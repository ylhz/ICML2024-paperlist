
# ICML 2024 Paper Summaries

This repository contains a collection of summaries for papers presented at the International Conference on Machine Learning (ICML) 2024. Each summary includes the following elements:

- **Title**: The title of the paper.
- **Authors**: The authors who contributed to the paper.
- **Affiliations**: The institutions or organizations with which the authors are associated.
- **TL;DR**: A concise summary generated by AI, providing an overview of the paper's key points and contributions.
- **Keywords**: Key themes, technical methods, and application areas identified for each paper.

> Note
Most of the content, except for the titles and authors, has been generated using AI tools. While efforts have been made to ensure accuracy, there may be errors or inaccuracies.

> Future Work
The repository is currently organized as a list, and additional detailed organization and content refinement are planned. Please stay tuned for updates as we work on enhancing the structure and content of these summaries.

---

## Papers List

- [Consistent Submodular Maximization](https://icml.cc/virtual/2024/poster/34755) (Poster)
  - **Authors:** [PAUL DUETTING](http://openreview.net/profile?id=~Paul_Duetting1), [Federico Fusco](http://openreview.net/profile?id=~Federico_Fusco1), [Silvio Lattanzi](http://openreview.net/profile?id=~Silvio_Lattanzi1), [Ashkan Norouzi-Fard](http://openreview.net/profile?id=~Ashkan_Norouzi-Fard2), [Morteza Zadimoghaddam](http://openreview.net/profile?id=~Morteza_Zadimoghaddam1)
  - **Affiliations:** Google Research, Sapienza University of Rome, Rome, Italy, Google Research, Google Research, Google Research
  - **TL;DR:** This paper investigates the problem of maximizing monotone submodular functions under cardinality constraints in a dynamic environment, focusing on maintaining a stable solution with bounded changes. The authors propose algorithms that balance consistency and approximation quality, demonstrating their effectiveness through experimental analysis.
  - **Keywords:** submodular maximization, consistency constraints, approximation algorithms, data mining, machine learning, data summarization, stability of solutions, dynamic environments, algorithms with trade-offs between consistency and approximation quality


- [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://icml.cc/virtual/2024/poster/32613) (Poster)
  - **Authors:** [Tri Dao](http://openreview.net/profile?id=~Tri_Dao1), [Albert Gu](http://openreview.net/profile?id=~Albert_Gu1)
  - **Affiliations:** Department of Computer Science, Princeton University, Machine Learning Department, Carnegie Mellon University
  - **TL;DR:** This paper establishes theoretical connections between Transformers and structured state-space models (SSMs), leading to the development of a new architecture, Mamba-2, which is significantly faster and competitive in language modeling. The authors aim to enhance the efficiency of SSMs by leveraging optimizations originally designed for Transformers.
  - **Keywords:** Transformers, State-Space Models, Language Modeling, Structured State-Space Models (SSMs), Linear Attention (LA), Structured Masked Attention (SMA), Efficiency issues in Transformers, Scaling in sequence length, Mamba-2 architecture, Theoretical connections between SSMs and attention, Semiseparable matrices, Dual forms, Tensor contractions


- [MusicRL: Aligning Music Generation to Human Preferences](https://icml.cc/virtual/2024/poster/34561) (Poster)
  - **Authors:** [Geoffrey Cideron](http://openreview.net/profile?id=~Geoffrey_Cideron1), [Sertan Girgin](http://openreview.net/profile?id=~Sertan_Girgin1), [Mauro Verzetti](http://openreview.net/profile?id=~Mauro_Verzetti1), [Damien Vincent](http://openreview.net/profile?id=~Damien_Vincent1), [Matej Kastelic](http://openreview.net/profile?id=~Matej_Kastelic1), [Zalán Borsos](http://openreview.net/profile?id=~Zal%C3%A1n_Borsos1), [Brian McWilliams](http://openreview.net/profile?id=~Brian_McWilliams2), [Victor Ungureanu](http://openreview.net/profile?id=~Victor_Ungureanu1), [Olivier Bachem](http://openreview.net/profile?id=~Olivier_Bachem1), [Olivier Pietquin](http://openreview.net/profile?id=~Olivier_Pietquin1), [Matthieu Geist](http://openreview.net/profile?id=~Matthieu_Geist1), [Léonard Hussenot](http://openreview.net/profile?id=~Leonard_Hussenot1), [Neil Zeghidour](http://openreview.net/profile?id=~Neil_Zeghidour1), [Andrea Agostinelli](http://openreview.net/profile?id=~Andrea_Agostinelli1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Cohere, Cohere, Google DeepMind, Kyutai, Google DeepMind
  - **TL;DR:** The study introduces MusicRL, a music generation system fine-tuned using human feedback to enhance the subjective appreciation of generated music. The results indicate that MusicRL-R and MusicRL-U outperform baseline models, highlighting the importance of human involvement in music generation model fine-tuning.
  - **Keywords:** music generation, human feedback, text-to-music models, reinforcement learning, autoregressive models, sequence-level rewards, subjectivity in musical appreciation, challenges in supervised training, MusicRL, MusicRL-R, MusicRL-U, MusicRL-RU, 300,000 pairwise preferences dataset, MusicLM, audio tokens, text adherence


- [Relaxing the Accurate Imputation Assumption in Doubly Robust Learning for Debiased Collaborative Filtering](https://icml.cc/virtual/2024/poster/34268) (Spotlight Poster)
  - **Authors:** [Haoxuan Li](http://openreview.net/profile?id=~Haoxuan_Li6), [Chunyuan Zheng](http://openreview.net/profile?id=~Chunyuan_Zheng1), [Shuyi Wang](http://openreview.net/profile?id=~Shuyi_Wang3), [Kunhan Wu](http://openreview.net/profile?id=~Kunhan_Wu1), [Eric Wang](http://openreview.net/profile?id=~Eric_Wang3), [Peng Wu](http://openreview.net/profile?id=~Peng_Wu5), [zhi geng](http://openreview.net/profile?id=~Zhi_Geng1), [Xu Chen](http://openreview.net/profile?id=~Xu_Chen13), [Xiao-Hua Zhou](http://openreview.net/profile?id=~Xiao-Hua_Zhou1)
  - **Affiliations:** Peking University, Peking University, University of Pennsylvania, Carnegie Mellon University, Zhejiang University, Beijing Technology and Business University, Beijing Technology and Business University, Renmin University of China, Peking University
  - **TL;DR:** This paper proposes novel doubly robust estimators for debiasing collaborative filtering in recommender systems, addressing the challenges of inaccurate pseudo-labelings and sampling selection bias. The proposed methods demonstrate improved performance over state-of-the-art approaches on various datasets.
  - **Keywords:** Recommender systems, Debiasing methods, Doubly robust estimators, Propensity reconstruction learning, Attention mechanism, Collaborative filtering, Sampling selection bias, Inaccurate pseudo-labelings, Unbiased learning, Novel estimators, Improved performance on datasets, KUAIREC (dataset), Semi-synthetic datasets, Real-world datasets


- [Kernel-Based Evaluation of Conditional Biological Sequence Models](https://icml.cc/virtual/2024/poster/35095) (Poster)
  - **Authors:** [Pierre Glaser](http://openreview.net/profile?id=~Pierre_Glaser1), [Steffan Paul](http://openreview.net/profile?id=~Steffanie_Paul1), [Alissa M. Hummer](http://openreview.net/profile?id=~Alissa_M_Hummer1), [Charlotte Deane](http://openreview.net/profile?id=~Charlotte_Deane1), [Debora Marks](http://openreview.net/profile?id=~Debora_Susan_Marks1), [Alan Amin](http://openreview.net/profile?id=~Alan_Nawzad_Amin1)
  - **Affiliations:** Gatsby Computational Neuroscience Unit, London, UK, Systems Biology, Harvard Medical School, Boston, USA, Systems Biology, Harvard Medical School, Boston, USA; Department of Statistics, University of Oxford, Oxford, UK, Department of Statistics, University of Oxford, Oxford, UK, Harvard Medical School, Broad Institute, Boston, USA, Courant Institute, New York University, New York, USA
  - **TL;DR:** This study introduces kernel-based tools for evaluating conditional sequence models, focusing on a new metric called Augmented Conditional Maximum Mean Discrepancy (ACMMD) to assess model fit and reliability. The approach is demonstrated through the analysis of the ProteinMPNN model, revealing its limitations in fitting data across various protein families and allowing for hyperparameter tuning to improve performance.
  - **Keywords:** conditional sequence models, computational biology, protein design, Augmented Conditional Maximum Mean Discrepancy (ACMMD), model evaluation, genomics, protein design, model accuracy, model reliability, high-dimensional discrete-valued sequences, new evaluation metrics, hyperparameter tuning


- [Fair Off-Policy Learning from Observational Data](https://icml.cc/virtual/2024/poster/33022) (Poster)
  - **Authors:** [Dennis Frauen](http://openreview.net/profile?id=~Dennis_Frauen1), [Valentyn Melnychuk](http://openreview.net/profile?id=~Valentyn_Melnychuk1), [Stefan Feuerriegel](http://openreview.net/profile?id=~Stefan_Feuerriegel1)
  - **Affiliations:** LMU Munich; Munich Center for Machine Learning, LMU Munich; Munich Center for Machine Learning, LMU Munich; Munich Center for Machine Learning
  - **TL;DR:** This paper presents a novel framework for fair off-policy learning from observational data, addressing the challenges of ensuring fairness despite potentially discriminatory historical policies. The proposed neural network-based approach, FairPol, aims to learn optimal decision rules while providing theoretical guarantees and demonstrating effectiveness through extensive experiments.
  - **Keywords:** Fairness in algorithmic decision-making, Off-policy learning, Neural network-based framework, Fair representation learning, Algorithmic decision-making, Social applications, Systematic discrimination, Bias in observational data, Optimal policies under fairness notions, Generalization bounds, Simulated data, Real-world data


- [Nash Learning from Human Feedback](https://icml.cc/virtual/2024/poster/33786) (Spotlight Poster)
  - **Authors:** [REMI MUNOS](http://openreview.net/profile?id=~Remi_Munos1), [Michal Valko](http://openreview.net/profile?id=~Michal_Valko1), [Daniele Calandriello](http://openreview.net/profile?id=~Daniele_Calandriello1), [Mohammad Gheshlaghi Azar](http://openreview.net/profile?id=~Mohammad_Gheshlaghi_Azar1), [Mark Rowland](http://openreview.net/profile?id=~Mark_Rowland1), [Zhaohan Guo](http://openreview.net/profile?id=~Zhaohan_Daniel_Guo1), [Yunhao Tang](http://openreview.net/profile?id=~Yunhao_Tang1), [Matthieu Geist](http://openreview.net/profile?id=~Matthieu_Geist1), [Thomas Mesnard](http://openreview.net/profile?id=~Thomas_Mesnard2), [Côme Fiegel](http://openreview.net/profile?id=~C%C3%B4me_Fiegel1), [Andrea Michi](http://openreview.net/profile?id=~Andrea_Michi1), [Marco Selvi](http://openreview.net/profile?id=~Marco_Selvi1), [Sertan Girgin](http://openreview.net/profile?id=~Sertan_Girgin1), [Nikola Momchev](http://openreview.net/profile?id=~Nikola_Momchev1), [Olivier Bachem](http://openreview.net/profile?id=~Olivier_Bachem1), [Daniel Mankowitz](http://openreview.net/profile?id=~Daniel_J_Mankowitz2), [Doina Precup](http://openreview.net/profile?id=~Doina_Precup1), [Bilal Piot](http://openreview.net/profile?id=~Bilal_Piot1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, ENSAE Paris, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This study introduces Nash Learning from Human Feedback (NLHF) as a novel approach for fine-tuning large language models (LLMs) using pairwise human feedback, aiming to enhance alignment with human preferences. The proposed method, supported by a new algorithm called Nash-MD, demonstrates effective policy generation and addresses limitations of traditional reward models.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Large Language Models (LLMs), Pairwise Preference Model, Nash Learning, Nash-MD, Mirror Descent, Text Summarization, Alignment with Human Preferences, Limitations of Reward Models, Novel Algorithmic Solution, Regularized Nash Equilibrium, Bradley-Terry Model, Elo Ranking System


- [Position: A Call for Embodied AI](https://icml.cc/virtual/2024/poster/33518) (Poster)
  - **Authors:** [Giuseppe Paolo](http://openreview.net/profile?id=~Giuseppe_Paolo1), [Jonas Gonzalez-Billandon](http://openreview.net/profile?id=~Jonas_Gonzalez-Billandon1), [Balázs Kégl](http://openreview.net/profile?id=~Bal%C3%A1zs_K%C3%A9gl2)
  - **Affiliations:** Noah’s Ark Lab, Huawei Technologies France, Paris, France, London Research Center, London, UK, Noah’s Ark Lab, Huawei Technologies France, Paris, France
  - **TL;DR:** The paper proposes Embodied AI (E-AI) as a crucial step towards achieving Artificial General Intelligence (AGI), emphasizing the need for a theoretical framework that incorporates perception, action, memory, and learning. It highlights the limitations of current AI technologies, particularly Large Language Models, and calls for research focused on creating E-AI agents capable of effective interaction with humans and other intelligent entities.
  - **Keywords:** Embodied AI, Artificial General Intelligence (AGI), Large Language Models (LLMs), Cognitive architectures, active inference, Robotics, neuroscience, natural language processing, Static learning, alignment issues, confabulation, Theoretical framework for E-AI, guidelines for future research, AI communication, collaboration, coexistence


- [Calibration Bottleneck: Over-compressed Representations are Less Calibratable](https://icml.cc/virtual/2024/poster/33499) (Poster)
  - **Authors:** [Deng-Bao Wang](http://openreview.net/profile?id=~Deng-Bao_Wang1), [Min-Ling Zhang](http://openreview.net/profile?id=~Min-Ling_Zhang2)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Lab. of Computer Network and Information Integration (Southeast University), MOE, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Lab. of Computer Network and Information Integration (Southeast University), MOE, China
  - **TL;DR:** This study investigates the calibratability of deep neural networks, revealing that over-compression in representation layers hinders calibration. It proposes a new training method, progressively layer-peeled training (PLP), which enhances model calibration and maintains competitive predictive performance.
  - **Keywords:** model calibratability, uncertainty calibration, weight decay regularizer, temperature scaling (TS), histogram binning (HB), progressively layer-peeled training (PLP), deep neural networks (DNNs), high-dimensional prediction tasks, safety-critical decision-making, miscalibration of model confidence, over-compression of representation layers, improved model calibration, competitive predictive performance


- [MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts](https://icml.cc/virtual/2024/poster/33196) (Poster)
  - **Authors:** [Jianan Zhou](http://openreview.net/profile?id=~Jianan_Zhou1), [Zhiguang Cao](http://openreview.net/profile?id=~Zhiguang_Cao1), [Yaoxin Wu](http://openreview.net/profile?id=~Yaoxin_Wu2), [Wen Song](http://openreview.net/profile?id=~Wen_Song1), [Yining Ma](http://openreview.net/profile?id=~Yining_Ma1), [Jie Zhang](http://openreview.net/profile?id=~Jie_Zhang9), [Xu Chi](http://openreview.net/profile?id=~Xu_Chi1)
  - **Affiliations:** College of Computing and Data Science, Nanyang Technological University, Singapore, School of Computing and Information Systems, Singapore Management University, Singapore, Department of Information Systems, Eindhoven University of Technology, The Netherlands, Institute of Marine Science and Technology, Shandong University, China, College of Computing and Data Science, Nanyang Technological University, Singapore, College of Computing and Data Science, Nanyang Technological University, Singapore, Singapore Institute of Manufacturing Technology (SIMTech), Agency for Science, Technology and Research (A*STAR), Singapore
  - **TL;DR:** This paper presents MVMoE, a multi-task vehicle routing solver that utilizes a mixture-of-experts approach to enhance model capacity and achieve zero-shot generalization across various VRP variants. The proposed method demonstrates significant improvements in performance and efficiency compared to traditional neural solvers.
  - **Keywords:** Vehicle Routing Problems (VRPs), Neural Solvers, Multi-Task Learning, Mixture-of-Experts (MoE), Hierarchical Gating, Reinforcement Learning (RL), Logistics, Transportation, Manufacturing, NP-hard problems, Zero-shot generalization, Out-of-distribution data, Unified neural solver, Enhanced model capacity, Empirical performance improvements


- [Automated Statistical Model Discovery with Language Models](https://icml.cc/virtual/2024/poster/34737) (Poster)
  - **Authors:** [Michael Li](http://openreview.net/profile?id=~Michael_Y._Li1), [Emily Fox](http://openreview.net/profile?id=~Emily_Fox2), [Noah Goodman](http://openreview.net/profile?id=~Noah_Goodman1)
  - **Affiliations:** Department of Computer Science, Stanford University; Department of Statistics, Stanford University; Chan Zuckerberg Biohub – San Francisco; Department of Psychology, Stanford University, Department of Computer Science, Stanford University; Department of Statistics, Stanford University; Chan Zuckerberg Biohub – San Francisco, Department of Computer Science, Stanford University; Department of Psychology, Stanford University
  - **TL;DR:** This study presents a method for automated statistical model discovery using large language models, which iteratively propose and critique models. The approach successfully identifies models comparable to those designed by human experts while ensuring interpretability and flexibility in model selection.
  - **Keywords:** Automated statistical model discovery, Large language models, Probabilistic programs, Box’s Loop, Probabilistic modeling, Searching over a vast space of models, domain-specific constraints, Models on par with human expert designed models, interpretable extensions of classic models


- [DRED: Zero-Shot Transfer in Reinforcement Learning via Data-Regularised Environment Design](https://icml.cc/virtual/2024/poster/32810) (Poster)
  - **Authors:** [Samuel Garcin](http://openreview.net/profile?id=~Samuel_Garcin1), [James Doran](http://openreview.net/profile?id=~James_Doran1), [Shangmin Guo](http://openreview.net/profile?id=~Shangmin_Guo1), [Christopher Lucas](http://openreview.net/profile?id=~Christopher_G._Lucas1), [Stefano V. Albrecht](http://openreview.net/profile?id=~Stefano_V_Albrecht1)
  - **Affiliations:** School of Informatics, University of Edinburgh, Huawei, School of Informatics, University of Edinburgh, School of Informatics, University of Edinburgh, School of Informatics, University of Edinburgh
  - **TL;DR:** This study investigates the impact of environment sampling on the zero-shot generalisation ability of reinforcement learning agents and introduces data-regularised environment design (DRED) to improve performance by minimizing overfitting and distributional shifts. The findings suggest that prioritizing levels based on value loss can enhance the agent's ability to generalize to new environments effectively.
  - **Keywords:** Zero-shot generalisation, Reinforcement learning, Deep actor-critic architectures, Adaptive sampling strategies, Unsupervised environment design (UED), Autonomous agents, Environment settings, Generalisation to new environments, Overfitting, Distributional shift, Data-regularised environment design (DRED), Generative model for level generation, Mutual information, Training levels


- [Discovering Environments with XRM](https://icml.cc/virtual/2024/poster/33430) (Oral)
  - **Authors:** [Mohammad Pezeshki](http://openreview.net/profile?id=~Mohammad_Pezeshki1), [Diane Bouchacourt](http://openreview.net/profile?id=~Diane_Bouchacourt3), [Mark Ibrahim](http://openreview.net/profile?id=~Mark_Ibrahim1), [Nicolas Ballas](http://openreview.net/profile?id=~Nicolas_Ballas1), [Pascal Vincent](http://openreview.net/profile?id=~Pascal_Vincent1), [David Lopez-Paz](http://openreview.net/profile?id=~David_Lopez-Paz2)
  - **Affiliations:** FAIR at Meta, FAIR at Meta, FAIR at Meta, FAIR at Meta, Mila at Université de Montréal; CIFAR, FAIR at Meta
  - **TL;DR:** The paper introduces CROSS-RISK MINIMIZATION (XRM) as a method for automatic environment discovery to enhance out-of-distribution generalization in AI systems. XRM effectively trains twin networks to learn from training data while addressing the challenges of costly environment annotations and biases, achieving improved worst-group-accuracy.
  - **Keywords:** Out-of-distribution (OOD) generalization, automatic environment discovery, CROSS-RISK MINIMIZATION (XRM), empirical risk minimization (ERM), group distributionally robust optimization (GroupDRO), AI systems, healthcare, finance, self-driving vehicles, Costly environment annotations, human annotator biases, spurious correlations, underrepresented groups in training data, Hyper-parameter tuning, oracle worst-group-accuracy


- [Model-based Reinforcement Learning for Confounded POMDPs](https://icml.cc/virtual/2024/poster/34613) (Poster)
  - **Authors:** [Mao Hong](http://openreview.net/profile?id=~Mao_Hong1), [Zhengling Qi](http://openreview.net/profile?id=~Zhengling_Qi1), [Yanxun Xu](http://openreview.net/profile?id=~Yanxun_Xu1)
  - **Affiliations:** Department of Applied Mathematics and Statistics, Johns Hopkins University, Baltimore, United States, Department of Decision Sciences, George Washington University, Washington, DC, United States, Department of Applied Mathematics and Statistics, Johns Hopkins University, Baltimore, United States
  - **TL;DR:** This study presents a model-based offline reinforcement learning algorithm for confounded POMDPs, establishing a novel identification result for action effects and developing a two-stage estimation procedure for off-policy evaluation. The findings demonstrate the efficiency of the proposed method under certain conditions, contributing to optimal policy learning in partially observable environments.
  - **Keywords:** model-based reinforcement learning, confounded partially observable Markov decision processes (POMDPs), offline reinforcement learning (RL), nonparametric two-stage estimation, off-policy evaluation (OPE), conservative policy optimization, autonomous driving, healthcare, partial observability, confounding bias, offline data distribution, optimal policy learning, finite-sample upper bound on suboptimality


- [Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching](https://icml.cc/virtual/2024/poster/33435) (Poster)
  - **Authors:** [Yuchen Zhang](http://openreview.net/profile?id=~Yuchen_Zhang8), [Tianle Zhang](http://openreview.net/profile?id=~Tianle_Zhang4), [Kai Wang](http://openreview.net/profile?id=~Kai_Wang8), [Ziyao Guo](http://openreview.net/profile?id=~Ziyao_Guo1), [Yuxuan Liang](http://openreview.net/profile?id=~Yuxuan_Liang1), [Xavier Bresson](http://openreview.net/profile?id=~Xavier_Bresson6), [Wei Jin](http://openreview.net/profile?id=~Wei_Jin4), [Yang You](http://openreview.net/profile?id=~Yang_You1)
  - **Affiliations:** National University of Singapore, National University of Singapore, National University of Singapore, National University of Singapore, Hong Kong University of Science and Technology (Guangzhou), National University of Singapore, Emory University, National University of Singapore
  - **TL;DR:** This paper presents a novel approach to lossless graph condensation that enhances the performance of Graph Neural Networks by utilizing diverse supervision signals and an expanding window matching technique. The proposed method addresses the limitations of existing techniques, particularly in large-scale graph datasets, and demonstrates superior results through extensive experiments.
  - **Keywords:** Graph condensation, Graph Neural Networks (GNNs), Trajectory matching, Curriculum learning, Expanding window matching, Large-scale graph datasets, Graph-related applications, Lossless condensation, Performance gap between condensed and original graphs, New method for lossless graph condensation, Knowledge extraction from expert trajectories, Citeseer


- [Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance](https://icml.cc/virtual/2024/poster/32929) (Poster)
  - **Authors:** [Chiraag Kaushik](http://openreview.net/profile?id=~Chiraag_Kaushik1), [Ran Liu](http://openreview.net/profile?id=~Ran_Liu2), [Chi-Heng Lin](http://openreview.net/profile?id=~Chi-Heng_Lin1), [Amrit Khera](http://openreview.net/profile?id=~Amrit_Khera1), [Matthew Jin](http://openreview.net/profile?id=~Matthew_Y_Jin1), [Wenrui Ma](http://openreview.net/profile?id=~Wenrui_Ma3), [Vidya Muthukumar](http://openreview.net/profile?id=~Vidya_Muthukumar3), [Eva Dyer](http://openreview.net/profile?id=~Eva_L_Dyer1)
  - **Affiliations:** Georgia Institute of Technology, Georgia, the USA, Georgia Institute of Technology, Georgia, the USA, Georgia Institute of Technology, Georgia, the USA; Samsung Research, None, Georgia Institute of Technology, Georgia, the USA, Stanford University, California, the USA, Georgia Institute of Technology, Georgia, the USA, Georgia Institute of Technology, Georgia, the USA, Georgia Institute of Technology, Georgia, the USA
  - **TL;DR:** This study introduces the concept of spectral imbalance as a source of class disparities in classification models, highlighting that even balanced datasets can exhibit significant performance gaps across classes. The authors develop a theoretical framework to analyze these disparities and propose methods to mitigate the issue through data augmentation and encoder evaluation.
  - **Keywords:** class bias, spectral imbalance, classification models, Gaussian mixture model, theoretical framework, data augmentation strategies, machine learning, pretrained models, class disparities, performance gaps across classes, class-dependent generalization, new framework for studying class-dependent generalization, insights into pretrained features


- [Position: Towards Unified Alignment Between Agents, Humans, and Environment](https://icml.cc/virtual/2024/poster/34602) (Poster)
  - **Authors:** [Zonghan Yang](http://openreview.net/profile?id=~Zonghan_Yang1), [an liu](http://openreview.net/profile?id=~An_Liu4), [Zijun Liu](http://openreview.net/profile?id=~Zijun_Liu2), [Kaiming Liu](http://openreview.net/profile?id=~Kaiming_Liu1), [Fangzhou Xiong](http://openreview.net/profile?id=~Fangzhou_Xiong1), [Yile Wang](http://openreview.net/profile?id=~Yile_Wang1), [Zeyuan Yang](http://openreview.net/profile?id=~Zeyuan_Yang3), [Qingyuan Hu](http://openreview.net/profile?id=~Qingyuan_Hu3), [XinRui Chen](http://openreview.net/profile?id=~陈鑫睿1), [Zhenhe Zhang](http://openreview.net/profile?id=~Zhenhe_Zhang2), [Fuwen Luo](http://openreview.net/profile?id=~Fuwen_Luo1), [Zhicheng Guo](http://openreview.net/profile?id=~Zhicheng_Guo2), [Peng Li](http://openreview.net/profile?id=~Peng_Li2), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu19)
  - **Affiliations:** Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China, Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China, Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China; Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China
  - **TL;DR:** This study introduces the principles of Unified Alignment for Agents (UA2), advocating for the alignment of agents with human intentions, environmental dynamics, and self-constraints. The research demonstrates the importance of these principles through proof-of-concept studies and performance benchmarking in realistic environments, highlighting the need for improved general problem-solving abilities in autonomous agents.
  - **Keywords:** Autonomous agents, Unified Alignment for Agents (UA2), Foundation models, Large Language Models (LLMs), Large Multimodal Models (LMMs), Web task automation, open-ended world exploration, interactive coding, robotic tasks, Limited efficacy in complex environments, neglected factors in agent benchmarks, Principles of UA2, proof-of-concept studies, performance benchmarking, WebShop, AI alignment, AI safety


- [Effects of Exponential Gaussian Distribution on (Double Sampling) Randomized Smoothing](https://icml.cc/virtual/2024/poster/34410) (Poster)
  - **Authors:** [Youwei Shu](http://openreview.net/profile?id=~Youwei_Shu1), [Xi Xiao](http://openreview.net/profile?id=~Xi_Xiao1), [Derui Wang](http://openreview.net/profile?id=~Derui_Wang1), [Yuxin Cao](http://openreview.net/profile?id=~Yuxin_Cao1), [Siji Chen](http://openreview.net/profile?id=~Siji_Chen2), [Minhui Xue](http://openreview.net/profile?id=~Jason_Xue1), [Linyi Li](http://openreview.net/profile?id=~Linyi_Li1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19)
  - **Affiliations:** Shenzhen International Graduate School, Tsinghua University, Shenzhen International Graduate School, Tsinghua University, CSIRO’s Data61, Shenzhen International Graduate School, Tsinghua University, Shenzhen International Graduate School, Tsinghua University, CSIRO’s Data61, University of Illinois Urbana-Champaign; Simon Fraser University; University of Chicago, University of Illinois Urbana-Champaign; University of Chicago
  - **TL;DR:** This study investigates the effects of Exponential Standard Gaussian and Exponential General Gaussian distributions on Randomized Smoothing and Double Sampling Randomized Smoothing, revealing that ESG distributions provide consistent certification while EGG significantly enhances certified accuracy, particularly in high-dimensional settings. The findings suggest potential improvements in robustness certification against adversarial examples.
  - **Keywords:** Randomized Smoothing, Adversarial Examples, Robustness Certification, Exponential Standard Gaussian (ESG), Exponential General Gaussian (EGG), Double Sampling Randomized Smoothing (DSRS), Adversarial Attacks, Curse of Dimensionality, Certified Radius, Improved Certified Accuracy, ImageNet, ℓp Adversaries, Neyman-Pearson Lemma


- [InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization](https://icml.cc/virtual/2024/poster/35040) (Oral)
  - **Authors:** [Zhengyang Hu](http://openreview.net/profile?id=~Zhengyang_Hu1), [Song Kang](http://openreview.net/profile?id=~Song_Kang1), [Qunsong Zeng](http://openreview.net/profile?id=~Qunsong_Zeng1), [Kaibin Huang](http://openreview.net/profile?id=~Kaibin_Huang1), [Yanchao Yang](http://openreview.net/profile?id=~Yanchao_Yang1)
  - **Affiliations:** Department of Electrical and Electronic Engineering, the University of Hong Kong, School of Information Science and Technology, University of Science and Technology of China; Work done as an intern at HKU, Department of Electrical and Electronic Engineering, the University of Hong Kong, Department of Electrical and Electronic Engineering, the University of Hong Kong, Department of Electrical and Electronic Engineering, the University of Hong Kong; Musketeers Foundation Institute of Data Science, the University of Hong Kong
  - **TL;DR:** The study introduces InfoNet, a neural network designed for efficient mutual information estimation without the need for test-time optimization. It demonstrates improved performance and generalization across various data distributions, providing a balance between efficiency and accuracy.
  - **Keywords:** Mutual Information Estimation, Real-Time Applications, Neural Networks, Attention Mechanism, Intelligent Behavior, Data Streams, Efficiency in Estimation, Test-Time Optimization, InfoNet, Efficiency-Accuracy Trade-off


- [Learning Associative Memories with Gradient Descent](https://icml.cc/virtual/2024/poster/34783) (Poster)
  - **Authors:** [Vivien Cabannnes](http://openreview.net/profile?id=~Vivien_Cabannes1), [Berfin Simsek](http://openreview.net/profile?id=~Berfin_Simsek1), [Alberto Bietti](http://openreview.net/profile?id=~Alberto_Bietti1)
  - **Affiliations:** Meta AI, Flatiron, Flatiron
  - **TL;DR:** This study investigates the training dynamics of an associative memory model using gradient descent and cross-entropy loss, revealing insights into classification margins and the effects of token frequency imbalance. The findings highlight oscillatory behaviors during training and the implications of overparameterization and underparameterization in learning.
  - **Keywords:** associative memory, training dynamics, deep learning, gradient descent, cross-entropy loss, softmax, imbalance in token frequencies, memory interferences, suboptimal memorization schemes, logarithmic growth of classification margins, oscillatory transitory regimes, token embeddings, linear layer, high-dimensional embedding vectors, Transformer models


- [WARM: On the Benefits of Weight Averaged Reward Models](https://icml.cc/virtual/2024/poster/32924) (Poster)
  - **Authors:** [Alexandre Rame](http://openreview.net/profile?id=~Alexandre_Rame1), [Nino Vieillard](http://openreview.net/profile?id=~Nino_Vieillard1), [Léonard Hussenot](http://openreview.net/profile?id=~Leonard_Hussenot1), [Robert Dadashi](http://openreview.net/profile?id=~Robert_Dadashi2), [Geoffrey Cideron](http://openreview.net/profile?id=~Geoffrey_Cideron1), [Olivier Bachem](http://openreview.net/profile?id=~Olivier_Bachem1), [Johan Ferret](http://openreview.net/profile?id=~Johan_Ferret1)
  - **Affiliations:** Google DeepMind, None, None, None, None, None, None
  - **TL;DR:** This study introduces Weight Averaged Reward Models (WARM) to address reward hacking in large language models by mitigating distribution shifts and preference inconsistencies. The proposed method enhances the quality and alignment of LLM predictions, achieving a significant win rate over traditional single reward model approaches.
  - **Keywords:** Large Language Models, Human Preferences, Reinforcement Learning, Reinforcement Learning from Human Feedback (RLHF), Weight Averaged Reward Models (WARM), Conversational Assistants, Summarization Tasks, Reward Hacking, Distribution Shifts, Preference Inconsistencies, Improved Quality and Alignment of LLM Predictions, Efficiency in Reward Modeling, Reward Models (RMs), Proxy RM, Policy Drift


- [TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge](https://icml.cc/virtual/2024/poster/34234) (Poster)
  - **Authors:** [Young Kwon](http://openreview.net/profile?id=~Young_D._Kwon1), [Rui Li](http://openreview.net/profile?id=~Rui_Li11), [Stylianos Venieris](http://openreview.net/profile?id=~Stylianos_Venieris1), [Jagmohan Chauhan](http://openreview.net/profile?id=~Jagmohan_Chauhan1), [Nicholas Lane](http://openreview.net/profile?id=~Nicholas_Donald_Lane1), [Cecilia Mascolo](http://openreview.net/profile?id=~Cecilia_Mascolo1)
  - **Affiliations:** Department of Computer Science and Technology, University of Cambridge, United Kingdom; Samsung AI Center, Cambridge, United Kingdom, Samsung AI Center, Cambridge, United Kingdom, Samsung AI Center, Cambridge, United Kingdom, School of Electronics and Computer Science, University of Southampton, United Kingdom, Samsung AI Center, Cambridge, United Kingdom, Department of Computer Science and Technology, University of Cambridge, United Kingdom
  - **TL;DR:** The study presents TinyTrain, an innovative on-device training approach that addresses the challenges of data scarcity and resource limitations in edge devices by selectively updating parts of the model. TinyTrain significantly improves training efficiency and accuracy while maintaining a minimal memory footprint, outperforming existing methods.
  - **Keywords:** On-device training, User personalisation, Privacy, Task-adaptive sparse-update method, Fine-tuning, IoT devices, Microcontroller units (MCUs), Edge devices, Data scarcity, Limited memory and compute resources, Long training time, Accuracy loss, TinyTrain method, Reduced computation and memory footprint, Improved training efficiency, Deep neural networks (DNNs), Multiply-accumulate (MAC) count


- [Batch and match: black-box variational inference with a score-based divergence](https://icml.cc/virtual/2024/poster/33622) (Spotlight Poster)
  - **Authors:** [Diana Cai](http://openreview.net/profile?id=~Diana_Cai1), [Chirag Modi](http://openreview.net/profile?id=~Chirag_Modi1), [Loucas Pillaud-Vivien](http://openreview.net/profile?id=~Loucas_Pillaud-Vivien1), [Charles Margossian](http://openreview.net/profile?id=~Charles_Margossian1), [Robert Gower](http://openreview.net/profile?id=~Robert_M._Gower1), [David Blei](http://openreview.net/profile?id=~David_Blei2), [Lawrence Saul](http://openreview.net/profile?id=~Lawrence_K._Saul3)
  - **Affiliations:** Center for Computational Mathematics, Flatiron Institute, Center for Computational Mathematics, Flatiron Institute; Center for Computational Astrophysics, Flatiron Institute, Center for Computational Mathematics, Flatiron Institute; CERMICS Laboratory, Ecole des Ponts ParisTech, Center for Computational Mathematics, Flatiron Institute, Center for Computational Mathematics, Flatiron Institute, Department of Statistics, Department of Computer Science, Columbia University, Center for Computational Mathematics, Flatiron Institute
  - **TL;DR:** This paper introduces a novel approach to black-box variational inference called batch and match (BaM), which utilizes a score-based divergence to improve convergence speed and stability compared to traditional methods. The authors demonstrate that BaM converges exponentially quickly to the target mean and covariance, outperforming existing implementations in terms of gradient evaluations.
  - **Keywords:** black-box variational inference, probabilistic modeling, score-based divergence, Gaussian variational families, stochastic evidence lower bound (ELBO), hierarchical models, deep generative models, high variance of gradient estimates, sensitivity to hyperparameters, convergence issues in high-dimensional problems, batch and match (BaM) method, closed-form proximal update, exponential convergence to target mean and covariance


- [Fool Your (Vision and) Language Model with Embarrassingly Simple Permutations](https://icml.cc/virtual/2024/poster/34427) (Poster)
  - **Authors:** [Yongshuo Zong](http://openreview.net/profile?id=~Yongshuo_Zong1), [Tingyang Yu](http://openreview.net/profile?id=~Tingyang_Yu1), [Ruchika Chavhan](http://openreview.net/profile?id=~Ruchika_Chavhan1), [Bingchen Zhao](http://openreview.net/profile?id=~Bingchen_Zhao1), [Timothy Hospedales](http://openreview.net/profile?id=~Timothy_Hospedales1)
  - **Affiliations:** University of Edinburgh, EPFL, University of Edinburgh, University of Edinburgh, University of Edinburgh
  - **TL;DR:** This study investigates the vulnerability of large language and vision-language models to adversarial permutations in multiple-choice question answering, revealing significant accuracy degradation. The findings highlight the need for a deeper understanding of model robustness before deploying these systems in real-world applications.
  - **Keywords:** robustness of language models, vulnerability in multiple-choice question answering (MCQA), permutation sensitivity, adversarial permutation, education, recruitment exams, model brittleness, accuracy degradation due to answer permutation, empirical demonstration of vulnerabilities, performance metrics analysis, MMLU dataset, large language models (LLMs), vision-language models (VLLMs), multiple-choice question answering (MCQA)


- [Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response](https://icml.cc/virtual/2024/poster/34506) (Oral)
  - **Authors:** [Junyi Zou](http://openreview.net/profile?id=~Bob_Junyi_Zou1), [Matthew Levine](http://openreview.net/profile?id=~Matthew_E_Levine1), [Dessi Zaharieva](http://openreview.net/profile?id=~Dessi_P._Zaharieva1), [Ramesh Johari](http://openreview.net/profile?id=~Ramesh_Johari1), [Emily Fox](http://openreview.net/profile?id=~Emily_Fox2)
  - **Affiliations:** Institute for Computational and Mathematical Engineering, Stanford University, Broad Institute of MIT and Harvard, Department of Pediatrics, Stanford University, Department of Management Science and Engineering, Stanford University, Department of Statistics and Department of Computer Science, Stanford University; Chan Zuckerberg Biohub – San Francisco
  - **TL;DR:** This study presents a hybrid model that combines mechanistic ODE-based dynamics with neural network components to improve causal modeling and predictive performance in glucose dynamics post-exercise for individuals with type 1 diabetes. The proposed approach effectively addresses the challenges of causal grounding in hybrid models while achieving state-of-the-art results.
  - **Keywords:** Hybrid models, Causal modeling, Interpretable models, Ordinary differential equations (ODEs), Neural networks, Type 1 diabetes management, Glucose dynamics modeling, Causal grounding loss in hybrid models, Learning from small datasets, Observational data challenges, Hybrid loss combining causal and predictive loss, State-of-the-art predictive performance, Continuous glucose monitoring (CGM), Counterfactual reasoning


- [Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference](https://icml.cc/virtual/2024/poster/33303) (Oral)
  - **Authors:** [JIAN XU](http://openreview.net/profile?id=~JIAN_XU5), [Delu Zeng](http://openreview.net/profile?id=~Delu_Zeng4), [John Paisley](http://openreview.net/profile?id=~John_Paisley1)
  - **Affiliations:** South China University of Technology, Guangzhou, China, South China University of Technology, Guangzhou, China, Columbia University, New York, USA
  - **TL;DR:** This paper introduces Denoising Diffusion Variational Inference (DDVI) as a method to improve posterior inference of inducing points in Deep Gaussian Processes (DGPs), addressing biases in traditional variational methods. The proposed approach combines score matching and stochastic differential equations to enhance model efficiency and reduce computational complexity.
  - **Keywords:** Deep Gaussian Processes, Bayesian deep learning, Denoising Diffusion Variational Inference (DDVI), Stochastic Differential Equation (SDE), Variational Inference, Posterior distribution approximation, computational complexity, bias in variational inference, Novel explicit variational lower bound for marginal likelihood function, Inducing points, KL divergence, score matching


- [Planning, Fast and Slow: Online Reinforcement Learning with Action-Free Offline Data via Multiscale Planners](https://icml.cc/virtual/2024/poster/34439) (Poster)
  - **Authors:** [Chengjie Wu](http://openreview.net/profile?id=~Chengjie_Wu1), [Hao Hu](http://openreview.net/profile?id=~Hao_Hu3), [yiqin yang](http://openreview.net/profile?id=~Yiqin_Yang1), [Ning Zhang](http://openreview.net/profile?id=~Ning_Zhang2), [Chongjie Zhang](http://openreview.net/profile?id=~Chongjie_Zhang1)
  - **Affiliations:** Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China, Institute of Automation, Chinese Academy of Sciences, China, Department of Computer Science & Engineering, Washington University in St. Louis, MO, USA, Department of Computer Science & Engineering, Washington University in St. Louis, MO, USA
  - **TL;DR:** This paper investigates the use of passive data to enhance online reinforcement learning, proposing a novel algorithm called Multiscale State-Centric Planners (MSCP) that effectively addresses challenges in long-horizon tasks. Empirical evaluations show that MSCP significantly outperforms existing methods by leveraging passive observations for actionable insights.
  - **Keywords:** reinforcement learning, passive RL, Multiscale State-Centric Planners (MSCP), online RL, offline RL, video data analysis, robotics manipulation, scientific discovery, distributional shift, extrapolation error, limited dataset coverage, long-horizon tasks, improved learning efficiency, dense training signals


- [LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery](https://icml.cc/virtual/2024/poster/33371) (Poster)
  - **Authors:** [Pingchuan Ma](http://openreview.net/profile?id=~Pingchuan_Ma3), [Johnson Tsun-Hsuan Wang](http://openreview.net/profile?id=~Tsun-Hsuan_Wang2), [Minghao Guo](http://openreview.net/profile?id=~Minghao_Guo1), [Zhiqing Sun](http://openreview.net/profile?id=~Zhiqing_Sun1), [Josh Tenenbaum](http://openreview.net/profile?id=~Joshua_B._Tenenbaum1), [Daniela Rus](http://openreview.net/profile?id=~Daniela_Rus1), [Chuang Gan](http://openreview.net/profile?id=~Chuang_Gan1), [Wojciech Matusik](http://openreview.net/profile?id=~Wojciech_Matusik2)
  - **Affiliations:** MIT CSAIL, MIT CSAIL, MIT CSAIL, CMU LTI, MIT BCS; Center for Brains, Minds and Machines, MIT CSAIL, UMass Amherst; MIT-IBM Watson AI Lab, MIT CSAIL
  - **TL;DR:** This study proposes a bilevel optimization framework called the Scientific Generative Agent (SGA) that combines the reasoning capabilities of large language models with the computational power of simulations to enhance scientific discovery. The framework demonstrates efficacy in discovering constitutive laws and designing molecules, revealing innovative solutions that challenge conventional human expectations.
  - **Keywords:** scientific discovery, large language models, simulations, bilevel optimization framework, Scientific Generative Agent (SGA), physics, chemistry, pharmacology, challenges in simulating observational feedback, grounding language with scientific discovery, novel solutions in constitutive law discovery, molecular design


- [Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://icml.cc/virtual/2024/poster/35068) (Poster)
  - **Authors:** [Wei-Lin Chiang](http://openreview.net/profile?id=~Wei-Lin_Chiang1), [Lianmin Zheng](http://openreview.net/profile?id=~Lianmin_Zheng2), [Ying Sheng](http://openreview.net/profile?id=~Ying_Sheng1), [Anastasios Angelopoulos](http://openreview.net/profile?id=~Anastasios_Nikolas_Angelopoulos1), [Tianle Li](http://openreview.net/profile?id=~Tianle_Li2), [Dacheng Li](http://openreview.net/profile?id=~Dacheng_Li1), [Banghua Zhu](http://openreview.net/profile?id=~Banghua_Zhu1), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang2), [Michael Jordan](http://openreview.net/profile?id=~Michael_Jordan1), [Joseph E Gonzalez](http://openreview.net/profile?id=~Joseph_E._Gonzalez1), [Ion Stoica](http://openreview.net/profile?id=~Ion_Stoica1)
  - **Affiliations:** UC Berkeley, UC Berkeley, Stanford, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UCSD, UC Berkeley, UC Berkeley, UC Berkeley
  - **TL;DR:** The paper introduces Chatbot Arena, an open platform designed to evaluate large language models (LLMs) based on human preferences through a crowdsourced pairwise comparison approach. It highlights the limitations of traditional static benchmarks and emphasizes the need for a dynamic evaluation method that reflects real-world usage.
  - **Keywords:** Large Language Models, Human Preference Evaluation, Pairwise Comparison, Crowdsourcing, Model Evaluation, Benchmarking, Alignment with Human Preferences, Limitations of Static Benchmarks, Open Evaluation Platform, Credibility of Evaluation Methods, LLMs (Large Language Models), Human Preference


- [Active Preference Learning for Large Language Models](https://icml.cc/virtual/2024/poster/34680) (Poster)
  - **Authors:** [William Muldrew](http://openreview.net/profile?id=~William_Muldrew1), [Peter Hayes](http://openreview.net/profile?id=~Peter_Hayes1), [Mingtian Zhang](http://openreview.net/profile?id=~Mingtian_Zhang1), [David Barber](http://openreview.net/profile?id=~David_Barber2)
  - **Affiliations:** Centre for Artificial Intelligence, University College London, London, UK, Centre for Artificial Intelligence, University College London, London, UK, Centre for Artificial Intelligence, University College London, London, UK, Centre for Artificial Intelligence, University College London, London, UK
  - **TL;DR:** This study develops an active learning strategy for Direct Preference Optimization (DPO) to enhance the efficiency of fine-tuning large language models using preference data. The proposed method improves both the learning rate and final performance compared to traditional reinforcement learning techniques.
  - **Keywords:** Large Language Models, Human-AI Alignment, Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), Fine-tuning Language Models, Preference Learning, Complexity of RLHF, Stability in Fine-tuning, Active Learning Strategy, Improved Learning Rate, Performance on Preference Data


- [On PI Controllers for Updating Lagrange Multipliers in Constrained Optimization](https://icml.cc/virtual/2024/poster/35138) (Poster)
  - **Authors:** [Motahareh Sohrabi](http://openreview.net/profile?id=~Motahareh_Sohrabi1), [Juan Ramirez](http://openreview.net/profile?id=~Juan_Ramirez2), [Tianyue Zhang](http://openreview.net/profile?id=~Tianyue_H._Zhang1), [Simon Lacoste-Julien](http://openreview.net/profile?id=~Simon_Lacoste-Julien1), [Jose Gallego-Posada](http://openreview.net/profile?id=~Jose_Gallego-Posada1)
  - **Affiliations:** Mila—Quebec AI Institute and DIRO, Université de Montréal, Mila—Quebec AI Institute and DIRO, Université de Montréal; Canada CIFAR AI Chair, Mila—Quebec AI Institute and DIRO, Université de Montréal, Mila—Quebec AI Institute and DIRO, Université de Montréal, Mila—Quebec AI Institute and DIRO, Université de Montréal
  - **TL;DR:** This paper introduces the νPI algorithm for updating Lagrange multipliers in constrained optimization, addressing the instability of traditional gradient descent-ascent methods. The proposed method demonstrates reliable stabilization of multiplier dynamics and generalizes existing momentum techniques, showing empirical success in various applications.
  - **Keywords:** Constrained optimization, Neural networks, νPI algorithm, Lagrangian min-max formulations, PI controllers, Fairness, Sparsity, Active learning, Reinforcement learning, Model quantization, Unstable oscillatory dynamics, Nonconvex optimization, Dual variable convergence, Stabilization of multiplier dynamics, Generalization of momentum methods, Lagrange multipliers, Gradient descent-ascent


- [CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay](https://icml.cc/virtual/2024/poster/34003) (Poster)
  - **Authors:** [Natasha Butt](http://openreview.net/profile?id=~Natasha_Butt1), [Blazej Manczak](http://openreview.net/profile?id=~Blazej_Manczak1), [Auke Wiggers](http://openreview.net/profile?id=~Auke_Wiggers1), [Corrado Rainone](http://openreview.net/profile?id=~Corrado_Rainone1), [David Zhang](http://openreview.net/profile?id=~David_W._Zhang1), [Michaël Defferrard](http://openreview.net/profile?id=~Micha%C3%ABl_Defferrard1), [Taco Cohen](http://openreview.net/profile?id=~Taco_Cohen1)
  - **Affiliations:** University of Amsterdam; Qualcomm AI Research, Qualcomm AI Research, Qualcomm AI Research, Qualcomm AI Research, Qualcomm AI Research, Qualcomm AI Research, Qualcomm AI Research; Qualcomm Technologies, Inc.
  - **TL;DR:** This paper introduces CodeIt, a novel method for self-improvement in language models that addresses the challenges of program synthesis by utilizing hindsight relabeling and prioritized experience replay. The approach achieves state-of-the-art performance on the Abstraction and Reasoning Corpus, solving 15% of the evaluation tasks and demonstrating effective inter-task generalization.
  - **Keywords:** language models, self-improvement, programming-by-examples, Code Iteration, hindsight relabeling, prioritized experience replay, Abstraction and Reasoning Corpus (ARC), general intelligence benchmarks, data sparsity in program synthesis, generalization between tasks, state-of-the-art performance on ARC, neuro-symbolic approach, ARC dataset


- [RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback](https://icml.cc/virtual/2024/poster/33772) (Poster)
  - **Authors:** [Yufei Wang](http://openreview.net/profile?id=~Yufei_Wang4), [Zhanyi Sun](http://openreview.net/profile?id=~Zhanyi_Sun1), [Jesse Zhang](http://openreview.net/profile?id=~Jesse_Zhang3), [Zhou Xian](http://openreview.net/profile?id=~Zhou_Xian1), [Erdem Biyik](http://openreview.net/profile?id=~Erdem_Biyik1), [David Held](http://openreview.net/profile?id=~David_Held1), [Zackory Erickson](http://openreview.net/profile?id=~Zackory_Erickson1)
  - **Affiliations:** Robotics Institute, Carnegie Mellon University, Robotics Institute, Carnegie Mellon University, Department of Computer Science, University of Southern California, Robotics Institute, Carnegie Mellon University, Department of Computer Science, University of Southern California, Robotics Institute, Carnegie Mellon University, Robotics Institute, Carnegie Mellon University
  - **TL;DR:** This paper introduces RL-VLM-F, a method that automates the generation of reward functions for reinforcement learning agents using text descriptions and visual observations, significantly reducing the need for human effort in reward engineering. The approach demonstrates effectiveness across various domains, outperforming previous methods that relied on large pretrained models.
  - **Keywords:** Reinforcement Learning, Reward Engineering, Vision Language Models, Preference Learning, Classic Control, Object Manipulation, Designing Reward Functions, High-Dimensional Environments, Automated Reward Generation, Effective Policies


- [Incorporating Information into Shapley Values: Reweighting via a Maximum Entropy Approach](https://icml.cc/virtual/2024/poster/34605) (Poster)
  - **Authors:** [Darya Biparva](http://openreview.net/profile?id=~Darya_Biparva1), [Donatello Materassi](http://openreview.net/profile?id=~Donatello_Materassi2)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA, Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA
  - **TL;DR:** The paper reconciles the computation of Shapley values with causal inference methods by applying a maximum entropy perspective, proposing two variations of Shapley values that incorporate prior information about the model. The findings suggest that blindly applying Occam's razor to Shapley values does not yield satisfactory explanations.
  - **Keywords:** Shapley values, Explainable AI (XAI), Maximum entropy approach, Additive feature attribution algorithms, Causal inference, Feature attribution, Interdependencies among features, Non-linear interactions, Variations of Shapley values based on entropy maximization, Occam’s razor


- [Exploiting Code Symmetries for Learning Program Semantics](https://icml.cc/virtual/2024/poster/34168) (Spotlight Poster)
  - **Authors:** [Kexin Pei](http://openreview.net/profile?id=~Kexin_Pei1), [Weichen Li](http://openreview.net/profile?id=~Weichen_Li1), [Qirui Jin](http://openreview.net/profile?id=~Qirui_Jin1), [Shuyang Liu](http://openreview.net/profile?id=~Shuyang_Liu2), [Scott Geng](http://openreview.net/profile?id=~Scott_Geng1), [Lorenzo Cavallaro](http://openreview.net/profile?id=~Lorenzo_Cavallaro1), [Junfeng Yang](http://openreview.net/profile?id=~Junfeng_Yang1), [Suman Jana](http://openreview.net/profile?id=~Suman_Jana1)
  - **Affiliations:** Columbia University; The University of Chicago, Columbia University, University of Michigan, Huazhong University of Science and Technology, University of Washington, University College London, Columbia University, Columbia University
  - **TL;DR:** This paper presents SYMC, a novel approach that incorporates code symmetries into the architecture of Large Language Models to enhance their ability to learn program semantics for automated program analysis. The results demonstrate that SYMC outperforms existing state-of-the-art models, including GPT-4, in various program analysis tasks without requiring pre-training.
  - **Keywords:** code semantics, Large Language Models (LLMs), program analysis, self-attention, group-theoretic framework, software engineering, security tasks, robustness of code LLMs, generalization to new code, SYMC model, improved generalization, performance on program analysis tasks, code symmetries, permutation group, program dependence graph


- [From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation](https://icml.cc/virtual/2024/poster/33302) (Poster)
  - **Authors:** [Kun Su](http://openreview.net/profile?id=~Kun_Su1), [Xiulong Liu](http://openreview.net/profile?id=~Xiulong_Liu1), [Eli Shlizerman](http://openreview.net/profile?id=~Eli_Shlizerman1)
  - **Affiliations:** Department of ECE, University of Washington, Seattle, United States, Department of ECE, University of Washington, Seattle, United States, Department of ECE, University of Washington, Seattle, United States; Department of Applied Math, University of Washington, Seattle, United States
  - **TL;DR:** This study introduces a unified framework called Vision to Audio and Beyond (VAB) that bridges audio-visual representation learning and vision-to-audio generation by leveraging latent spaces for contextual learning. The model demonstrates efficiency in generating high-quality audio from video and acquiring semantic audio-visual features, leading to competitive results in various audio-visual tasks.
  - **Keywords:** audio-visual representation learning, vision-to-audio generation, latent space modeling, masked audio token prediction, iterative-decoding, audio-visual retrieval, audio-visual classification, high-dimensional data, joint event comprehension, extensive training computations, unified model for audio-visual tasks, high-quality audio generation from video


- [Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach](https://icml.cc/virtual/2024/poster/33746) (Poster)
  - **Authors:** [Zixiao Wang](http://openreview.net/profile?id=~Zixiao_Wang6), [AmirEmad Ghassami](http://openreview.net/profile?id=~AmirEmad_Ghassami1), [Ilya Shpitser](http://openreview.net/profile?id=~Ilya_Shpitser1)
  - **Affiliations:** Department of Biostatistics Johns Hopkins University, Baltimore, MD, Department of Mathematics and Statistics, Boston University, Boston, MA, Department of Computer Science, Johns Hopkins University, Baltimore, MD
  - **TL;DR:** This study introduces a data fusion approach to identify and estimate parameters in settings with nonignorable missing data (MNAR) by augmenting MNAR datasets with auxiliary datasets that are missing at random (MAR). The authors derive inverse probability weighted estimators and demonstrate their effectiveness through simulations and applications.
  - **Keywords:** Nonignorable missing data, Data fusion, Missing not at random (MNAR), Inverse probability weighted (IPW) estimators, Healthcare, Economics, Social sciences, Missing data, Missingness mechanisms, Identification of parameters, Estimation strategies, Missing at random (MAR), Outcome-selection model, Pattern-mixture model


- [Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams](https://icml.cc/virtual/2024/poster/33386) (Poster)
  - **Authors:** [Brian Cho](http://openreview.net/profile?id=~Brian_M_Cho1), [Kyra Gan](http://openreview.net/profile?id=~Kyra_Gan1), [Nathan Kallus](http://openreview.net/profile?id=~Nathan_Kallus1)
  - **Affiliations:** Department of ORIE, Cornell Tech, New York, NY, USA, Department of ORIE, Cornell Tech, New York, NY, USA, Department of ORIE, Cornell Tech, New York, NY, USA
  - **TL;DR:** This paper introduces PEAK, a novel nonparametric sequential testing method for composite hypotheses across multiple data streams, which significantly reduces the number of samples needed for stopping while maintaining strong statistical power. The method demonstrates practical benefits in both synthetic and real-world applications, particularly in optimizing decision-making in sequential experiments.
  - **Keywords:** Sequential testing, Nonparametric hypothesis testing, Testing-by-betting framework, Expectation-based averaged capital (PEAK), Pure-exploration bandit problems, Digital interventions, Composite hypotheses, Type-I error control, Early stopping, Novel betting scheme, Non-asymptotic α-level test, Reduction in sample size, HeartSteps dataset


- [Towards Compositionality in Concept Learning](https://icml.cc/virtual/2024/poster/32808) (Poster)
  - **Authors:** [Adam Stein](http://openreview.net/profile?id=~Adam_Stein2), [Aaditya Naik](http://openreview.net/profile?id=~Aaditya_Naik1), [Yinjun Wu](http://openreview.net/profile?id=~Yinjun_Wu1), [Mayur Naik](http://openreview.net/profile?id=~Mayur_Naik1), [Eric Wong](http://openreview.net/profile?id=~Eric_Wong1)
  - **Affiliations:** Department of Computer and Information Science, University of Pennsylvania, Pennsylvania, USA, Department of Computer and Information Science, University of Pennsylvania, Pennsylvania, USA, School of Computer Science, Peking University, Beijing, China, Department of Computer and Information Science, University of Pennsylvania, Pennsylvania, USA, Department of Computer and Information Science, University of Pennsylvania, Pennsylvania, USA
  - **TL;DR:** This study proposes Compositional Concept Extraction (CCE) to discover compositional concept representations in foundation models, addressing the limitations of existing unsupervised methods. The evaluation demonstrates that CCE yields more compositional concepts and enhances accuracy in downstream classification tasks.
  - **Keywords:** Concept-based interpretability, Compositionality in concept learning, Compositional Concept Extraction (CCE), PCA, KMeans, Image data, Text data, Non-compositional concept extraction, Black-box nature of foundation models, Improved compositional concept representations, Better accuracy on classification tasks, CUB dataset, CLIP model


- [A Minimaximalist Approach to Reinforcement Learning from Human Feedback](https://icml.cc/virtual/2024/poster/34967) (Poster)
  - **Authors:** [Gokul Swamy](http://openreview.net/profile?id=~Gokul_Swamy1), [Christoph Dann](http://openreview.net/profile?id=~Christoph_Dann1), [Rahul Kidambi](http://openreview.net/profile?id=~Rahul_Kidambi1), [Steven Wu](http://openreview.net/profile?id=~Steven_Wu1), [Alekh Agarwal](http://openreview.net/profile?id=~Alekh_Agarwal2)
  - **Affiliations:** Carnegie Mellon University; Google Research, Google Research, Google Research, Carnegie Mellon University, Google Research
  - **TL;DR:** This paper introduces Self-Play Preference Optimization (SPO), a novel algorithm for reinforcement learning from human feedback that avoids the need for a reward model and adversarial training. The approach demonstrates significant efficiency and robustness in learning from human preferences, particularly in the presence of non-Markovian and stochastic conditions.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Preference-based Reinforcement Learning (PbRL), Self-Play Preference Optimization (SPO), Minimax Winner (MW), Robotics, Recommendation Systems, Retrieval Systems, Large Language Models (LLMs), Non-Markovian preferences, Intransitive preferences, Stochastic preferences, Compounding errors in offline approaches, Efficient learning methods, Robustness to noisy preferences


- [Efficient PAC Learnability of Dynamical Systems Over Multilayer Networks](https://icml.cc/virtual/2024/poster/35107) (Poster)
  - **Authors:** [Zirou Qiu](http://openreview.net/profile?id=~Zirou_Qiu1), [Abhijin Adiga](http://openreview.net/profile?id=~Abhijin_Adiga1), [Madhav Marathe](http://openreview.net/profile?id=~Madhav_V._Marathe1), [S. S. Ravi](http://openreview.net/profile?id=~S._S._Ravi1), [Daniel Rosenkrantz](http://openreview.net/profile?id=~Daniel_Rosenkrantz1), [Richard Stearns](http://openreview.net/profile?id=~Richard_Stearns1), [Anil Vullikanti](http://openreview.net/profile?id=~Anil_Vullikanti1)
  - **Affiliations:** University of Virginia, Charlottesville, VA, USA; Biocomplexity Institute and Initiative, University of Virginia, Charlottesville, VA, USA, Biocomplexity Institute and Initiative, University of Virginia, Charlottesville, VA, USA, University of Virginia, Charlottesville, VA, USA; Biocomplexity Institute and Initiative, University of Virginia, Charlottesville, VA, USA, Biocomplexity Institute and Initiative, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, University at Albany – SUNY, Albany, NY, USA, Department of Computer Science, University at Albany – SUNY, Albany, NY, USA, Department of Computer Science, University at Albany – SUNY, Albany, NY, USA, University of Virginia, Charlottesville, VA, USA; Biocomplexity Institute and Initiative, University of Virginia, Charlottesville, VA, USA
  - **TL;DR:** This study investigates the learnability of dynamical systems over multilayer networks, presenting an efficient PAC learning algorithm that requires few training examples to infer unknown systems. The findings provide a tight analysis of model complexity and establish theoretical foundations for future research in multilayer dynamical systems.
  - **Keywords:** networked dynamical systems, multilayer networks, learnability, PAC learning algorithm, Natarajan dimension, cascading phenomena, contagion propagation, learning unknown interaction functions, model complexity, efficient learning guarantees, theoretical foundations for multilayer dynamical systems, interaction functions, threshold interaction functions, social contagions


- [Fast Decision Boundary based Out-of-Distribution Detector](https://icml.cc/virtual/2024/poster/33500) (Poster)
  - **Authors:** [Litian Liu](http://openreview.net/profile?id=~Litian_Liu1), [Yao Qin](http://openreview.net/profile?id=~Yao_Qin1)
  - **Affiliations:** MIT, UC Santa Barbara
  - **TL;DR:** This paper presents a computationally-efficient Out-of-Distribution (OOD) detection method that leverages feature distances to decision boundaries without relying on auxiliary models. The proposed approach demonstrates improved efficiency and effectiveness compared to state-of-the-art methods, making it suitable for real-time applications.
  - **Keywords:** Out-of-Distribution (OOD) detection, AI safety, Feature distance to decision boundaries, closed-form estimation, Autonomous driving, AI systems deployment, Computational overhead in OOD detection, reliance on auxiliary models, Hyperparameter-free OOD detector, efficiency-effectiveness trade-off improvement, CIFAR-10, SVHN


- [Diffuse, Sample, Project: Plug-And-Play Controllable Graph Generation](https://icml.cc/virtual/2024/poster/33350) (Poster)
  - **Authors:** [Kartik Sharma](http://openreview.net/profile?id=~Kartik_Sharma1), [Srijan Kumar](http://openreview.net/profile?id=~Srijan_Kumar1), [Rakshit Trivedi](http://openreview.net/profile?id=~Rakshit_Trivedi1)
  - **Affiliations:** Georgia Institute of Technology, Atlanta, GA, USA, Georgia Institute of Technology, Atlanta, GA, USA, Massachusetts Institute of Technology, Cambridge, MA, USA
  - **TL;DR:** The study introduces PRODIGY, a novel approach for controlled graph generation using diffusion models, enabling precise control over graph properties while satisfying hard constraints. The method achieves up to 100% constraint satisfaction for various graph types, marking a significant advancement in interpretable graph generation.
  - **Keywords:** graph generation, controlled graph generation, diffusion models, PRODIGY (Projected Diffusion), diffusion models, drug discovery, network optimization, social network analysis, controlling properties of generated graphs, handling hard constraints, up to 100% constraint satisfaction, precise control in graph generation


- [Multi-group Learning for Hierarchical Groups](https://icml.cc/virtual/2024/poster/33229) (Poster)
  - **Authors:** [Samuel Deng](http://openreview.net/profile?id=~Samuel_Deng1), [Daniel Hsu](http://openreview.net/profile?id=~Daniel_Hsu1)
  - **Affiliations:** Department of Computer Science, Columbia University, Department of Computer Science, Columbia University
  - **TL;DR:** This study extends multi-group learning to hierarchical groups, presenting an algorithm that outputs an interpretable decision tree predictor with near-optimal sample complexity. Empirical evaluations demonstrate its effectiveness in achieving generalization across real datasets with hierarchical structures.
  - **Keywords:** multi-group learning, hierarchical structure, decision tree predictor, boosting-based algorithm, medical imaging, facial recognition, object recognition, natural language processing, subgroup performance, fairness in predictions, near-optimal sample complexity, group-wise error rates, agnostic PAC learning, hierarchical group structure


- [Encodings for Prediction-based Neural Architecture Search](https://icml.cc/virtual/2024/poster/33452) (Poster)
  - **Authors:** [Yash Akhauri](http://openreview.net/profile?id=~Yash_Akhauri1), [Mohamed Abdelfattah](http://openreview.net/profile?id=~Mohamed_S_Abdelfattah1)
  - **Affiliations:** Cornell University, New York, USA, Cornell University, New York, USA
  - **TL;DR:** This paper investigates various encoding methods for Neural Architecture Search (NAS) and introduces FLAN, a predictor that significantly reduces the cost of training accuracy predictors while enhancing sample efficiency. The study categorizes encodings into structural, learned, and score-based types, demonstrating their impact on NAS optimization.
  - **Keywords:** Neural Architecture Search (NAS), Predictor-based methods, Accuracy predictors, Zero-cost proxies, Unsupervised pretraining, Structural encodings, Learned encodings, Neural network design, Optimization, Computational cost, Sample efficiency, FLAN: Flow Attention for NAS, Unified encodings, Cost reduction for training NAS accuracy predictors, NASBench-101, NASBench-201, NASBench-301, Network Design Spaces (NDS), TransNASBench-101


- [Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models](https://icml.cc/virtual/2024/poster/33107) (Poster)
  - **Authors:** [Andy Zhou](http://openreview.net/profile?id=~Andy_Zhou2), [Kai Yan](http://openreview.net/profile?id=~Kai_Yan1), [Michal Shlapentokh-Rothman](http://openreview.net/profile?id=~Michal_Shlapentokh-Rothman1), [Haohan Wang](http://openreview.net/profile?id=~Haohan_Wang1), [Yu-Xiong Wang](http://openreview.net/profile?id=~Yu-Xiong_Wang1)
  - **Affiliations:** University of Illinois Urbana-Champaign; Lapis Labs, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign
  - **TL;DR:** This paper introduces Language Agent Tree Search (LATS), a framework that enhances the capabilities of language models in reasoning, acting, and planning by integrating Monte Carlo Tree Search. The experimental results demonstrate LATS's effectiveness in various decision-making tasks, achieving state-of-the-art performance in programming and competitive results in web navigation.
  - **Keywords:** Language models, autonomous agents, decision-making, Monte Carlo Tree Search, in-context learning, Programming, interactive question-answering, web navigation, mathematics, Limitations of simple acting processes, need for deliberate decision-making, Language Agent Tree Search (LATS), improved reasoning performance, state-of-the-art accuracy on HumanEval, HumanEval, WebShop


- [Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation](https://icml.cc/virtual/2024/poster/33547) (Poster)
  - **Authors:** [Xinyi Wang](http://openreview.net/profile?id=~Xinyi_Wang2), [Alfonso Amayuelas](http://openreview.net/profile?id=~Alfonso_Amayuelas2), [Kexun Zhang](http://openreview.net/profile?id=~Kexun_Zhang1), [Liangming Pan](http://openreview.net/profile?id=~Liangming_Pan1), [Wenhu Chen](http://openreview.net/profile?id=~Wenhu_Chen3), [William Wang](http://openreview.net/profile?id=~William_Yang_Wang2)
  - **Affiliations:** Department of Computer Science, University of California, Santa Barbara, Department of Computer Science, University of California, Santa Barbara, Language Technologies Institute, Carnegie Mellon University, Department of Computer Science, University of California, Santa Barbara, Cheriton School of Computer Science, University of Waterloo, Department of Computer Science, University of California, Santa Barbara
  - **TL;DR:** This study investigates how pre-trained language models derive reasoning capabilities through the aggregation of indirect reasoning paths, particularly in the context of knowledge graphs and chain-of-thought reasoning. The findings suggest that enhancing training with random walk paths can significantly improve multi-step reasoning performance in real-world applications.
  - **Keywords:** reasoning ability, pre-trained language models, next-token prediction, random walk paths, knowledge graphs, chain-of-thought reasoning, multi-step reasoning, logical reasoning, improved reasoning performance, aggregation of reasoning paths, multiple KG and CoT datasets, large language models (LLMs), reasoning graphs


- [On the Asymptotic Distribution of the Minimum Empirical Risk](https://icml.cc/virtual/2024/poster/34848) (Poster)
  - **Authors:** [Jacob Westerhout](http://openreview.net/profile?id=~Jacob_Westerhout1), [TrungTin Nguyen](http://openreview.net/profile?id=~TrungTin_Nguyen1), [Xin Guo](http://openreview.net/profile?id=~Xin_Guo5), [Hien Nguyen](http://openreview.net/profile?id=~Hien_Duy_Nguyen1)
  - **Affiliations:** School of Mathematics and Physics, The University of Queensland, St Lucia, QLD 4072, Australia, School of Mathematics and Physics, The University of Queensland, St Lucia, QLD 4072, Australia, School of Mathematics and Physics, The University of Queensland, St Lucia, QLD 4072, Australia, School of Computing, Engineering, and Mathematical Sciences, La Trobe University, Bundoora, VIC 3086, Australia; Institute of Mathematics for Industry, Kyushu University, Nishi Ward, Fukuoka 819-0395, Japan
  - **TL;DR:** This paper characterizes the asymptotic distribution of the minimum empirical risk (MER) under various conditions, improving upon previous assumptions. The findings enable the construction of consistent confidence sets and hypothesis tests, with applications illustrated in neural network contexts.
  - **Keywords:** Empirical Risk Minimization (ERM), Minimum Empirical Risk (MER), Statistical Inference, Asymptotic Distribution, Bootstrap, Penalized Model Selection, Neural Networks, Non-independent and identically distributed data, Discontinuous loss functions, Non-Euclidean spaces, Asymptotic distributions for MERs, Consistent confidence sets, Consistent hypothesis tests


- [APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference](https://icml.cc/virtual/2024/poster/32904) (Oral)
  - **Authors:** [Bowen Zhao](http://openreview.net/profile?id=~Bowen_Zhao3), [Hannaneh Hajishirzi](http://openreview.net/profile?id=~Hannaneh_Hajishirzi1), [Qingqing Cao](http://openreview.net/profile?id=~Qingqing_Cao1)
  - **Affiliations:** University of Washington; Allen Institute for Artificial Intelligence, University of Washington; Allen Institute for Artificial Intelligence, Apple, work done at the University of Washington
  - **TL;DR:** The study introduces APT, a method that adaptively prunes and tunes parameters in large language models to enhance both training and inference efficiency. APT achieves up to 98% task performance retention while significantly reducing memory usage and speeding up fine-tuning processes.
  - **Keywords:** Efficient training, Inference efficiency, Language models, Adaptive pruning, Tuning, Parameter-efficient fine-tuning (PEFT), Structured pruning, High training and inference costs, Memory consumption, APT method, Performance maintenance, Speedup in fine-tuning, Reduction in memory footprint, RoBERTa, T5, LLaMA, LoRA, Transformer


- [Chain of Code: Reasoning with a Language Model-Augmented Code Emulator](https://icml.cc/virtual/2024/poster/32784) (Oral)
  - **Authors:** [Chengshu Li](http://openreview.net/profile?id=~Chengshu_Li1), [Jacky Liang](http://openreview.net/profile?id=~Jacky_Liang1), [Andy Zeng](http://openreview.net/profile?id=~Andy_Zeng3), [Xinyun Chen](http://openreview.net/profile?id=~Xinyun_Chen1), [Karol Hausman](http://openreview.net/profile?id=~Karol_Hausman2), [Dorsa Sadigh](http://openreview.net/profile?id=~Dorsa_Sadigh1), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1), [Li Fei-Fei](http://openreview.net/profile?id=~Li_Fei-Fei1), [Fei Xia](http://openreview.net/profile?id=~Fei_Xia1), [brian ichter](http://openreview.net/profile?id=~brian_ichter1)
  - **Affiliations:** Department of Computer Science, Stanford University, California, USA, Google DeepMind, California, USA, Google DeepMind, California, USA, Google DeepMind, California, USA, Department of Computer Science, Stanford University; Google DeepMind, California, USA, Department of Computer Science, Stanford University; Google DeepMind, California, USA, Google DeepMind, California, USA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, California, USA, Department of Computer Science, Stanford University, California, USA, Google DeepMind, California, USA, Google DeepMind, California, USA
  - **TL;DR:** This paper introduces Chain of Code (CoC), a method that enhances language model reasoning by integrating code-writing and emulation techniques. The results show that CoC significantly outperforms previous methods, broadening the scope of reasoning tasks that language models can effectively address.
  - **Keywords:** Language Models, Code Reasoning, Chain of Thought, Code Emulation, Undefined behaviors in code execution, Complex reasoning tasks, Chain of Code (CoC), LM-augmented code emulator


- [Sample as you Infer: Predictive Coding with Langevin Dynamics](https://icml.cc/virtual/2024/poster/34917) (Poster)
  - **Authors:** [Umais Zahid](http://openreview.net/profile?id=~Umais_Zahid1), [Qinghai Guo](http://openreview.net/profile?id=~Qinghai_Guo1), [Zafeirios Fountas](http://openreview.net/profile?id=~Zafeirios_Fountas1)
  - **Affiliations:** Huawei Technologies R&D, London, UK; Huawei Technologies Co., Ltd., Shenzhen, Guangdong, China, Huawei Technologies Co., Ltd., Shenzhen, Guangdong, China, Huawei Technologies R&D, London, UK
  - **TL;DR:** This paper introduces Langevin Predictive Coding (LPC), a novel algorithm for deep generative model learning that enhances predictive coding with Gaussian noise and Langevin sampling techniques. The results show that LPC achieves superior sample quality and faster convergence compared to traditional variational autoencoders (VAEs) while maintaining competitive performance across key metrics.
  - **Keywords:** predictive coding, deep generative models, Langevin sampling, variational lower bound, Riemannian Langevin methods, adaptive SGD, robustness to sampling step size, training unsupervised deep generative models, Langevin Predictive Coding (LPC), superior sample quality, faster convergence, benchmark datasets, Gaussian noise, evidence lower bound (ELBO), hierarchical latent Gaussian generative models, Bayesian brain hypothesis, cognitive sciences


- [Ensemble Pruning for Out-of-distribution Generalization](https://icml.cc/virtual/2024/poster/33502) (Poster)
  - **Authors:** [Fengchun Qiao](http://openreview.net/profile?id=~Fengchun_Qiao1), [Xi Peng](http://openreview.net/profile?id=~Xi_Peng1)
  - **Affiliations:** DeepREAL Lab, Department of Computer and Information Sciences, University of Delaware, DE, USA, DeepREAL Lab, Department of Computer and Information Sciences, University of Delaware, DE, USA
  - **TL;DR:** This paper proposes a novel optimization framework for ensemble pruning that enhances out-of-distribution generalization by selecting complementary models with high predictive diversity. The approach is model-agnostic and demonstrates superior performance in both multi- and single-source out-of-distribution scenarios.
  - **Keywords:** Ensemble learning, Out-of-distribution generalization, Ensemble pruning, Combinatorial optimization, Redundant models, Predictive diversity, Distribution shifts, Optimization framework, Model-agnostic approach, Deep neural networks, Topology graph


- [Online Matrix Completion: A Collaborative Approach with Hott Items](https://icml.cc/virtual/2024/poster/34880) (Poster)
  - **Authors:** [Dheeraj Baby](http://openreview.net/profile?id=~Dheeraj_Baby1), [Soumyabrata Pal](http://openreview.net/profile?id=~Soumyabrata_Pal1)
  - **Affiliations:** Dept. of Computer Science, UC Santa Barbara, California, United States, Adobe, Bangalore, India
  - **TL;DR:** This study addresses the low rank matrix completion problem in an online setting, proposing two efficient algorithms that leverage user collaboration to improve recommendation accuracy. The algorithms achieve near-optimal regret guarantees, significantly enhancing the performance of recommendation systems.
  - **Keywords:** online matrix completion, collaborative filtering, recommendation systems, PHASEDCLUSTERE-LIM, DETERMINANTELIM, low rank matrix completion, recommendation systems, user preference learning, exploration vs. exploitation, noisy rewards, sub-optimal item elimination, near-optimal per-user regret, regret guarantees


- [TENG: Time-Evolving Natural Gradient for Solving PDEs With Deep Neural Nets Toward Machine Precision](https://icml.cc/virtual/2024/poster/32799) (Poster)
  - **Authors:** [Zhuo Chen](http://openreview.net/profile?id=~Zhuo_Chen8), [Jacob McCarran](http://openreview.net/profile?id=~Jacob_McCarran1), [Esteban Vizcaino](http://openreview.net/profile?id=~Esteban_Vizcaino1), [Marin Soljačić](http://openreview.net/profile?id=~Marin_Soljacic1), [Di Luo](http://openreview.net/profile?id=~Di_Luo1)
  - **Affiliations:** Department of Physics, Massachusetts Institute of Technology; NSF AI Institute for Artificial Intelligence and Fundamental Interactions, Department of Physics, Massachusetts Institute of Technology; NSF AI Institute for Artificial Intelligence and Fundamental Interactions, Department of Physics, Massachusetts Institute of Technology; NSF AI Institute for Artificial Intelligence and Fundamental Interactions, Department of Physics, Massachusetts Institute of Technology; NSF AI Institute for Artificial Intelligence and Fundamental Interactions, Department of Physics, Massachusetts Institute of Technology; NSF AI Institute for Artificial Intelligence and Fundamental Interactions; Department of Physics, Harvard University
  - **TL;DR:** This paper introduces the Time-Evolving Natural Gradient (TENG) method for solving partial differential equations (PDEs) using neural networks, achieving high accuracy and machine precision. The effectiveness of TENG is demonstrated through its superior performance on various PDEs, including the heat equation, Allen-Cahn equation, and Burgers’ equation.
  - **Keywords:** Partial Differential Equations (PDEs), Neural Networks, Time-Evolving Natural Gradient (TENG), TENG-Euler, TENG-Heun, Natural Gradient Optimization, Computational Mathematics, Data-Driven Discovery, Initial Value Problems, Accuracy Challenges in PDE Solutions, High Accuracy in Neural-Network-Based PDE Solutions, Step-by-Step Optimizations, Machine Precision, Time-Dependent Variational Principles, Optimization-Based Time Integration


- [Extracting Training Data From Document-Based VQA Models](https://icml.cc/virtual/2024/poster/32989) (Poster)
  - **Authors:** [Francesco Pinto](http://openreview.net/profile?id=~Francesco_Pinto1), [Nathalie Rauschmayr](http://openreview.net/profile?id=~Nathalie_Rauschmayr1), [Florian Tramer](http://openreview.net/profile?id=~Florian_Tram%C3%A8r1), [Phil Torr](http://openreview.net/profile?id=~Philip_Torr1), [Federico Tombari](http://openreview.net/profile?id=~Federico_Tombari1)
  - **Affiliations:** Department of Engineering of Science, University of Oxford, UK; Google, Zurich, Switzerland; None, Google, Zurich, Switzerland, ETH Zurich, Zurich, Switzerland, Department of Engineering of Science, University of Oxford, UK, Google, Zurich, Switzerland
  - **TL;DR:** This study investigates the memorization behavior of Vision-Language Models in Document-Based Visual Question Answering, revealing that these models can extract sensitive information even when it is not present in the input. The authors propose a mitigation strategy to prevent the extractability of Personal Identifiable Information (PII).
  - **Keywords:** Document-Based Visual Question Answering, Vision-Language Models, Visual Question Answering, Memorization of sensitive information, privacy risk, Mitigation strategy for extractable PII, DocVQA dataset, Personal Identifiable Information (PII), extractability


- [NeWRF: A Deep Learning Framework for Wireless Radiation Field Reconstruction and Channel Prediction](https://icml.cc/virtual/2024/poster/35164) (Poster)
  - **Authors:** [Haofan Lu](http://openreview.net/profile?id=~Haofan_Lu2), [Christopher Vattheuer](http://openreview.net/profile?id=~Christopher_Vattheuer1), [Baharan Mirzasoleiman](http://openreview.net/profile?id=~Baharan_Mirzasoleiman1), [Omid Abari](http://openreview.net/profile?id=~Omid_Abari2)
  - **Affiliations:** Department of Computer Science, University of California Los Angeles (UCLA), Los Angeles, United States, Department of Computer Science, University of California Los Angeles (UCLA), Los Angeles, United States, Department of Computer Science, University of California Los Angeles (UCLA), Los Angeles, United States, Department of Computer Science, University of California Los Angeles (UCLA), Los Angeles, United States
  - **TL;DR:** The study introduces NeWRF, a deep learning framework designed to predict wireless channels, significantly reducing the time and cost associated with traditional site surveys. The framework effectively utilizes sparse channel measurements to accurately predict wireless signal quality at unvisited locations, addressing common issues in wireless network deployments.
  - **Keywords:** wireless channel prediction, deep learning, Neural Radiance Fields (NeRF), wireless network deployments, site surveys, dead spots, dropped signals, measurement density, NeWRF framework, accurate channel prediction


- [Image Hijacks: Adversarial Images can Control Generative Models at Runtime](https://icml.cc/virtual/2024/poster/34839) (Poster)
  - **Authors:** [Luke Bailey](http://openreview.net/profile?id=~Luke_Bailey1), [Euan Ong](http://openreview.net/profile?id=~Euan_Ong1), [Stuart Russell](http://openreview.net/profile?id=~Stuart_Russell1), [Scott Emmons](http://openreview.net/profile?id=~Scott_Emmons1)
  - **Affiliations:** Harvard University, Cambridge University, University of California, Berkeley, University of California, Berkeley
  - **TL;DR:** This study introduces the concept of image hijacks, which are adversarial images that can manipulate the behavior of vision-language models (VLMs) at inference time. The authors demonstrate that these hijacks can achieve over 80% success in various attacks, raising significant concerns about the security of foundation models against input-based adversarial attacks.
  - **Keywords:** image hijacks, adversarial attacks, vision-language models (VLMs), Behaviour Matching algorithm, Prompt Matching method, security of foundation models, input-based attacks, adversarial robustness, adversarial images controlling VLMs, automated attacks, high success rates, VLMs, LLaVA, CLIP, LLaMA-2, large language models (LLMs), adversarial machine learning


- [Benchmarking Deletion Metrics with the Principled Explanations](https://icml.cc/virtual/2024/poster/34014) (Poster)
  - **Authors:** [Yipei Wang](http://openreview.net/profile?id=~Yipei_Wang1), [Xiaoqian Wang](http://openreview.net/profile?id=~Xiaoqian_Wang1)
  - **Affiliations:** Elmore Family School of Electrical and Computer Engineering, Purdue University, IN, USA, Elmore Family School of Electrical and Computer Engineering, Purdue University, IN, USA
  - **TL;DR:** This paper introduces the TRAjectory importanCE (TRACE) framework for evaluating attribution-based explanation methods using insertion/deletion metrics. It demonstrates that TRACE provides optimal results and addresses critical issues such as the out-of-distribution problem in model predictions.
  - **Keywords:** Explainable Artificial Intelligence (XAI), Attribution Methods, Insertion/Deletion Metrics, TRAjectory importanCE (TRACE) framework, Out-of-Distribution (OOD) issue, Black-box nature of DNNs, Benchmarking insertion/deletion metrics, Evaluation metrics for attribution methods


- [How Language Model Hallucinations Can Snowball](https://icml.cc/virtual/2024/poster/34536) (Poster)
  - **Authors:** [Muru Zhang](http://openreview.net/profile?id=~Muru_Zhang1), [Ofir Press](http://openreview.net/profile?id=~Ofir_Press1), [William Merrill](http://openreview.net/profile?id=~William_Merrill1), [Alisa Liu](http://openreview.net/profile?id=~Alisa_Liu1), [Noah Smith](http://openreview.net/profile?id=~Noah_A._Smith2)
  - **Affiliations:** Paul G. Allen School of Computer Science and Engineering, University of Washington, Princeton University; Princeton Language and Intelligence, Center for Data Science, New York University, Paul G. Allen School of Computer Science and Engineering, University of Washington, Paul G. Allen School of Computer Science and Engineering, University of Washington; Allen Institute for AI
  - **TL;DR:** This study investigates how language models can generate hallucinations that they recognize as incorrect, demonstrating that early mistakes can lead to further inaccuracies. The findings highlight the need for better understanding and mitigation of hallucination phenomena in language models.
  - **Keywords:** Language Model Hallucinations, Knowledge Gaps in Language Models, Question-Answering Datasets, Zero-Shot Chain-of-Thought Prompting, Information-Seeking, Problem-Solving, Hallucination of Incorrect Statements, Early Mistakes Leading to More Mistakes, Identification of Incorrect Claims by Language Models, GPT-3.5, GPT-4, LLaMA2-70B-chat


- [MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data](https://icml.cc/virtual/2024/poster/34942) (Poster)
  - **Authors:** [Paul Scotti](http://openreview.net/profile?id=~Paul_Steven_Scotti1), [Mihir Tripathy](http://openreview.net/profile?id=~Mihir_Tripathy1), [Cesar Kadir Torrico Villanueva](http://openreview.net/profile?id=~Cesar_Torrico1), [Reese Kneeland](http://openreview.net/profile?id=~Reese_Kneeland1), [Tong Chen](http://openreview.net/profile?id=~Tong_Chen14), [Ashutosh Narang](http://openreview.net/profile?id=~Ashutosh_Narang1), [Charan Santhirasegaran](http://openreview.net/profile?id=~Charan_Santhirasegaran1), [Jonathan Xu](http://openreview.net/profile?id=~Jonathan_Xu1), [Thomas Naselaris](http://openreview.net/profile?id=~Thomas_Naselaris3), [Kenneth Norman](http://openreview.net/profile?id=~Kenneth_A._Norman2), [Tanishq Abraham](http://openreview.net/profile?id=~Tanishq_Mathew_Abraham1)
  - **Affiliations:** Stability AI; Medical AI Research Center (MedARC); Princeton Neuroscience Institute, Medical AI Research Center (MedARC), Medical AI Research Center (MedARC), University of Minnesota, The University of Sydney, Medical AI Research Center (MedARC), Medical AI Research Center (MedARC), University of Waterloo, University of Minnesota, Princeton Neuroscience Institute, Stability AI; Medical AI Research Center (MedARC)
  - **TL;DR:** The study presents a novel approach for reconstructing visual perception from fMRI data using only 1 hour of training data by leveraging shared-subject models and functional alignment. This method significantly enhances out-of-subject generalization and achieves state-of-the-art results in image retrieval and reconstruction metrics.
  - **Keywords:** visual perception reconstruction, fMRI data analysis, shared-subject models, functional alignment procedure, CLIP, Stable Diffusion XL, neuroscience, brain imaging, image reconstruction, data sparsity, high-quality reconstruction with limited data, improved out-of-subject generalization, state-of-the-art image retrieval and reconstruction metrics, Natural Scenes Dataset, CLIP, Stable Diffusion, fMRI, latent space, pixel space


- [PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling](https://icml.cc/virtual/2024/poster/33980) (Poster)
  - **Authors:** [Phong Nguyen](http://openreview.net/profile?id=~Phong_C.H._Nguyen1), [Xinlun Cheng](http://openreview.net/profile?id=~Xinlun_Cheng1), [Shahab Azarfar](http://openreview.net/profile?id=~Shahab_Azarfar1), [Pradeep Seshadri](http://openreview.net/profile?id=~Pradeep_Kumar_Seshadri1), [Yen Nguyen](http://openreview.net/profile?id=~Yen_Thi_Nguyen1), [Munho Kim](http://openreview.net/profile?id=~Munho_Kim1), [Sanghun Choi](http://openreview.net/profile?id=~Sanghun_Choi1), [H. Udaykumar](http://openreview.net/profile?email=hs-kumar%40uiowa.edu), [Stephen Baek](http://openreview.net/profile?id=~Stephen_Baek1)
  - **Affiliations:** School of Data Science, University of Virginia, United States, School of Data Science, University of Virginia, United States; Department of Astronomy, University of Virginia, United States, School of Data Science, University of Virginia, United States, Department of Mechanical Engineering, University of Iowa, United States, Department of Mechanical Engineering, University of Iowa, United States, School of Mechanical Engineering, Kyungpook National University, Republic of Korea, School of Mechanical Engineering, Kyungpook National University, Republic of Korea, Department of Mechanical Engineering, University of Iowa, United States, School of Data Science, University of Virginia, United States; Department of Mechanical and Aerospace Engineering, University of Virginia, United States
  - **TL;DR:** The study presents PARCv2, an advanced model for simulating unsteady and advection-dominant physics problems using a physics-aware recurrent convolutional approach. It demonstrates improved capabilities in modeling complex dynamics, particularly in fluid dynamics and energetic materials, compared to existing models.
  - **Keywords:** Physics-aware deep learning, spatiotemporal dynamics, Recurrent convolutions, differentiator-integrator architecture, differential operators, Fluid dynamics, energetic materials, Unsteady dynamics, fast transients, advection-dominated systems, nonlinear field evolution, PARCv2 model, hybrid integral solver, long-time predictions, Partial differential equations (PDEs), advection-reaction-diffusion equations


- [Prediction Accuracy of Learning in Games : Follow-the-Regularized-Leader meets Heisenberg](https://icml.cc/virtual/2024/poster/33792) (Poster)
  - **Authors:** [Yi Feng](http://openreview.net/profile?id=~Yi_Feng3), [Georgios Piliouras](http://openreview.net/profile?id=~Georgios_Piliouras1), [Xiao Wang](http://openreview.net/profile?id=~Xiao_Wang4)
  - **Affiliations:** Shanghai University of Finance and Economics, Shanghai, China, Google DeepMind, London, United Kingdom, Shanghai University of Finance and Economics, Shanghai, China; Key Laboratory of Interdisciplinary Research of Computation and Economics, China
  - **TL;DR:** This study investigates the accuracy of predictions in deterministic learning dynamics of zero-sum games, focusing on observer uncertainty and covariance evolution. It establishes a Heisenberg-type inequality for the Follow-the-Regularized-Leader algorithm and demonstrates that Symplectic discretization improves prediction accuracy in learning dynamics.
  - **Keywords:** prediction accuracy, learning dynamics, zero-sum games, Follow-the-Regularized-Leader (FTRL), Euler discretization, Symplectic discretization, machine learning, game theory, observer uncertainty, prediction challenges in learning dynamics, growth rates of covariance information, Heisenberg-type inequality for FTRL


- [Scalable AI Safety via Doubly-Efficient Debate](https://icml.cc/virtual/2024/poster/34905) (Oral)
  - **Authors:** [Jonah Brown-Cohen](http://openreview.net/profile?id=~Jonah_Brown-Cohen1), [Geoffrey Irving](http://openreview.net/profile?id=~Geoffrey_Irving2), [Georgios Piliouras](http://openreview.net/profile?id=~Georgios_Piliouras1)
  - **Affiliations:** Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK
  - **TL;DR:** This paper presents a new set of debate protocols aimed at improving AI safety by allowing honest strategies to verify the alignment of stochastic AI systems using polynomial simulation steps. The findings suggest that these protocols can effectively manage the complexities of human oversight in high-stakes AI applications, such as legal document drafting.
  - **Keywords:** AI Safety, AI Alignment, Debate Protocols, Natural Language Processing, Legal Document Drafting, Human Oversight Limitations, Complexity of Tasks, New Debate Protocols, Efficient Human Judgement Utilization, Large Language Models (LLMs), Stochastic AI Systems


- [Unsupervised Concept Discovery Mitigates Spurious Correlations](https://icml.cc/virtual/2024/poster/33213) (Poster)
  - **Authors:** [Md Rifat Arefin](http://openreview.net/profile?id=~Md_Rifat_Arefin1), [Yan Zhang](http://openreview.net/profile?id=~Yan_Zhang1), [Aristide Baratin](http://openreview.net/profile?id=~Aristide_Baratin1), [Francesco Locatello](http://openreview.net/profile?id=~Francesco_Locatello1), [Irina Rish](http://openreview.net/profile?id=~Irina_Rish1), [Dianbo Liu](http://openreview.net/profile?id=~Dianbo_Liu2), [Kenji Kawaguchi](http://openreview.net/profile?id=~Kenji_Kawaguchi1)
  - **Affiliations:** Mila, University of Montreal, Canada, Samsung - SAIT AI Lab, Montreal, Canada, Samsung - SAIT AI Lab, Montreal, Canada, Institute of Science and Technology Austria, Mila, University of Montreal, Canada, National University of Singapore, National University of Singapore
  - **TL;DR:** This paper introduces CoBalT, a novel method that leverages unsupervised object-centric learning to mitigate spurious correlations in deep learning models without requiring human-labeled subgroup annotations. The approach demonstrates competitive performance on benchmark datasets, addressing the challenge of model brittleness and unintended biases.
  - **Keywords:** spurious correlations, unsupervised learning, object-centric learning, concept balancing, vector quantization, contrastive learning, image classification, representation learning, spurious correlations, model brittleness, unintended biases, CoBalT (concept balancing technique), robust classification, benchmark datasets for sub-population shifts


- [GeoMFormer: A General Architecture for Geometric Molecular Representation Learning](https://icml.cc/virtual/2024/poster/33785) (Poster)
  - **Authors:** [Tianlang Chen](http://openreview.net/profile?id=~Tianlang_Chen2), [Shengjie Luo](http://openreview.net/profile?id=~Shengjie_Luo1), [Di He](http://openreview.net/profile?id=~Di_He1), [Shuxin Zheng](http://openreview.net/profile?id=~Shuxin_Zheng1), [Tie-Yan Liu](http://openreview.net/profile?id=~Tie-Yan_Liu1), [Liwei Wang](http://openreview.net/profile?id=~Liwei_Wang1)
  - **Affiliations:** School of EECS, Peking University, National Key Laboratory of General Artificial Intelligence; School of Intelligence Science and Technology, Peking University, National Key Laboratory of General Artificial Intelligence; School of Intelligence Science and Technology, Peking University, Microsoft Research AI4Science, Microsoft Research AI4Science, National Key Laboratory of General Artificial Intelligence; School of Intelligence Science and Technology; Center for Machine Learning Research, Peking University
  - **TL;DR:** This study introduces GeoMFormer, a novel Transformer-based architecture designed for geometric molecular representation learning that effectively captures both invariant and equivariant features. Extensive experiments demonstrate its strong performance across various molecular modeling tasks, addressing the need for a flexible framework in this domain.
  - **Keywords:** Molecular modeling, Geometric representation learning, Transformer-based models, Cross-attention modules, Quantum mechanics, Molecular systems, Invariance and equivariance in molecular representation, GeoMFormer architecture, Strong performance on invariant and equivariant tasks


- [The Fundamental Limits of Least-Privilege Learning](https://icml.cc/virtual/2024/poster/33742) (Poster)
  - **Authors:** [Theresa Stadler](http://openreview.net/profile?id=~Theresa_Stadler1), [Bogdan Kulynych](http://openreview.net/profile?id=~Bogdan_Kulynych1), [Michael Gastpar](http://openreview.net/profile?id=~Michael_Gastpar1), [Nicolas Papernot](http://openreview.net/profile?id=~Nicolas_Papernot1), [Carmela Troncoso](http://openreview.net/profile?id=~Carmela_Troncoso1)
  - **Affiliations:** EPFL, Lausanne, Switzerland, Lausanne University Hospital & University of Lausanne, Switzerland, EPFL, Lausanne, Switzerland, University of Toronto & Vector Institute, Toronto, Canada, EPFL, Lausanne, Switzerland
  - **TL;DR:** This study formalizes the least-privilege principle in machine learning, demonstrating a fundamental trade-off between the utility of feature representations for a task and the potential leakage of sensitive information. The findings indicate that achieving high utility while completely preventing inference of unrelated attributes is not feasible.
  - **Keywords:** least-privilege learning, data misuse prevention, feature representations, feature mappings, machine learning as a service (MLaaS), information leakage, unintended inferences, data misuse, formalisation of least-privilege principle, trade-off between utility and leakage, Conditional Entropy Bottleneck


- [Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs](https://icml.cc/virtual/2024/poster/32846) (Oral)
  - **Authors:** [Yeonhong Park](http://openreview.net/profile?id=~Yeonhong_Park1), [Jake Hyun](http://openreview.net/profile?id=~Jake_Hyun1), [SangLyul Cho](http://openreview.net/profile?id=~SangLyul_Cho1), [Bonggeun Sim](http://openreview.net/profile?id=~Bonggeun_Sim1), [Jae W. Lee](http://openreview.net/profile?id=~Jae_W._Lee1)
  - **Affiliations:** Seoul National University, Seoul National University, Seoul National University, Seoul National University, Seoul National University
  - **TL;DR:** This paper introduces any-precision LLM, a method for the low-cost deployment of multiple, different-sized Large Language Models (LLMs) by leveraging a lightweight quantization technique. The proposed solution significantly reduces deployment costs while maintaining model quality and inference throughput.
  - **Keywords:** Large Language Models, any-precision LLM, quantization, post-training quantization, deployment of LLMs, high deployment costs, memory costs, training multiple model versions, lightweight method for any-precision quantization, efficient serving of LLMs


- [Towards Modular LLMs by Building and Reusing a Library of LoRAs](https://icml.cc/virtual/2024/poster/35197) (Poster)
  - **Authors:** [Oleksiy Ostapenko](http://openreview.net/profile?id=~Oleksiy_Ostapenko1), [Zhan Su](http://openreview.net/profile?id=~Zhan_Su1), [Edoardo Ponti](http://openreview.net/profile?id=~Edoardo_Ponti1), [Laurent Charlin](http://openreview.net/profile?id=~Laurent_Charlin1), [Nicolas Le Roux](http://openreview.net/profile?id=~Nicolas_Le_Roux2), [Lucas Caccia](http://openreview.net/profile?id=~Lucas_Caccia1), [Alessandro Sordoni](http://openreview.net/profile?id=~Alessandro_Sordoni2)
  - **Affiliations:** Microsoft Research; Mila — Quebec AI Institute; Université de Montréal, Mila — Quebec AI Institute; University of Copenhagen, University of Edinburgh, Mila — Quebec AI Institute; HEC Montréal; Canada CIFAR AI Chair, Microsoft Research; Mila — Quebec AI Institute; Université de Montréal; Canada CIFAR AI Chair, Microsoft Research, Microsoft Research; Mila — Quebec AI Institute; Université de Montréal
  - **TL;DR:** This study focuses on building and reusing a library of parameter-efficient adapters for large language models (LLMs) to enhance their performance on new tasks through techniques like model-based clustering and a novel routing mechanism. The findings demonstrate that these methods lead to superior generalization compared to traditional joint training approaches.
  - **Keywords:** Modular LLMs, Parameter-efficient adapters, Zero-shot generalization, LoRA (Low-Rank Adaptation), Model-based clustering (MBC), Arrow routing mechanism, Large language models (LLMs), Multi-task learning, Improving LLM performance on new tasks, Task generalization, Dynamic selection of relevant adapters, Superior generalization to new tasks


- [Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models](https://icml.cc/virtual/2024/poster/34826) (Spotlight Poster)
  - **Authors:** [Louis Sharrock](http://openreview.net/profile?id=~Louis_Sharrock1), [Jack Simons](http://openreview.net/profile?id=~Jack_Simons1), [Song Liu](http://openreview.net/profile?id=~Song_Liu1), [Mark Beaumont](http://openreview.net/profile?id=~Mark_Beaumont1)
  - **Affiliations:** Department of Mathematics and Statistics, Lancaster University, UK; School of Mathematics, University of Bristol, UK, School of Mathematics, University of Bristol, UK, School of Mathematics, University of Bristol, UK, School of Mathematics, University of Bristol, UK
  - **TL;DR:** This paper introduces Sequential Neural Posterior Score Estimation (SNPSE), a novel score-based method for Bayesian inference in simulator-based models, which effectively reduces simulation costs by leveraging conditional score-based diffusion models. The method demonstrates comparable or superior performance to existing state-of-the-art techniques like Sequential Neural Posterior Estimation (SNPE) across various numerical examples.
  - **Keywords:** Bayesian inference, likelihood-free inference, simulator-based models, Sequential Neural Posterior Score Estimation (SNPSE), score-based methods, conditional score-based diffusion models, Neuroscience, evolutionary biology, ecology, epidemiology, climate science, cosmology, high-energy physics, econometrics, Absence of a tractable likelihood function, inference from data, New methods for Bayesian inference, sequential training procedures


- [Early Time Classification with Accumulated Accuracy Gap Control](https://icml.cc/virtual/2024/poster/35195) (Poster)
  - **Authors:** [Liran Ringel](http://openreview.net/profile?id=~Liran_Ringel1), [Regev Cohen](http://openreview.net/profile?id=~Regev_Cohen1), [Daniel Freedman](http://openreview.net/profile?id=~Daniel_Freedman2), [Michael Elad](http://openreview.net/profile?id=~Michael_Elad3), [Yaniv Romano](http://openreview.net/profile?id=~Yaniv_Romano1)
  - **Affiliations:** Department of Computer Science, Technion—Israel Institute of Technology, Haifa, Israel; Department of Electrical and Computer Engineering, Technion—Israel Institute of Technology, Haifa, Israel, Verily AI, Israel, Verily AI, Israel, Verily AI, Israel, Department of Computer Science, Technion—Israel Institute of Technology, Haifa, Israel; Department of Electrical and Computer Engineering, Technion—Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This paper introduces a statistical framework for early time series classification that allows for accurate predictions without processing the entire input stream. The proposed method significantly reduces the number of timesteps required for classification while maintaining rigorous accuracy gap control.
  - **Keywords:** Early time series classification, predictive inference, Statistical framework, calibrated stopping rule, Learn-then-Test calibration, Reading comprehension, real-time song identification, computational tomography, Accuracy gap control, early halt times, Early stopping mechanism, reduction of timesteps for classification, Sequential classifier, i.i.d. instances


- [Sequential Disentanglement by Extracting Static Information From A Single Sequence Element](https://icml.cc/virtual/2024/poster/34752) (Poster)
  - **Authors:** [Nimrod Berman](http://openreview.net/profile?id=~Nimrod_Berman1), [Ilan Naiman](http://openreview.net/profile?id=~Ilan_Naiman1), [Idan Arbiv](http://openreview.net/profile?id=~Idan_Arbiv1), [Gal Fadlon](http://openreview.net/profile?id=~Gal_Fadlon1), [Omri Azencot](http://openreview.net/profile?id=~Omri_Azencot1)
  - **Affiliations:** Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel, Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel, Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel, Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel, Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel
  - **TL;DR:** This study presents a novel architecture for unsupervised sequential disentanglement that effectively mitigates information leakage by conditioning on a single sample. The proposed method outperforms existing approaches on various benchmarks in generation and prediction tasks.
  - **Keywords:** Unsupervised sequential disentanglement, Representation learning, Variational autoencoders (VAEs), Dynamic extensions, Time series analysis, Video processing, Audio processing, Information leakage, Non-disentangled representation, Novel architecture for disentanglement, Variational framework, Static factors, Dynamic factors, Latent codes


- [Robust Universal Adversarial Perturbations](https://icml.cc/virtual/2024/poster/34115) (Poster)
  - **Authors:** [Changming Xu](http://openreview.net/profile?id=~Changming_Xu2), [Gagandeep Singh](http://openreview.net/profile?id=~Gagandeep_Singh1)
  - **Affiliations:** Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, USA, VMWare, California, USA; Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, USA
  - **TL;DR:** This study introduces a method for generating Universal Adversarial Perturbations (UAPs) that are robust against real-world transformations, significantly improving their effectiveness in practical attack scenarios. The proposed UAPs demonstrate up to 23% greater robustness compared to existing state-of-the-art methods.
  - **Keywords:** Universal Adversarial Perturbations, Adversarial Machine Learning, Iterative algorithm, probabilistic robustness bounds, Image classification, audio classification, Non-robustness of adversarial perturbations, real-world transformations, Robust UAPs, improved robustness against transformations, CIFAR-10, ILSVRC 2012, Deep Neural Networks (DNNs), adversarial perturbations


- [Model Assessment and Selection under Temporal Distribution Shift](https://icml.cc/virtual/2024/poster/32794) (Poster)
  - **Authors:** [Elise Han](http://openreview.net/profile?id=~Elise_Han1), [Chengpiao Huang](http://openreview.net/profile?id=~Chengpiao_Huang1), [Kaizheng Wang](http://openreview.net/profile?id=~Kaizheng_Wang1)
  - **Affiliations:** Department of Computer Science, Columbia University, New York, NY, United States, Department of Industrial Engineering and Operations Research, Columbia University, New York, NY, United States, Department of Industrial Engineering and Operations Research, Columbia University, New York, NY, United States; Data Science Institute, Columbia University, New York, NY, United States
  - **TL;DR:** This paper presents adaptive methods for model assessment and selection in environments with temporal distribution shifts, utilizing a rolling window approach to estimate generalization errors. The proposed techniques enable effective comparison of models and demonstrate adaptability to changing data distributions.
  - **Keywords:** model assessment, model selection, temporal distribution shift, rolling window approach, pairwise comparisons, single-elimination tournament, non-stationarity in data, model performance degradation, distribution shift, adaptive model selection, generalization error estimation


- [Minimizing $f$-Divergences by Interpolating Velocity Fields](https://icml.cc/virtual/2024/poster/33281) (Poster)
  - **Authors:** [Song Liu](http://openreview.net/profile?id=~Song_Liu1), [Jiahao Yu](http://openreview.net/profile?id=~Jiahao_Yu4), [Jack Simons](http://openreview.net/profile?id=~Jack_Simons1), [Mingxuan Yi](http://openreview.net/profile?id=~Mingxuan_Yi1), [Mark Beaumont](http://openreview.net/profile?id=~Mark_Beaumont1)
  - **Affiliations:** University of Bristol, Bristol, UK, University of Bristol, Bristol, UK, University of Bristol, Bristol, UK, University of Bristol, Bristol, UK, University of Bristol, Bristol, UK
  - **TL;DR:** This study presents a method for minimizing f-divergences between particle and target distributions by estimating velocity fields through interpolation techniques, addressing issues of overfitting in previous density ratio estimations. The proposed approach demonstrates effectiveness in applications such as domain adaptation and missing data imputation.
  - **Keywords:** machine learning, statistical divergence minimization, particle distribution, Wasserstein Gradient Flow, Stein Variational Gradient Descent, density ratio estimation, interpolation techniques, domain adaptation, data imputation, generative modeling, overfitting, statistical discrepancy, unnormalized target density functions, consistent estimators, improved velocity field estimation, f-divergence, Kullback-Leibler divergence, probability flow ODE


- [Low-Cost High-Power Membership Inference Attacks](https://icml.cc/virtual/2024/poster/32909) (Oral)
  - **Authors:** [Sajjad Zarifzadeh](http://openreview.net/profile?id=~Sajjad_Zarifzadeh1), [Philippe Liu](http://openreview.net/profile?id=~Philippe_Liu1), [Reza Shokri](http://openreview.net/profile?id=~Reza_Shokri1)
  - **Affiliations:** National University of Singapore (NUS), CS Department, National University of Singapore (NUS), CS Department, National University of Singapore (NUS), CS Department
  - **TL;DR:** This paper presents a novel statistical test for robust membership inference attacks (RMIA) that operates with low computational overhead and superior test power compared to prior methods. The findings indicate that RMIA effectively enhances the differentiation between member and non-member data points, laying the groundwork for practical data privacy risk assessment in machine learning.
  - **Keywords:** Membership inference attacks, data privacy risk assessment, Robust membership inference attacks (RMIA), likelihood ratio tests, Machine learning, Information leakage, computational cost of attacks, performance instability, Novel statistical test for MIA, enhanced differentiation between member and non-member data points


- [Bridging Mini-Batch and Asymptotic Analysis in Contrastive Learning: From InfoNCE to Kernel-Based Losses](https://icml.cc/virtual/2024/poster/33984) (Poster)
  - **Authors:** [Panagiotis Koromilas](http://openreview.net/profile?id=~Panagiotis_Koromilas1), [Giorgos Bouritsas](http://openreview.net/profile?id=~Giorgos_Bouritsas1), [Theodoros Giannakopoulos](http://openreview.net/profile?id=~Theodoros_Giannakopoulos1), [Mihalis Nicolaou](http://openreview.net/profile?id=~Mihalis_Nicolaou1), [Yannis Panagakis](http://openreview.net/profile?id=~Yannis_Panagakis1)
  - **Affiliations:** Department of Informatics and Telecommunications, National and Kapodistrian University of Athens; Archimedes AI/Athena Research Center, Department of Informatics and Telecommunications, National and Kapodistrian University of Athens; Archimedes AI/Athena Research Center, NCSR "Demokritos", The Cyprus Institute, Department of Informatics and Telecommunications, National and Kapodistrian University of Athens; Archimedes AI/Athena Research Center
  - **TL;DR:** This study analyzes various contrastive learning losses, demonstrating that they share the same optimal solutions under certain conditions, leading to the introduction of a new objective called Decoupled Hyperspherical Energy Loss (DHEL). The empirical results indicate that DHEL enhances performance and robustness across different batch sizes and hyperparameters in computer vision tasks.
  - **Keywords:** Contrastive Learning, Representation Learning, InfoNCE, Kernel Contrastive Learning (KCL), Decoupled Hyperspherical Energy Loss (DHEL), Computer Vision, Memory issues with large batches, Sensitivity to temperature hyperparameter, Dimensionality collapse, Hard-negative sampling strategies, Improved downstream performance, Robustness across batch sizes and hyperparameters, Reduced dimensionality collapse, Hyperspherical Energy Minimisation (HEM)


- [Emergence of In-Context Reinforcement Learning from Noise Distillation](https://icml.cc/virtual/2024/poster/33784) (Poster)
  - **Authors:** [Ilya Zisman](http://openreview.net/profile?id=~Ilya_Zisman1), [Vladislav Kurenkov](http://openreview.net/profile?id=~Vladislav_Kurenkov1), [Alexander Nikulin](http://openreview.net/profile?id=~Alexander_Nikulin1), [Viacheslav Sinii](http://openreview.net/profile?id=~Viacheslav_Sinii1), [Sergey Kolesnikov](http://openreview.net/profile?id=~Sergey_Kolesnikov1)
  - **Affiliations:** AIRI, Moscow, Russia; Skoltech, Moscow, Russia, AIRI, Moscow, Russia; Innopolis University, Kazan, Russia, AIRI, Moscow, Russia; MIPT, Moscow, Russia, Tinkoff, Moscow, Russia; Innopolis University, Kazan, Russia, Tinkoff, Moscow, Russia
  - **TL;DR:** This study introduces ADε, a novel data acquisition method that enables in-context reinforcement learning through synthetic noise injection, addressing the challenges of sample inefficiency and generalization in RL. Experimental results show that this approach allows RL agents to outperform the best suboptimal policies in learning datasets by a significant margin.
  - **Keywords:** In-Context Reinforcement Learning, Noise Distillation, Reinforcement Learning, Transformers, Noise Injection Curriculum, Sample Inefficiency, Generalization to New Tasks, ADε (new data acquisition approach), Learning Histories Generation, Meta-RL, Curriculum Learning


- [Subgoal-based Demonstration Learning for Formal Theorem Proving](https://icml.cc/virtual/2024/poster/33040) (Poster)
  - **Authors:** [Xueliang Zhao](http://openreview.net/profile?id=~Xueliang_Zhao1), [Wenda Li](http://openreview.net/profile?id=~Wenda_Li1), [Lingpeng Kong](http://openreview.net/profile?id=~Lingpeng_Kong1)
  - **Affiliations:** The University of Hong Kong, University of Edinburgh, The University of Hong Kong
  - **TL;DR:** This paper introduces a subgoal-based demonstration learning framework to enhance the efficiency of proof search in large language models for formal theorem proving. The proposed methods significantly improve proof accuracy and sampling efficiency, demonstrating the potential of LLMs in automated theorem proving.
  - **Keywords:** formal theorem proving, large language models (LLMs), subgoal-based demonstration learning, reinforcement learning, diffusion models, software verification, research-level mathematics, proof search efficiency, proof accuracy, increased proof accuracy, improved sampling efficiency, miniF2F benchmark


- [Vector Quantization Pretraining for EEG Time Series with Random Projection and Phase Alignment](https://icml.cc/virtual/2024/poster/34865) (Poster)
  - **Authors:** [Haokun Gui](http://openreview.net/profile?id=~Haokun_GUI1), [Xiucheng Li](http://openreview.net/profile?id=~Xiucheng_Li2), [Xinyang Chen](http://openreview.net/profile?id=~Xinyang_Chen1)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), China, School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), China, School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), China
  - **TL;DR:** This study introduces VQ-MTM, a self-supervised learning model for EEG time series analysis that utilizes random projection and phase alignment to enhance seizure detection and classification. The model demonstrates significant performance improvements over existing methods across multiple datasets.
  - **Keywords:** EEG time series analysis, self-supervised learning, Vector Quantization, BERT-style modeling, random-projection quantization, phase-aligning module, Time-Phase-Shift Equivariance of Fourier Transform, Neurological disorder diagnosis, seizure detection and classification, Data sparsity, labor-intensive diagnosis, rare disorder types, VQ-MTM model, improved performance in seizure detection and classification, Five real-world datasets, large-scale datasets


- [Consistent Diffusion Meets Tweedie: Training Exact Ambient Diffusion Models with Noisy Data](https://icml.cc/virtual/2024/poster/34110) (Poster)
  - **Authors:** [Giannis Daras](http://openreview.net/profile?id=~Giannis_Daras1), [Alexandros Dimakis](http://openreview.net/profile?id=~Alex_Dimakis1), [Constantinos Daskalakis](http://openreview.net/profile?id=~Constantinos_Costis_Daskalakis1)
  - **Affiliations:** Department of Computer Science, University of Texas at Austin; Archimedes AI, Department of Electrical and Computer Engineering, University of Texas at Austin, Department of Electrical Engineering and Computer Science, MIT
  - **TL;DR:** This paper presents a novel framework for training diffusion models using only corrupted data, addressing the memorization issue prevalent in existing models. The proposed method effectively samples from the uncorrupted distribution and reduces memorization while maintaining competitive performance.
  - **Keywords:** Ambient diffusion, diffusion models, corrupted data, Tweedie’s formula, consistency loss function, Image generation, MRI, black-hole imaging, Memorization of training examples, copyright and privacy concerns, training with corrupted data, Exact framework for learning diffusion models, optimal denoisers, Stable Diffusion XL


- [Simple linear attention language models balance the recall-throughput tradeoff](https://icml.cc/virtual/2024/poster/33515) (Spotlight Poster)
  - **Authors:** [Simran Arora](http://openreview.net/profile?id=~Simran_Arora1), [Sabri Eyuboglu](http://openreview.net/profile?id=~Sabri_Eyuboglu1), [Michael Zhang](http://openreview.net/profile?id=~Michael_Zhang4), [Aman Timalsina](http://openreview.net/profile?id=~Aman_Timalsina1), [Silas Alberti](http://openreview.net/profile?id=~Silas_Alberti2), [James Zou](http://openreview.net/profile?id=~James_Zou1), [Atri Rudra](http://openreview.net/profile?id=~Atri_Rudra1), [Christopher Re](http://openreview.net/profile?id=~Christopher_Re1)
  - **Affiliations:** Stanford University, Stanford University, Stanford University, University of Buffalo, Stanford University, Stanford University, University of Buffalo, Stanford University
  - **TL;DR:** This study investigates the efficiency of language models, particularly focusing on the recall-throughput tradeoff. The proposed BASED architecture combines linear and sliding window attention, achieving significant improvements in recall and throughput compared to existing models.
  - **Keywords:** language models, recall, efficiency, attention, linear attention, sliding window attention, BASED architecture, language generation, recall-intensive tasks, memory consumption, recall-throughput tradeoff, BASED architecture, improved throughput, enhanced recall, KV-cache, sub-quadratic models, perplexity, Pareto frontier


- [$\texttt{MoE-RBench}$: Towards Building Reliable Language Models with Sparse Mixture-of-Experts](https://icml.cc/virtual/2024/poster/34261) (Poster)
  - **Authors:** [Guanjie Chen](http://openreview.net/profile?id=~Guanjie_Chen3), [Xinyu Zhao](http://openreview.net/profile?id=~Xinyu_Zhao3), [Tianlong Chen](http://openreview.net/profile?id=~Tianlong_Chen1), [Yu Cheng](http://openreview.net/profile?id=~Yu_Cheng1)
  - **Affiliations:** Shanghai Artificial Intelligence Laboratory; Shanghai Jiao Tong University, The University of North Carolina at Chapel Hill, The University of North Carolina at Chapel Hill; MIT; Harvard University, The Chinese University of Hong Kong
  - **TL;DR:** This study introduces MoE-RBench, a framework for assessing the reliability of Sparse Mixture-of-Experts models, highlighting their performance in safety, adversarial resilience, and robustness. The findings suggest that with proper training settings, MoE models can outperform dense language models in reliability, addressing critical issues in downstream applications.
  - **Keywords:** Mixture-of-Experts (MoE), Large Language Models (LLMs), reliability assessment, Sparse Mixture-of-Experts (SMoE), conditional computation, language modeling, translation, vision, multimodality, reliability issues, harmful content generation, false information, performance drops, domain transfer instability, comprehensive assessment of MoE reliability, insights into adapting pre-trained MoE models, Transformer, adversarial attacks, out-of-distribution robustness, safety, hallucination, AI safety


- [Human-like Category Learning by Injecting Ecological Priors from Large Language Models into Neural Networks](https://icml.cc/virtual/2024/poster/33082) (Poster)
  - **Authors:** [Akshay Kumar Jagadish](http://openreview.net/profile?id=~Akshay_Kumar_Jagadish1), [Julian Coda-Forno](http://openreview.net/profile?id=~Julian_Coda-Forno1), [Mirko Thalmann](http://openreview.net/profile?id=~Mirko_Thalmann1), [Eric Schulz](http://openreview.net/profile?id=~Eric_Schulz1), [Marcel Binz](http://openreview.net/profile?id=~Marcel_Binz1)
  - **Affiliations:** Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany
  - **TL;DR:** This study demonstrates that large language models can generate ecologically valid category learning tasks, and introduces a new model called ecologically rational meta-learned inference (ERMI) that explains human category learning better than existing cognitive models. The findings suggest that human cognition can be understood through the lens of ecological rationality, with implications for developing more effective cognitive models.
  - **Keywords:** Ecological rationality, Category learning, Human cognition, Large language models, Meta-learning, Ecologically rational meta-learned inference (ERMI), Cognitive science, Classification tasks, Defining ecologically valid tasks, Building rational models for cognitive tasks, ERMI model, Better explanation of human data, State-of-the-art performance on OpenML-CC18, OpenML-CC18


- [Unified Generation, Reconstruction, and Representation: Generalized Diffusion with Adaptive Latent Encoding-Decoding](https://icml.cc/virtual/2024/poster/33342) (Poster)
  - **Authors:** [Guangyi Liu](http://openreview.net/profile?id=~Guangyi_Liu1), [Yu Wang](http://openreview.net/profile?id=~Yu_Wang24), [Zeyu Feng](http://openreview.net/profile?id=~Zeyu_Feng2), [Qiyu Wu](http://openreview.net/profile?id=~Qiyu_Wu2), [Liping Tang](http://openreview.net/profile?id=~Liping_Tang2), [Yuan Gao](http://openreview.net/profile?id=~Yuan_Gao11), [Zhen Li](http://openreview.net/profile?id=~Zhen_Li6), [Shuguang Cui](http://openreview.net/profile?id=~Shuguang_Cui1), [Julian McAuley](http://openreview.net/profile?id=~Julian_McAuley1), [Zichao Yang](http://openreview.net/profile?id=~Zichao_Yang1), [Eric Xing](http://openreview.net/profile?id=~Eric_Xing1), [Zhiting Hu](http://openreview.net/profile?id=~Zhiting_Hu3)
  - **Affiliations:** MBZUAI, UC San Diego, UC San Diego, University of Tokyo, MBZUAI, Stanford University, CUHK-Shenzhen, CUHK-Shenzhen, UC San Diego, CMU, MBZUAI; CMU, UC San Diego
  - **TL;DR:** This paper introduces Generalized Encoding-Decoding Diffusion Probabilistic Models (EDDPMs) that unify the capabilities of generation, reconstruction, and representation across various data types. Extensive experiments demonstrate EDDPMs' flexibility and superior performance compared to existing models in handling diverse tasks.
  - **Keywords:** deep generative models, encoding-decoding, diffusion models, Generalized Encoding-Decoding Diffusion Probabilistic Models (EDDPMs), Gaussian noising-denoising, variational autoencoders (VAEs), generative adversarial networks (GANs), text synthesis, protein sequence generation, image generation, limitations of existing models, need for flexible data handling, integration of core capabilities, enhanced performance across data types


- [ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy](https://icml.cc/virtual/2024/poster/34818) (Poster)
  - **Authors:** [Kirill Vishniakov](http://openreview.net/profile?id=~Kirill_Vishniakov1), [Zhiqiang Shen](http://openreview.net/profile?id=~Zhiqiang_Shen1), [Zhuang Liu](http://openreview.net/profile?id=~Zhuang_Liu1)
  - **Affiliations:** MBZUAI, MBZUAI, Meta AI Research
  - **TL;DR:** This study conducts a comparative analysis of ConvNet and Vision Transformer models across supervised and CLIP training paradigms, revealing significant differences in model behaviors despite similar ImageNet accuracies. The findings emphasize the need for nuanced evaluation metrics beyond traditional accuracy measures to better inform model selection for specific applications.
  - **Keywords:** computer vision, model selection, model evaluation, ConvNet, Vision Transformer, CLIP, image classification, model robustness, model overfitting, performance evaluation, transferability, comparative analysis, model characteristics, ImageNet


- [Principled Preferential Bayesian Optimization](https://icml.cc/virtual/2024/poster/33758) (Oral)
  - **Authors:** [Wenjie Xu](http://openreview.net/profile?id=~Wenjie_Xu3), [Wenbin Wang](http://openreview.net/profile?id=~Wenbin_Wang6), [Yuning Jiang](http://openreview.net/profile?id=~Yuning_Jiang5), [Bratislav Svetozarevic](http://openreview.net/profile?id=~Bratislav_Svetozarevic1), [Colin Jones](http://openreview.net/profile?id=~Colin_Jones1)
  - **Affiliations:** Automatic Control Laboratory, EPFL, Lausanne, Switzerland; Urban Energy Systems Laboratory, Empa, Zurich, Switzerland, Automatic Control Laboratory, EPFL, Lausanne, Switzerland, Automatic Control Laboratory, EPFL, Lausanne, Switzerland, Urban Energy Systems Laboratory, Empa, Zurich, Switzerland; The Institute for Artificial Intelligence Research and Development of Serbia, Serbia, Automatic Control Laboratory, EPFL, Lausanne, Switzerland
  - **TL;DR:** This study presents a novel approach to preferential Bayesian optimization that utilizes preference feedback to optimize black-box functions, introducing an optimistic algorithm with theoretical guarantees on cumulative regret and convergence. Experimental results demonstrate that the proposed method outperforms existing heuristics in various applications.
  - **Keywords:** Preferential Bayesian Optimization, Black-box Function Optimization, Likelihood Ratio, Gaussian Processes, Surrogate Modeling, Visual Design Optimization, Thermal Comfort Optimization, Robotic Gait Optimization, Preference Feedback, Cumulative Regret, Global Convergence, Optimistic Algorithm, Information-Theoretic Bound, Estimated Best Solution


- [CogBench: a large language model walks into a psychology lab](https://icml.cc/virtual/2024/poster/34100) (Poster)
  - **Authors:** [Julian Coda-Forno](http://openreview.net/profile?id=~Julian_Coda-Forno1), [Marcel Binz](http://openreview.net/profile?id=~Marcel_Binz1), [Jane Wang](http://openreview.net/profile?id=~Jane_X_Wang1), [Eric Schulz](http://openreview.net/profile?id=~Eric_Schulz1)
  - **Affiliations:** Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Google DeepMind, London, UK, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany
  - **TL;DR:** This study introduces CogBench, a benchmark for evaluating large language models (LLMs) using behavioral metrics from cognitive psychology experiments. Key findings reveal that larger models and those trained with reinforcement learning from human feedback exhibit more human-like behavior and improved performance, while open-source models are less risk-prone than proprietary ones.
  - **Keywords:** Large Language Models, Cognitive Psychology, Behavioral Metrics, Statistical Multilevel Modeling, Reinforcement Learning from Human Feedback (RLHF), Prompt-Engineering Techniques, AI Evaluation, Human Behavior Alignment, Evaluation Challenges of LLMs, Opacity of Model Behavior, Introduction of CogBench, Insights on Model Size and RLHF Impact


- [Learning a Diffusion Model Policy from Rewards via Q-Score Matching](https://icml.cc/virtual/2024/poster/35083) (Poster)
  - **Authors:** [Michael Psenka](http://openreview.net/profile?id=~Michael_Psenka1), [Alejandro Escontrela](http://openreview.net/profile?id=~Alejandro_Escontrela1), [Pieter Abbeel](http://openreview.net/profile?id=~Pieter_Abbeel2), [Yi Ma](http://openreview.net/profile?id=~Yi_Ma4)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, University of California, Berkeley, Department of Electrical Engineering and Computer Science, University of California, Berkeley, Department of Electrical Engineering and Computer Science, University of California, Berkeley, Department of Electrical Engineering and Computer Science, University of California, Berkeley
  - **TL;DR:** This paper introduces a novel policy update method called Q-score matching for reinforcement learning using diffusion models, addressing the challenges of optimizing policies in continuous action spaces. The proposed method demonstrates the ability to learn effective policies through off-policy reinforcement learning, showing promise in simulated environments.
  - **Keywords:** reinforcement learning, diffusion models, Q-score matching, behavior cloning, off-policy reinforcement learning, robotics, continuous action spaces, optimization of policies, sampling challenges in continuous spaces, new policy update method, geometric perspective on policy optimization


- [Averaging $n$-step Returns Reduces Variance in Reinforcement Learning](https://icml.cc/virtual/2024/poster/33314) (Poster)
  - **Authors:** [Brett Daley](http://openreview.net/profile?id=~Brett_Daley1), [Martha White](http://openreview.net/profile?id=~Martha_White1), [Marlos C. Machado](http://openreview.net/profile?id=~Marlos_C._Machado1)
  - **Affiliations:** Department of Computing Science, University of Alberta; Alberta Machine Intelligence Institute; Canada CIFAR AI Chair, Department of Computing Science, University of Alberta; Alberta Machine Intelligence Institute; Canada CIFAR AI Chair, Department of Computing Science, University of Alberta; Alberta Machine Intelligence Institute; Canada CIFAR AI Chair
  - **TL;DR:** This study demonstrates that averaging n-step returns into compound returns reduces variance in reinforcement learning, leading to improved sample efficiency. The findings suggest that these methods can enhance the performance of deep RL agents like DQN and PPO.
  - **Keywords:** reinforcement learning, multistep returns, sample efficiency, n-step returns, λ-returns, temporal-difference learning, deep reinforcement learning, value function learning, variance in multistep returns, sample complexity, compound returns, variance reduction, two-bootstrap returns


- [SiT: Symmetry-invariant Transformers for Generalisation in Reinforcement Learning](https://icml.cc/virtual/2024/poster/34004) (Poster)
  - **Authors:** [Matthias Weissenbacher](http://openreview.net/profile?id=~Matthias_Weissenbacher1), [Rishabh Agarwal](http://openreview.net/profile?id=~Rishabh_Agarwal2), [Yoshinobu Kawahara](http://openreview.net/profile?id=~Yoshinobu_Kawahara1)
  - **Affiliations:** RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; Graduate School of Information Science and Technology, Osaka University, Japan, Google DeepMind, RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; Graduate School of Information Science and Technology, Osaka University, Japan
  - **TL;DR:** This paper introduces the Symmetry-Invariant Transformer (SiT), a novel architecture designed to enhance generalization in reinforcement learning by leveraging local and global symmetries. The results demonstrate SiT's superior performance over traditional Vision Transformers on various benchmarks, highlighting its sample efficiency and adaptability to new environments.
  - **Keywords:** reinforcement learning, generalization, Symmetry-Invariant Transformer (SiT), Graph Symmetric Attention (GSA), self-attention, image-based reinforcement learning, MiniGrid, Procgen, Atari, CIFAR10, out-of-distribution generalization, data augmentation challenges, sample inefficiency, improved generalization, sample efficiency, MiniGrid, Procgen, Atari 100k, CIFAR10, Vision Transformers (ViTs), local and global symmetries, equivariance, invariance


- [Linguistic Calibration of Long-Form Generations](https://icml.cc/virtual/2024/poster/32959) (Poster)
  - **Authors:** [Neil Band](http://openreview.net/profile?id=~Neil_Band1), [Xuechen Li](http://openreview.net/profile?id=~Xuechen_Li1), [Tengyu Ma](http://openreview.net/profile?id=~Tengyu_Ma1), [Tatsunori Hashimoto](http://openreview.net/profile?id=~Tatsunori_Hashimoto1)
  - **Affiliations:** Department of Computer Science, Stanford University, Department of Computer Science, Stanford University, Department of Computer Science, Stanford University, Department of Computer Science, Stanford University
  - **TL;DR:** This study introduces a framework for linguistic calibration of long-form generations in language models, enabling them to convey confidence levels in their claims. The findings demonstrate that the calibrated model significantly improves user decision-making by providing probabilistic predictions alongside generated content.
  - **Keywords:** Linguistic calibration, long-form generations, language models, Reinforcement learning, supervised finetuning, Decision-making, biomedical questions, scientific questions, Hallucination, knowledge gaps in language models, Calibrated long-form generations, improved user decision-making, Llama 2 7B, AI Safety, AI Alignment, Hallucination


- [Optimally Improving Cooperative Learning in a Social Setting](https://icml.cc/virtual/2024/poster/33982) (Poster)
  - **Authors:** [Shahrzad Haddadan](http://openreview.net/profile?id=~Shahrzad_Haddadan1), [Cheng Xin](http://openreview.net/profile?id=~Cheng_Xin2), [Jie Gao](http://openreview.net/profile?id=~Jie_Gao6)
  - **Affiliations:** Rutgers Business School, Piscataway, NJ, USA, Department of Computer Science, Rutgers University, Piscataway, NJ, USA, Department of Computer Science, Rutgers University, Piscataway, NJ, USA
  - **TL;DR:** This study investigates how to optimally correct a few classifiers in a network of agents to enhance overall prediction accuracy, presenting a polynomial time algorithm for aggregate optimization and demonstrating the NP-hard nature of egalitarian optimization. The findings have significant implications for cooperative learning in various applications, including cybersecurity and social networks.
  - **Keywords:** Cooperative learning, Networked agents, Classification task, Polynomial time algorithm, Approximation algorithms, Cybersecurity, Online social networks, Erroneous classifiers, Accuracy maximization, NP-hard optimization, Optimization algorithms, Performance guarantees, Synthetic data, Real data


- [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://icml.cc/virtual/2024/poster/32874) (Poster)
  - **Authors:** [Piotr Nawrot](http://openreview.net/profile?id=~Piotr_Nawrot1), [Adrian Łańcucki](http://openreview.net/profile?id=~Adrian_%C5%81a%C5%84cucki1), [Marcin Chochowski](http://openreview.net/profile?id=~Marcin_Chochowski1), [David Tarjan](http://openreview.net/profile?email=dtarjan%40nvidia.com), [Edoardo Ponti](http://openreview.net/profile?id=~Edoardo_Ponti1)
  - **Affiliations:** NVIDIA; University of Edinburgh, NVIDIA; University of Wrocław, NVIDIA, NVIDIA, NVIDIA
  - **TL;DR:** This study introduces Dynamic Memory Compression (DMC) to enhance the efficiency of large language models during inference by reducing the memory load of key-value caches. The method achieves significant throughput improvements while maintaining performance, allowing for longer contexts and larger batches within existing memory constraints.
  - **Keywords:** Large Language Models, Transformers, Memory Efficiency, Dynamic Memory Compression (DMC), Grouped Query Attention (GQA), Auto-regressive Inference, Conversational AI, Inefficiency in memory usage, excessive memory load during generation, Increased throughput, cache compression, performance preservation, Llama 2, NVIDIA H100 GPU, Key-Value Cache, Attention Mechanisms


- [On Positivity Condition for Causal Inference](https://icml.cc/virtual/2024/poster/34935) (Poster)
  - **Authors:** [Inwoo Hwang](http://openreview.net/profile?id=~Inwoo_Hwang1), [Yesong Choe](http://openreview.net/profile?email=yesong%40snu.ac.kr), [Yeahoon Kwon](http://openreview.net/profile?id=~Yeahoon_Kwon1), [Sanghack Lee](http://openreview.net/profile?id=~Sanghack_Lee1)
  - **Affiliations:** Artificial Intelligence Institute, Seoul National University, Seoul, South Korea, Graduate School of Data Science, Seoul National University, Seoul, South Korea, Graduate School of Data Science, Seoul National University, Seoul, South Korea, Artificial Intelligence Institute, Seoul National University, Seoul, South Korea; Graduate School of Data Science, Seoul National University, Seoul, South Korea
  - **TL;DR:** The study investigates the positivity condition necessary for causal effect identification in observational studies, particularly in scenarios where strict positivity may not hold. It explores various methodologies to derive identification formulas without relying on strict positivity, ultimately proposing a positivity-aware identification algorithm.
  - **Keywords:** Causal inference, observational studies, causal effect identification, do-calculus, Q-decomposition, adjustment criterion, backdoor criterion, g-computation, Strict positivity, unmeasured confounders, non-identifiability, Positivity-aware identification algorithm, identification formulas, Causal diagrams, causal graph, semi-Markovian model


- [Stable Differentiable Causal Discovery](https://icml.cc/virtual/2024/poster/34386) (Poster)
  - **Authors:** [Achille Nazaret](http://openreview.net/profile?id=~Achille_Nazaret1), [Justin Hong](http://openreview.net/profile?id=~Justin_Hong1), [Elham Azizi](http://openreview.net/profile?id=~Elham_Azizi1), [David Blei](http://openreview.net/profile?id=~David_Blei2)
  - **Affiliations:** Department of Computer Science, Columbia University, New York, USA; Irving Institute for Cancer Dynamics, Columbia University, New York, USA, Department of Computer Science, Columbia University, New York, USA; Irving Institute for Cancer Dynamics, Columbia University, New York, USA, Department of Computer Science, Columbia University, New York, USA; Irving Institute for Cancer Dynamics, Columbia University, New York, USA; Department of Biomedical Engineering, Columbia University, New York, USA, Department of Computer Science, Columbia University, New York, USA; Department of Statistics, Columbia University, New York, USA
  - **TL;DR:** This paper introduces Stable Differentiable Causal Discovery (SDCD), a new method for inferring causal relationships represented as directed acyclic graphs (DAGs), which addresses the numerical instability and scalability issues of existing methods. SDCD demonstrates improved convergence speed and accuracy, making it applicable to datasets with thousands of variables.
  - **Keywords:** Causal Discovery, Directed Acyclic Graphs (DAGs), Differentiable Causal Discovery (DCD), Stable Differentiable Causal Discovery (SDCD), Spectral Acyclicity Constraint, Biology, Climate Science, Economics, NP-hard problem, numerical instability, scalability issues, New method (SDCD), improved convergence speed and accuracy, scalable to thousands of variables, Observational data, interventional data


- [Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective](https://icml.cc/virtual/2024/poster/32768) (Poster)
  - **Authors:** [Wu Lin](http://openreview.net/profile?id=~Wu_Lin2), [Felix Dangel](http://openreview.net/profile?id=~Felix_Dangel1), [Runa Eschenhagen](http://openreview.net/profile?id=~Runa_Eschenhagen1), [Juhan Bae](http://openreview.net/profile?id=~Juhan_Bae2), [Richard E Turner](http://openreview.net/profile?id=~Richard_E_Turner1), [Alireza Makhzani](http://openreview.net/profile?id=~Alireza_Makhzani1)
  - **Affiliations:** Vector Institute, Canada; University of Toronto, Canada, Vector Institute, Canada, Cambridge University, United Kingdom, University of Toronto, Canada, Cambridge University, United Kingdom, Vector Institute, Canada; University of Toronto, Canada
  - **TL;DR:** This study investigates the effects of removing the square root from adaptive gradient methods, revealing that square-root-free methods can close the generalization gap to SGD on convolutional architectures while maintaining performance on transformers. The findings suggest new insights into adaptive methods and their role in modern training strategies.
  - **Keywords:** adaptive gradient methods, second-order methods, deep learning, Adam, SGD, convolutional architectures, diagonal and non-diagonal adaptive methods, transformers, convolutional neural networks (CNNs), generalization gap, performance discrepancies between adaptive methods and SGD, square-root-free adaptive methods, preconditioner invariance, improved training performance


- [Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers](https://icml.cc/virtual/2024/poster/33870) (Poster)
  - **Authors:** [Katherine Crowson](http://openreview.net/profile?id=~Katherine_Crowson1), [Stefan Baumann](http://openreview.net/profile?id=~Stefan_Andreas_Baumann1), [Alex Birch](http://openreview.net/profile?id=~Alex_Birch1), [Tanishq Abraham](http://openreview.net/profile?id=~Tanishq_Mathew_Abraham1), [Daniel Kaplan](http://openreview.net/profile?id=~Daniel_Z_Kaplan2), [Enrico Shippole](http://openreview.net/profile?id=~Enrico_Shippole1)
  - **Affiliations:** Stability AI, United States, CompVis @ LMU Munich, Germany, Birchlabs, England, United Kingdom, Stability AI, United States, realiz.ai, New York, United States, Independent Researcher, Florida, United States
  - **TL;DR:** The paper introduces the Hourglass Diffusion Transformer (HDiT), a novel image-generative model that efficiently scales with pixel count for high-resolution training directly in pixel-space. HDiT achieves competitive performance with existing models and sets a new state-of-the-art for diffusion models on FFHQ-10242.
  - **Keywords:** image generation, high-resolution synthesis, Hourglass Diffusion Transformer (HDiT), Transformer architecture, convolutional U-Nets, image editing, video and audio generation, fine detail representation, training complexity, state-of-the-art performance on FFHQ-10242, competitive performance on ImageNet-2562, ImageNet, FFHQ, diffusion models, latent diffusion models (LDMs), CNN-transformer-hybrid


- [Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error](https://icml.cc/virtual/2024/poster/33033) (Oral)
  - **Authors:** [Haoran Li](http://openreview.net/profile?id=~Haoran_Li17), [Zicheng Zhang](http://openreview.net/profile?id=~Zicheng_Zhang3), [Wang Luo](http://openreview.net/profile?id=~Wang_Luo1), [Congying Han](http://openreview.net/profile?id=~Congying_Han1), [Yudong Hu](http://openreview.net/profile?id=~Yudong_Hu1), [Tiande Guo](http://openreview.net/profile?id=~Tiande_Guo1), [Shichen Liao](http://openreview.net/profile?id=~Shichen_Liao1)
  - **Affiliations:** School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China, School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China, School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China, School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China, School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China, School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China, School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This study investigates the existence of an optimal robust policy (ORP) in deep reinforcement learning under adversarial conditions, introducing a consistency assumption of policy (CAP) and demonstrating that the Bellman optimal policy serves as the ORP. The proposed Consistent Adversarial Robust Deep Q-Network (CAR-DQN) effectively minimizes Bellman Infinity-error, showcasing superior performance across various benchmarks.
  - **Keywords:** Adversarial robustness, Deep reinforcement learning (DRL), Bellman optimal policy, Consistent Adversarial Robust Deep Q-Network (CAR-DQN), L∞-norm, Optimal robust policy (ORP), state-adversarial robustness, adversarial attacks, Existence of deterministic and stationary ORP, minimization of Bellman Infinity-error, Markov decision process (MDP), state-adversarial paradigm, Bellman optimality equations


- [A Unified Framework for Learning with Nonlinear Model Classes from Arbitrary Linear Samples](https://icml.cc/virtual/2024/poster/32752) (Poster)
  - **Authors:** [Ben Adcock](http://openreview.net/profile?id=~Ben_Adcock1), [Juan Cardenas](http://openreview.net/profile?id=~Juan_M._Cardenas1), [Nick Dexter](http://openreview.net/profile?id=~Nick_Dexter1)
  - **Affiliations:** Department of Mathematics, Simon Fraser University, Burnaby, BC, Canada, Ann and H. J. Smead Department of Aerospace Engineering Sciences, University of Colorado Boulder, Boulder, Colorado, USA, Department of Scientific Computing, Florida State University, Tallahassee, Florida, USA
  - **TL;DR:** This paper introduces a unified framework for learning with nonlinear model classes from arbitrary linear samples, establishing learning guarantees that relate training data to model class for effective generalization. The framework accommodates various learning problems, including compressed sensing and active learning, providing a comprehensive approach to analyzing learning challenges.
  - **Keywords:** Learning from training data, Nonlinear model classes, Random linear measurements, Learning guarantees, Compressed sensing, Active learning, Regression, Learning guarantees, Generalization bounds, Framework for learning, Variation of model class, Hilbert spaces, Finite-dimensional subspaces


- [Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices](https://icml.cc/virtual/2024/poster/33252) (Poster)
  - **Authors:** [Nathaniel Cohen](http://openreview.net/profile?id=~Nathaniel_Cohen1), [Vladimir Kulikov](http://openreview.net/profile?id=~Vladimir_Kulikov1), [Matan Kleiner](http://openreview.net/profile?id=~Matan_Kleiner1), [Inbar Huberman-Spiegelglas](http://openreview.net/profile?id=~Inbar_Huberman-Spiegelglas1), [Tomer Michaeli](http://openreview.net/profile?id=~Tomer_Michaeli1)
  - **Affiliations:** Mines Paris – PSL Research University, Paris, France; Technion – Israel Institute of Technology, Haifa, Israel, Technion – Israel Institute of Technology, Haifa, Israel, Technion – Israel Institute of Technology, Haifa, Israel, Technion – Israel Institute of Technology, Haifa, Israel, Technion – Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This paper presents Slicedit, a novel zero-shot method for text-based video editing that utilizes pretrained text-to-image diffusion models to enhance temporal consistency in videos with complex nonrigid motion. The method effectively edits videos while preserving the original structure and motion, demonstrating significant advantages over existing techniques.
  - **Keywords:** video editing, text-to-image diffusion models, diffusion models, spatiotemporal slices, video editing, image synthesis, temporal consistency, nonrigid motion, occlusions, Slicedit method, enhanced temporal consistency


- [The Expressive Power of Path-Based Graph Neural Networks](https://icml.cc/virtual/2024/poster/33339) (Poster)
  - **Authors:** [Caterina Graziani](http://openreview.net/profile?id=~Caterina_Graziani1), [Tamara Drucks](http://openreview.net/profile?id=~Tamara_Drucks1), [Fabian Jogl](http://openreview.net/profile?id=~Fabian_Jogl1), [Monica Bianchini](http://openreview.net/profile?id=~Monica_Bianchini1), [franco scarselli](http://openreview.net/profile?id=~franco_scarselli1), [Thomas Gärtner](http://openreview.net/profile?id=~Thomas_G%C3%A4rtner2)
  - **Affiliations:** Department of Information Engineering and Mathematics, University of Siena, Siena, Italy, RUML, TU Wien, Vienna, Austria; CAIML, TU Wien, Vienna, Austria, RUML, TU Wien, Vienna, Austria; CAIML, TU Wien, Vienna, Austria, Department of Information Engineering and Mathematics, University of Siena, Siena, Italy, Department of Information Engineering and Mathematics, University of Siena, Siena, Italy, RUML, TU Wien, Vienna, Austria
  - **TL;DR:** This study introduces PATH-WL, a novel class of path-based graph neural networks that enhances expressive power by utilizing paths and shortest path distance information. The findings demonstrate that PATH-WL can count cycles and distinguish a broader range of graph classes compared to existing methods, establishing a new hierarchy of expressive graph neural networks.
  - **Keywords:** graph neural networks, expressive power, path-based methods, PATH-WL, color refinement algorithms, message passing, strongly regular graphs, graph isomorphism, limitations of 1-WL, counting cycles, distinguishing non-isomorphic graphs, new hierarchy of expressive GNNs, empirical results on graph classes, Weisfeiler-Leman (1-WL), k-WL


- [Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning](https://icml.cc/virtual/2024/poster/32733) (Poster)
  - **Authors:** [Saber Malekmohammadi](http://openreview.net/profile?id=~Saber_Malekmohammadi1), [Yaoliang Yu](http://openreview.net/profile?id=~Yaoliang_Yu1), [YANG CAO](http://openreview.net/profile?id=~YANG_CAO10)
  - **Affiliations:** School of Computer Science, University of Waterloo, Waterloo, Canada; Vector Institute, Toronto, Canada, School of Computer Science, University of Waterloo, Waterloo, Canada; Vector Institute, Toronto, Canada, Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan
  - **TL;DR:** This paper presents Robust-HDP, an algorithm designed to enhance utility and convergence speed in heterogeneous differentially private federated learning systems, particularly in scenarios with untrusted servers. The method effectively estimates noise levels in clients' model updates, addressing challenges related to varying privacy requirements and dataset sizes.
  - **Keywords:** Federated Learning, Differential Privacy, Robust-HDP, Local Differential Privacy (LDP), Central Differential Privacy (CDP), Robust PCA (RPCA), Heterogeneity in privacy requirements, noise level variation in model updates, untrusted server challenges, Improved utility and convergence speed, noise level estimation in model updates, Differential Privacy (DP), privacy parameter (ϵ), DPSGD algorithm


- [Environment Design for Inverse Reinforcement Learning](https://icml.cc/virtual/2024/poster/34748) (Oral)
  - **Authors:** [Thomas Kleine Buening](http://openreview.net/profile?id=~Thomas_Kleine_Buening1), [Victor Villin](http://openreview.net/profile?id=~Victor_Villin1), [Christos Dimitrakakis](http://openreview.net/profile?id=~Christos_Dimitrakakis1)
  - **Affiliations:** The Alan Turing Institute, London, UK, Université de Neuchâtel, Neuchâtel, Switzerland, Université de Neuchâtel, Neuchâtel, Switzerland
  - **TL;DR:** This study proposes a framework for adaptive environment design in Inverse Reinforcement Learning to improve sample-efficiency and robustness in learning reward functions from expert demonstrations. The authors demonstrate that intelligently selecting environments can enhance the learning process, addressing challenges related to overfitting and changes in environment dynamics.
  - **Keywords:** Inverse Reinforcement Learning, Adaptive Environment Design, Bayesian IRL, Maximum Entropy IRL, Autonomous Decision-Making, Robotics, Low Sample-Efficiency, Overfitting to Environment Dynamics, Improved Sample-Efficiency, Robustness of Learned Rewards


- [A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://icml.cc/virtual/2024/poster/33565) (Oral)
  - **Authors:** [Andrew Lee](http://openreview.net/profile?id=~Andrew_Lee2), [Xiaoyan Bai](http://openreview.net/profile?id=~Xiaoyan_Bai1), [Itamar Pres](http://openreview.net/profile?id=~Itamar_Pres1), [Martin Wattenberg](http://openreview.net/profile?id=~Martin_Wattenberg1), [Jonathan K. Kummerfeld](http://openreview.net/profile?id=~Jonathan_K._Kummerfeld2), [Rada Mihalcea](http://openreview.net/profile?id=~Rada_Mihalcea1)
  - **Affiliations:** University of Michigan, Ann Arbor, U.S.A., University of Michigan, Ann Arbor, U.S.A., University of Michigan, Ann Arbor, U.S.A., Harvard University, Cambridge, Massachusetts, University of Sydney, Sydney, Australia, University of Michigan, Ann Arbor, U.S.A.
  - **TL;DR:** This study investigates the mechanisms by which direct preference optimization (DPO) aligns pre-trained language models to reduce toxicity. It reveals that while toxic capabilities are not removed, they are bypassed, and provides insights into how these models can be un-aligned.
  - **Keywords:** alignment algorithms, toxicity reduction, pre-trained language models, direct preference optimization (DPO), reinforcement learning with human preferences (RLHF), proximal policy optimization (PPO), singular value decomposition (SVD), natural language processing, toxicity, undesirable behaviors in language models, mechanisms of alignment, un-aligning models, understanding model behavior, pairwise preference dataset, large language models, GPT2-medium, Llama2-7b


- [A Dynamic Algorithm for Weighted Submodular Cover Problem](https://icml.cc/virtual/2024/poster/32825) (Oral)
  - **Authors:** [Kiarash Banihashem](http://openreview.net/profile?id=~Kiarash_Banihashem1), [Samira Goudarzi](http://openreview.net/profile?id=~Samira_Goudarzi1), [MohammadTaghi Hajiaghayi](http://openreview.net/profile?id=~MohammadTaghi_Hajiaghayi1), [Peyman Jabbarzade](http://openreview.net/profile?id=~Peyman_Jabbarzade1), [Morteza Monemizadeh](http://openreview.net/profile?id=~Morteza_Monemizadeh1)
  - **Affiliations:** Department of Computer Science, University of Maryland, MD, USA, Department of Computer Science, University of Maryland, MD, USA, Department of Computer Science, University of Maryland, MD, USA, Department of Computer Science, University of Maryland, MD, USA, Department of Mathematics and Computer Science, TU Eindhoven, the Netherlands
  - **TL;DR:** This study introduces a dynamic algorithm for the weighted submodular cover problem, focusing on maintaining an approximately optimal solution amidst element insertions and deletions. The proposed randomized algorithm achieves a (1 − O(ϵ), O(ϵ−1))-bicriteria approximation with low query complexity per update.
  - **Keywords:** submodular cover problem, dynamic setting, optimization, randomized algorithm, bicriteria approximation, data summarization, active learning, network inference, video analysis, facility location, maintaining approximately optimal solution, low query complexity, updates to ground set, (1 − O(ϵ), O(ϵ−1))-bicriteria approximation, monotone submodular function, ground set


- [A Tale of Tails: Model Collapse as a Change of Scaling Laws](https://icml.cc/virtual/2024/poster/34339) (Poster)
  - **Authors:** [Elvis Dohmatob](http://openreview.net/profile?id=~Elvis_Dohmatob1), [Yunzhen Feng](http://openreview.net/profile?id=~Yunzhen_Feng1), [Pu Yang](http://openreview.net/profile?id=~Pu_Yang3), [Francois Charton](http://openreview.net/profile?id=~Francois_Charton1), [Julia Kempe](http://openreview.net/profile?id=~Julia_Kempe1)
  - **Affiliations:** Meta FAIR, Center for Data Science, New York University; Courant Institute, New York University, School of Mathematical Sciences, Peking University, Meta FAIR, Center for Data Science, New York University; Courant Institute, New York University
  - **TL;DR:** This study investigates how the introduction of synthetic data into training datasets affects the scaling laws of AI models, potentially leading to model collapse. The authors develop a theoretical framework to analyze various decay phenomena and validate their findings through experiments with a transformer model.
  - **Keywords:** synthetic data, model collapse, scaling laws, transformer, large language model, text generation, arithmetic tasks, loss of scaling, un-learning of skills, contamination of datasets, theoretical framework of model collapse, decay phenomena, LAION-5B, Llama2, generative AI, neural scaling laws, AIGC (AI Generated Content)


- [Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning](https://icml.cc/virtual/2024/poster/34011) (Poster)
  - **Authors:** [Haoxin Liu](http://openreview.net/profile?id=~haoxin_liu1), [Harshavardhan Kamarthi](http://openreview.net/profile?id=~Harshavardhan_Kamarthi1), [Lingkai Kong](http://openreview.net/profile?id=~Lingkai_Kong1), [Zhiyuan Zhao](http://openreview.net/profile?id=~Zhiyuan_Zhao1), [Chao Zhang](http://openreview.net/profile?id=~Chao_Zhang15), [B. Aditya Prakash](http://openreview.net/profile?id=~B._Aditya_Prakash2)
  - **Affiliations:** School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA
  - **TL;DR:** This paper proposes FOIL, a model-agnostic framework that enhances time-series forecasting models' out-of-distribution generalization capabilities through invariant learning. The approach addresses challenges posed by unobserved variables and environment inference, achieving performance improvements of up to 85% across various forecasting models.
  - **Keywords:** Time-series forecasting, Out-of-distribution generalization, Invariant learning, Invariant learning, Surrogate loss, Multi-head network, Public health, Finance, Urban computing, Out-of-distribution (OOD) generalization, Unobserved variables, Temporal distribution shifts, FOIL framework, Improved performance of TSF models


- [From Coarse to Fine: Enable Comprehensive Graph Self-supervised Learning with Multi-granular Semantic Ensemble](https://icml.cc/virtual/2024/poster/34367) (Oral)
  - **Authors:** [Qianlong Wen](http://openreview.net/profile?id=~Qianlong_Wen1), [Mingxuan Ju](http://openreview.net/profile?id=~Mingxuan_Ju1), [Zhongyu Ouyang](http://openreview.net/profile?id=~Zhongyu_Ouyang1), [Chuxu Zhang](http://openreview.net/profile?id=~Chuxu_Zhang2), [Yanfang Ye](http://openreview.net/profile?id=~Yanfang_Ye1)
  - **Affiliations:** Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA, Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Snap Inc., Bellevue, WA, USA, Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA, School of Computer Science, Brandeis University, Waltham, MA, USA, Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA
  - **TL;DR:** This study introduces the Multi-granularity Graph Semantic Ensemble (MGSE) framework to enhance self-supervised learning in graph models by capturing multi-granular knowledge. Experimental results demonstrate that MGSE can improve the performance of existing graph SSL frameworks by up to 9.2%.
  - **Keywords:** Self-supervised learning (SSL), Graph learning, Multi-granularity Graph Semantic Ensemble, Knowledge Distillation, Drug discovery, Protein analysis, Social network analysis, Data sparsity, Generalization across downstream tasks, Performance improvement in graph SSL frameworks, Graph neural networks (GNNs), Teacher-student model


- [Sparser, Better, Deeper, Stronger: Improving Static Sparse Training with Exact Orthogonal Initialization](https://icml.cc/virtual/2024/poster/32889) (Poster)
  - **Authors:** [Aleksandra I. Nowak](http://openreview.net/profile?id=~Aleksandra_Nowak1), [Łukasz Gniecki](http://openreview.net/profile?id=~%C5%81ukasz_Gniecki1), [Filip Szatkowski](http://openreview.net/profile?id=~Filip_Szatkowski1), [Jacek Tabor](http://openreview.net/profile?id=~Jacek_Tabor1)
  - **Affiliations:** Jagiellonian University, Faculty of Mathematics and Computer Science; Jagiellonian University, Doctoral School of Exact and Natural Sciences; IDEAS NCBR, Jagiellonian University, Faculty of Mathematics and Computer Science, Warsaw University of Technology, Jagiellonian University, Faculty of Mathematics and Computer Science
  - **TL;DR:** This study introduces Exact Orthogonal Initialization (EOI) as a novel method for static sparse training, demonstrating its effectiveness in training highly sparse neural networks without the need for residual connections or normalization techniques. The findings highlight the importance of weight initialization in optimizing sparse models from scratch.
  - **Keywords:** static sparse training, neural network compression, Exact Orthogonal Initialization (EOI), Givens rotations, sparse initialization, optimization challenges, superior effectiveness and efficiency of EOI, training highly sparse networks, MLP (Multi-Layer Perceptron), CNN (Convolutional Neural Networks)


- [TIC-TAC: A Framework For Improved Covariance Estimation In Deep Heteroscedastic Regression](https://icml.cc/virtual/2024/poster/32623) (Poster)
  - **Authors:** [Megh Shukla](http://openreview.net/profile?id=~Megh_Shukla1), [Mathieu Salzmann](http://openreview.net/profile?id=~Mathieu_Salzmann1), [Alexandre Alahi](http://openreview.net/profile?id=~Alexandre_Alahi3)
  - **Affiliations:** Ecole Polytechnique Fédérale de Lausanne (EPFL); Swiss Data Science Center (SDSC), Ecole Polytechnique Fédérale de Lausanne (EPFL); Swiss Data Science Center (SDSC), Ecole Polytechnique Fédérale de Lausanne (EPFL)
  - **TL;DR:** This study presents a framework called TIC-TAC for improving covariance estimation in deep heteroscedastic regression, addressing the challenges of sub-optimal convergence. The proposed methods, TIC and TAC, enhance the accuracy of covariance predictions and facilitate better optimization outcomes.
  - **Keywords:** deep heteroscedastic regression, covariance estimation, Taylor Induced Covariance (TIC), Task Agnostic Correlations (TAC), sub-optimal convergence, challenges in covariance estimation, improved convergence of negative log-likelihood, accurate covariance learning, synthetic datasets, real-world datasets


- [On The Fairness Impacts of Hardware Selection in Machine Learning](https://icml.cc/virtual/2024/poster/32742) (Poster)
  - **Authors:** [Sree Harsha Nelaturu](http://openreview.net/profile?id=~Sree_Harsha_Nelaturu1), [Nishaanth Kanna](http://openreview.net/profile?id=~Nishaanth_Kanna_Ravichandran1), [Cuong Tran](http://openreview.net/profile?id=~Cuong_Tran1), [Sara Hooker](http://openreview.net/profile?id=~Sara_Hooker2), [Ferdinando Fioretto](http://openreview.net/profile?id=~Ferdinando_Fioretto1)
  - **Affiliations:** Cohere For AI Community; Saarland University, Cohere For AI Community, Dyania Health; University of Virginia, Cohere For AI, University of Virginia; Cohere For AI
  - **TL;DR:** This study investigates how hardware selection impacts fairness and performance in machine learning models, revealing that different hardware can exacerbate disparities among demographic groups. The authors propose a theoretical framework to quantify these disparities and suggest strategies to mitigate hardware-induced performance imbalances.
  - **Keywords:** fairness in machine learning, hardware selection, machine learning as-a-service, model training and deployment, performance disparities, ethical application of ML, gradient flow variations, loss surface differences, strategy for mitigating performance imbalances, theoretical framework for quantifying disparities


- [Practical Hamiltonian Monte Carlo on Riemannian Manifolds via Relativity Theory](https://icml.cc/virtual/2024/poster/34558) (Poster)
  - **Authors:** [Kai Xu](http://openreview.net/profile?id=~Kai_Xu4), [Hong Ge](http://openreview.net/profile?id=~Hong_Ge1)
  - **Affiliations:** MIT-IBM Watson AI Lab, Cambridge MA, United States, University of Cambridge, Cambridge, United Kingdom
  - **TL;DR:** This paper presents a method to enhance the stability of Hamiltonian Monte Carlo sampling on Riemannian manifolds by introducing position-dependent velocity norms, which effectively mitigate numerical errors in high curvature regions. The proposed approach generalizes existing techniques and offers a more robust algorithm for sampling from relativistic momentum distributions.
  - **Keywords:** Hamiltonian Monte Carlo, Riemannian Manifolds, Numerical Stability, Hamiltonian dynamics, momentum distributions, numerical integration, Statistical physics, neuroscience, bioinformatics, social science, machine learning, Integration instability, high curvature regions, numerical errors, Position-dependent velocity norms, tractable algorithms for relativistic momentum distributions


- [Challenges in Training PINNs: A Loss Landscape Perspective](https://icml.cc/virtual/2024/poster/33180) (Oral)
  - **Authors:** [Pratik Rathore](http://openreview.net/profile?id=~Pratik_Rathore1), [Weimu Lei](http://openreview.net/profile?id=~Weimu_Lei1), [Zachary Frangella](http://openreview.net/profile?id=~Zachary_Frangella1), [Lu Lu](http://openreview.net/profile?id=~Lu_Lu1), [Madeleine Udell](http://openreview.net/profile?id=~Madeleine_Udell1)
  - **Affiliations:** Department of Electrical Engineering, Stanford University, Stanford, CA, USA; ICME, Stanford University, Stanford, CA, USA, ICME, Stanford University, Stanford, CA, USA, Department of Management Science & Engineering, Stanford University, Stanford, CA, USA, Department of Statistics and Data Science, Yale University, New Haven, CT, USA, ICME, Stanford University, Stanford, CA, USA; Department of Management Science & Engineering, Stanford University, Stanford, CA, USA
  - **TL;DR:** This study investigates the challenges of training Physics-Informed Neural Networks (PINNs) by analyzing the loss landscape and proposes a novel second-order optimizer, NysNewton-CG (NNCG), which enhances performance compared to traditional methods. The findings highlight the importance of combining first- and second-order optimization techniques to effectively minimize the PINN loss function and improve solutions to complex partial differential equations.
  - **Keywords:** Physics-Informed Neural Networks (PINNs), optimization challenges in training neural networks, Adam optimizer, L-BFGS, Adam+L-BFGS, NysNewton-CG (NNCG), Solving Partial Differential Equations (PDEs), Ill-conditioning in loss landscape, difficulties in minimizing PINN loss function, Improved optimization strategies for training PINNs, insights into loss landscape


- [High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise](https://icml.cc/virtual/2024/poster/34640) (Oral)
  - **Authors:** [Eduard Gorbunov](http://openreview.net/profile?id=~Eduard_Gorbunov1), [Abdurakhmon Sadiev](http://openreview.net/profile?id=~Abdurakhmon_Sadiev1), [Marina Danilova](http://openreview.net/profile?id=~Marina_Danilova1), [Samuel Horváth](http://openreview.net/profile?id=~Samuel_Horv%C3%A1th1), [Gauthier Gidel](http://openreview.net/profile?id=~Gauthier_Gidel1), [Pavel Dvurechenskii](http://openreview.net/profile?id=~Pavel_Dvurechensky1), [Alexander Gasnikov](http://openreview.net/profile?id=~Alexander_Gasnikov1), [Peter Richtarik](http://openreview.net/profile?id=~Peter_Richt%C3%A1rik1)
  - **Affiliations:** Mohamed bin Zayed University of Artificial Intelligence, UAE, King Abdullah University of Science and Technology, KSA, Moscow Institute of Physics and Technology, Russia, Mohamed bin Zayed University of Artificial Intelligence, UAE, Université de Montréal and Mila, Canada; Canada CIFAR AI Chair, Weierstrass Institute for Applied Analysis and Stochastics, Germany, University Innopolis, Russia; Ivannikov Institute for System Programming RAS, Russia; Skolkovo Institute of Science and Technology, Russia, King Abdullah University of Science and Technology, KSA
  - **TL;DR:** This paper presents new stochastic methods for composite and distributed optimization that utilize gradient clipping to achieve high-probability convergence results, addressing limitations in existing methods under heavy-tailed noise. The findings contribute to the theoretical understanding and practical application of optimization techniques in machine learning contexts.
  - **Keywords:** high-probability convergence, stochastic optimization, composite optimization, distributed optimization, Prox-SGD, Parallel SGD, gradient clipping, machine learning, federated learning, heavy-tailed noise, convergence issues, optimization under weak assumptions, new stochastic methods, high-probability convergence results, methods for variational inequalities, variational inequalities, strongly convex problems


- [Adversarial Attacks on Combinatorial Multi-Armed Bandits](https://icml.cc/virtual/2024/poster/35177) (Poster)
  - **Authors:** [Rishab Balasubramanian](http://openreview.net/profile?id=~Rishab_Balasubramanian1), [Jiawei Li](http://openreview.net/profile?id=~Jiawei_Li10), [Tadepalli Prasad](http://openreview.net/profile?id=~Prasad_Tadepalli3), [Huazheng Wang](http://openreview.net/profile?id=~Huazheng_Wang1), [Qingyun Wu](http://openreview.net/profile?id=~Qingyun_Wu2), [Haoyu Zhao](http://openreview.net/profile?id=~Haoyu_Zhao1)
  - **Affiliations:** Oregon State University, University of Illinois Urbana-Champaign, Oregon State University, Oregon State University, Pennsylvania State University, Princeton University
  - **TL;DR:** This study investigates reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB) and establishes conditions for their attackability, revealing that the attackability depends on whether the bandit instance is known to the adversary. The findings indicate that adversarial attacks on CMAB are complex and lack a universal attack strategy due to the unknown environment.
  - **Keywords:** Adversarial attacks, Combinatorial Multi-armed Bandits (CMAB), Reward poisoning attacks, Attack algorithm, Online advertising, Recommendation, Ranking, Influence maximization, Attackability of CMAB, Vulnerability and robustness to poisoning attacks, Sufficient and necessary condition for attackability, Validation through experiments, Multi-armed bandits (MAB), Semi-bandit feedback


- [Safe Exploration in Dose Finding Clinical Trials with Heterogeneous Participants](https://icml.cc/virtual/2024/poster/33735) (Poster)
  - **Authors:** [Isabel Chien](http://openreview.net/profile?id=~Isabel_Chien2), [Wessel Bruinsma](http://openreview.net/profile?id=~Wessel_P_Bruinsma1), [Javier Gonzalez](http://openreview.net/profile?id=~Javier_Gonzalez2), [Richard E Turner](http://openreview.net/profile?id=~Richard_E_Turner1)
  - **Affiliations:** University of Cambridge, Cambridge, UK, Microsoft Research AI for Science, Microsoft Research, University of Cambridge, Cambridge, UK
  - **TL;DR:** The study presents SAFE-T, an adaptive dose-finding procedure that prioritizes participant safety and efficacy while accommodating heterogeneous populations. It demonstrates improved performance over traditional methods in identifying optimal drug doses, addressing ethical concerns in clinical trials.
  - **Keywords:** dose-finding clinical trials, participant heterogeneity, adaptive trial methods, Safe Allocation for Exploration of Treatments (SAFE-T), non-parametric multi-output Gaussian process models, Bayesian optimization, drug development, clinical trials, participant safety, participant benefit, toxicity, efficacy, ethical concerns, societal health inequalities, accurate final dose recommendations, theoretical guarantees for safety constraints


- [Parameterized Physics-informed Neural Networks for Parameterized PDEs](https://icml.cc/virtual/2024/poster/33141) (Oral)
  - **Authors:** [Woojin Cho](http://openreview.net/profile?id=~Woojin_Cho1), [Minju Jo](http://openreview.net/profile?id=~Minju_Jo1), [Haksoo Lim](http://openreview.net/profile?id=~Haksoo_Lim1), [Kookjin Lee](http://openreview.net/profile?id=~Kookjin_Lee1), [Dongeun Lee](http://openreview.net/profile?id=~Dongeun_Lee1), [Sanghyun Hong](http://openreview.net/profile?id=~Sanghyun_Hong1), [Noseong Park](http://openreview.net/profile?id=~Noseong_Park1)
  - **Affiliations:** Yonsei University; Arizona State University, LG CNS, Yonsei University, Arizona State University, Texas A&M University-Commerce, Oregon State University, KAIST
  - **TL;DR:** This paper introduces parameterized physics-informed neural networks (P2INNs) to efficiently model solutions of parameterized PDEs, significantly improving accuracy and parameter efficiency compared to traditional PINNs. The findings demonstrate P2INNs' effectiveness in overcoming known limitations of existing methods in scientific machine learning applications.
  - **Keywords:** Physics-informed neural networks, Scientific machine learning, Parameterized physics-informed neural networks (P2INNs), Neural networks, Design optimization, Uncertainty quantification, Repetitive and time-consuming training of PINNs, Evaluation of PDE solutions in parameter space, Improved accuracy and parameter efficiency of P2INNs, Overcoming failure modes of PINNs, Partial differential equations (PDEs), Latent representation, Governing physical laws


- [Position: The Causal Revolution Needs Scientific Pragmatism](https://icml.cc/virtual/2024/poster/33566) (Poster)
  - **Authors:** [Joshua Loftus](http://openreview.net/profile?id=~Joshua_R._Loftus1)
  - **Affiliations:** Department of Statistics, London School of Economics, London, UK
  - **TL;DR:** The paper argues for the adoption of scientific pragmatism to facilitate the progress of causal models in empirical sciences, which are currently hindered by conflicting academic perspectives. It emphasizes the importance of using causal models as tools for hypothetical reasoning to unlock their potential benefits.
  - **Keywords:** Causal models, Scientific pragmatism, Knowledge generation, Causal models, Predictive models, Empirical sciences, Health sciences, Social sciences, Economics, Computer science, Statistics, Stalled progress in causal methods, Scientific perfectionism, System-centric inductive biases, Structural causal models (SCMs), Directed acyclic graphs (DAGs)


- [Agnostic Interactive Imitation Learning: New Theory and Practical Algorithms](https://icml.cc/virtual/2024/poster/33883) (Poster)
  - **Authors:** [Yichen Li](http://openreview.net/profile?id=~Yichen_Li3), [Chicheng Zhang](http://openreview.net/profile?id=~Chicheng_Zhang1)
  - **Affiliations:** Department of Computer Science, University of Arizona, Tucson, AZ, USA, Department of Computer Science, University of Arizona, Tucson, AZ, USA
  - **TL;DR:** This study introduces new algorithms for interactive imitation learning that allow learners to query experts for action annotations, aiming to learn competitive policies with minimal expert input. The proposed methods, MFTPL-P and BOOTSTRAP-DAGGER, demonstrate significant improvements over existing imitation learning approaches in continuous control tasks.
  - **Keywords:** Interactive Imitation Learning, Agnostic Learning, MFTPL-P (Mixed Follow the Perturbed Leader with Poisson perturbations), BOOTSTRAP-DAGGER, Continuous Control Tasks, Covariate Shift, Data Collection Methods, Oracle-efficient algorithms, Finite-sample guarantees


- [PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency](https://icml.cc/virtual/2024/poster/33175) (Poster)
  - **Authors:** [Yeonsung Jung](http://openreview.net/profile?id=~Yeonsung_Jung1), [Heecheol Yun](http://openreview.net/profile?id=~Heecheol_Yun1), [Joonhyung Park](http://openreview.net/profile?id=~Joonhyung_Park1), [Jin-Hwa Kim](http://openreview.net/profile?id=~Jin-Hwa_Kim1), [Eunho Yang](http://openreview.net/profile?id=~Eunho_Yang1)
  - **Affiliations:** Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea, Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea, Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea, NAVER AI Lab, Republic of Korea; AI Institute of Seoul National University, Republic of Korea, Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea; AITRICS, Republic of Korea
  - **TL;DR:** This paper introduces PruNeRF, a segment-centric dataset pruning framework that effectively identifies and removes distractors in Neural Radiance Fields (NeRF) training images. The proposed method demonstrates improved robustness against distractors, addressing a significant challenge in 3D scene learning.
  - **Keywords:** Neural Radiance Fields (NeRF), 3D scene learning, dataset pruning, Influence Functions, depth-based reprojection, segmentation, 3D scene synthesis, image processing, Vulnerability to distractors, dataset curation challenges, inconsistency in supervision, PruNeRF framework, improved robustness against distractors


- [Bridging discrete and continuous state spaces: Exploring the Ehrenfest process in time-continuous diffusion models](https://icml.cc/virtual/2024/poster/34853) (Poster)
  - **Authors:** [Ludwig Winkler](http://openreview.net/profile?id=~Ludwig_Winkler1), [Lorenz Richter](http://openreview.net/profile?id=~Lorenz_Richter1), [Manfred Opper](http://openreview.net/profile?id=~Manfred_Opper1)
  - **Affiliations:** Technical University of Berlin, Zuse Institute Berlin; dida Datenschmiede GmbH, Technical University of Berlin; University of Birmingham; University of Potsdam
  - **TL;DR:** This study explores the relationship between time-continuous Markov jump processes on discrete state spaces and their continuous-state diffusion counterparts, particularly focusing on the Ehrenfest process. The authors propose a new algorithm for training the time-reversal of these processes, demonstrating its effectiveness through numerical experiments.
  - **Keywords:** generative modeling, stochastic processes, time-reversal, Markov jump processes, Ornstein-Uhlenbeck process, stochastic differential equations (SDEs), denoising score matching, discrete data, text, images, graph structures, biological data, bridging discrete and continuous state spaces, improved convergence, new algorithms for training time-reversal of Markov jump processes, Ehrenfest process, score functions, rate functions


- [Conformal Validity Guarantees Exist for Any Data Distribution (and How to Find Them)](https://icml.cc/virtual/2024/poster/34552) (Poster)
  - **Authors:** [Drew Prinster](http://openreview.net/profile?id=~Drew_Prinster1), [Samuel Stanton](http://openreview.net/profile?id=~Samuel_Don_Stanton1), [Anqi Liu](http://openreview.net/profile?id=~Anqi_Liu2), [Suchi Saria](http://openreview.net/profile?id=~Suchi_Saria1)
  - **Affiliations:** Department of Computer Science, Johns Hopkins University, Baltimore, MD, U.S.A., Prescient Design, Genentech, New York City, NY, U.S.A., Department of Computer Science, Johns Hopkins University, Baltimore, MD, U.S.A., Department of Computer Science, Johns Hopkins University, Baltimore, MD, U.S.A.
  - **TL;DR:** This paper demonstrates that conformal prediction can be extended to any joint data distribution, addressing the limitations of previous methods that assumed exchangeability. The authors propose specific algorithms for practical applications in AI/ML contexts, particularly in scenarios involving covariate shifts due to active learning and black-box optimization.
  - **Keywords:** uncertainty quantification, conformal prediction, AI/ML risk management, conformal prediction, weighted exchangeability, black-box optimization, active learning, data distribution shifts, uncertainty estimation challenges, algorithms for covariate shifts, empirical evaluation methods, feedback-loop shifts, quasi-exchangeability


- [Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation](https://icml.cc/virtual/2024/poster/32883) (Poster)
  - **Authors:** [Michelle Pan](http://openreview.net/profile?id=~Michelle_Pan1), [Mariah Schrum](http://openreview.net/profile?id=~Mariah_L_Schrum1), [Vivek Myers](http://openreview.net/profile?id=~Vivek_Myers1), [Erdem Biyik](http://openreview.net/profile?id=~Erdem_Biyik1), [Anca Dragan](http://openreview.net/profile?id=~Anca_Dragan1)
  - **Affiliations:** Department of Electrical Engineering and Computer Sciences, UC Berkeley, Department of Electrical Engineering and Computer Sciences, UC Berkeley, Department of Electrical Engineering and Computer Sciences, UC Berkeley, Department of Computer Science, University of Southern California, Department of Electrical Engineering and Computer Sciences, UC Berkeley
  - **TL;DR:** This study presents the Coprocessor Actor Critic, a model-based reinforcement learning approach for adaptive brain stimulation aimed at treating neurological conditions like Parkinson's disease and post-stroke motor deficits. The proposed method demonstrates improved sample efficiency and task success compared to traditional methods, highlighting its potential for personalized rehabilitation strategies.
  - **Keywords:** adaptive brain stimulation, neurological conditions, brain-computer interface, model-based reinforcement learning (MBRL), coprocessor policy learning, rehabilitation, stroke recovery, motor control, patient heterogeneity, closed-loop coprocessor policies, motor deficits, improved sample efficiency, task success, individualized coprocessor policies


- [Learning to Continually Learn with the Bayesian Principle](https://icml.cc/virtual/2024/poster/34415) (Poster)
  - **Authors:** [Soochan Lee](http://openreview.net/profile?id=~Soochan_Lee1), [Hyeonseong Jeon](http://openreview.net/profile?id=~Hyeonseong_Jeon2), [Jaehyeon Son](http://openreview.net/profile?id=~Jaehyeon_Son1), [Gunhee Kim](http://openreview.net/profile?id=~Gunhee_Kim1)
  - **Affiliations:** Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea, Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea, Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea, Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea
  - **TL;DR:** This study proposes a novel meta-continual learning framework that leverages sequential Bayesian updates to mitigate catastrophic forgetting in neural networks while maintaining their representational power. The approach demonstrates improved performance and scalability, making it applicable across various domains.
  - **Keywords:** continual learning, meta-learning, sequential Bayesian update, stochastic gradient descent, catastrophic forgetting, NP-hard problem, meta-continual learning framework, improved performance, scalability


- [Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data](https://icml.cc/virtual/2024/poster/34567) (Poster)
  - **Authors:** [Xuran Meng](http://openreview.net/profile?id=~Xuran_Meng1), [Difan Zou](http://openreview.net/profile?id=~Difan_Zou1), [Yuan Cao](http://openreview.net/profile?id=~Yuan_Cao1)
  - **Affiliations:** Department of Statistics and Actuarial Science, University of Hong Kong, Hong Kong, Department of Computer Science, University of Hong Kong, Hong Kong, Department of Statistics and Actuarial Science, University of Hong Kong, Hong Kong; Department of Mathematics, University of Hong Kong, Hong Kong
  - **TL;DR:** This study investigates the benign overfitting phenomenon in two-layer ReLU convolutional neural networks for XOR-type classification tasks, demonstrating that these networks can achieve near Bayes-optimal accuracy even in the presence of label-flipping noise. The findings establish conditions under which over-parameterized models can effectively learn complex patterns that linear models cannot.
  - **Keywords:** benign overfitting, deep learning, over-parameterization, two-layer ReLU convolutional neural networks (CNNs), gradient descent, binary classification, XOR-type classification tasks, overfitting, label-flipping noise, sample complexity, signal-to-noise ratio, near Bayes-optimal accuracy, upper and lower bounds of test error


- [Repoformer: Selective Retrieval for Repository-Level Code Completion](https://icml.cc/virtual/2024/poster/33155) (Oral)
  - **Authors:** [Di Wu](http://openreview.net/profile?id=~Di_Wu14), [Wasi Ahmad](http://openreview.net/profile?id=~Wasi_Uddin_Ahmad1), [Dejiao Zhang](http://openreview.net/profile?id=~Dejiao_Zhang1), [Murali Krishna Ramanathan](http://openreview.net/profile?id=~Murali_Krishna_Ramanathan1), [Xiaofei Ma](http://openreview.net/profile?id=~Xiaofei_Ma1)
  - **Affiliations:** University of California Los Angeles, AWS AI Labs, AWS AI Labs, AWS AI Labs, AWS AI Labs
  - **TL;DR:** This paper proposes a selective retrieval framework for repository-level code completion that enhances efficiency and performance by avoiding unnecessary retrievals. The framework achieves state-of-the-art results and demonstrates significant speed improvements while maintaining output quality.
  - **Keywords:** repository-level code completion, retrieval-augmented generation (RAG), self-supervised learning, selective retrieval, inefficiency in retrieval, performance degradation from irrelevant information, state-of-the-art performance, 70% inference speedup, RepoEval, CrossCodeEval, CrossCodeLongEval, code language models (code LMs), cross-file contexts


- [Closing the Gap: Achieving Global Convergence (Last Iterate) of Actor-Critic under Markovian Sampling with Neural Network Parametrization](https://icml.cc/virtual/2024/poster/32956) (Spotlight Poster)
  - **Authors:** [Mudit Gaur](http://openreview.net/profile?id=~Mudit_Gaur1), [Amrit Singh Bedi](http://openreview.net/profile?id=~Amrit_Bedi1), [Di Wang](http://openreview.net/profile?id=~Di_Wang1), [Vaneet Aggarwal](http://openreview.net/profile?id=~Vaneet_Aggarwal1)
  - **Affiliations:** Department of Statistics, Purdue University, West Lafayette, IN, U.S.A, Department of Computer Science, University of Central Florida, Department of Computer Science, KAUST, School of IE and School of ECE, Purdue University, West Lafayette, IN, U.S.A
  - **TL;DR:** This study provides a comprehensive theoretical analysis of Actor-Critic algorithms, addressing practical aspects such as multi-layer neural network parametrization and Markovian sampling. The authors establish global convergence sample complexity bounds, highlighting the importance of aligning theoretical models with real-world applications.
  - **Keywords:** Actor-Critic algorithms, reinforcement learning, Multi-layer neural network parametrization, Markovian sampling, Games, network scheduling, robotics, autonomous driving, video streaming, Gap between theoretical analysis and practical implementations, global convergence, Global convergence sample complexity bounds, weak gradient domination property, MMCLG criteria


- [SqueezeLLM: Dense-and-Sparse Quantization](https://icml.cc/virtual/2024/poster/35187) (Poster)
  - **Authors:** [Sehoon Kim](http://openreview.net/profile?id=~Sehoon_Kim1), [Coleman Hooper](http://openreview.net/profile?id=~Coleman_Richard_Charles_Hooper1), [Amir Gholaminejad](http://openreview.net/profile?id=~Amir_Gholami2), [Zhen Dong](http://openreview.net/profile?id=~Zhen_Dong3), [Xiuyu Li](http://openreview.net/profile?id=~Xiuyu_Li1), [Sheng Shen](http://openreview.net/profile?id=~Sheng_Shen2), [Michael Mahoney](http://openreview.net/profile?id=~Michael_W._Mahoney1), [EECS Kurt Keutzer](http://openreview.net/profile?id=~Kurt_Keutzer1)
  - **Affiliations:** UC Berkeley, UC Berkeley, UC Berkeley; ICSI; LBNL, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley; ICSI; LBNL, UC Berkeley
  - **TL;DR:** This study introduces SqueezeLLM, a post-training quantization framework that enables lossless compression of generative large language models to ultra-low precisions of up to 3-bit, significantly improving performance and reducing memory bandwidth bottlenecks. The framework achieves up to 2.3× speedup on an A6000 GPU compared to the FP16 baseline.
  - **Keywords:** Generative Large Language Models, Model Deployment, Quantization, Sensitivity-based Non-uniform Quantization, Dense-and-Sparse Decomposition, Memory Bandwidth Bottleneck, Resource Requirements for Inference, Lossless Compression, 3-bit Quantization, Performance Improvement, LLaMA Models, A6000 GPU


- [Disparate Impact on Group Accuracy of Linearization for Private Inference](https://icml.cc/virtual/2024/poster/34916) (Poster)
  - **Authors:** [Saswat Das](http://openreview.net/profile?id=~Saswat_Das2), [Marco Romanelli](http://openreview.net/profile?id=~Marco_Romanelli1), [Ferdinando Fioretto](http://openreview.net/profile?id=~Ferdinando_Fioretto1)
  - **Affiliations:** University of Virginia, Charlottesville, VA, USA, New York University, New York, NY, USA, University of Virginia, Charlottesville, VA, USA
  - **TL;DR:** This study investigates the impact of linearizing ReLU activations in neural networks for private inference, revealing that while it reduces computational costs, it disproportionately harms the accuracy of minority groups. The authors propose a fine-tuning strategy to mitigate these fairness issues.
  - **Keywords:** Private Inference, Fairness in Machine Learning, Linearization of Non-linear Activations, ReLU Activations, Machine Learning, Facial Recognition, Computational Challenge of Cryptographic Computations, Disparate Impact on Accuracy for Minority Groups, Mitigation Strategies for Accuracy Loss, Mathematical Interpretation of Decision Boundaries, UTKFaces, ResNet18


- [Enabling Uncertainty Estimation in Iterative Neural Networks](https://icml.cc/virtual/2024/poster/34213) (Poster)
  - **Authors:** [Nikita Durasov](http://openreview.net/profile?id=~Nikita_Durasov1), [Doruk Oner](http://openreview.net/profile?id=~Doruk_Oner1), [Jonathan Donier](http://openreview.net/profile?id=~Jonathan_Donier1), [Hieu Le](http://openreview.net/profile?id=~Hieu_Le2), [EPFL Pascal Fua](http://openreview.net/profile?id=~Pascal_Fua1)
  - **Affiliations:** Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, Neural Concept SA, Lausanne, Switzerland, Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, Computer Vision Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland
  - **TL;DR:** This paper presents a method for uncertainty estimation in iterative neural networks by leveraging the convergence rate of successive outputs as a proxy for accuracy. The approach achieves state-of-the-art results with lower computational costs compared to traditional methods like ensembles, demonstrating practical applications in road detection and aerodynamic property estimation.
  - **Keywords:** uncertainty estimation, iterative neural networks, road detection, aerodynamic properties estimation, convergence rate, accuracy of predictions, state-of-the-art uncertainty estimates, computational cost reduction


- [SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals](https://icml.cc/virtual/2024/poster/34078) (Poster)
  - **Authors:** [Rahul Thapa](http://openreview.net/profile?id=~Rahul_Thapa1), [Bryan He](http://openreview.net/profile?id=~Bryan_He1), [Magnus Ruud Kjaer](http://openreview.net/profile?id=~Magnus_Ruud_Kjaer1), [Hyatt Moore](http://openreview.net/profile?id=~Hyatt_Moore_IV1), [Gauri Ganjoo](http://openreview.net/profile?id=~Gauri_Ganjoo1), [Emmanuel Mignot](http://openreview.net/profile?id=~Emmanuel_Mignot1), [James Zou](http://openreview.net/profile?id=~James_Zou1)
  - **Affiliations:** Department of Biomedical Data Science, Stanford University, Department of Computer Science, Stanford University, Department of Health Technology, Technical University of Denmark, Department of Psychiatry and Behavioral Sciences, Stanford University, Department of Psychiatry and Behavioral Sciences, Stanford University, Department of Psychiatry and Behavioral Sciences, Stanford University; Co-senior authors, Department of Biomedical Data Science, Stanford University; Department of Computer Science, Stanford University; Co-senior authors
  - **TL;DR:** This study introduces SleepFM, a multi-modal foundation model for sleep analysis that leverages a large polysomnography dataset to improve sleep stage classification and sleep disordered breathing detection through a novel contrastive learning approach. The model outperforms traditional methods, demonstrating the importance of integrating diverse physiological signals for comprehensive sleep health assessment.
  - **Keywords:** multi-modal representation learning, sleep analysis, contrastive learning, logistic regression, convolutional neural networks (CNN), sleep disorder detection, sleep stage classification, manual visual inspection of sleep data, reliance on labeled data, integration of diverse physiological signals, SleepFM model, improved performance metrics (AUROC, AUPRC), polysomnography dataset, 14,000 participants, 100,000 hours of recordings, Brain Activity Signals (BAS), sleep disordered breathing (SDB)


- [An LLM Compiler for Parallel Function Calling](https://icml.cc/virtual/2024/poster/32829) (Poster)
  - **Authors:** [Sehoon Kim](http://openreview.net/profile?id=~Sehoon_Kim1), [Suhong Moon](http://openreview.net/profile?id=~Suhong_Moon1), [Ryan Tabrizi](http://openreview.net/profile?id=~Ryan_Tabrizi1), [Nicholas Lee](http://openreview.net/profile?id=~Nicholas_Lee1), [Michael Mahoney](http://openreview.net/profile?id=~Michael_W._Mahoney1), [EECS Kurt Keutzer](http://openreview.net/profile?id=~Kurt_Keutzer1), [Amir Gholaminejad](http://openreview.net/profile?id=~Amir_Gholami2)
  - **Affiliations:** UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley; ICSI; LBNL, UC Berkeley, UC Berkeley; ICSI
  - **TL;DR:** This paper introduces LLMCompiler, a system designed to execute function calls in parallel, addressing the inefficiencies of sequential function calling in Large Language Models. The results demonstrate significant improvements in latency, cost, and accuracy compared to existing methods like ReAct.
  - **Keywords:** Large Language Models, Function Calling, Parallel Execution, LLMCompiler, Function Calling Planner, Task Fetching Unit, Executor, High latency, Cost inefficiency, Inaccurate behavior in sequential function calling, Latency speedup, Cost savings, Accuracy improvement, ReAct, Tool Calling, Classical Compilers


- [Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints](https://icml.cc/virtual/2024/poster/33462) (Poster)
  - **Authors:** [Yunsheng Tian](http://openreview.net/profile?id=~Yunsheng_Tian1), [Ane Zuniga](http://openreview.net/profile?id=~Ane_Zuniga1), [Xinwei Zhang](http://openreview.net/profile?id=~Xinwei_Zhang1), [Johannes P. Dürholt](http://openreview.net/profile?id=~Johannes_P._D%C3%BCrholt1), [Payel Das](http://openreview.net/profile?id=~Payel_Das1), [Jie Chen](http://openreview.net/profile?id=~Jie_Chen1), [Wojciech Matusik](http://openreview.net/profile?id=~Wojciech_Matusik2), [Mina Konakovic Lukovic](http://openreview.net/profile?id=~Mina_Konakovic_Lukovic1)
  - **Affiliations:** MIT CSAIL, USA, MIT CSAIL, USA, MIT-IBM Watson AI Lab, IBM Research, USA, Evonik Operations GmbH, Germany, MIT-IBM Watson AI Lab, IBM Research, USA, MIT-IBM Watson AI Lab, IBM Research, USA, MIT CSAIL, USA, MIT CSAIL, USA
  - **TL;DR:** This paper presents BE-CBO, a novel Bayesian optimization method designed to efficiently explore the boundary between feasible and infeasible designs in the presence of unknown physical constraints. The method demonstrates superior performance compared to existing techniques through extensive experiments on both synthetic and real-world benchmarks.
  - **Keywords:** Bayesian optimization, black-box functions, unknown constraints, BE-CBO (Bayesian Exploration-Constrained Bayesian Optimization), ensemble of neural networks, Gaussian Processes, Engineering design, materials science, formulation development, Unknown physical constraints, optimization of feasible and infeasible regions, New optimization method (BE-CBO), improved performance in boundary exploration


- [Q-Star Meets Scalable Posterior Sampling: Bridging Theory and Practice via HyperAgent](https://icml.cc/virtual/2024/poster/34173) (Poster)
  - **Authors:** [Yingru Li](http://openreview.net/profile?id=~Yingru_Li1), [Jiawei Xu](http://openreview.net/profile?id=~Jiawei_Xu1), [Lei Han](http://openreview.net/profile?id=~Lei_Han1), [Zhi-Quan Luo](http://openreview.net/profile?id=~Zhi-Quan_Luo1)
  - **Affiliations:** The Chinese University of Hong Kong, Shenzhen; Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen, Tencent AI and Robotics X, The Chinese University of Hong Kong, Shenzhen; Shenzhen Research Institute of Big Data
  - **TL;DR:** This paper introduces HyperAgent, a novel reinforcement learning algorithm that efficiently approximates posteriors related to optimal action-value functions, demonstrating significant performance improvements in large-scale benchmarks. The algorithm achieves logarithmic computational complexity and sublinear regret, addressing key challenges in practical RL deployment.
  - **Keywords:** reinforcement learning, hypermodel framework, HyperAgent, Q⋆ function, DDQN, Deep Sea exploration, Atari benchmarks, large state spaces, computational complexity, data efficiency, exploration vs. exploitation, efficient incremental approximation, sublinear regret, logarithmic per-step computational complexity


- [Zero-Sum Positional Differential Games as a Framework for Robust Reinforcement Learning: Deep Q-Learning Approach](https://icml.cc/virtual/2024/poster/33937) (Poster)
  - **Authors:** [Anton Plaksin](http://openreview.net/profile?id=~Anton_Plaksin1), [Vitaly Kalev](http://openreview.net/profile?id=~Vitaly_Kalev1)
  - **Affiliations:** Yandex, Moscow, Russia, IMM UB RAS, Yekaterinburg, Russia
  - **TL;DR:** This paper introduces a framework for Robust Reinforcement Learning using positional differential game theory, demonstrating that a shared Q-function can effectively address uncertainties in agent policies. The proposed Isaacs Deep Q-Network algorithms outperform existing RRL and Multi-Agent RL methods in various environments.
  - **Keywords:** Robust Reinforcement Learning, Positional Differential Games, Deep Q-Learning, Q-function, Minimax and Maximin Bellman equations, Real-world applications, Control systems, Uncertainty, Disturbances, Non-stationarity, Robust policies, Isaacs Deep Q-Network algorithms, Centralized Q-learning approach, Adversarial agent


- [Leveraging Attractor Dynamics in Spatial Navigation for Better Language Parsing](https://icml.cc/virtual/2024/poster/33490) (Spotlight Poster)
  - **Authors:** [Xiaolong Zou](http://openreview.net/profile?id=~Xiaolong_Zou1), [Xingxing Cao](http://openreview.net/profile?id=~Xingxing_Cao1), [Xiaojiao Yang](http://openreview.net/profile?id=~Xiaojiao_Yang1), [Bo Hong](http://openreview.net/profile?id=~Bo_Hong2)
  - **Affiliations:** Qiyuan Lab, Beijing, China, Qiyuan Lab, Beijing, China, Qiyuan Lab, Beijing, China, Qiyuan Lab, Beijing, China
  - **TL;DR:** This study investigates the shared computational mechanisms of the hippocampal formation in spatial navigation and language comprehension by developing a prefrontal-hippocampal-entorhinal model (PHE-trinity). The findings suggest that attractor dynamics can enhance learning efficiency and generalization in language parsing tasks, revealing insights into the dynamic mechanisms underlying these cognitive functions.
  - **Keywords:** spatial navigation, language comprehension, hippocampal formation, modular continuous attractor neural network, grid network, language command parsing, relationship between spatial navigation and language processing, systematic generalization, efficient learning, dynamic mechanism for syntactic structure representation, SCAN dataset


- [Infinite-Horizon Distributionally Robust Regret-Optimal Control](https://icml.cc/virtual/2024/poster/33404) (Poster)
  - **Authors:** [Taylan Kargin](http://openreview.net/profile?id=~Taylan_Kargin1), [Joudi Hajar](http://openreview.net/profile?id=~Joudi_Hajar1), [Vikrant Malik](http://openreview.net/profile?id=~Vikrant_Malik1), [Babak Hassibi](http://openreview.net/profile?id=~Babak_Hassibi1)
  - **Affiliations:** California Institute of Technology, California Institute of Technology, California Institute of Technology, California Institute of Technology
  - **TL;DR:** This study focuses on infinite-horizon distributionally robust control of linear systems with quadratic costs, aiming to minimize the worst-case expected regret compared to a non-causal policy. The authors develop an efficient algorithm for optimal control and a method for constructing a near-optimal state-space controller, avoiding the computational burden of traditional approaches.
  - **Keywords:** distributionally robust control, regret-optimal control, linear systems, Wasserstein-2 ambiguity set, frequency-domain algorithm, convex optimization, uncertainty in control systems, performance decline due to disturbances, near-optimal state-space controller, worst-case expected regret minimization, H∞-norm, causal policy, non-causal policy, bounded energy disturbances


- [Irregular Multivariate Time Series Forecasting: A Transformable Patching Graph Neural Networks Approach](https://icml.cc/virtual/2024/poster/33940) (Poster)
  - **Authors:** [Weijia Zhang](http://openreview.net/profile?id=~Weijia_Zhang4), [Chenlong Yin](http://openreview.net/profile?id=~Chenlong_Yin1), [Hao Liu](http://openreview.net/profile?id=~Hao_Liu17), [Xiaofang Zhou](http://openreview.net/profile?id=~Xiaofang_Zhou3), [Hui Xiong](http://openreview.net/profile?id=~Hui_Xiong1)
  - **Affiliations:** The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology (Guangzhou)
  - **TL;DR:** This study introduces Transformable Patching Graph Neural Networks (T-PATCHGNN) for forecasting Irregular Multivariate Time Series (IMTS), addressing challenges related to irregular sampling and inter-time series correlations. The proposed method demonstrates superior performance on a comprehensive IMTS forecasting benchmark across various scientific domains.
  - **Keywords:** Irregular Multivariate Time Series (IMTS) forecasting, time series analysis, Transformable Patching Graph Neural Networks (T-PATCHGNN), time-adaptive graph neural networks, Healthcare, biomechanics, climate science, astronomy, Irregular sampling intervals, missing data, intra-time series dependency modeling, inter-time series correlation modeling, New forecasting method (T-PATCHGNN), improved modeling of dynamic inter-time series correlation


- [Unbiased Multi-Label Learning from Crowdsourced Annotations](https://icml.cc/virtual/2024/poster/33627) (Poster)
  - **Authors:** [Mingxuan Xia](http://openreview.net/profile?id=~Mingxuan_Xia1), [Zenan Huang](http://openreview.net/profile?id=~Zenan_Huang1), [Runze Wu](http://openreview.net/profile?id=~Runze_Wu1), [Gengyu Lyu](http://openreview.net/profile?id=~Gengyu_Lyu3), [Junbo Zhao](http://openreview.net/profile?id=~Junbo_Zhao1), [Gang Chen](http://openreview.net/profile?id=~Gang_Chen6), [Haobo Wang](http://openreview.net/profile?id=~Haobo_Wang1)
  - **Affiliations:** School of Software Technology, Zhejiang University, Ningbo, China; State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China, State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China, Fuxi AI Lab, NetEase Inc., Hangzhou, China, Faculty of Information Technology, Beijing University of Technology, Beijing, China, State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China, State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China, School of Software Technology, Zhejiang University, Ningbo, China; State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China
  - **TL;DR:** This study introduces an unbiased risk estimator for Crowdsourced Multi-Label Learning (CMLL) that addresses the challenges of unreliable labels from annotators. The proposed method enhances performance by leveraging label correlations and provides a theoretical foundation for generalization error bounds.
  - **Keywords:** Crowdsourced Multi-Label Learning (CMLL), Multi-Label Learning (MLL), Unbiased risk estimator, Decoupled autoencoder framework, Image recognition, Document classification, Protein function prediction, Unreliable labels from annotators, Data annotation challenges, Generalization error bound, Improved performance through label correlation exploitation, Crowdsourced transition matrices, True label inferring, Crowdsourced Multi-Label Inference (CMLI)


- [A Geometric Explanation of the Likelihood OOD Detection Paradox](https://icml.cc/virtual/2024/poster/34582) (Poster)
  - **Authors:** [Hamidreza Kamkari](http://openreview.net/profile?id=~Hamidreza_Kamkari1), [Brendan Ross](http://openreview.net/profile?id=~Brendan_Leigh_Ross1), [Jesse Cresswell](http://openreview.net/profile?id=~Jesse_C._Cresswell1), [Anthony Caterini](http://openreview.net/profile?id=~Anthony_L._Caterini1), [Rahul G. Krishnan](http://openreview.net/profile?id=~Rahul_G_Krishnan1), [Gabriel Loaiza-Ganem](http://openreview.net/profile?id=~Gabriel_Loaiza-Ganem1)
  - **Affiliations:** Layer 6 AI; University of Toronto; Vector Institute, Layer 6 AI, Layer 6 AI, Layer 6 AI, University of Toronto; Vector Institute, Layer 6 AI
  - **TL;DR:** This study investigates the paradox of likelihood-based deep generative models assigning higher likelihoods to out-of-distribution data from simpler sources while failing to generate such data. The authors propose a new method for OOD detection that combines likelihoods with local intrinsic dimension estimates, achieving state-of-the-art results.
  - **Keywords:** Out-of-distribution (OOD) detection, deep generative models (DGMs), Likelihood-based models, normalizing flows (NFs), score-based diffusion models, Autonomous driving, medical diagnostics, finance, medical imaging, OOD detection reliability, paradox of high likelihoods for OOD data, low probability mass, Method for OOD detection using likelihoods and local intrinsic dimension (LID) estimates


- [Position: Evolving AI Collectives Enhance Human Diversity and Enable Self-Regulation](https://icml.cc/virtual/2024/poster/32843) (Poster)
  - **Authors:** [Shiyang Lai](http://openreview.net/profile?id=~Shiyang_Lai1), [Yujin Potter](http://openreview.net/profile?id=~Yujin_Potter1), [Junsol Kim](http://openreview.net/profile?id=~Junsol_Kim1), [Richard Zhuang](http://openreview.net/profile?id=~Richard_Zhuang2), [Dawn Song](http://openreview.net/profile?id=~Dawn_Song1), [James Evans](http://openreview.net/profile?id=~James_Evans1)
  - **Affiliations:** Department of Sociology & Knowledge Lab, University of Chicago, Department of Electrical Engineering and Computer Sciences, UC Berkeley, Department of Sociology & Knowledge Lab, University of Chicago, Department of Electrical Engineering and Computer Sciences, UC Berkeley, Department of Electrical Engineering and Computer Sciences, UC Berkeley, Department of Sociology & Knowledge Lab, University of Chicago; Santa Fe Institute
  - **TL;DR:** This study explores the formation of free-formed AI collectives that can enhance human diversity and self-regulation in online environments. It highlights the potential for these decentralized AI systems to reduce toxic behavior and suggests avenues for further research on AI moderation and ethical considerations.
  - **Keywords:** AI collectives, human diversity, self-regulation, Large language models (LLMs), decentralized AI subjectivities, Online environments, AI moderation, Toxic behavior online, anti-social behavior, Emergent AI collectives, cross-moderation opportunities


- [Accelerating Heterogeneous Federated Learning with Closed-form Classifiers](https://icml.cc/virtual/2024/poster/33597) (Poster)
  - **Authors:** [Eros Fanì](http://openreview.net/profile?id=~Eros_Fan%C3%AC1), [Raffaello Camoriano](http://openreview.net/profile?id=~Raffaello_Camoriano1), [Barbara Caputo](http://openreview.net/profile?id=~Barbara_Caputo1), [Marco Ciccone](http://openreview.net/profile?id=~Marco_Ciccone1)
  - **Affiliations:** Department of Computing and Control Engineering, Polytechnic University of Turin, Italy; Istituto Italiano di Tecnologia, Genoa, Italy; CINI Consortium, Rome, Italy, Department of Computing and Control Engineering, Polytechnic University of Turin, Italy; Istituto Italiano di Tecnologia, Genoa, Italy, Department of Computing and Control Engineering, Polytechnic University of Turin, Italy; Istituto Italiano di Tecnologia, Genoa, Italy; CINI Consortium, Rome, Italy, Department of Computing and Control Engineering, Polytechnic University of Turin, Italy
  - **TL;DR:** The study introduces Federated Recursive Ridge Regression (FED3R) to address challenges in Federated Learning caused by statistical heterogeneity, demonstrating significant improvements in convergence speed and resource efficiency. The method is particularly effective in cross-device scenarios and can be fine-tuned with existing FL algorithms.
  - **Keywords:** Federated Learning, Statistical Heterogeneity, Ridge Regression, Closed-form Classifiers, Cross-device Scenarios, Client Drift, Biased Local Solutions, Convergence Speed, Data Distribution Heterogeneity, Federated Recursive Ridge Regression (FED3R), FED3R with Fine-Tuning (FED3R+FT)


- [Learning Scale-Aware Spatio-temporal Implicit Representation for Event-based Motion Deblurring](https://icml.cc/virtual/2024/poster/32818) (Poster)
  - **Authors:** [Wei Yu](http://openreview.net/profile?id=~Wei_Yu14), [Jianing Li](http://openreview.net/profile?id=~Jianing_Li4), [Shengping Zhang](http://openreview.net/profile?id=~Shengping_Zhang1), [Xiangyang Ji](http://openreview.net/profile?id=~Xiangyang_Ji1)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China, School of Computer Science, Peking University, Beijing, China, School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China, Department of Automation, Tsinghua University, Beijing, China
  - **TL;DR:** This study introduces a Scale-Aware Spatio-temporal Network (SASNet) for effectively restoring blurred images using event streams at arbitrary scales, addressing the challenges posed by unknown spatial and temporal scales. The proposed method outperforms existing techniques, particularly in high-speed motion scenarios, and is validated on a newly created high-resolution dataset.
  - **Keywords:** Event-based motion deblurring, computational photography, Scale-Aware Spatio-temporal Network (SASNet), Spatial Implicit Representation Module (SIRM), Temporal Implicit Representation Module (TIRM), Image restoration, robotics, autonomous vehicles, Unknown scales of images and events, motion blur, Improved deblurring performance, high-resolution hybrid dataset (H2D), High-resolution Hybrid Deblur (H2D) dataset, GoPro dataset, Event Vision Sensors (EVS), CMOS Image Sensors (CIS)


- [Why Do You Grok? A Theoretical Analysis on Grokking Modular Addition](https://icml.cc/virtual/2024/poster/33679) (Poster)
  - **Authors:** [Mohamad Amin Mohamadi](http://openreview.net/profile?id=~Mohamad_Amin_Mohamadi1), [Zhiyuan Li](http://openreview.net/profile?id=~Zhiyuan_Li2), [Lei Wu](http://openreview.net/profile?id=~Lei_Wu1), [Danica J Sutherland](http://openreview.net/profile?id=~Danica_J._Sutherland1)
  - **Affiliations:** Toyota Technological Institute at Chicago; Computer Science Department, University of British Columbia, Toyota Technological Institute at Chicago, School of Mathematical Sciences, Peking University, Computer Science Department, University of British Columbia; Alberta Machine Intelligence Institute
  - **TL;DR:** This study provides a theoretical explanation for the "grokking" phenomenon in neural networks, demonstrating that models can generalize after initially overfitting, particularly in the context of modular addition. The findings suggest a transition from kernel-like behavior to more complex dynamics in gradient descent as a key factor in achieving generalization.
  - **Keywords:** grokking, generalization patterns, overfitting, gradient descent, one-hidden-layer quadratic networks, modular addition, algorithmic tasks, model generalization, population error, representation learning, theoretical explanation of grokking, empirical evidence of network behavior, kernel regime, rich regime, permutation-equivariant model


- [An Intrinsic Vector Heat Network](https://icml.cc/virtual/2024/poster/33023) (Poster)
  - **Authors:** [Alexander Gao](http://openreview.net/profile?id=~Alexander_Gao1), [Maurice Chu](http://openreview.net/profile?id=~Maurice_Chu1), [Mubbasir Kapadia](http://openreview.net/profile?id=~Mubbasir_Kapadia2), [Ming Lin](http://openreview.net/profile?id=~Ming_Lin2), [Hsueh-Ti Derek Liu](http://openreview.net/profile?id=~Hsueh-Ti_Derek_Liu1)
  - **Affiliations:** Roblox Research; Department of Computer Science, University of Maryland, College Park, USA, Roblox Core AI, Roblox Core AI, Department of Computer Science, University of Maryland, College Park, USA, Roblox Research; Department of Computer Science, University of Maryland, College Park, USA
  - **TL;DR:** This paper presents a novel neural network architecture for learning tangent vector fields on manifold surfaces in 3D, utilizing a trainable vector heat diffusion module to maintain essential invariances. The proposed method is validated on triangle meshes and demonstrates effectiveness in quadrilateral mesh generation.
  - **Keywords:** Tangent vector fields, Neural network architecture, Manifold surfaces, Vector heat diffusion module, Vector-valued neurons, Quadrilateral mesh generation, Scientific computation, Robotic navigation, Invariance to rigid motion, Isometric deformation, Choice of local tangent bases, Novel architecture for learning tangent vector fields, Empirical validation of invariant properties, Triangle meshes, Riemannian manifolds, Scalar-valued architectures, Geometric deep learning


- [Simplicity Bias via Global Convergence of Sharpness Minimization](https://icml.cc/virtual/2024/poster/33913) (Poster)
  - **Authors:** [Khashayar Gatmiry](http://openreview.net/profile?id=~Khashayar_Gatmiry1), [Zhiyuan Li](http://openreview.net/profile?id=~Zhiyuan_Li2), [Sashank J. Reddi](http://openreview.net/profile?id=~Sashank_J._Reddi1), [Stefanie Jegelka](http://openreview.net/profile?id=~Stefanie_Jegelka3)
  - **Affiliations:** Massachusetts Institute of Technology, Toyota Technological Institute at Chicago, Google Research, Massachusetts Institute of Technology
  - **TL;DR:** This study investigates the simplicity bias in neural networks trained with label noise SGD, demonstrating that it converges to a model replicating a single linear feature across all neurons, resulting in a low-rank feature matrix. The findings highlight a connection between sharpness minimization and the geometry of the loss landscape, contributing to the understanding of generalization in neural networks.
  - **Keywords:** simplicity bias, generalization ability of neural networks, stochastic gradient descent (SGD), label noise SGD, sharpness minimization, low complexity, high-dimensional training data, loss landscape, convergence to low-rank feature matrix, local geodesic convexity of Hessian trace, two-layer neural networks, rank one feature matrix


- [Mean Field Langevin Actor-Critic: Faster Convergence and Global Optimality beyond Lazy Learning](https://icml.cc/virtual/2024/poster/34537) (Poster)
  - **Authors:** [Kakei Yamamoto](http://openreview.net/profile?id=~Kakei_Yamamoto1), [Kazusato Oko](http://openreview.net/profile?id=~Kazusato_Oko1), [Zhuoran Yang](http://openreview.net/profile?id=~Zhuoran_Yang1), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1)
  - **Affiliations:** MIT, Cambridge, MA, The University of Tokyo, Tokyo, Japan; Center for Advanced Intelligence Project, RIKEN, Yale University, New Haven, CT, The University of Tokyo, Tokyo, Japan; Center for Advanced Intelligence Project, RIKEN
  - **TL;DR:** This study introduces the mean-field Langevin TD learning and policy gradient methods to enhance feature learning in deep reinforcement learning, demonstrating linear convergence to the globally optimal policy and accurate value function identification. The findings contribute to a deeper understanding of neural reinforcement learning beyond traditional lazy training approaches.
  - **Keywords:** deep reinforcement learning, optimal policy determination, mean-field Langevin TD learning (MFLTD), mean-field Langevin policy gradient (MFLPG), policy gradient, temporal-difference (TD) learning, Wasserstein gradient flows, nonconvexity of expected total reward, bias of semi-gradient optimization, challenges in neural network optimization, linear convergence towards globally optimal policy, accurate identification of true value function, actor-critic method, Kullback-Leibler divergence


- [Revisiting Context Aggregation for Image Matting](https://icml.cc/virtual/2024/poster/32897) (Poster)
  - **Authors:** [Qinglin Liu](http://openreview.net/profile?id=~Qinglin_Liu1), [Xiaoqian Lv](http://openreview.net/profile?id=~Xiaoqian_Lv1), [Quanling Meng](http://openreview.net/profile?id=~Quanling_Meng1), [Zonglin Li](http://openreview.net/profile?id=~Zonglin_Li1), [Xiangyuan Lan](http://openreview.net/profile?id=~Xiangyuan_Lan4), [Shuo Yang](http://openreview.net/profile?id=~Shuo_Yang5), [Shengping Zhang](http://openreview.net/profile?id=~Shengping_Zhang1), [Liqiang Nie](http://openreview.net/profile?id=~Liqiang_Nie2)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China, School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China, School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China, School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China; Peng Cheng Laboratory, Shenzhen, China, Department of Computer Science, The University of Hong Kong, Hong Kong, China, Department of Computer Science, The University of Hong Kong, Hong Kong, China, School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China; Peng Cheng Laboratory, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China
  - **TL;DR:** This study revisits context aggregation mechanisms in image matting, revealing that a basic encoder-decoder network can achieve superior performance without complex context aggregation modules. The proposed AEMatter network, utilizing a Hybrid-Transformer backbone and large image training strategy, significantly outperforms existing state-of-the-art methods.
  - **Keywords:** Image Matting, Context Aggregation, Encoder-Decoder Network, Hybrid-Transformer, Appearance-Enhanced Axis-Wise Learning (AEAL), Image Editing, Film Post-Production, Context Scale Shift, Matting Performance Degradation, AEMatter Network, Large Image Training Strategy, Five Popular Matting Datasets


- [Robustness of Nonlinear Representation Learning](https://icml.cc/virtual/2024/poster/34481) (Oral)
  - **Authors:** [Simon Buchholz](http://openreview.net/profile?id=~Simon_Buchholz1), [Bernhard Schölkopf](http://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1)
  - **Affiliations:** Max Planck Institute for Intelligent Systems, Tübingen, Germany; Tübingen AI Center, Tübingen, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany; Tübingen AI Center, Tübingen, Germany; ELLIS Institute, Tübingen, Germany
  - **TL;DR:** This study investigates the robustness of nonlinear representation learning in slightly misspecified settings, focusing on approximate identifiability in Independent Component Analysis (ICA) with nearly isometric mixing functions. The findings suggest that the mixing matrix and independent components can be approximately recovered, which has significant implications for unsupervised representation learning in real-world data.
  - **Keywords:** Unsupervised representation learning, Robustness, Causal Representation Learning, Independent Component Analysis (ICA), Nonlinear representation learning, Misspecification, Identifiability, Latent variable identification, Approximate identifiability results, Recovery of mixing matrix, Local isometry, Mixing functions


- [Mind the Boundary: Coreset Selection via Reconstructing the Decision Boundary](https://icml.cc/virtual/2024/poster/33392) (Poster)
  - **Authors:** [Shuo Yang](http://openreview.net/profile?id=~Shuo_Yang5), [Zhe Cao](http://openreview.net/profile?id=~Zhe_Cao3), [Sheng Guo](http://openreview.net/profile?id=~Sheng_Guo4), [Ruiheng Zhang](http://openreview.net/profile?id=~Ruiheng_Zhang1), [Ping Luo](http://openreview.net/profile?id=~Ping_Luo2), [Shengping Zhang](http://openreview.net/profile?id=~Shengping_Zhang1), [Liqiang Nie](http://openreview.net/profile?id=~Liqiang_Nie2)
  - **Affiliations:** The University of Hong Kong, Hong Kong, Beijing Institute of Technology, China, MYBank, Ant Group, China, Beijing Institute of Technology, China, The University of Hong Kong, Hong Kong; Shanghai AI Lab, China, Harbin Institute of Technology, China, Harbin Institute of Technology, China
  - **TL;DR:** This study introduces a novel coreset selection method that reconstructs the decision boundary of deep neural networks, achieving a 50% data pruning rate on the ImageNet-1K dataset with minimal accuracy loss. The findings highlight the method's effectiveness and its potential for cross-architecture transferability in model training.
  - **Keywords:** Coreset selection, Generalization capability, Deep neural networks, Geometry-based methods, Coreset construction, Image recognition, Data sparsity, Computational cost, Generalization error, 50% data pruning rate, Cross-architecture transferability, ImageNet-1K


- [Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling](https://icml.cc/virtual/2024/poster/32711) (Poster)
  - **Authors:** [Mingze Wang](http://openreview.net/profile?id=~Mingze_Wang2), [Zeping Min](http://openreview.net/profile?id=~Zeping_Min1), [Lei Wu](http://openreview.net/profile?id=~Lei_Wu1)
  - **Affiliations:** School of Mathematical Sciences, Peking University, Beijing, China; Center for Machine Learning Research, Peking University, Beijing, China, School of Mathematical Sciences, Peking University, Beijing, China, School of Mathematical Sciences, Peking University, Beijing, China; Center for Machine Learning Research, Peking University, Beijing, China
  - **TL;DR:** This study introduces a novel algorithm, Progressive Rescaling Gradient Descent (PRGD), which maximizes the margin of linearly separable data at an exponential rate, significantly outperforming existing gradient-based algorithms that achieve only polynomial rates. The findings suggest that PRGD can enhance generalization performance in both linearly separable and non-separable datasets.
  - **Keywords:** margin maximization, gradient-based algorithms, linearly separable data, Progressive Rescaling Gradient Descent (PRGD), gradient descent (GD), normalized gradient descent (NGD), deep learning, machine learning, margin maximization bias, inefficiency in GD/NGD, exponential rate of margin maximization, theoretical findings, generalization performance enhancement, ℓ2-margin, centripetal velocity


- [Conformal Prediction with Learned Features](https://icml.cc/virtual/2024/poster/33774) (Poster)
  - **Authors:** [Shayan Kiyani](http://openreview.net/profile?id=~Shayan_Kiyani2), [George J. Pappas](http://openreview.net/profile?id=~George_J._Pappas1), [Hamed Hassani](http://openreview.net/profile?id=~Hamed_Hassani2)
  - **Affiliations:** Electrical and Systems Engineering Department, University of Pennsylvania, USA, Electrical and Systems Engineering Department, University of Pennsylvania, USA, Electrical and Systems Engineering Department, University of Pennsylvania, USA
  - **TL;DR:** This paper introduces Partition Learning Conformal Prediction (PLCP), a framework designed to enhance the conditional validity of prediction sets by learning uncertainty-guided features from calibration data. Experimental results demonstrate that PLCP outperforms existing methods in terms of coverage and length across various datasets.
  - **Keywords:** conformal prediction, conditional guarantees, Partition Learning Conformal Prediction (PLCP), alternating gradient descent, healthcare, prediction sets, nontrivial prediction sets, full conditional coverage, marginal coverage, improved conditional validity, theoretical analysis, superior performance in coverage and length, real-world datasets, synthetic datasets


- [Symmetry Induces Structure and Constraint of Learning](https://icml.cc/virtual/2024/poster/34892) (Poster)
  - **Authors:** [Liu Ziyin](http://openreview.net/profile?id=~Liu_Ziyin1)
  - **Affiliations:** MIT; NTT Research
  - **TL;DR:** This study explores the impact of loss function symmetries on the learning behavior of machine learning models, demonstrating that mirror reflection symmetries impose constraints on model parameters. The findings reveal that these constraints can lead to phenomena such as sparsity and low rankness, providing insights into the design of algorithms that enforce hard constraints in a differentiable manner.
  - **Keywords:** symmetry in neural networks, learning behavior, loss function, stochastic gradient descent (SGD), gradient descent (GD), constraints on model parameters, loss of plasticity, collapse phenomena, mirror reflection symmetry, constrained symmetric solutions, sparsity, low rankness, homogeneous ensembling


- [Position: Standardization of Behavioral Use Clauses is Necessary for the Adoption of Responsible Licensing of AI](https://icml.cc/virtual/2024/poster/34888) (Poster)
  - **Authors:** [Daniel McDuff](http://openreview.net/profile?id=~Daniel_McDuff1), [Tim Korjakow](http://openreview.net/profile?id=~Tim_Korjakow2), [Scott Cambo](http://openreview.net/profile?id=~Scott_Cambo1), [Jesse Benjamin](http://openreview.net/profile?id=~Jesse_Josua_Benjamin1), [Jenny Lee](http://openreview.net/profile?email=leewall%40gmail.com), [Yacine Jernite](http://openreview.net/profile?id=~Yacine_Jernite1), [Carlos Muñoz Ferrandis](http://openreview.net/profile?id=~Carlos_Mu%C3%B1oz_Ferrandis1), [Aaron Gokaslan](http://openreview.net/profile?id=~Aaron_Gokaslan1), [Alek Tarkowski](http://openreview.net/profile?email=alek%40openfuture.eu), [Joseph Lindley](http://openreview.net/profile?email=joseph.lindley%40gmail.com), [A. Feder Cooper](http://openreview.net/profile?id=~A._Feder_Cooper1), [Danish Contractor](http://openreview.net/profile?id=~Danish_Contractor2)
  - **Affiliations:** Responsible AI Licenses; University of Washington, USA, Responsible AI Licenses; Technical University of Berlin, None, Responsible AI Licenses; Lancaster University, Responsible AI Licenses; None, Hugging Face, Alinia AI, Cornell University, Open Future Foundation, Responsible AI Licenses; Lancaster University, The Center for Generative AI, Law, and Policy Research, None
  - **TL;DR:** The paper advocates for the standardization of behavioral use clauses in responsible AI licenses to mitigate risks associated with AI technology while allowing for necessary customization in specific contexts. It highlights the significant adoption of these licenses and the need for clarity to avoid user confusion and dilution of impact.
  - **Keywords:** Responsible AI, Licensing, Behavioral Use Clauses, Mixed-methods methodology, Clustering of license clauses, Qualitative interviews, Quantitative analysis, AI asset management, Software and model repositories, Negligent or malicious uses of AI, Accountability challenges in decentralized systems, Standardization of responsible AI licenses, Customization of behavioral restrictions, Responsible AI Licenses, Behavioral-use clauses


- [Watermark Stealing in Large Language Models](https://icml.cc/virtual/2024/poster/33848) (Poster)
  - **Authors:** [Nikola Jovanović](http://openreview.net/profile?id=~Nikola_Jovanovi%C4%871), [Robin Staab](http://openreview.net/profile?id=~Robin_Staab1), [Martin Vechev](http://openreview.net/profile?id=~Martin_Vechev1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Department of Computer Science, ETH Zurich, Department of Computer Science, ETH Zurich
  - **TL;DR:** This study identifies watermark stealing as a significant vulnerability in current LLM watermarking schemes, demonstrating that attackers can effectively spoof and scrub watermarks for under $50 with an average success rate exceeding 80%. The findings challenge the perceived robustness of these schemes and highlight the urgent need for more secure watermarking methods.
  - **Keywords:** LLM watermarking, AI-generated content detection, Watermark stealing, spooﬁng attacks, scrubbing attacks, Vulnerability of watermarking schemes, adversarial attacks, Automated watermark stealing algorithm, comprehensive study of spooﬁng and scrubbing, Large Language Models (LLMs), watermarking


- [Adaptively Perturbed Mirror Descent for Learning in Games](https://icml.cc/virtual/2024/poster/34804) (Poster)
  - **Authors:** [Kenshi Abe](http://openreview.net/profile?id=~Kenshi_Abe1), [Kaito Ariu](http://openreview.net/profile?id=~Kaito_Ariu1), [Mitsuki Sakamoto](http://openreview.net/profile?id=~Mitsuki_Sakamoto1), [Atsushi Iwasaki](http://openreview.net/profile?id=~Atsushi_Iwasaki2)
  - **Affiliations:** CyberAgent, Tokyo, Japan; University of Electro-Communications, Tokyo, Japan, CyberAgent, Tokyo, Japan, CyberAgent, Tokyo, Japan, University of Electro-Communications, Tokyo, Japan
  - **TL;DR:** This study introduces Adaptively Perturbed Mirror Descent (APMD), a novel technique for achieving last-iterate convergence to Nash equilibria in monotone games, even in the presence of noise. The proposed method demonstrates significantly accelerated convergence rates by adaptively adjusting the perturbation magnitude based on the strategy profile's proximity to an equilibrium.
  - **Keywords:** Learning in games, Nash equilibrium, Payoff perturbation, Mirror Descent (MD), Adaptively Perturbed MD (APMD), Optimistic learning algorithms, Monotone games, Cournot competition, Zero-sum games, Noise in feedback, Convergence challenges, Strategy profile dynamics, Last-iterate convergence, Accelerated convergence, Perturbation adjustment


- [AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers](https://icml.cc/virtual/2024/poster/33480) (Poster)
  - **Authors:** [Reduan Achtibat](http://openreview.net/profile?id=~Reduan_Achtibat1), [Sayed Mohammad Vakilzadeh Hatefi](http://openreview.net/profile?id=~Sayed_Mohammad_Vakilzadeh_Hatefi1), [Maximilian Dreyer](http://openreview.net/profile?id=~Maximilian_Dreyer1), [Aakriti Jain](http://openreview.net/profile?id=~Aakriti_Jain1), [Thomas Wiegand](http://openreview.net/profile?id=~Thomas_Wiegand1), [Sebastian Lapuschkin](http://openreview.net/profile?id=~Sebastian_Lapuschkin1), [Wojciech Samek](http://openreview.net/profile?id=~Wojciech_Samek1)
  - **Affiliations:** Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany; Technische Universität Berlin, 10587 Berlin, Germany; BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany, Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany; Technische Universität Berlin, 10587 Berlin, Germany; BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany, Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany; Technische Universität Berlin, 10587 Berlin, Germany; BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany, Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany; Technische Universität Berlin, 10587 Berlin, Germany; BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany, Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany; Technische Universität Berlin, 10587 Berlin, Germany; BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany, Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany; Technische Universität Berlin, 10587 Berlin, Germany; BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany, Fraunhofer Heinrich-Hertz-Institute, 10587 Berlin, Germany; Technische Universität Berlin, 10587 Berlin, Germany; BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany
  - **TL;DR:** This study introduces AttnLRP, an extension of Layer-wise Relevance Propagation that effectively attributes both input and latent representations in transformer models, addressing challenges of biased predictions and hallucinations. The proposed method demonstrates superior faithfulness and computational efficiency compared to existing techniques, facilitating a deeper understanding of model behavior.
  - **Keywords:** Explainability, Large Language Models, Transformer Models, Layer-wise Relevance Propagation, Attention Mechanism, Text Generation, Image Generation, Biased Predictions, Hallucinations, Model Behavior Understanding, Faithful Attributions, Concept-Based Explanations, LLaMa 2, Mixtral 8x7b, Flan-T5, Vision Transformer Architectures, Attention Layers, Feed-Forward Network (FFN)


- [A Dynamical Model of Neural Scaling Laws](https://icml.cc/virtual/2024/poster/33116) (Poster)
  - **Authors:** [Blake Bordelon](http://openreview.net/profile?id=~Blake_Bordelon1), [Alexander Atanasov](http://openreview.net/profile?id=~Alexander_Atanasov1), [Cengiz Pehlevan](http://openreview.net/profile?id=~Cengiz_Pehlevan2)
  - **Affiliations:** SEAS, Harvard University; Kempner Institute, Harvard University, Department of Physics, Harvard University, SEAS, Harvard University; Kempner Institute, Harvard University
  - **TL;DR:** This study introduces a solvable model to analyze neural scaling laws, revealing that performance improves predictably with training time, dataset size, and model size. The findings highlight an asymmetric compute-optimal scaling rule and the dynamics of training convergence, providing insights into the optimal trade-offs in deep learning architectures.
  - **Keywords:** neural scaling laws, deep learning, gradient descent, random feature model, language models, vision models, performance scaling, training time, model size, compute-optimal scaling law, asymmetric scaling rule


- [Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning](https://icml.cc/virtual/2024/poster/34491) (Poster)
  - **Authors:** [Idan Achituve](http://openreview.net/profile?id=~Idan_Achituve1), [Idit Diamant](http://openreview.net/profile?id=~Idit_Diamant1), [Arnon Netzer](http://openreview.net/profile?id=~Arnon_Netzer1), [Gal Chechik](http://openreview.net/profile?id=~Gal_Chechik1), [Ethan Fetaya](http://openreview.net/profile?id=~Ethan_Fetaya1)
  - **Affiliations:** Faculty of Engineering, Bar-Ilan University, Israel; Sony Semiconductor Israel, Sony Semiconductor Israel, Sony Semiconductor Israel, Department of Computer Science, Bar-Ilan University, Israel, Faculty of Engineering, Bar-Ilan University, Israel
  - **TL;DR:** This study introduces a novel gradient aggregation method for multi-task learning using Bayesian inference to account for uncertainty in gradient dimensions. The approach improves performance by effectively managing the sensitivity of gradients across multiple tasks.
  - **Keywords:** Multi-task learning (MTL), Bayesian inference, Gradient aggregation, probability distribution over parameters, Autonomous vehicles, real-time inference tasks, Sensitivity in gradient dimensions, performance degradation in MTL, Novel gradient aggregation procedure, uncertainty estimates for gradients, State-of-the-art performance


- [Optimal Coresets for Low-Dimensional Geometric Median](https://icml.cc/virtual/2024/poster/34837) (Poster)
  - **Authors:** [Peyman Afshani](http://openreview.net/profile?id=~Peyman_Afshani1), [Chris Schwiegelshohn](http://openreview.net/profile?id=~Chris_Schwiegelshohn1)
  - **Affiliations:** Department of Computer Science, Aarhus University, Denmark, Department of Computer Science, Aarhus University, Denmark
  - **TL;DR:** This study investigates coresets for approximating the cost of median queries in low-dimensional spaces, providing matching upper and lower bounds on the number of points in the coreset. The findings highlight the efficiency of coresets in big data analysis and their implications for geometric median problems.
  - **Keywords:** coresets, geometric median, big data analysis, approximating cost with respect to median queries, high-dimensional data, coreset construction, bounds on coreset size, Euclidean norm, ε-coreset


- [FrameQuant: Flexible Low-Bit Quantization for Transformers](https://icml.cc/virtual/2024/poster/32713) (Poster)
  - **Authors:** [Harshavardhan Adepu](http://openreview.net/profile?id=~Harshavardhan_Adepu1), [Zhanpeng Zeng](http://openreview.net/profile?id=~Zhanpeng_Zeng1), [Li Zhang](http://openreview.net/profile?id=~Li_Zhang28), [Vikas Singh](http://openreview.net/profile?id=~Vikas_Singh1)
  - **Affiliations:** University of Wisconsin-Madison, University of Wisconsin-Madison, Google Research, University of Wisconsin-Madison; Google Research
  - **TL;DR:** This paper introduces FrameQuant, a novel Post-Training Quantization scheme that enables quantization of Transformer models to two bits with minimal accuracy loss, addressing the challenges of high resource demands in deploying large models. The method leverages Fusion Frames to enhance robustness against quantization errors, promising significant efficiency gains.
  - **Keywords:** Post-Training Quantization, Transformer Models, Efficiency in Model Deployment, Fusion Frames, Low-Bit Quantization, Natural Language Processing, Vision Transformers, Image Classification, High compute and memory/storage footprint, Model deployment challenges, Resource-constrained devices, Two-bit quantization scheme, Robustness to quantization error, Transformers, Large Language Models (LLMs), Vision Transformers (VITs)


- [Learning to Play Atari in a World of Tokens](https://icml.cc/virtual/2024/poster/32760) (Poster)
  - **Authors:** [Pranav Agarwal](http://openreview.net/profile?id=~Pranav_Agarwal1), [Sheldon Andrews](http://openreview.net/profile?id=~Sheldon_Andrews1), [Samira Ebrahimi Kahou](http://openreview.net/profile?id=~Samira_Ebrahimi_Kahou1)
  - **Affiliations:** École de Technologie Supérieure, Mila, None, École de Technologie Supérieure, Roblox, None, University of Calgary, Mila, Canada CIFAR AI Chair
  - **TL;DR:** This study introduces DART, a sample-efficient method for model-based reinforcement learning that utilizes discrete representations to improve world modeling and learning behavior. DART outperforms previous state-of-the-art methods on the Atari 100k benchmark, achieving a median human-normalized score of 0.790 and surpassing human performance in 9 out of 26 games.
  - **Keywords:** Model-based reinforcement learning, Transformers, Discrete abstract representations, Transformer-decoder, Transformer-encoder, Atari games, Reinforcement learning environments, Sample inefficiency, Partial observability, Compounding error problems, DART (Discrete Abstract Representations for Transformer-based Learning), Improved sample efficiency, Atari 100k


- [Probabilistic Generating Circuits - Demystified](https://icml.cc/virtual/2024/poster/34563) (Oral)
  - **Authors:** [Sanyam Agarwal](http://openreview.net/profile?id=~Sanyam_Agarwal2), [Markus Bläser](http://openreview.net/profile?id=~Markus_Bl%C3%A4ser1)
  - **Affiliations:** Saarland University, Saarland Informatics Campus, Saarbrücken, Germany, Saarland University, Saarland Informatics Campus, Saarbrücken, Germany
  - **TL;DR:** This paper investigates probabilistic generating circuits (PGCs) and demonstrates that their power stems from allowing negative weights rather than their representation method. It also shows that PGCs for categorical variables with larger image sizes do not support tractable marginalization unless NP = P, while PCs with negative weights can effectively model these variables.
  - **Keywords:** probabilistic modeling, tractable probabilistic inference, probabilistic generating circuits (PGCs), probabilistic circuits (PCs), determinantal point processes (DPPs), probability generating polynomial, intractable probabilistic inference, marginalization challenges, transformation of PGCs to PCs with negative weights, tractable marginalization for categorical variables, negative weights, set-multilinear polynomials


- [The Non-linear $F$-Design and Applications to Interactive Learning](https://icml.cc/virtual/2024/poster/34243) (Poster)
  - **Authors:** [Alekh Agarwal](http://openreview.net/profile?id=~Alekh_Agarwal2), [Jian Qian](http://openreview.net/profile?id=~Jian_Qian2), [Alexander Rakhlin](http://openreview.net/profile?id=~Alexander_Rakhlin1), [Tong Zhang](http://openreview.net/profile?id=~Tong_Zhang2)
  - **Affiliations:** Google, MIT, MIT, UIUC
  - **TL;DR:** This paper introduces the F-design, a generalization of G-optimal design for non-linear function classes, and demonstrates its effectiveness in various interactive machine learning tasks. The authors provide algorithms for constructing designs with a bounded F-condition number, yielding state-of-the-art results in data collection and exploration.
  - **Keywords:** Non-linear experimental design, Interactive machine learning, F-design, G-design, Algorithms for design construction, Data collection, Confidence band construction, Contextual bandits, Model-free reinforcement learning, Active learning, Data collection efficiency, Sample-constrained scenarios, F-condition number, State-of-the-art results in interactive learning, Eluder dimension


- [BeigeMaps: Behavioral Eigenmaps for Reinforcement Learning from Images](https://icml.cc/virtual/2024/poster/33147) (Poster)
  - **Authors:** [Sandesh Adhikary](http://openreview.net/profile?id=~Sandesh_Adhikary1), [Anqi Li](http://openreview.net/profile?id=~Anqi_Li1), [Byron Boots](http://openreview.net/profile?id=~Byron_Boots1)
  - **Affiliations:** Computer Science and Engineering, University of Washington, Seattle, WA (USA), Computer Science and Engineering, University of Washington, Seattle, WA (USA); NVIDIA, Computer Science and Engineering, University of Washington, Seattle, WA (USA)
  - **TL;DR:** The study proposes Behavioral Eigenmaps (BeigeMaps) as a new representation learning method for reinforcement learning agents that addresses the challenges of high-dimensional image observations. The findings demonstrate that BeigeMaps can improve policy performance in prior behavioral distance-based RL algorithms.
  - **Keywords:** Reinforcement Learning, Representation Learning, Behavioral Distances, Bisimulation Metric, Isometric Mapping, Image Observations, Control Policies, High-dimensional Data, Poor Sample Efficiency, Weak Generalization Capability, Behavioral Eigenmaps (BeigeMaps), Improved Policy Performance


- [Gaussian Processes on Cellular Complexes](https://icml.cc/virtual/2024/poster/33677) (Poster)
  - **Authors:** [Mathieu Alain](http://openreview.net/profile?id=~Mathieu_Alain1), [So Takao](http://openreview.net/profile?id=~So_Takao1), [Brooks Paige](http://openreview.net/profile?id=~Brooks_Paige1), [Marc Deisenroth](http://openreview.net/profile?id=~Marc_Peter_Deisenroth1)
  - **Affiliations:** Centre for Artificial Intelligence, University College London, London, UK, Centre for Artificial Intelligence, University College London, London, UK; Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena, CA, USA, Centre for Artificial Intelligence, University College London, London, UK, Centre for Artificial Intelligence, University College London, London, UK
  - **TL;DR:** This paper proposes Gaussian processes on cellular complexes to model polyadic relations and capture interactions between higher-order cells, addressing the limitations of traditional graphs in uncertainty quantification. The authors introduce a novel cellular Matérn kernel that generalizes existing methods and allows for directed predictions on various cell types.
  - **Keywords:** Gaussian processes, cellular complexes, polyadic relations, Gaussian processes, Matérn kernel, Machine learning, signal processing, Uncertainty quantification, limitations of graphs, Novel kernels for cellular complexes, Graph neural networks (GNNs), graph kernel machines, Topological inductive biases, higher-order cells


- [How to Escape Sharp Minima with Random Perturbations](https://icml.cc/virtual/2024/poster/32855) (Poster)
  - **Authors:** [Kwangjun Ahn](http://openreview.net/profile?id=~Kwangjun_Ahn2), [Ali Jadbabaie](http://openreview.net/profile?id=~Ali_Jadbabaie1), [Suvrit Sra](http://openreview.net/profile?id=~Suvrit_Sra1)
  - **Affiliations:** MIT; Microsoft Research; TU Munich, MIT, MIT; TU Munich
  - **TL;DR:** This study formalizes the concept of flat minima in optimization and explores efficient algorithms for finding them, highlighting the significance of flat minima in improving prediction performance in machine learning applications. The findings support the effectiveness of methods like sharpness-aware minimization in achieving flatter minima.
  - **Keywords:** flat minima, optimization algorithms, machine learning, gradient-based algorithm, sharpness-aware minimization (SAM), image classification, language processing, local/global minima, prediction performance, approximate flat minima, efficient algorithms, Hessian, empirical risk


- [Not all distributional shifts are equal: Fine-grained robust conformal inference](https://icml.cc/virtual/2024/poster/35127) (Poster)
  - **Authors:** [Jiahao Ai](http://openreview.net/profile?id=~Jiahao_Ai1), [Zhimei Ren](http://openreview.net/profile?id=~Zhimei_Ren1)
  - **Affiliations:** School of Mathematical Sciences, Peking University, Beijing, China, Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, USA
  - **TL;DR:** This study presents a fine-grained framework for uncertainty quantification in predictive models that addresses distributional shifts by distinguishing between shifts in covariate distributions and conditional relationships. The proposed method demonstrates improved robustness and efficiency in generating valid prediction intervals compared to existing approaches.
  - **Keywords:** uncertainty quantification, distributional shifts, predictive models, conformal inference, distributionally robust learning, sensitivity analysis, individual treatment effects, performance drop under distributional shifts, non-exchangeability of training and test data, valid prediction intervals, improved efficiency in prediction sets, f-divergence, covariate distribution shift


- [Learning Mixtures of Gaussian Processes through Random Projection](https://icml.cc/virtual/2024/poster/34399) (Poster)
  - **Authors:** [Emmanuel Akeweje](http://openreview.net/profile?id=~Emmanuel_Akeweje1), [Mimi Zhang](http://openreview.net/profile?id=~Mimi_Zhang2)
  - **Affiliations:** School of Computer Science and Statistics, Trinity College Dublin, Ireland; I-Form Advanced Manufacturing Research Centre, Science Foundation Ireland, Ireland, School of Computer Science and Statistics, Trinity College Dublin, Ireland; I-Form Advanced Manufacturing Research Centre, Science Foundation Ireland, Ireland
  - **TL;DR:** This study presents an ensemble clustering framework to identify latent cluster labels in functional data generated from Gaussian process mixtures, significantly reducing computational complexity compared to existing methods. The proposed approach allows for independent learning of each Gaussian process component after uncovering hidden cluster labels, with theoretical guarantees on identifiability and learnability.
  - **Keywords:** Gaussian processes, ensemble clustering, functional data, Gaussian mixture model (GMM), univariate GMM, parameter estimation, Cluster analysis, statistical analysis of functional data, Computational complexity, identifiability and learnability of Gaussian process mixtures, Consensus clustering, theoretical guarantees, Synthetic datasets, real datasets


- [LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions](https://icml.cc/virtual/2024/poster/33810) (Poster)
  - **Authors:** [Victor Agostinelli III](http://openreview.net/profile?id=~Victor_Agostinelli_III1), [Sanghyun Hong](http://openreview.net/profile?id=~Sanghyun_Hong1), [Lizhong Chen](http://openreview.net/profile?id=~Lizhong_Chen2)
  - **Affiliations:** Oregon State University, OR USA, Oregon State University, OR USA, Oregon State University, OR USA
  - **TL;DR:** The study introduces LeaPformer, a novel approach to enhance linear transformers for autoregressive and simultaneous tasks by utilizing learned proportions instead of static positional representations. It demonstrates superior performance in quality-throughput trade-offs on various benchmarks, including language modeling and speech-to-text translation.
  - **Keywords:** Linear Transformers, Autoregressive Tasks, Simultaneous Tasks, Position-based re-weighting functions, Dynamic proportions, Natural Language Processing, Speech-to-Text Translation, Quadratic complexity of attention mechanisms, Sequence length dependency, LeaPformer, Quality-throughput trade-off, Long-Range Arena benchmark, Wikitext-103, Transformers, Efficient Attention Mechanisms


- [Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise](https://icml.cc/virtual/2024/poster/33366) (Poster)
  - **Authors:** [Kwangjun Ahn](http://openreview.net/profile?id=~Kwangjun_Ahn2), [Zhiyu Zhang](http://openreview.net/profile?id=~Zhiyu_Zhang1), [Yunbum Kook](http://openreview.net/profile?id=~Yunbum_Kook1), [Yan Dai](http://openreview.net/profile?id=~Yan_Dai1)
  - **Affiliations:** MIT; Microsoft Research, Harvard University, Georgia Tech, Tsinghua University
  - **TL;DR:** This study provides a theoretical perspective on the Adam optimizer by framing it within the online learning of updates, revealing its correspondence to the Follow-the-Regularized-Leader (FTRL) framework. The findings emphasize the significance of Adam's algorithmic components, particularly momentum and discounting factors, in enhancing optimization performance.
  - **Keywords:** Adam optimizer, online learning, optimization algorithms, Follow-the-Regularized-Leader (FTRL), stochastic gradient descent (SGD), Deep learning, Transformer-based neural networks, Theoretical understanding of algorithmic components, convergence rates, Insights into the benefits of momentum and discounting factors in optimization


- [Physics of Language Models: Part 3.1, Knowledge Storage and Extraction](https://icml.cc/virtual/2024/poster/34955) (Spotlight Poster)
  - **Authors:** [Zeyuan Allen-Zhu](http://openreview.net/profile?id=~Zeyuan_Allen-Zhu1), [Yuanzhi Li](http://openreview.net/profile?id=~Yuanzhi_Li1)
  - **Affiliations:** Meta / FAIR Labs, USA, MBZUAI, UAE
  - **TL;DR:** This paper investigates how large language models (LLMs) memorize and extract knowledge, revealing that effective knowledge extraction requires data augmentation during pretraining. The authors recommend rewriting pretraining data and incorporating more instruction-finetuning data to enhance knowledge extraction capabilities.
  - **Keywords:** Knowledge extraction, Language models, Memorization, Transformer-based models, Linear probing, Knowledge extraction challenges, Memorization vs. extraction, Recommendations for LLM pretraining, Knowledge augmentation, Controlled biography dataset, Large language models (LLMs), Factual knowledge


- [Revisiting Inexact Fixed-Point Iterations for Min-Max Problems: Stochasticity and Structured Nonconvexity](https://icml.cc/virtual/2024/poster/33210) (Poster)
  - **Authors:** [Ahmet Alacaoglu](http://openreview.net/profile?id=~Ahmet_Alacaoglu2), [Donghwan Kim](http://openreview.net/profile?id=~Donghwan_Kim2), [Stephen Wright](http://openreview.net/profile?id=~Stephen_Wright1)
  - **Affiliations:** University of Wisconsin–Madison, USA; University of British Columbia, Canada, KAIST, Republic of Korea, University of Wisconsin–Madison, USA
  - **TL;DR:** This study addresses constrained, L-smooth, potentially stochastic nonconvex-nonconcave min-max problems, providing optimal complexity guarantees under cohypomonotonicity and weak MVI conditions. The authors present improved convergence analyses and refined methods for inexact iterations, relevant to applications in reinforcement learning and adversarial machine learning.
  - **Keywords:** min-max problems, nonconvex-nonconcave optimization, stochastic optimization, inexact fixed-point iterations, Halpern iteration, Krasnosel’ski˘ı-Mann iteration, multilevel Monte Carlo estimator, reinforcement learning, generative adversarial networks (GANs), adversarial machine learning, nonconvexity, cohypomonotonicity, weak Minty Variational Inequality (MVI), optimal complexity guarantees, convergence analysis improvements, ρ-cohypomonotonicity, Lipschitz continuity, indicator function


- [Byzantine-Robust Federated Learning: Impact of Client Subsampling and Local Updates](https://icml.cc/virtual/2024/poster/34407) (Poster)
  - **Authors:** [Youssef Allouah](http://openreview.net/profile?id=~Youssef_Allouah1), [Sadegh Farhadkhani](http://openreview.net/profile?id=~Sadegh_Farhadkhani1), [Rachid Guerraoui](http://openreview.net/profile?id=~Rachid_Guerraoui1), [Nirupam Gupta](http://openreview.net/profile?id=~Nirupam_Gupta1), [Rafael Pinot](http://openreview.net/profile?id=~Rafael_Pinot1), [Geovani Rizk](http://openreview.net/profile?id=~Geovani_Rizk1), [Sasha Voitovych](http://openreview.net/profile?id=~Sasha_Voitovych1)
  - **Affiliations:** EPFL, EPFL, EPFL, EPFL, Sorbonne Université, LPSM, EPFL, University of Toronto
  - **TL;DR:** This study investigates the robustness of federated learning against Byzantine clients by analyzing the effects of client subsampling and local updates. The findings reveal that careful management of these factors is crucial for optimal convergence and performance in federated learning systems.
  - **Keywords:** Federated Learning, Byzantine Robustness, FedAvg, FedRo, Image Classification, Adversarial Clients, Byzantine Clients, Client Subsampling, Local Updates, Robust Aggregation Rule, Convergence Conditions, FEMNIST, CIFAR-10


- [The Privacy Power of Correlated Noise in Decentralized Learning](https://icml.cc/virtual/2024/poster/34985) (Poster)
  - **Authors:** [Youssef Allouah](http://openreview.net/profile?id=~Youssef_Allouah1), [Anastasiia Koloskova](http://openreview.net/profile?id=~Anastasia_Koloskova2), [Aymane Firdoussi](http://openreview.net/profile?id=~Aymane_El_Firdoussi1), [Martin Jaggi](http://openreview.net/profile?id=~Martin_Jaggi1), [Rachid Guerraoui](http://openreview.net/profile?id=~Rachid_Guerraoui1)
  - **Affiliations:** EPFL, Switzerland, EPFL, Switzerland, EPFL, Switzerland, EPFL, Switzerland, EPFL, Switzerland
  - **TL;DR:** This paper introduces DECOR, a decentralized learning method that utilizes correlated noise to enhance privacy while maintaining utility in model training. It demonstrates that DECOR achieves optimal privacy-utility trade-offs under a new relaxation of local differential privacy, SecLDP, addressing privacy concerns in decentralized settings.
  - **Keywords:** Decentralized learning, Privacy, Decentralized SGD, Differential privacy (DP), Local differential privacy (LDP), SecLDP, Privacy concerns, Data exposure, Network sparsity, DECOR method, Privacy accountant, Correlated noise, Gaussian noise


- [Nonlinear Filtering with Brenier Optimal Transport Maps](https://icml.cc/virtual/2024/poster/33624) (Poster)
  - **Authors:** [Mohammad Al-Jarrah](http://openreview.net/profile?id=~Mohammad_Al-Jarrah1), [Niyizhen Jin](http://openreview.net/profile?id=~Niyizhen_Jin1), [Bamdad Hosseini](http://openreview.net/profile?id=~Bamdad_Hosseini1), [Amirhossein Taghvaei](http://openreview.net/profile?id=~Amirhossein_Taghvaei1)
  - **Affiliations:** Department of Aeronautics & Astronautics, University of Washington, Seattle, Department of Applied Mathematics, University of Washington, Seattle, Department of Applied Mathematics, University of Washington, Seattle, Department of Aeronautics & Astronautics, University of Washington, Seattle
  - **TL;DR:** This paper presents a novel method for nonlinear filtering using Brenier optimal transport maps to compute the conditional distribution of a stochastic dynamical system's state from noisy observations. The proposed method addresses limitations of conventional particle filters, particularly in high-dimensional and non-Gaussian scenarios, demonstrating improved sample efficiency and scalability.
  - **Keywords:** Nonlinear filtering, Stochastic dynamical systems, Brenier optimal transport (OT) maps, Sequential importance resampling (SIR) particle filters, Ensemble Kalman filter, Neural networks, Degenerate likelihoods, High-dimensional states, Weight degeneracy issue, Sample efficiency, High-dimensional scalability, Multi-modal distributions, Hidden Markov process, Conditional probability kernels


- [Hyperbolic Optimizer as a Dynamical System](https://icml.cc/virtual/2024/poster/33246) (Poster)
  - **Authors:** [Nico Alvarado](http://openreview.net/profile?id=~Nico_Alvarado1), [Hans Lobel](http://openreview.net/profile?id=~Hans_Lobel1)
  - **Affiliations:** Department of Computer Science, Pontificia Universidad Católica de Chile; National Center of Artificial Intelligence, Chile; Millennium Institute Foundational Research on Data, Chile, Department of Computer Science, Pontificia Universidad Católica de Chile; National Center of Artificial Intelligence, Chile; Millennium Institute Foundational Research on Data, Chile; Department of Transport and Logistics Engineering, Pontificia Universidad Católica de Chile
  - **TL;DR:** This study introduces a hyperbolic optimizer based on ADMM tailored for hyperbolic geometry, linking it to a non-linear ordinary differential equation. The research emphasizes the importance of stability analysis for effective implementation in real-world applications, particularly in hyperbolic neural networks.
  - **Keywords:** hyperbolic geometry, dynamical systems, optimization algorithms, Riemannian optimization, ADMM (Alternating Direction Method of Multipliers), non-linear ordinary differential equations, hyperbolic neural networks, hierarchical data representation, challenges of classical optimizers in non-Euclidean spaces, new hyperbolic optimizer, stability analysis through ODE linearization, hyperbolic spaces, Lyapunov stability, Poincaré ball model


- [Robust and Conjugate Gaussian Process Regression](https://icml.cc/virtual/2024/poster/34974) (Spotlight Poster)
  - **Authors:** [Matias Altamirano](http://openreview.net/profile?id=~Matias_Altamirano2), [Francois-Xavier Briol](http://openreview.net/profile?id=~Francois-Xavier_Briol1), [Jeremias Knoblauch](http://openreview.net/profile?id=~Jeremias_Knoblauch1)
  - **Affiliations:** Department of Statistical Science, University College London, London, United Kingdom, Department of Statistical Science, University College London, London, United Kingdom, Department of Statistical Science, University College London, London, United Kingdom
  - **TL;DR:** This paper presents a method for robust and conjugate Gaussian process regression (RCGP) that maintains closed-form conditioning while addressing the challenges posed by outliers in data. The proposed RCGP method demonstrates strong empirical performance across various applications, including Bayesian optimisation and sparse variational Gaussian processes.
  - **Keywords:** Gaussian Process Regression, Robustness in Inference, Generalised Bayesian Inference, Conjugate Gaussian Process, Bayesian Optimisation, Sparse Variational Gaussian Processes, Outliers in Data, Non-robust Inferences, Provably Robust and Conjugate Gaussian Process (RCGP)


- [Position: Stop Making Unscientific AGI Performance Claims](https://icml.cc/virtual/2024/poster/34773) (Poster)
  - **Authors:** [Patrick Altmeyer](http://openreview.net/profile?id=~Patrick_Altmeyer1), [Andrew Demetriou](http://openreview.net/profile?id=~Andrew_M._Demetriou1), [Antony Bartlett](http://openreview.net/profile?id=~Antony_Bartlett1), [Cynthia C. S. Liem](http://openreview.net/profile?id=~Cynthia_C._S._Liem2)
  - **Affiliations:** Department of Intelligent Systems, Delft University of Technology, Delft, the Netherlands, Department of Intelligent Systems, Delft University of Technology, Delft, the Netherlands, Department of Intelligent Systems, Delft University of Technology, Delft, the Netherlands, Department of Intelligent Systems, Delft University of Technology, Delft, the Netherlands
  - **TL;DR:** The paper argues against the interpretation of spurious correlations in large language models as evidence of Artificial General Intelligence (AGI), emphasizing the need for caution in AI research communication. It highlights the tendency of humans to anthropomorphize AI capabilities and calls for adherence to principles of academic integrity.
  - **Keywords:** Artificial General Intelligence (AGI), Large Language Models (LLMs), Random projections, matrix decompositions, deep autoencoders, transformers, Misinterpretation of AI capabilities, spurious correlations in model representations, Academic integrity, methodological setup


- [Triple Changes Estimator for Targeted Policies](https://icml.cc/virtual/2024/poster/32647) (Spotlight Poster)
  - **Authors:** [Sina Akbari](http://openreview.net/profile?id=~Sina_Akbari1), [Negar Kiyavash](http://openreview.net/profile?id=~Negar_Kiyavash1)
  - **Affiliations:** EPFL, Switzerland, EPFL, Switzerland
  - **TL;DR:** This study extends the triple difference estimator within the changes-in-changes framework to provide a more robust method for estimating causal effects, particularly in the context of Medicaid expansion's impact on children's preventive care. The findings highlight the importance of addressing the parallel trends assumption to avoid biased estimates in observational studies.
  - **Keywords:** difference-in-differences (DiD), triple difference estimator, changes-in-changes (CiC), triple changes estimator, Medicaid expansion, children's preventive care, parallel trends assumption, bias in estimates, extension of the triple difference estimator, identification assumptions


- [Online conformal prediction with decaying step sizes](https://icml.cc/virtual/2024/poster/35102) (Poster)
  - **Authors:** [Anastasios Angelopoulos](http://openreview.net/profile?id=~Anastasios_Nikolas_Angelopoulos1), [Rina Barber](http://openreview.net/profile?id=~Rina_Barber1), [Stephen Bates](http://openreview.net/profile?id=~Stephen_Bates1)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley CA USA, Department of Statistics, University of Chicago, Chicago IL USA, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge MA USA
  - **TL;DR:** This paper presents a method for online conformal prediction that utilizes decaying step sizes to provide robust coverage guarantees for both arbitrary and I.I.D. data sequences. The proposed method improves practical properties by ensuring that coverage remains close to the desired level at every time point, particularly in stable distributions.
  - **Keywords:** online conformal prediction, uncertainty quantification, decaying step sizes, conformal score function, time-series forecasting, medicine, robotics, finance, epidemiology, coverage probability, prediction sets, historical fraction of miscovered labels, simultaneous best-case and worst-case guarantees, long-run coverage guarantee, convergence guarantee


- [Distinguishing the Knowable from the Unknowable with Language Models](https://icml.cc/virtual/2024/poster/32819) (Poster)
  - **Authors:** [Gustaf Ahdritz](http://openreview.net/profile?id=~Gustaf_Ahdritz2), [Tian Qin](http://openreview.net/profile?id=~Tian_Qin3), [Nikhil Vyas](http://openreview.net/profile?id=~Nikhil_Vyas1), [Boaz Barak](http://openreview.net/profile?id=~Boaz_Barak2), [Benjamin Edelman](http://openreview.net/profile?id=~Benjamin_L._Edelman1)
  - **Affiliations:** Harvard University, Harvard University, Harvard University, Harvard University, Harvard University
  - **TL;DR:** This study investigates the distinction between epistemic and aleatoric uncertainty in large language models (LLMs) by using larger models as proxies for ground truth. The authors propose methods to identify these uncertainties at the token level, demonstrating that LLMs contain internal representations that can enhance model confidence indicators.
  - **Keywords:** epistemic uncertainty, aleatoric uncertainty, large language models (LLMs), linear probes, embeddings, unsupervised methods, In-Context Learning Test (ICLT), identifying uncertainty types in language model outputs, improved indicators of model confidence, token-level uncertainty classification


- [Scalable Online Exploration via Coverability](https://icml.cc/virtual/2024/poster/34698) (Poster)
  - **Authors:** [Philip Amortila](http://openreview.net/profile?id=~Philip_Amortila1), [Dylan Foster](http://openreview.net/profile?id=~Dylan_J_Foster1), [Akshay Krishnamurthy](http://openreview.net/profile?id=~Akshay_Krishnamurthy1)
  - **Affiliations:** University of Illinois, Urbana-Champaign, Microsoft Research, Microsoft Research
  - **TL;DR:** This paper introduces L1-Coverage as a new exploration objective in reinforcement learning, aimed at improving exploration efficiency in high-dimensional environments. The authors demonstrate that L1-Coverage facilitates effective policy optimization and enables computationally efficient algorithms for both model-based and model-free reinforcement learning.
  - **Keywords:** exploration in reinforcement learning, high-dimensional domains, L1-Coverage, policy optimization, policy gradient, Q-learning, sample complexity, computational efficiency, exploration challenges, efficient model-based and model-free algorithms, exploration objectives, MDP (Markov Decision Process), coverability


- [Stationarity without mean reversion in improper Gaussian processes](https://icml.cc/virtual/2024/poster/34936) (Poster)
  - **Authors:** [Luca Ambrogioni](http://openreview.net/profile?id=~Luca_Ambrogioni1)
  - **Affiliations:** Donders Institute for Brain, Cognition and Behaviour, Radboud University
  - **TL;DR:** This paper introduces a method to use improper Gaussian process priors with infinite variance to create stationary processes that do not exhibit mean reversion, addressing issues in traditional GP regression. The authors present a family of non-positive kernels that maintain desirable properties of standard stationary kernels while avoiding pathological behaviors.
  - **Keywords:** Gaussian Processes, Stationarity, Non-Mean Reversion, Improper Gaussian Processes, Non-Positive Kernels, Machine Learning, Spatial Statistics, Statistical Signal Processing, Mean Reversion, Pathological Behavior in GP Regression, Non-Reverting Covariance Functions, Analytical Posterior Distributions, Synthetic Data, Real Data, Covariance Function, Kernel, Squared Exponential, Matérn Class


- [Causal Action Influence Aware Counterfactual Data Augmentation](https://icml.cc/virtual/2024/poster/34911) (Poster)
  - **Authors:** [Núria Armengol Urpí](http://openreview.net/profile?id=~N%C3%BAria_Armengol_Urp%C3%AD1), [Marco Bagatella](http://openreview.net/profile?id=~Marco_Bagatella1), [Marin Vlastelica](http://openreview.net/profile?id=~Marin_Vlastelica1), [Georg Martius](http://openreview.net/profile?id=~Georg_Martius1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Zurich, Switzerland; Max Planck Institute for Intelligent Systems, Tübingen, Germany, Department of Computer Science, ETH Zurich, Zurich, Switzerland; Max Planck Institute for Intelligent Systems, Tübingen, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany, Department of Computer Science, University of Tübingen, Tübingen, Germany
  - **TL;DR:** The study introduces CAIAC, a data augmentation method that enhances offline learning for robots by utilizing causal action influence to create synthetic transitions from existing datasets. This approach significantly improves the robustness of learning algorithms against distributional shifts and causal confusion.
  - **Keywords:** Offline learning, data augmentation, causal reasoning, Causal action influence (CAI), counterfactual reasoning, Robotics, complex behavior teaching, Causal confusion, distributional shift, spurious correlations, CAIAC method for data augmentation, improved robustness of offline learning algorithms


- [No Dimensional Sampling Coresets for Classification](https://icml.cc/virtual/2024/poster/33304) (Spotlight Poster)
  - **Authors:** [Meysam Alishahi](http://openreview.net/profile?id=~Meysam_Alishahi1), [Jeff Phillips](http://openreview.net/profile?id=~Jeff_M._Phillips1)
  - **Affiliations:** Kahlert School of Computing, University of Utah, Salt Lake City, Utah, USA; None, Kahlert School of Computing, University of Utah, Salt Lake City, Utah, USA; visiting ScaDS.AI, University of Leipzig and MPI for Math in the Sciences, Leipzig, Germany
  - **TL;DR:** This paper refines and generalizes the concept of coresets for classification problems using a sensitivity sampling framework, introducing the first no dimensional coresets that do not depend on the dimension of the data. The findings provide sample complexity bounds and ensure approximation guarantees for various loss functions, enhancing the efficiency of machine learning models.
  - **Keywords:** Coresets, Classification Problems, Sensitivity Sampling, Sensitivity Sampling, Radamacher Complexity, Machine Learning, Approximation Guarantees, Data Size Reduction, Performance on New Data, No Dimensional Coresets, Sample Complexity Bounds


- [Robust Graph Matching when Nodes are Corrupt](https://icml.cc/virtual/2024/poster/33877) (Poster)
  - **Authors:** [Taha Ameen Ur Rahman](http://openreview.net/profile?id=~Taha_Ameen1), [Bruce Hajek](http://openreview.net/profile?id=~Bruce_Hajek1)
  - **Affiliations:** Department of Electrical and Computer Engineering; Coordinated Science Laboratory, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA, Department of Electrical and Computer Engineering; Coordinated Science Laboratory, University of Illinois Urbana-Champaign, Urbana, IL 61801, USA
  - **TL;DR:** This study introduces two models for matching correlated graphs in the presence of corrupt nodes, demonstrating that while detection of corrupt nodes is impossible in an adversarial setting, robust estimators can still match a positive fraction of nodes when only one network is compromised. The findings highlight the importance of considering node corruption in graph matching algorithms.
  - **Keywords:** Graph matching, Node corruption, k-core estimator, maximum overlap estimator, Social networks, Biological networks, Computer vision, Natural language processing, Node corruption, Graph isomorphism, Necessary conditions for estimators, Robust algorithms, Correlated graphs, Edge-correlated networks


- [Learning the Target Network in Function Space](https://icml.cc/virtual/2024/poster/33310) (Poster)
  - **Authors:** [Kavosh Asadi](http://openreview.net/profile?id=~Kavosh_Asadi1), [Yao Liu](http://openreview.net/profile?id=~Yao_Liu1), [Shoham Sabach](http://openreview.net/profile?id=~Shoham_Sabach1), [Ming Yin](http://openreview.net/profile?id=~Ming_Yin4), [Rasool Fakoor](http://openreview.net/profile?id=~Rasool_Fakoor1)
  - **Affiliations:** Amazon, Amazon, Technion; Amazon, Princeton University, Amazon
  - **TL;DR:** The study introduces Lookahead-Replicate (LR), a novel algorithm for learning the value function in reinforcement learning that maintains equivalence in function space rather than parameter space. Empirical results show that LR significantly enhances deep reinforcement learning performance on the Atari benchmark.
  - **Keywords:** reinforcement learning, value function approximation, Lookahead-Replicate (LR), Bellman operator, learning accurate value function, function approximation in large state spaces, convergent behavior in learning, improved deep RL performance, Atari benchmark


- [How Free is Parameter-Free Stochastic Optimization?](https://icml.cc/virtual/2024/poster/34927) (Spotlight Poster)
  - **Authors:** [Amit Attia](http://openreview.net/profile?id=~Amit_Attia1), [Tomer Koren](http://openreview.net/profile?id=~Tomer_Koren1)
  - **Affiliations:** Blavatnik School of Computer Science, Tel Aviv University; Google Research Tel Aviv, Blavatnik School of Computer Science, Tel Aviv University; Google Research Tel Aviv
  - **TL;DR:** This study investigates the existence of fully parameter-free stochastic optimization methods that achieve competitive convergence rates without requiring significant knowledge of problem parameters. It presents a hyperparameter search technique that outperforms existing state-of-the-art algorithms in both non-convex and convex settings.
  - **Keywords:** parameter-free stochastic optimization, stochastic gradient descent, hyperparameter search technique, adaptive methods, parameter-free methods, machine learning, statistical learning problems, tuning of algorithmic parameters, unknown problem parameters, fully parameter-free methods, convergence rates, lower bounds


- [Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing](https://icml.cc/virtual/2024/poster/33356) (Poster)
  - **Authors:** [Alaa Anani](http://openreview.net/profile?id=~Alaa_Anani1), [Tobias Lorenz](http://openreview.net/profile?id=~Tobias_Lorenz1), [Bernt Schiele](http://openreview.net/profile?id=~Bernt_Schiele1), [Mario Fritz](http://openreview.net/profile?id=~Mario_Fritz1)
  - **Affiliations:** CISPA Helmhotz Center for Information Security; Max Planck Institute for Informatics, CISPA Helmhotz Center for Information Security, Max Planck Institute for Informatics, CISPA Helmhotz Center for Information Security
  - **TL;DR:** This paper presents a novel adaptive hierarchical certification method for semantic segmentation that reduces abstain rates by certifying unstable components at coarser semantic levels while maintaining theoretical guarantees. The proposed method demonstrates improved certified accuracy and lower abstain rates compared to existing state-of-the-art techniques.
  - **Keywords:** Certification for machine learning, Semantic segmentation, Adversarial robustness, Randomized smoothing, Adaptive hierarchical certification, Autonomous driving, Medical imaging, Video surveillance, Object detection, High abstain rates, Model uncertainty, Non-robustness to adversarial perturbations, Certified Information Gain (CIG), Lower abstain rate, Cityscapes, PASCAL-Context, ACDC, COCO-Stuff, Safety-critical domains


- [Random features models: a way to study the success of naive imputation](https://icml.cc/virtual/2024/poster/34736) (Poster)
  - **Authors:** [Alexis Ayme](http://openreview.net/profile?id=~Alexis_Ayme1), [Claire Boyer](http://openreview.net/profile?id=~Claire_Boyer1), [Aymeric Dieuleveut](http://openreview.net/profile?id=~Aymeric_Dieuleveut1), [Erwan Scornet](http://openreview.net/profile?id=~Erwan_Scornet1)
  - **Affiliations:** Sorbonne Université, CNRS, Laboratoire de Probabilités, Statistique et Modélisation (LPSM), F-75005 Paris, France; Institut Universitaire de France (IUF), Sorbonne Université, CNRS, Laboratoire de Probabilités, Statistique et Modélisation (LPSM), F-75005 Paris, France; Institut Universitaire de France (IUF), CMAP, UMR7641, Ecole Polytechnique, IP Paris, 91128 Palaiseau, France, Sorbonne Université, CNRS, Laboratoire de Probabilités, Statistique et Modélisation (LPSM), F-75005 Paris, France
  - **TL;DR:** This study investigates the effectiveness of naive imputation methods for handling missing data, demonstrating that the bias introduced is negligible in high-dimensional linear predictors and remains relevant even in low dimensions. The authors establish theoretical bounds for stochastic gradient predictors applied to zero-imputed data, suggesting favorable outcomes even under more complex missing data scenarios.
  - **Keywords:** missing data, naive imputation, predictive performance, random features model, stochastic gradient descent (SGD), machine learning, data analysis, missing completely at random (MCAR), bias in imputation, finite-sample bounds, consistency of imputation strategies


- [Delaunay Graph: Addressing Over-Squashing and Over-Smoothing Using Delaunay Triangulation](https://icml.cc/virtual/2024/poster/32801) (Poster)
  - **Authors:** [Hugo Attali](http://openreview.net/profile?id=~Hugo_Attali1), [Davide Buscaldi](http://openreview.net/profile?id=~Davide_Buscaldi2), [Nathalie Pernelle](http://openreview.net/profile?id=~Nathalie_Pernelle1)
  - **Affiliations:** LIPN, Universite Sorbonne Nord, LIPN, Universite Sorbonne Nord, LIPN, Universite Sorbonne Nord
  - **TL;DR:** This study introduces a novel approach using Delaunay Triangulation to construct graphs from features, addressing the issues of over-squashing and over-smoothing in Graph Neural Networks. The proposed method consistently outperforms existing graph rewiring techniques, enhancing information propagation efficiency.
  - **Keywords:** Graph Neural Networks (GNNs), Information Propagation, Delaunay Triangulation, Message-Passing Paradigm, Graph Rewiring, Chemistry, Information Retrieval, Social Network Analysis, Knowledge Graphs, Over-Squashing, Over-Smoothing, Heterophilic Case, Bottlenecks in Graphs, Novel Graph Construction Method, Improved Information Propagation


- [An amortized approach to non-linear mixed-effects modeling based on neural posterior estimation](https://icml.cc/virtual/2024/poster/32838) (Poster)
  - **Authors:** [Jonas Arruda](http://openreview.net/profile?id=~Jonas_Arruda1), [Yannik Schälte](http://openreview.net/profile?email=yannik.schaelte%40gmail.com), [Clemens Peiter](http://openreview.net/profile?email=cpeiter%40uni-bonn.de), [Olga Teplytska](http://openreview.net/profile?email=oteplytska%40uni-bonn.de), [Ulrich Jaehde](http://openreview.net/profile?email=u.jaehde%40uni-bonn.de), [Jan Hasenauer](http://openreview.net/profile?id=~Jan_Hasenauer1)
  - **Affiliations:** Life and Medical Sciences Institute, University of Bonn, 53115 Bonn, Germany; Computational Health Center, Helmholtz Zentrum München, 85764 Neuherberg, Germany, Life and Medical Sciences Institute, University of Bonn, 53115 Bonn, Germany; Computational Health Center, Helmholtz Zentrum München, 85764 Neuherberg, Germany, Life and Medical Sciences Institute, University of Bonn, 53115 Bonn, Germany, Pharmaceutical Institute, University of Bonn, 53121 Bonn, Germany, Pharmaceutical Institute, University of Bonn, 53121 Bonn, Germany, Life and Medical Sciences Institute, University of Bonn, 53115 Bonn, Germany; Computational Health Center, Helmholtz Zentrum München, 85764 Neuherberg, Germany
  - **TL;DR:** This study presents a novel machine learning-based approach using neural density estimation to efficiently estimate parameters in non-linear mixed-effects models for heterogeneous populations. The method demonstrates significant flexibility and scalability in applications within cell biology and pharmacology compared to traditional techniques.
  - **Keywords:** Non-linear mixed-effects models, Heterogeneous populations, Neural density estimation, Conditional normalizing flows, Cell biology, Pharmacology, Computational challenges in parameter inference, Individual-level likelihood formulation, Amortized parameter estimation, Efficient inference of population parameters


- [Memory Consolidation Enables Long-Context Video Understanding](https://icml.cc/virtual/2024/poster/32983) (Spotlight Poster)
  - **Authors:** [Ivana Balazevic](http://openreview.net/profile?id=~Ivana_Balazevic1), [Yuge Shi](http://openreview.net/profile?id=~Yuge_Shi2), [Pinelopi Papalampidi](http://openreview.net/profile?id=~Pinelopi_Papalampidi1), [Rahma Chaabouni](http://openreview.net/profile?id=~Rahma_Chaabouni1), [Skanda Koppula](http://openreview.net/profile?id=~Skanda_Koppula1), [Olivier Henaff](http://openreview.net/profile?id=~Olivier_J_Henaff1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This study introduces the Memory-Consolidated Vision Transformer (MC-ViT), which effectively extends the temporal context of video understanding by utilizing a memory bank of past activations without modifying the underlying architecture. MC-ViT achieves state-of-the-art performance on long-context video tasks while using significantly fewer parameters than existing models.
  - **Keywords:** long-context video understanding, memory consolidation, transformer architectures, memory bank, fine-tuning, video processing, artificial vision systems, short temporal contexts, quadratic complexity, scaling issues, Memory-Consolidated Vision Transformer (MC-ViT), state-of-the-art performance, EgoSchema, Perception Test, Diving48, redundancy reduction, streaming setting


- [Private Vector Mean Estimation in the Shuffle Model: Optimal Rates Require Many Messages](https://icml.cc/virtual/2024/poster/34119) (Poster)
  - **Authors:** [Hilal Asi](http://openreview.net/profile?id=~Hilal_Asi1), [Vitaly Feldman](http://openreview.net/profile?id=~Vitaly_Feldman1), [Jelani Nelson](http://openreview.net/profile?id=~Jelani_Nelson2), [Huy Nguyen](http://openreview.net/profile?id=~Huy_Nguyen3), [Kunal Talwar](http://openreview.net/profile?id=~Kunal_Talwar1), [Samson Zhou](http://openreview.net/profile?id=~Samson_Zhou1)
  - **Affiliations:** Apple, Apple, UC Berkeley, Northeastern, Apple, Texas A&M University
  - **TL;DR:** This study addresses private vector mean estimation in the shuffle model, proposing a multi-message protocol that achieves optimal error with a specific message complexity. It also analyzes the robustness of the protocol against malicious users and establishes the necessity of multiple messages for optimal performance.
  - **Keywords:** private vector mean estimation, shuffle model, federated learning, multi-message protocol, single-message protocol, differential privacy, federated analytics, machine learning model training, statistical analysis, privacy error, optimal error, robustness to malicious users, optimal message complexity, mean squared error analysis, local differential privacy, Encode, Shuffle, Analyze (ESA) model


- [Practical Performance Guarantees for Pipelined DNN Inference](https://icml.cc/virtual/2024/poster/34028) (Spotlight Poster)
  - **Authors:** [Aaron Archer](http://openreview.net/profile?id=~Aaron_Archer1), [Matthew Fahrbach](http://openreview.net/profile?id=~Matthew_Fahrbach1), [Kuikui Liu](http://openreview.net/profile?id=~Kuikui_Liu1), [Prakash Prabhu](http://openreview.net/profile?id=~Prakash_Prabhu1)
  - **Affiliations:** Google, Google, MIT, Google
  - **TL;DR:** This study focuses on optimizing pipeline parallelism for DNN inference by partitioning model graphs and minimizing the bottleneck stage's running time. The authors present effective algorithms and demonstrate that their novel MIP relaxations significantly improve lower bounds, closing the optimality gap by a factor of nearly 10.
  - **Keywords:** pipeline parallelism, deep neural network (DNN) inference, mixed-integer programming (MIP), model graph partitioning, bottleneck stage, communication overhead, running time balance, improved lower bounds, practical algorithms, diverse testbed of 369 production models


- [Fast Algorithms for Hypergraph PageRank with Applications to Semi-Supervised Learning](https://icml.cc/virtual/2024/poster/32899) (Poster)
  - **Authors:** [Konstantinos Ameranis](http://openreview.net/profile?id=~Konstantinos_Ameranis1), [Adela DePavia](http://openreview.net/profile?id=~Adela_Frances_DePavia1), [Lorenzo Orecchia](http://openreview.net/profile?id=~Lorenzo_Orecchia1), [Erasmo Tani](http://openreview.net/profile?id=~Erasmo_Tani1)
  - **Affiliations:** Department of Computer Science, University of Chicago, Chicago, USA, Computational and Applied Mathematics, University of Chicago, Chicago, USA, Department of Computer Science, University of Chicago, Chicago, USA, Department of Computer Science, University of Chicago, Chicago, USA
  - **TL;DR:** This paper presents scalable algorithms for hypergraph PageRank and hypergraph Laplacian systems to enhance semi-supervised learning by effectively capturing higher-order relationships. The proposed methods demonstrate significant speed improvements, enabling broader applications of hypergraph models in large-scale settings.
  - **Keywords:** semi-supervised learning, hypergraph models, hypergraph PageRank, hypergraph Laplacian systems, higher-order relations, group membership, scalability of hypergraph models, scalable algorithms, speed-ups on hypergraph tasks, benchmark instances of semi-supervised learning, hypergraph primitives, Dirichlet energy, Laplacian system


- [Bipartite Matching in Massive Graphs: A Tight Analysis of EDCS](https://icml.cc/virtual/2024/poster/34595) (Poster)
  - **Authors:** [Amir Azarmehr](http://openreview.net/profile?id=~Amir_Azarmehr1), [Soheil Behnezhad](http://openreview.net/profile?id=~Soheil_Behnezhad1), [Mohammad Roghani](http://openreview.net/profile?id=~Mohammad_Roghani1)
  - **Affiliations:** Khoury College of Computer Science, Northeastern University, Boston, USA, Khoury College of Computer Science, Northeastern University, Boston, USA, Department of Management Science and Engineering, Stanford University, Stanford, USA
  - **TL;DR:** This paper analyzes the edge-degree constrained subgraph (EDCS) for maximum matching in massive graphs, revealing that the optimal sparsity parameter β = 6 achieves an approximation ratio of 0.677, surpassing the previously believed limit of 2/3. The findings suggest that increasing β is not necessary for improving approximation ratios.
  - **Keywords:** Maximum matching, Graph sparsification, Edge-degree constrained subgraph (EDCS), Data mining, Resource allocation, Online advertisement, Balanced clustering, Handling massive graphs, Memory limitations in processing, Approximation ratio analysis, Improved matching size


- [A Rate-Distortion View of Uncertainty Quantification](https://icml.cc/virtual/2024/poster/32634) (Poster)
  - **Authors:** [Ifigeneia Apostolopoulou](http://openreview.net/profile?id=~Ifigeneia_Apostolopoulou1), [Benjamin Eysenbach](http://openreview.net/profile?id=~Benjamin_Eysenbach1), [Frank Nielsen](http://openreview.net/profile?id=~Frank_Nielsen1), [Artur Dubrawski](http://openreview.net/profile?id=~Artur_Dubrawski2)
  - **Affiliations:** Machine Learning Department, AutonLab, Carnegie Mellon University, Computer Science Department, Princeton University, Sony Computer Science Laboratories Inc., Tokyo, Japan, Machine Learning Department, AutonLab, Carnegie Mellon University
  - **TL;DR:** This paper introduces the Distance Aware Bottleneck (DAB) method to enhance deep neural networks' ability to quantify uncertainty by measuring the distance of new examples from a learned codebook of training data. The proposed method achieves superior out-of-distribution detection and misclassification prediction compared to existing techniques, all while providing deterministic uncertainty estimates with a single forward pass.
  - **Keywords:** Uncertainty Quantification, Deep Learning, Distance Aware Bottleneck (DAB), Gaussian Processes, Deterministic Uncertainty Methods (DUMs), Out-Of-Distribution (OOD) Detection, Reliable uncertainty estimation, model confidence, misclassification prediction, Improved OOD detection, deterministic uncertainty estimates, CIFAR-10, CIFAR-100, SVHN, Information Bottleneck, feature collapse


- [Constrained Ensemble Exploration for Unsupervised Skill Discovery](https://icml.cc/virtual/2024/poster/34770) (Poster)
  - **Authors:** [Chenjia Bai](http://openreview.net/profile?id=~Chenjia_Bai2), [Rushuai Yang](http://openreview.net/profile?id=~Rushuai_Yang1), [Qiaosheng Zhang](http://openreview.net/profile?id=~Qiaosheng_Zhang2), [Kang Xu](http://openreview.net/profile?id=~Kang_Xu2), [Yi Chen](http://openreview.net/profile?id=~Yi_Chen18), [Ting Xiao](http://openreview.net/profile?id=~Ting_Xiao1), [Xuelong Li](http://openreview.net/profile?id=~Xuelong_Li2)
  - **Affiliations:** Shanghai Artificial Intelligence Laboratory; Shenzhen Research Institute of Northwestern Polytechnical University, Hong Kong University of Science and Technology, Shanghai Artificial Intelligence Laboratory, Tencent, Hong Kong University of Science and Technology, East China University of Science and Technology; The Institute of Artificial Intelligence (TeleAI), China Telecom, Shanghai Artificial Intelligence Laboratory; The Institute of Artificial Intelligence (TeleAI), China Telecom
  - **TL;DR:** This paper presents a novel unsupervised reinforcement learning framework called Constrained Ensemble exploration for Skill Discovery (CeSD), which utilizes an ensemble of skills to enhance state coverage and learn distinguishable behaviors without relying on extrinsic rewards. The proposed method demonstrates superior performance in various downstream tasks compared to existing approaches.
  - **Keywords:** Unsupervised Reinforcement Learning, Skill Discovery, Ensemble of Skills, State-Distribution Constraints, Value Functions, Game AI, Self-Driving Cars, Robotic Manipulation, Locomotion Tasks, Static Skills, Poor State Coverage, High-Dimensional Environments, Novel Framework (CeSD), Well-Explored Ensemble Skills, Mutual Information, Empowerment, Feature Clustering


- [Simulation of Graph Algorithms with Looped Transformers](https://icml.cc/virtual/2024/poster/33696) (Poster)
  - **Authors:** [Artur Back de Luca](http://openreview.net/profile?id=~Artur_Back_de_Luca1), [Kimon Fountoulakis](http://openreview.net/profile?id=~Kimon_Fountoulakis1)
  - **Affiliations:** David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada, David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada
  - **TL;DR:** This study investigates the capability of looped transformer networks to simulate various graph algorithms, including Dijkstra’s, BFS, and DFS, demonstrating that the architecture can handle graphs of varying sizes without increasing the number of parameters. The findings indicate a limit to simulation due to finite precision, while also achieving Turing Completeness with constant width.
  - **Keywords:** graph algorithms, neural networks, algorithmic reasoning, looped transformer, attention heads, adjacency matrix, simulation of algorithms, finite precision, Turing Completeness, multitask model, Dijkstra’s algorithm, Breadth-first search (BFS), Depth-first search (DFS), Kosaraju’s algorithm, strongly connected components (SCC)


- [Diffusion Models Demand Contrastive Guidance for Adversarial Purification to Advance](https://icml.cc/virtual/2024/poster/35110) (Poster)
  - **Authors:** [Mingyuan Bai](http://openreview.net/profile?id=~Mingyuan_Bai1), [Wei Huang](http://openreview.net/profile?id=~Wei_Huang6), [Li Tenghui](http://openreview.net/profile?id=~Li_Tenghui1), [Andong Wang](http://openreview.net/profile?id=~Andong_Wang1), [Junbin Gao](http://openreview.net/profile?id=~Junbin_Gao1), [Cesar F Caiafa](http://openreview.net/profile?id=~Cesar_F_Caiafa1), [Qibin Zhao](http://openreview.net/profile?id=~Qibin_Zhao1)
  - **Affiliations:** Tensor Learning Team, Center of Advanced Intelligence Project, RIKEN, Tokyo, 1030027, JAPAN, Deep Learning Theory Team, Center of Advanced Intelligence Project, RIKEN, Tokyo, 1030027, JAPAN, School of Automation, Guangdong University of Technology, Guangzhou, 510006, CHINA, Key Laboratory of Intelligent Detection and the Internet of Things in Manufacturing, Ministry of Education, Guangzhou, 510006, CHINA, Discipline of Business Analytics, The University of Sydney Business School, The University of Sydney, Darlington, NSW, 2006, AUSTRALIA, Instituto Argentino de Radioastronomía, CONICET CCT La Plata/CIC-PBA/UNLP, V. Elisa, 1894, ARGENTINA, Tensor Learning Team, Center of Advanced Intelligence Project, RIKEN, Tokyo, 1030027, JAPAN
  - **TL;DR:** This study proposes a method to enhance adversarial purification using diffusion models guided by contrastive loss, demonstrating significant improvements over existing adversarial training methods. Extensive experiments show that the proposed approach effectively removes adversarial attacks while preserving semantic information across various datasets.
  - **Keywords:** Adversarial defense, Adversarial purification, Diffusion models, Contrastive guidance, Image classification, Deep learning, Adversarial attacks, Misclassification of DNNs, Improved adversarial purification methods, Theoretical derivation of noise levels, CIFAR-10, CIFAR-100, German Traffic Sign Recognition Benchmark, ImageNet, ResNet, WideResNet, Generative models


- [On the Complexity of Finite-Sum Smooth Optimization under the Polyak–Łojasiewicz Condition](https://icml.cc/virtual/2024/poster/33207) (Spotlight Poster)
  - **Authors:** [Yunyan Bai](http://openreview.net/profile?id=~Yunyan_Bai1), [Yuxing Liu](http://openreview.net/profile?id=~Yuxing_Liu1), [Luo Luo](http://openreview.net/profile?id=~Luo_Luo1)
  - **Affiliations:** School of Data Science, Fudan University, Shanghai, China, School of Data Science, Fudan University, Shanghai, China, School of Data Science, Fudan University, Shanghai, China; Shanghai Key Laboratory for Contemporary Applied Mathematics, Shanghai, China
  - **TL;DR:** This paper investigates the complexity of finite-sum smooth optimization under the Polyak–Łojasiewicz condition, demonstrating that any gradient method requires a significant number of incremental first-order oracle calls to achieve an ε-suboptimal solution. It also provides lower bounds for communication and time costs in distributed settings and proposes a decentralized first-order method that approaches these bounds.
  - **Keywords:** Finite-sum optimization, Polyak–Łojasiewicz condition, Incremental first-order oracle (IFO) methods, Gradient descent (GD), Deep neural networks, Reinforcement learning, Optimal control, Matrix recovery, Nonconvex optimization, Complexity of finding stationary points, Lower bounds for communication rounds, time cost, and local first-order oracle calls; Decentralized first-order method


- [On the Identifiability of Switching Dynamical Systems](https://icml.cc/virtual/2024/poster/34570) (Poster)
  - **Authors:** [Carles Balsells-Rodas](http://openreview.net/profile?id=~Carles_Balsells-Rodas1), [Yixin Wang](http://openreview.net/profile?id=~Yixin_Wang1), [Yingzhen Li](http://openreview.net/profile?id=~Yingzhen_Li1)
  - **Affiliations:** Imperial College London, University of Michigan, Imperial College London
  - **TL;DR:** This study investigates the identifiability of Switching Dynamical Systems, establishing identification conditions for Markov Switching Models and demonstrating their practical applications in segmenting high-dimensional time series and causal discovery in climate data. The findings contribute to the theoretical understanding of sequential latent variable models and propose estimation algorithms for identifiable systems.
  - **Keywords:** Identifiability, Latent Variable Models, Switching Dynamical Systems, Markov Switching Models, Non-linear Gaussians, Affine Transformations, High-dimensional Time Series, Causal Discovery, Climate Data, Identifiability Analysis, Sequential Generative Models, Estimation Algorithms, Identification Conditions, State-space Models, Hidden Markov Models, Autoregressive Connections


- [Combinatorial Approximations for Cluster Deletion: Simpler, Faster, and Better](https://icml.cc/virtual/2024/poster/34522) (Poster)
  - **Authors:** [Vicente Balmaseda](http://openreview.net/profile?id=~Vicente_Balmaseda1), [Ying Xu](http://openreview.net/profile?id=~Ying_Xu11), [Yixin Cao](http://openreview.net/profile?id=~Yixin_Cao5), [Nate Veldt](http://openreview.net/profile?id=~Nate_Veldt2)
  - **Affiliations:** Department of Computer Science and Engineering, Texas A&M University, College Station, Texas, USA, Department of Computing, Hong Kong Polytechnic University, Hong Kong, China, Department of Computing, Hong Kong Polytechnic University, Hong Kong, China, Department of Computer Science and Engineering, Texas A&M University, College Station, Texas, USA
  - **TL;DR:** This paper presents improved approximation algorithms for the NP-hard cluster deletion problem, enhancing previous guarantees from 4 to 3. The authors also introduce a new combinatorial approach that is more scalable for practical applications.
  - **Keywords:** Cluster deletion, graph clustering, correlation clustering, Approximation algorithms, linear programming, Computational biology, social network analysis, NP-hard problems, edge deletion to form cliques, Improved approximation guarantees, combinatorial algorithms


- [How Learning by Reconstruction Produces Uninformative Features For Perception](https://icml.cc/virtual/2024/poster/33801) (Poster)
  - **Authors:** [Randall Balestriero](http://openreview.net/profile?id=~Randall_Balestriero1), [Yann LeCun](http://openreview.net/profile?id=~Yann_LeCun1)
  - **Affiliations:** Brown University, NYU
  - **TL;DR:** The study investigates the misalignment between learning to reconstruct data and learning for perception, demonstrating that reconstruction can lead to uninformative features. It finds that using denoising strategies can improve the alignment and performance on perception tasks.
  - **Keywords:** representation learning, perception, reconstruction, deep Autoencoder, denoising, image recognition, supervised learning, misalignment between reconstruction and perception, uninformative features, long training schedules, improved accuracy through noise strategies, detection of non-beneficial noise strategies, TinyImagenet


- [On Mechanistic Knowledge Localization in Text-to-Image Generative Models](https://icml.cc/virtual/2024/poster/33449) (Poster)
  - **Authors:** [Samyadeep Basu](http://openreview.net/profile?id=~Samyadeep_Basu1), [Keivan Rezaei](http://openreview.net/profile?id=~Keivan_Rezaei1), [Priyatham Kattakinda](http://openreview.net/profile?id=~Priyatham_Kattakinda1), [Vlad Morariu](http://openreview.net/profile?id=~Vlad_I_Morariu1), [Nanxuan Zhao](http://openreview.net/profile?id=~Nanxuan_Zhao1), [Ryan A Rossi](http://openreview.net/profile?id=~Ryan_A._Rossi2), [Varun Manjunatha](http://openreview.net/profile?id=~Varun_Manjunatha1), [Soheil Feizi](http://openreview.net/profile?id=~Soheil_Feizi2)
  - **Affiliations:** University of Maryland, University of Maryland, University of Maryland, Adobe Research, Adobe Research, Adobe Research, Adobe Research, University of Maryland
  - **TL;DR:** This study introduces mechanistic localization in text-to-image generative models to identify layers controlling visual attributes, facilitating efficient model editing. The authors present methods LOCOGEN and LOCOEDIT, addressing challenges in knowledge localization and model editing in recent models like SD-XL and DeepFloyd.
  - **Keywords:** text-to-image generative models, mechanistic localization, model editing, causal tracing, LOCOGEN, LOCOEDIT, image generation, model editing, challenges in model editing, knowledge localization, efficient model editing methods, neuron-level model editing, LAION-5B, MS-COCO, UNet, CLIP text-encoder, Stable-Diffusion


- [Analyzing $D^\alpha$ seeding for $k$-means](https://icml.cc/virtual/2024/poster/33650) (Poster)
  - **Authors:** [Etienne Bamas](http://openreview.net/profile?id=~Etienne_Bamas1), [Sai Ganesh Nagarajan](http://openreview.net/profile?id=~Sai_Ganesh_Nagarajan1), [Ola Svensson](http://openreview.net/profile?id=~Ola_Svensson2)
  - **Affiliations:** ETH AI Center, Zurich, Switzerland, Zuse Institute Berlin, EPFL
  - **TL;DR:** This paper analyzes the Dα seeding algorithm for k-means clustering, demonstrating that for any α > 2, it provides improved approximation guarantees for the standard k-means cost. The findings indicate that the choice of α significantly influences the clustering performance, particularly in relation to the distribution of data points.
  - **Keywords:** clustering, k-means, Dα seeding, k-means++, Dα seeding algorithm, (k,α)-clustering cost, approximation guarantees, O(log k) approximation, experimental validation, Gaussian distributions, mixing weights


- [Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions](https://icml.cc/virtual/2024/poster/34109) (Spotlight Poster)
  - **Authors:** [Yongqiang Cai](http://openreview.net/profile?id=~Yongqiang_Cai1)
  - **Affiliations:** School of Mathematical Sciences, Laboratory of Mathematics and Complex Systems, MOE, Beijing Normal University, Beijing, 100875, China
  - **TL;DR:** This study investigates the universal approximation capabilities of deep neural networks by demonstrating the existence of a finite vocabulary of mappings that can approximate any continuous function on a compact domain. The findings suggest a novel compositional model for regular languages and highlight the potential of sequence modeling in addressing non-sequential problems.
  - **Keywords:** deep learning, sequence modeling, universal approximation, composite functions, linear mappings, nonlinear mappings, residual networks, neural ODE, natural language processing, image recognition, reinforcement learning, transforming non-sequential problems into sequential ones, handling long-term dependencies, finite vocabulary for universal approximation, approximation power of mapping compositions, RNNs, Transformers, BERT, GPT, ResNets


- [Neural Networks Learn Statistics of Increasing Complexity](https://icml.cc/virtual/2024/poster/34431) (Poster)
  - **Authors:** [Nora Belrose](http://openreview.net/profile?id=~Nora_Belrose1), [Quintin Pope](http://openreview.net/profile?id=~Quintin_Pope1), [Lucia Quirke](http://openreview.net/profile?id=~Lucia_Quirke1), [Alex Mallen](http://openreview.net/profile?id=~Alex_Troy_Mallen1), [Xiaoli Fern](http://openreview.net/profile?id=~Xiaoli_Fern1)
  - **Affiliations:** EleutherAI, Oregon State University, EleutherAI, EleutherAI, Oregon State University
  - **TL;DR:** This study investigates the distributional simplicity bias (DSB) in neural networks, demonstrating that they initially learn low-order statistics of data distributions before higher-order correlations. The findings reveal that early-training networks can effectively generalize to maximum-entropy distributions that match the low-order statistics of their training data, but this ability diminishes later in training.
  - **Keywords:** distributional simplicity bias, neural networks, low-order statistics, Taylor expansion, optimal transport methods, image classification, synthetic data generation, generalization behavior, reliance on statistics of different orders, criteria for model sensitivity to statistics, empirical evidence for DSB, maximum-entropy distributions, token n-gram frequencies, embedding vectors


- [Relational DNN Verification With Cross Executional Bound Refinement](https://icml.cc/virtual/2024/poster/34470) (Poster)
  - **Authors:** [Debangshu Banerjee](http://openreview.net/profile?id=~Debangshu_Banerjee2), [Gagandeep Singh](http://openreview.net/profile?id=~Gagandeep_Singh1)
  - **Affiliations:** Department of Computer Science, University of Illinois Urbana-Champaign, USA, Department of Computer Science, University of Illinois Urbana-Champaign, USA; VMware Research, USA
  - **TL;DR:** This paper presents RACoon, a scalable relational verifier for deep neural networks that improves the precision of verifying relational properties by utilizing cross-execution dependencies across all layers. The study addresses the limitations of existing DNN verification techniques, particularly in handling universal adversarial perturbations and other relational properties.
  - **Keywords:** relational properties, deep neural networks (DNNs), verification, scalable relational verifier, cross-execution dependencies, MILP (Mixed Integer Linear Program), autonomous driving, medical diagnosis, robustness against universal adversarial perturbations (UAP), imprecise relational properties, computational expense of verification, RACoon (the developed verifier), improved precision over SOTA baselines, universal adversarial perturbations (UAP), worst-case hamming distance


- [Neural Diffusion Models](https://icml.cc/virtual/2024/poster/32683) (Poster)
  - **Authors:** [Grigory Bartosh](http://openreview.net/profile?id=~Grigory_Bartosh1), [Dmitry Vetrov](http://openreview.net/profile?id=~Dmitry_P._Vetrov1), [Christian Andersson Naesseth](http://openreview.net/profile?id=~Christian_A._Naesseth1)
  - **Affiliations:** University of Amsterdam, Constructor University, Bremen, University of Amsterdam
  - **TL;DR:** This paper introduces Neural Diffusion Models (NDMs), a framework that allows for non-linear, time-dependent transformations in generative modeling, significantly improving the performance of diffusion models. NDMs achieve state-of-the-art results on various image generation benchmarks, including ImageNet and CelebA-HQ, by optimizing the reverse process and utilizing learnable transformations.
  - **Keywords:** generative models, diffusion models, Neural Diffusion Models (NDMs), non-linear transformations, variational bound, stochastic differential equations (SDE), ordinary differential equations (ODE), image generation, data augmentation, unsupervised learning, limitations of fixed forward processes, optimization of reverse processes, state-of-the-art results, simulation-free training, learnable transformations, MNIST, CIFAR-10, ImageNet, CelebA-HQ


- [Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies](https://icml.cc/virtual/2024/poster/34464) (Poster)
  - **Authors:** [Brian Bartoldson](http://openreview.net/profile?id=~Brian_R._Bartoldson1), [James Diffenderfer](http://openreview.net/profile?id=~James_Diffenderfer1), [Konstantinos Parasyris](http://openreview.net/profile?id=~Konstantinos_Parasyris2), [Bhavya Kailkhura](http://openreview.net/profile?id=~Bhavya_Kailkhura1)
  - **Affiliations:** Lawrence Livermore National Laboratory, Lawrence Livermore National Laboratory, Lawrence Livermore National Laboratory, Lawrence Livermore National Laboratory
  - **TL;DR:** This study investigates the challenges of achieving adversarial robustness in image classifiers, particularly on the CIFAR10 dataset, revealing that while scaling can improve performance, a plateau in robustness exists around 90%. The findings suggest that current adversarial attack formulations need rethinking to produce valid images consistent with their original labels.
  - **Keywords:** Adversarial robustness, Image classification, Adversarial training, Scaling laws, Image recognition, Adversarial machine learning, Robustness to ℓ∞-norm bounded perturbations, Limitations of current adversarial defenses, New scaling laws for adversarial robustness, Improved compute-efficient models, CIFAR10, AutoAttack


- [The Role of Learning Algorithms in Collective Action](https://icml.cc/virtual/2024/poster/34555) (Poster)
  - **Authors:** [Omri Ben-Dov](http://openreview.net/profile?id=~Omri_Ben-Dov1), [Jake Fawkes](http://openreview.net/profile?id=~Jake_Fawkes1), [Samira Samadi](http://openreview.net/profile?id=~Samira_Samadi1), [Amartya Sanyal](http://openreview.net/profile?id=~Amartya_Sanyal1)
  - **Affiliations:** Max Planck Institute for Intelligent Systems, Tübingen, Germany; Tübingen AI Center, Department of Statistics, University of Oxford, Max Planck Institute for Intelligent Systems, Tübingen, Germany; Tübingen AI Center, Max Planck Institute for Intelligent Systems, Tübingen, Germany
  - **TL;DR:** This study investigates how the choice of learning algorithms affects the success of collective action in machine learning, revealing that small collectives perform better with Distributionally Robust Optimization (DRO), while larger collectives may underperform contrary to expectations. The findings emphasize the importance of considering algorithm properties in collective action strategies.
  - **Keywords:** Collective action in machine learning, impact of learning algorithms, Distributionally Robust Optimization (DRO), Stochastic Gradient Descent (SGD), Performance on minority sub-populations, group error optimization, Higher success of small collectives with DRO, simplicity bias in learning algorithms, Bayes classifiers, empirical risk minimization (ERM), simplicity bias


- [By Tying Embeddings You Are Assuming the Distributional Hypothesis](https://icml.cc/virtual/2024/poster/32648) (Spotlight Poster)
  - **Authors:** [Bertolotti Francesco](http://openreview.net/profile?id=~Francesco_Bertolotti1), [Walter Cazzola](http://openreview.net/profile?id=~Walter_Cazzola1)
  - **Affiliations:** Department of Computer Science, Università degli Studi di Milano, Milan, Italy, Department of Computer Science, Università degli Studi di Milano, Milan, Italy
  - **TL;DR:** This study investigates the impact of tied input-output embeddings in language models, revealing that such embeddings reflect the distributional hypothesis where semantically similar words share similar representations. The findings suggest that weight tying is effective when the distributional hypothesis is valid for the data, providing insights into the organization of embeddings in foundational language models.
  - **Keywords:** tied input-output embeddings, distributional hypothesis, semantic space, Natural Language Processing (NLP), semantic organization of embeddings, effectiveness of weight tying, foundational language models, semantic equivalence


- [Refining Minimax Regret for Unsupervised Environment Design](https://icml.cc/virtual/2024/poster/34295) (Poster)
  - **Authors:** [Michael Beukman](http://openreview.net/profile?id=~Michael_Beukman1), [Samuel Coward](http://openreview.net/profile?id=~Samuel_Coward1), [Michael Matthews](http://openreview.net/profile?id=~Michael_Matthews4), [Mattie Fellows](http://openreview.net/profile?id=~Mattie_Fellows1), [Minqi Jiang](http://openreview.net/profile?id=~Minqi_Jiang1), [Michael Dennis](http://openreview.net/profile?id=~Michael_D_Dennis1), [Jakob Foerster](http://openreview.net/profile?id=~Jakob_Nicolaus_Foerster1)
  - **Affiliations:** University of Oxford, University of Oxford, University of Oxford, University of Oxford, University College London, UC Berkeley, University of Oxford
  - **TL;DR:** This study introduces Bayesian level-perfect minimax regret (BLP) as a refinement of the minimax regret objective in unsupervised environment design, addressing the issue of learning stagnation caused by training on high-regret levels. The proposed ReMiDi algorithm enables continued learning and improvement beyond the limitations of traditional minimax regret policies.
  - **Keywords:** Unsupervised Environment Design, Reinforcement Learning, Minimax Regret, Bayesian Level-Perfect MMR, ReMiDi Algorithm, Regret Stagnation, Learning Stagnation, BLP Policies, Robustness Guarantees


- [VNN: Verification-Friendly Neural Networks with Hard Robustness Guarantees](https://icml.cc/virtual/2024/poster/33424) (Poster)
  - **Authors:** [Anahita Baninajjar](http://openreview.net/profile?id=~Anahita_Baninajjar1), [Ahmed Rezine](http://openreview.net/profile?id=~Ahmed_Rezine1), [Amir Aminifar](http://openreview.net/profile?id=~Amir_Aminifar1)
  - **Affiliations:** Department of Electrical and Information Technology, Lund University, Lund, Sweden, Department of Computer and Information Science, Linköping University, Linköping, Sweden, Department of Electrical and Information Technology, Lund University, Lund, Sweden
  - **TL;DR:** This study introduces Verification-Friendly Neural Networks (VNNs) that balance prediction performance with formal verification capabilities, addressing scalability challenges in verifying Deep Neural Networks (DNNs). The proposed framework demonstrates significant efficiency improvements, allowing for verification of up to 76 times more samples while maintaining accuracy in safety-critical applications.
  - **Keywords:** Verification-Friendly Neural Networks, Deep Neural Networks, Formal Verification, Post-training optimization, Sparsity enforcement, Safety-critical applications, Medical domain (epileptic seizure detection, cardiac arrhythmia detection), Lack of formal correctness guarantees, Scalability challenges in verification, Over-approximation in verification frameworks, Robustness establishment for VNNs, Efficiency in verification time, Increased sample verification capability, MNIST, CHB-MIT, MIT-BIH, Adversarial examples, Verification frameworks


- [Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation](https://icml.cc/virtual/2024/poster/33037) (Poster)
  - **Authors:** [Luca Beurer-Kellner](http://openreview.net/profile?id=~Luca_Beurer-Kellner1), [Marc Fischer](http://openreview.net/profile?id=~Marc_Fischer1), [Martin Vechev](http://openreview.net/profile?id=~Martin_Vechev1)
  - **Affiliations:** Department of Computer Science, ETH Zürich, Switzerland, Department of Computer Science, ETH Zürich, Switzerland, Department of Computer Science, ETH Zürich, Switzerland
  - **TL;DR:** This paper presents DOMINO, a novel decoding algorithm for large language models that enforces syntactic constraints during text generation with minimal performance overhead. The method significantly improves task accuracy and can achieve nearly double the speed of traditional unconstrained decoding approaches.
  - **Keywords:** Large Language Models, Constrained Generation, Constrained Decoding, Subword Alignment, Speculative Decoding, Performance Overhead, Task Accuracy, Token Misalignment, DOMINO Algorithm, Speedup in Decoding


- [Sarah Frank-Wolfe: Methods for Constrained Optimization with Best Rates and Practical Features](https://icml.cc/virtual/2024/poster/33715) (Poster)
  - **Authors:** [Aleksandr Beznosikov](http://openreview.net/profile?id=~Aleksandr_Beznosikov1), [David Dobre](http://openreview.net/profile?id=~David_Dobre1), [Gauthier Gidel](http://openreview.net/profile?id=~Gauthier_Gidel1)
  - **Affiliations:** Innopolis University, Russia; Skolkovo Institute of Science and Technology, Russia; Moscow Institute of Physics and Technology, Russia; Yandex, Russia, Universite de’ Montreal and Mila, Canada, Universite de’ Montreal and Mila, Canada; Canada CIFAR AI Chair
  - **TL;DR:** This paper presents two new variants of the Frank-Wolfe method for stochastic finite-sum minimization, achieving the best convergence rates for both convex and non-convex functions while avoiding the need for large batches or full gradients. The proposed methods are experimentally validated to demonstrate faster theoretical rates compared to existing approaches.
  - **Keywords:** Constrained optimization, Stochastic optimization, Frank-Wolfe method, Stochastic finite-sum minimization, Machine learning, Empirical risk minimization, Large datasets, Expensive full gradient computation, Projection-free methods, New variants of Frank-Wolfe algorithms, Improved convergence guarantees, Linear minimization oracle (LMO), Conditional Gradient algorithm


- [Monotone Individual Fairness](https://icml.cc/virtual/2024/poster/34933) (Poster)
  - **Authors:** [Yahav Bechavod](http://openreview.net/profile?id=~Yahav_Bechavod1)
  - **Affiliations:** Department of Computer and Information Sciences, University of Pennsylvania
  - **TL;DR:** This paper addresses the challenge of ensuring individual fairness in online learning by extending existing frameworks to allow for more flexible auditing schemes. The authors present new algorithms that improve both predictive accuracy and fairness, achieving better bounds on regret and fairness violations compared to previous methods.
  - **Keywords:** individual fairness, online learning, predictive accuracy, monotone aggregation functions, oracle-efficient algorithms, lending, hiring, education, healthcare, fairness violations, regret in online learning, improved bounds for oracle-efficient algorithms, computational efficiency, Lipschitz condition, adversarial online learning


- [Position: Scaling Simulation is Neither Necessary Nor Sufficient for In-the-Wild Robot Manipulation](https://icml.cc/virtual/2024/poster/34363) (Poster)
  - **Authors:** [Homanga Bharadhwaj](http://openreview.net/profile?id=~Homanga_Bharadhwaj1)
  - **Affiliations:** The Robotics Institute, School of Computer Science, Carnegie Mellon University
  - **TL;DR:** This paper critiques the reliance on scaling robotic simulations for achieving effective real-world manipulation, arguing that such scaling is neither necessary nor sufficient for developing robots that can generalize across diverse tasks while adhering to human preferences. The authors emphasize the unique challenges of real-world environments and the need for more principled approaches in robotic manipulation research.
  - **Keywords:** robotic manipulation, real-world deployment, human preferences, scaling simulation, zero-shot generalization, dynamic environments, high-dimensional state-space


- [CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations](https://icml.cc/virtual/2024/poster/33364) (Poster)
  - **Authors:** [Jules Berman](http://openreview.net/profile?id=~Jules_Berman1), [Benjamin Peherstorfer](http://openreview.net/profile?id=~Benjamin_Peherstorfer2)
  - **Affiliations:** Courant Institute of Mathematical Sciences, New York University, New York, NY 10012, USA, Courant Institute of Mathematical Sciences, New York University, New York, NY 10012, USA
  - **TL;DR:** This study presents Continuous Low-Rank Adaptation (CoLoRA) for efficiently modeling the evolution of solution fields in parameterized partial differential equations, allowing for rapid predictions under varying physics parameters and initial conditions. CoLoRA significantly outperforms classical methods in speed and accuracy, particularly in data-scarce scenarios.
  - **Keywords:** Reduced modeling, Continuous Low-Rank Adaptation (CoLoRA), Parameterized Partial Differential Equations (PDEs), Low-Rank Adaptation (LoRA), Galerkin-optimal approximations, Science and engineering, Fluid mechanics, Heat transfer, Data scarcity, High computational cost of numerical methods, Kolmogorov barrier, Nonlinear reduced modeling, Efficient approximation of solution fields


- [Best of Both Worlds Guarantees for Smoothed Online Quadratic Optimization](https://icml.cc/virtual/2024/poster/33346) (Poster)
  - **Authors:** [Neelkamal Bhuyan](http://openreview.net/profile?id=~Neelkamal_Bhuyan1), [Debankur Mukherjee](http://openreview.net/profile?id=~Debankur_Mukherjee1), [Adam Wierman](http://openreview.net/profile?id=~Adam_Wierman1)
  - **Affiliations:** H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA, USA, H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, GA, USA, Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena, CA, USA
  - **TL;DR:** This study investigates the smoothed online quadratic optimization (SOQO) problem, presenting a new online optimal algorithm called Lazy Adaptive Interpolation (LAI) that performs well in both adversarial and stochastic settings. The findings highlight a trade-off between adversarial and stochastic performance, leading to a best-of-both-worlds algorithm that balances robust adversarial performance with near-optimal stochastic outcomes.
  - **Keywords:** Smoothed Online Quadratic Optimization (SOQO), Adversarial and Stochastic Settings, Lazy Adaptive Interpolation (LAI), Dynamic Interpolation Algorithm, Smart Grid Management, Adaptive Control, Data Center Management, Quadratic Hitting Cost, ℓ2-norm Switching Cost, Stochastic Analysis, Online Optimal Algorithm, Stochastic-Adversarial Trade-off


- [Controlling Behavioral Diversity in Multi-Agent Reinforcement Learning](https://icml.cc/virtual/2024/poster/32991) (Poster)
  - **Authors:** [Matteo Bettini](http://openreview.net/profile?id=~Matteo_Bettini1), [Ryan Kortvelesy](http://openreview.net/profile?id=~Ryan_Kortvelesy1), [Amanda Prorok](http://openreview.net/profile?id=~Amanda_Prorok1)
  - **Affiliations:** Department of Computer Science, University of Cambridge, Cambridge, UK, Department of Computer Science, University of Cambridge, Cambridge, UK, Department of Computer Science, University of Cambridge, Cambridge, UK
  - **TL;DR:** This study introduces Diversity Control (DiCo), a method for precisely controlling behavioral diversity in Multi-Agent Reinforcement Learning (MARL) without altering the learning objective. The method is theoretically validated and empirically demonstrated to enhance performance and sample efficiency in MARL tasks.
  - **Keywords:** Multi-Agent Reinforcement Learning (MARL), Behavioral Diversity, Diversity Control (DiCo), Actor-Critic Algorithms, Cooperative and Competitive Tasks in MARL, Controlling diversity to an exact value, Sample Efficiency, Novel method for diversity control, Theoretical proofs of diversity achievement, Policy Parameter Sharing, Homogeneous and Heterogeneous Policies


- [Total Variation Distance Meets Probabilistic Inference](https://icml.cc/virtual/2024/poster/34925) (Poster)
  - **Authors:** [Arnab Bhattacharyya](http://openreview.net/profile?id=~Arnab_Bhattacharyya1), [Sutanu Gayen](http://openreview.net/profile?id=~Sutanu_Gayen1), [Kuldeep S. Meel](http://openreview.net/profile?id=~Kuldeep_S._Meel2), [Dimitrios Myrisiotis](http://openreview.net/profile?id=~Dimitrios_Myrisiotis1), [A. Pavan](http://openreview.net/profile?id=~A._Pavan1), [N. Vinodchandran](http://openreview.net/profile?id=~N._V._Vinodchandran2)
  - **Affiliations:** School of Computing, National University of Singapore, Singapore, Department of Computer Science and Engineering, Indian Institute of Technology Kanpur, India, Department of Computer Science, University of Toronto, Canada, CNRS@CREATE LTD., Singapore, Department of Computer Science, Iowa State University, USA, School of Computing, University of Nebraska - Lincoln, USA
  - **TL;DR:** This paper establishes a novel connection between total variation distance estimation and probabilistic inference, leading to a fully polynomial randomized approximation scheme for estimating TV distances in Bayesian networks with small treewidth. The findings provide efficient algorithms for statistical distance estimation, enhancing the understanding of high-dimensional distributions.
  - **Keywords:** Total Variation Distance, Probabilistic Inference, Graphical Models, Fully Polynomial Randomized Approximation Scheme (FPRAS), Relative Approximation, Bayesian Networks, High-Dimensional Distributions, Estimation of TV Distances, Approximation Schemes, New notion of partial couplings, Efficient algorithms for TV distance estimation, Bayes nets, Treewidth


- [Naive Bayes Classifiers over Missing Data: Decision and Poisoning](https://icml.cc/virtual/2024/poster/34069) (Poster)
  - **Authors:** [Song Bian](http://openreview.net/profile?id=~Song_Bian4), [Xiating Ouyang](http://openreview.net/profile?id=~Xiating_Ouyang1), [ZHIWEI FAN](http://openreview.net/profile?id=~ZHIWEI_FAN1), [Paris Koutris](http://openreview.net/profile?id=~Paraschos_Koutris1)
  - **Affiliations:** Department of Computer Sciences, University of Wisconsin-Madison, Madison WI, USA, Department of Computer Sciences, University of Wisconsin-Madison, Madison WI, USA, Department of Computer Sciences, University of Wisconsin-Madison, Madison WI, USA, Department of Computer Sciences, University of Wisconsin-Madison, Madison WI, USA
  - **TL;DR:** This study investigates the certifiable robustness of Naive Bayes Classifiers on dirty datasets with missing values, demonstrating efficient algorithms for determining robustness and addressing data poisoning attacks. The findings suggest that understanding data quality can reduce the need for exhaustive data cleaning processes.
  - **Keywords:** certifiable robustness, machine learning classifiers, dirty datasets, Naive Bayes Classifiers (NBC), polynomial time algorithms, missing values, data cleaning, data poisoning attacks, efficient algorithms, robustness decision-making


- [Biharmonic Distance of Graphs and its Higher-Order Variants: Theoretical Properties with Applications to Centrality and Clustering](https://icml.cc/virtual/2024/poster/35046) (Poster)
  - **Authors:** [Mitchell Black](http://openreview.net/profile?id=~Mitchell_Black1), [Lucy Lin](http://openreview.net/profile?id=~Lucy_Lin1), [Weng-Keen Wong](http://openreview.net/profile?id=~Weng-Keen_Wong1), [Amir Nayyeri](http://openreview.net/profile?id=~Amir_Nayyeri1)
  - **Affiliations:** School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon, USA, School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon, USA, School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon, USA, School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon, USA
  - **TL;DR:** This study investigates the biharmonic distance as a variant of effective resistance in graphs, demonstrating its significance in measuring edge importance to global topology. The authors introduce clustering algorithms based on this distance and explore its applications in edge centrality and graph clustering.
  - **Keywords:** biharmonic distance, effective resistance, graph connectivity, clustering algorithms, k-harmonic distance, centrality, graph clustering, measuring distance between vertices, global topology of graphs, theoretical results connecting biharmonic distance to graph connectivity measures


- [Position: Explain to Question not to Justify](https://icml.cc/virtual/2024/poster/33069) (Poster)
  - **Authors:** [Przemyslaw Biecek](http://openreview.net/profile?id=~Przemyslaw_Biecek2), [Wojciech Samek](http://openreview.net/profile?id=~Wojciech_Samek1)
  - **Affiliations:** MI2.AI, University of Warsaw, Poland; MI2.AI, Warsaw University of Technology, Poland, Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, Germany; Department of Electrical Engineering and Computer Science, Technical University of Berlin, Germany; BIFOLD - Berlin Institute for the Foundations of Learning and Data, Germany
  - **TL;DR:** This paper discusses the current crisis in Explainable Artificial Intelligence (XAI) due to conflicting goals and highlights the under-explored area of model/validation-oriented explanations (RED XAI) as a promising research avenue. The authors argue for the need for more methods to enhance explainability and ensure the safety of AI systems.
  - **Keywords:** Explainable Artificial Intelligence (XAI), RED XAI, BLUE XAI, AI safety, model validation, Divergent goals in XAI, flawed AI models, need for explainability methods, New challenges in RED XAI, methods for questioning models, Interpretability, trust, fairness, causal reasoning


- [Improving fine-grained understanding in image-text pre-training](https://icml.cc/virtual/2024/poster/34962) (Poster)
  - **Authors:** [Ioana Bica](http://openreview.net/profile?id=~Ioana_Bica1), [Anastasija Ilic](http://openreview.net/profile?id=~Anastasija_Ilic1), [Matthias Bauer](http://openreview.net/profile?id=~Matthias_Bauer1), [Goker Erdogan](http://openreview.net/profile?id=~Goker_Erdogan1), [Matko Bošnjak](http://openreview.net/profile?id=~Matko_Bo%C5%A1njak2), [Christos Kaplanis](http://openreview.net/profile?id=~Christos_Kaplanis2), [Alexey Gritsenko](http://openreview.net/profile?id=~Alexey_A._Gritsenko1), [Matthias Minderer](http://openreview.net/profile?id=~Matthias_Minderer1), [Charles Blundell](http://openreview.net/profile?id=~Charles_Blundell1), [Razvan Pascanu](http://openreview.net/profile?id=~Razvan_Pascanu1), [Jovana Mitrovic](http://openreview.net/profile?id=~Jovana_Mitrovic1)
  - **Affiliations:** Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, Zurich, Switzerland, Google DeepMind, Zurich, Switzerland, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK
  - **TL;DR:** This paper introduces SPARse fine-grained Contrastive alignment (SPARC) to enhance multimodal representations from image-text pairs, addressing the challenge of fine-grained visual information loss. The method shows improved performance on both coarse-grained and fine-grained tasks, including classification and object detection.
  - **Keywords:** fine-grained multimodal representations, image-text pre-training, SPARse fine-grained Contrastive alignment (SPARC), sparse similarity metric, fine-grained sequence-wise loss, image classification, retrieval, object detection, segmentation, discarding fine-grained visual information, poor performance on localization, counting, understanding spatial relationships, improved performance on fine-grained tasks, learning representations that encode global and local information


- [Generalization in Kernel Regression Under Realistic Assumptions](https://icml.cc/virtual/2024/poster/34118) (Spotlight Poster)
  - **Authors:** [Daniel Barzilai](http://openreview.net/profile?id=~Daniel_Barzilai1), [Ohad Shamir](http://openreview.net/profile?id=~Ohad_Shamir1)
  - **Affiliations:** Weizmann Institute of Science, Weizmann Institute of Science
  - **TL;DR:** This paper provides a unified theory to upper bound the excess risk of kernel regression under realistic assumptions, demonstrating that many kernels exhibit self-regularization properties that lead to good generalization even in high-dimensional settings. The findings highlight the implications for understanding generalization in over-parameterized models, particularly neural networks.
  - **Keywords:** kernel regression, generalization, over-parameterization, self-regularization, eigenvalue perturbation bounds, neural networks, high-dimensional data, bias-variance tradeoff, overfitting, noise fitting, upper bounds for excess risk, convergence rates for regularized regression, Neural Tangent Kernel (NTK), Gaussian Process Kernel (GPK)


- [Standardized Interpretable Fairness Measures for Continuous Risk Scores](https://icml.cc/virtual/2024/poster/34651) (Poster)
  - **Authors:** [Ann-Kristin Becker](http://openreview.net/profile?id=~Ann-Kristin_Becker1), [Oana Dumitrasc](http://openreview.net/profile?id=~Oana_Dumitrasc1), [Klaus Broelemann](http://openreview.net/profile?id=~Klaus_Broelemann1)
  - **Affiliations:** SCHUFA Holding AG, Wiesbaden, Germany, SCHUFA Holding AG, Wiesbaden, Germany, SCHUFA Holding AG, Wiesbaden, Germany
  - **TL;DR:** This paper proposes a standardized method for measuring fairness in continuous risk scores using the Wasserstein distance, which effectively quantifies group disparities and outperforms traditional ROC-based measures. The findings highlight the importance of monitoring biases across different models and populations to ensure non-discriminatory decision-making processes.
  - **Keywords:** fairness measures, group disparities, continuous risk scores, Wasserstein distance, standardized fairness measures, finance, education, social media, medicine, algorithmic fairness, discrimination based on protected characteristics, biases in risk scoring, novel approach to quantifying group disparities, monitoring bias over time, ROC-based fairness measures


- [Why do Variational Autoencoders Really Promote Disentanglement?](https://icml.cc/virtual/2024/poster/34754) (Poster)
  - **Authors:** [Pratik Bhowal](http://openreview.net/profile?id=~Pratik_Bhowal1), [Achint Soni](http://openreview.net/profile?id=~Achint_Soni1), [Sirisha Rambhatla](http://openreview.net/profile?id=~Sirisha_Rambhatla1)
  - **Affiliations:** NVIDIA, India, Department of Computer Science, University of Waterloo, Ontario, Canada, Department of Management Science and Engineering, University of Waterloo, Ontario, Canada
  - **TL;DR:** This study investigates why Variational Autoencoders (VAEs) effectively promote disentangled representation learning (DRL), focusing on the orthogonality properties of the decoder. The findings provide theoretical insights and experimental corroboration, emphasizing the importance of understanding DRL capabilities in real-world VAEs.
  - **Keywords:** Disentangled Representation Learning (DRL), Variational Autoencoders (VAEs), Computer Vision, Text-based Media Generation, Understanding disentanglement in real-world VAEs, latent space rotation, Theoretical establishment of orthogonality properties of the decoder, dSprites dataset


- [Scale-Free Image Keypoints Using Differentiable Persistent Homology](https://icml.cc/virtual/2024/poster/33468) (Poster)
  - **Authors:** [Giovanni Barbarani](http://openreview.net/profile?id=~Giovanni_Barbarani1), [Francesco Vaccarino](http://openreview.net/profile?id=~Francesco_Vaccarino1), [Gabriele Trivigno](http://openreview.net/profile?id=~Gabriele_Trivigno1), [Marco Guerra](http://openreview.net/profile?id=~Marco_Guerra1), [Gabriele Berton](http://openreview.net/profile?id=~Gabriele_Berton1), [Carlo Masone](http://openreview.net/profile?id=~Carlo_Masone1)
  - **Affiliations:** Department of Mathematical Sciences "Giuseppe Luigi Lagrange", Politecnico di Torino, Italy, Department of Mathematical Sciences "Giuseppe Luigi Lagrange", Politecnico di Torino, Italy, Department of Control and Computer Engineering, Politecnico di Torino, Italy; Department of Control and Computer Engineering, Politecnico di Torino, Italy, Institut Fourier, Université Grenoble Alpes, France, Department of Control and Computer Engineering, Politecnico di Torino, Italy, Department of Control and Computer Engineering, Politecnico di Torino, Italy
  - **TL;DR:** This paper presents MorseDet, a novel keypoint detection model that utilizes Morse theory and persistent homology to address scale dependency and flexibility issues in existing methods. The proposed approach demonstrates competitive performance in keypoint repeatability, marking a significant advancement in topology-based learning for feature detection.
  - **Keywords:** keypoint detection, computer vision, topological learning, Morse theory, persistent homology, differentiable loss function, robotics, image retrieval, visual localization, SLAM, 3D reconstruction, place recognition, scale dependency, lack of flexibility in existing methods, MorseDet (topology-based learning model), competitive performance in keypoint repeatability


- [Dynamic Survival Analysis with Controlled Latent States](https://icml.cc/virtual/2024/poster/32720) (Poster)
  - **Authors:** [Linus Bleistein](http://openreview.net/profile?id=~Linus_Bleistein1), [Van NGUYEN](http://openreview.net/profile?id=~Van_Tuan_NGUYEN1), [Adeline Fermanian](http://openreview.net/profile?id=~Adeline_Fermanian1), [Agathe Guilloux](http://openreview.net/profile?id=~Agathe_Guilloux1)
  - **Affiliations:** Inria Paris, F-75015 Paris, France; Centre de Recherche des Cordeliers, INSERM, Université de Paris, Sorbonne Université, F-75006 Paris, France; LaMME, UEVE and UMR 8071, Paris Saclay University, F-91042, Evry, France, Inria Paris, F-75015 Paris, France; LOPF, Califrais’ Machine Learning Lab, Paris, France; Laboratoire de Probabilités, Statistique et Modélisation, LPSM, Univ. Paris Cité, F-75005, Paris, France, LOPF, Califrais’ Machine Learning Lab, Paris, France, Inria Paris, F-75015 Paris, France; Centre de Recherche des Cordeliers, INSERM, Université de Paris, Sorbonne Université, F-75006 Paris, France
  - **TL;DR:** This study introduces a novel approach for learning individual-specific intensities of counting processes using controlled differential equations and neural estimators. The proposed models, including a signature-based estimator called CoxSig, demonstrate strong performance across various real-world datasets in fields such as finance and healthcare.
  - **Keywords:** survival analysis, time-to-event data, individual-specific intensities, controlled differential equations, neural controlled differential equations, signature-based estimator (CoxSig), finance, predictive maintenance, food supply chain management, healthcare, predicting event occurrence times, modeling individual-specific intensity, handling time-dependent data, theoretical learning guarantees, flexible models for intensity prediction, counting processes, Cox models, Hawkes processes, deep architectures


- [Stability Evaluation through Distributional Perturbation Analysis](https://icml.cc/virtual/2024/poster/33827) (Poster)
  - **Authors:** [Jose Blanchet](http://openreview.net/profile?id=~Jose_Blanchet1), [Peng Cui](http://openreview.net/profile?id=~Peng_Cui1), [Jiajin Li](http://openreview.net/profile?id=~Jiajin_Li2), [Jiashuo Liu](http://openreview.net/profile?id=~Jiashuo_Liu1)
  - **Affiliations:** Department of Management Science and Engineering, Stanford University, Department of Computer Science and Technology, Tsinghua University, Department of Management Science and Engineering, Stanford University, Department of Management Science and Engineering, Stanford University; Department of Computer Science and Technology, Tsinghua University
  - **TL;DR:** This paper proposes a stability evaluation criterion based on distributional perturbations to assess the reliability of learning models in out-of-sample environments. The authors demonstrate the practical utility of their criterion through empirical studies, addressing challenges such as data corruptions and sub-population shifts.
  - **Keywords:** stability evaluation, distributional perturbations, optimal transport (OT) discrepancy, strong duality theorem, healthcare, economics, self-driving, out-of-sample performance, data corruptions, sub-population shifts, stability evaluation criterion, tractable convex formulations, computational methods


- [Multi-Patch Prediction: Adapting Language Models for Time Series Representation Learning](https://icml.cc/virtual/2024/poster/34033) (Poster)
  - **Authors:** [Yuxuan Bian](http://openreview.net/profile?id=~Yuxuan_Bian1), [Xuan Ju](http://openreview.net/profile?id=~Xuan_Ju1), [Jiangtong Li](http://openreview.net/profile?id=~Jiangtong_Li2), [Zhijian Xu](http://openreview.net/profile?id=~Zhijian_Xu1), [Dawei Cheng](http://openreview.net/profile?id=~Dawei_Cheng1), [Qiang Xu](http://openreview.net/profile?id=~Qiang_Xu1)
  - **Affiliations:** The Chinese University of Hong Kong; Tongji University, The Chinese University of Hong Kong, Tongji University, The Chinese University of Hong Kong, Tongji University, The Chinese University of Hong Kong
  - **TL;DR:** This study introduces aLLM4TS, a framework that adapts Large Language Models for time-series representation learning by reconceptualizing forecasting as a self-supervised, multi-patch prediction task. The framework demonstrates superior performance in capturing temporal dynamics and enhancing transferability across various downstream tasks.
  - **Keywords:** time-series representation learning, self-supervised learning, multi-patch prediction, Large Language Models (LLMs), patch-wise decoding, causal continual pre-training, time-series analysis, forecasting, classification, anomaly detection, challenges in generating comprehensive time-series representations, temporal dynamics capture, aLLM4TS framework, enhanced transferability of temporal representations


- [Beyond ELBOs: A Large-Scale Evaluation of Variational Methods for Sampling](https://icml.cc/virtual/2024/poster/33461) (Poster)
  - **Authors:** [Denis Blessing](http://openreview.net/profile?id=~Denis_Blessing1), [Xiaogang Jia](http://openreview.net/profile?id=~Xiaogang_Jia1), [Johannes Esslinger](http://openreview.net/profile?id=~Johannes_Esslinger1), [Francisco Vargas](http://openreview.net/profile?id=~Francisco_Vargas1), [Gerhard Neumann](http://openreview.net/profile?id=~Gerhard_Neumann2)
  - **Affiliations:** Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany, Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany, Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany, University of Cambridge, Cambridge, United Kingdom, Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany
  - **TL;DR:** This study introduces a comprehensive benchmark for evaluating variational methods for sampling, addressing the challenges of disparate performance measures and mode collapse. The findings highlight the strengths and weaknesses of existing methods, providing valuable insights for future developments in sampling techniques.
  - **Keywords:** Variational Inference, Monte Carlo methods, sampling methods, Annealed Importance Sampling (AIS), Sequential Monte Carlo (SMC), integral probability metrics (IPMs), maximum mean discrepancy, Wasserstein distance, Bayesian statistics, natural sciences, Intractable probability distributions, mode collapse, lack of standardized evaluation protocols, Benchmark for evaluating sampling methods, novel metrics for quantifying mode collapse


- [Shifted Interpolation for Differential Privacy](https://icml.cc/virtual/2024/poster/34351) (Poster)
  - **Authors:** [Jinho Bok](http://openreview.net/profile?id=~Jinho_Bok1), [Weijie Su](http://openreview.net/profile?id=~Weijie_J_Su1), [Jason Altschuler](http://openreview.net/profile?id=~Jason_Altschuler1)
  - **Affiliations:** Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA, USA, Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA, USA, Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA, USA
  - **TL;DR:** This paper presents a refined analysis of differential privacy in the context of noisy gradient descent algorithms, establishing the "privacy amplification by iteration" phenomenon within the f-differential privacy framework. The findings lead to tighter privacy bounds and the first exact privacy analysis for strongly convex optimization settings.
  - **Keywords:** differential privacy, private optimization, machine learning, noisy gradient descent, f-differential privacy, shifted interpolated processes, privacy leakage, tight characterizations, convex optimization, privacy amplification by iteration, exact privacy analysis, generalizations beyond divergence-based relaxations, convex losses, strongly convex optimization, stochastic gradient descent


- [Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features](https://icml.cc/virtual/2024/poster/34393) (Poster)
  - **Authors:** [Simone Bombari](http://openreview.net/profile?id=~Simone_Bombari1), [Marco Mondelli](http://openreview.net/profile?id=~Marco_Mondelli1)
  - **Affiliations:** Institute of Science and Technology, Austria, Institute of Science and Technology, Austria
  - **TL;DR:** This study investigates the concept of word sensitivity in attention layers compared to fully connected architectures, demonstrating that attention layers exhibit higher word sensitivity, which enhances their generalization capabilities in NLP tasks. The findings suggest that the softmax function in attention layers plays a crucial role in this property, leading to better performance in capturing contextual meaning.
  - **Keywords:** word sensitivity, attention layers, transformers, natural language processing, random features, softmax, BERT-Base, NLP tasks, contextual meaning, semantic changes, generalization bounds, high word sensitivity, low word sensitivity in random features, imdb review dataset


- [Position: Machine Learning-powered Assessments of the EU Digital Services Act Aid Quantify Policy Impacts on Online Harms](https://icml.cc/virtual/2024/poster/33384) (Poster)
  - **Authors:** [Eleonora Bonel](http://openreview.net/profile?id=~Eleonora_Bonel1), [Luca Nannini](http://openreview.net/profile?id=~Luca_Nannini1), [Davide Bassi](http://openreview.net/profile?id=~Davide_Bassi2), [Michele Maggini](http://openreview.net/profile?id=~Michele_Joshua_Maggini2)
  - **Affiliations:** École d’Affaires Publique, Sciences Po, Paris, France, Centro Singular de Investigación en Tecnoloxías Intelixentes da USC (CiTIUS), Santiago de Compostela, Spain; Minsait, Indra Sistemas SA, Madrid, Spain, Centro Singular de Investigación en Tecnoloxías Intelixentes da USC (CiTIUS), Santiago de Compostela, Spain, Centro Singular de Investigación en Tecnoloxías Intelixentes da USC (CiTIUS), Santiago de Compostela, Spain
  - **TL;DR:** The paper evaluates the impact of the EU Digital Services Act on mitigating online harms, particularly disinformation, while highlighting the potential of machine learning techniques to assess regulatory effectiveness. It calls for improved data access and methodological approaches to better understand the DSA's real-world implications.
  - **Keywords:** Machine Learning, Digital Services Act, Online Harms, Disinformation, Large Language Models, Generative Modelling, Online Platforms, Policy Evaluation, Disinformation Spread, Algorithmic Accountability, Content Moderation, Quantifying Policy Impacts, Data-Driven Approaches


- [Improving Neural Additive Models with Bayesian Principles](https://icml.cc/virtual/2024/poster/35179) (Poster)
  - **Authors:** [Kouroche Bouchiat](http://openreview.net/profile?id=~Kouroche_Bouchiat1), [Alexander Immer](http://openreview.net/profile?id=~Alexander_Immer1), [Hugo Yèche](http://openreview.net/profile?id=~Hugo_Y%C3%A8che1), [Gunnar Ratsch](http://openreview.net/profile?id=~Gunnar_Ratsch1), [Vincent Fortuin](http://openreview.net/profile?id=~Vincent_Fortuin1)
  - **Affiliations:** ETH Zürich, Zürich, Switzerland; None, ETH Zürich, Zürich, Switzerland; Max Planck Institute for Intelligent Systems, Tübingen, Germany, ETH Zürich, Zürich, Switzerland, ETH Zürich, Zürich, Switzerland, Helmholtz AI, Munich, Germany; TU Munich, Munich, Germany; Munich Center for Machine Learning, Munich, Germany
  - **TL;DR:** This study proposes a Bayesian variant of Neural Additive Models (LA-NAMs) that enhances model transparency and uncertainty estimation while enabling feature selection and interaction ranking. The proposed method demonstrates improved performance on tabular datasets and real-world medical tasks compared to existing models.
  - **Keywords:** Neural Additive Models, Bayesian Principles, Transparency in Deep Learning, Laplace Approximation, Empirical Bayes Procedure, Tabular Datasets, Medical Tasks, Lack of Calibrated Uncertainties, Overconfidence in Models, Feature Selection, Improved Uncertainty Estimation, Feature Interaction Selection, Generalized Additive Models, Deep Neural Networks


- [Applying language models to algebraic topology: generating simplicial cycles using multi-labeling in Wu's formula](https://icml.cc/virtual/2024/poster/33428) (Poster)
  - **Authors:** [Kirill Brilliantov](http://openreview.net/profile?id=~Kirill_Brilliantov1), [Fedor Pavutnitskiy](http://openreview.net/profile?id=~Fedor_Pavutnitskiy1), [Dmitrii A. Pasechniuk](http://openreview.net/profile?id=~Dmitry_Pasechnyuk1), [German Magai](http://openreview.net/profile?id=~German_Magai1)
  - **Affiliations:** ETH, Zurich, Beijing Institute of Mathematical Sciences and Applications, Beijing; Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi; Ivannikov Institute for System Programming of the Russian Academy of Sciences, Moscow, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi; Ivannikov Institute for System Programming of the Russian Academy of Sciences, Moscow, HSE University, Moscow; Noeon Research, Tokyo
  - **TL;DR:** This paper explores the use of machine learning to generate simplicial cycles in algebraic topology, specifically through the lens of Wu's formula. The authors propose a framework that reformulates the problem as sampling from algorithmic datasets, aiming to enhance understanding of the group-theoretic structure of homotopy groups.
  - **Keywords:** algebraic topology, homotopy groups, machine learning, language modeling, multi-labeling, simplicial groups, mathematical research, generating simplicial cycles, sampling from intersections of normal subgroups, proof-of-concept framework, algorithmic datasets, Dyck languages, Wu’s formula, simplicial cycles


- [Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation](https://icml.cc/virtual/2024/poster/33343) (Poster)
  - **Authors:** [Gavin Brown](http://openreview.net/profile?id=~Gavin_R_Brown1), [Krishnamurthy Dvijotham](http://openreview.net/profile?id=~Krishnamurthy_Dj_Dvijotham1), [Georgina Evans](http://openreview.net/profile?id=~Georgina_Evans1), [Daogao Liu](http://openreview.net/profile?id=~Daogao_Liu1), [Adam Smith](http://openreview.net/profile?id=~Adam_Smith1), [Abhradeep Guha Thakurta](http://openreview.net/profile?id=~Abhradeep_Guha_Thakurta1)
  - **Affiliations:** Paul G Allen School of Computer Science and Engineering, University of Washington; Boston University, Google DeepMind, Google DeepMind, Paul G Allen School of Computer Science and Engineering, University of Washington; Boston University, Department of Computer Science, Boston University, Google DeepMind
  - **TL;DR:** This paper presents an improved analysis of differentially private gradient descent for linear regression, demonstrating that the sample complexity can be linearly dependent on the data dimension. The authors derive tighter error bounds and establish methods for constructing adaptive conﬁdence intervals for the empirical optimizer.
  - **Keywords:** differential privacy, linear regression, gradient descent, noisy gradient descent (DP-GD), Gaussian distribution, machine learning, statistical learning, sample complexity, parameter estimation, privacy distortion, tighter error bounds, conﬁdence intervals, empirical optimizer, Gaussian process, ordinary least squares estimator


- [Semantically-correlated memories in a dense associative model](https://icml.cc/virtual/2024/poster/33231) (Poster)
  - **Authors:** [Thomas F Burns](http://openreview.net/profile?id=~Thomas_F_Burns1)
  - **Affiliations:** Institute for Computational and Experimental Research in Mathematics, Brown University, USA; SciAI Center, Cornell University, USA; Neural Coding and Brain Computing Unit, OIST Graduate University, Japan
  - **TL;DR:** This study introduces the Correlated Dense Associative Memory (CDAM) model, which integrates auto- and hetero-association for continuous-valued memory patterns. The model demonstrates effectiveness in real-world applications, including image retrieval and simulating finite automata, while revealing distinct dynamical modes of memory association.
  - **Keywords:** associative memory, continuous-valued memory patterns, Correlated Dense Associative Memory (CDAM), anti-Hebbian learning rules, image retrieval, neuroscience experiments, finite automata simulation, multi-scale representations, dynamic attractors


- [Differentially Private Bias-Term Fine-tuning of Foundation Models](https://icml.cc/virtual/2024/poster/33451) (Poster)
  - **Authors:** [Zhiqi Bu](http://openreview.net/profile?id=~Zhiqi_Bu1), [Yu-Xiang Wang](http://openreview.net/profile?id=~Yu-Xiang_Wang1), [Sheng Zha](http://openreview.net/profile?id=~Sheng_Zha1), [George Karypis](http://openreview.net/profile?id=~George_Karypis1)
  - **Affiliations:** Amazon AI, Amazon AI; University of California, San Diego, Amazon AI, Amazon AI
  - **TL;DR:** This study introduces differentially private bias-term fine-tuning (DP-BiTFiT) for large pre-trained models, achieving state-of-the-art accuracy while significantly improving computational efficiency. The method allows for effective fine-tuning on sensitive data without the extensive overhead typically associated with differential privacy.
  - **Keywords:** Differential Privacy, Fine-tuning, Large Pre-trained Models, Differentially Private Bias-Term Fine-tuning (DP-BiTFiT), Parameter Efficient Fine-tuning, Language Tasks, Vision Tasks, Privacy Constraints, Computational Overhead, High Accuracy, Efficiency in Fine-tuning


- [Fully-Dynamic Approximate Decision Trees With Worst-Case Update Time Guarantees](https://icml.cc/virtual/2024/poster/33567) (Poster)
  - **Authors:** [Marco Bressan](http://openreview.net/profile?id=~Marco_Bressan4), [Mauro Sozio](http://openreview.net/profile?id=~Mauro_Sozio2)
  - **Affiliations:** Department of Computer Science, University of Milan, Italy, Institut Polytechnique de Paris, Telecom Paris
  - **TL;DR:** This study presents a novel algorithm for maintaining decision trees in a fully-dynamic setting, ensuring both high-quality trees and efficient update times. The algorithm achieves optimal update time guarantees while addressing the challenges posed by adversarial sequences of data insertions and deletions.
  - **Keywords:** Fully-dynamic decision trees, Machine learning, Gini gain, Decision tree algorithms (ID3, C4.5), Data mining, Classification, Regression, Maintaining decision trees under adversarial updates, Dynamic dataset updates, Algorithms with worst-case update time guarantees, Quality maintenance of decision trees


- [Provably Neural Active Learning Succeeds via Prioritizing Perplexing Samples](https://icml.cc/virtual/2024/poster/33232) (Poster)
  - **Authors:** [Dake Bu](http://openreview.net/profile?id=~Dake_Bu1), [Wei Huang](http://openreview.net/profile?id=~Wei_Huang6), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1), [Ji Cheng](http://openreview.net/profile?id=~Ji_Cheng1), [Qingfu Zhang](http://openreview.net/profile?id=~Qingfu_Zhang1), [Zhiqiang Xu](http://openreview.net/profile?id=~zhiqiang_xu1), [Hau-San Wong](http://openreview.net/profile?id=~Hau-San_Wong1)
  - **Affiliations:** Department of Computer Science, City University of Hong Kong, Hong Kong SAR, Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, Department of Mathematical Informatics, the University of Tokyo, Tokyo, Japan, Department of Computer Science, City University of Hong Kong, Hong Kong SAR, Department of Computer Science, City University of Hong Kong, Hong Kong SAR, Mohamed bin Zayed University of Artificial Intelligence, Masdar, United Arab Emirates, Department of Computer Science, City University of Hong Kong, Hong Kong SAR
  - **TL;DR:** This study provides a unified explanation for the success of uncertainty-based and diversity-based query criteria in Neural Network-based Active Learning (NAL) by demonstrating that both prioritize samples containing yet-to-be-learned features. The findings indicate that these strategies lead to lower test errors with smaller labeled datasets compared to passive learning approaches.
  - **Keywords:** Neural Network-based Active Learning (NAL), Deep Active Learning (DAL), Uncertainty-based query criteria, Diversity-based query criteria, 2-layer Neural Networks, Data selection, Sample labeling, Inadequate learning of yet-to-be-learned features, Large test error in passive learning, Unified explanation for query criteria success, Small test error with small labeled set, Feature-noise data model, Easy-to-learn features, Hard-to-learn features


- [Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More](https://icml.cc/virtual/2024/poster/34903) (Poster)
  - **Authors:** [Fanchen Bu](http://openreview.net/profile?id=~Fanchen_Bu1), [Hyeonsoo Jo](http://openreview.net/profile?id=~Hyeonsoo_Jo1), [Soo Yong Lee](http://openreview.net/profile?id=~Soo_Yong_Lee1), [Sungsoo Ahn](http://openreview.net/profile?id=~Sungsoo_Ahn1), [Kijung Shin](http://openreview.net/profile?id=~Kijung_Shin2)
  - **Affiliations:** School of Electrical Engineering, KAIST, Daejeon, Republic of Korea; AI4CO Open-Source Community, Kim Jaechul Graduate School of Artificial Intelligence, KAIST, Seoul, Republic of Korea, Kim Jaechul Graduate School of Artificial Intelligence, KAIST, Seoul, Republic of Korea, Graduate School of Artificial Intelligence, POSTECH, Pohang, Republic of Korea; Department of Computer Science and Engineering, Pohang, Republic of Korea, Kim Jaechul Graduate School of Artificial Intelligence, KAIST, Seoul, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea
  - **TL;DR:** This study addresses challenges in unsupervised combinatorial optimization by deriving nontrivial objectives and derandomization methods under common conditions. The authors validate their approach through extensive experiments, demonstrating improvements in optimization quality and speed.
  - **Keywords:** Combinatorial optimization, Unsupervised learning for combinatorial optimization, Probabilistic method, Derandomization, Discrete optimization, Cardinality constraints, Derandomization challenges, Nontrivial objectives, UCOM2 (Unsupervised Combinatorial Optimization Under Commonly-involved Conditions)


- [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://icml.cc/virtual/2024/poster/34133) (Poster)
  - **Authors:** [Tianle Cai](http://openreview.net/profile?id=~Tianle_Cai1), [Yuhong Li](http://openreview.net/profile?id=~Yuhong_Li2), [Zhengyang Geng](http://openreview.net/profile?id=~Zhengyang_Geng1), [Hongwu Peng](http://openreview.net/profile?id=~Hongwu_Peng1), [Jason Lee](http://openreview.net/profile?id=~Jason_D._Lee1), [Deming Chen](http://openreview.net/profile?id=~Deming_Chen1), [Tri Dao](http://openreview.net/profile?id=~Tri_Dao1)
  - **Affiliations:** Princeton University; Together AI, University of Illinois Urbana-Champaign, Carnegie Mellon University, University of Connecticut, Princeton University, University of Illinois Urbana-Champaign, Princeton University; Together AI
  - **TL;DR:** The paper introduces MEDUSA, a framework that enhances the inference speed of Large Language Models by adding extra decoding heads for parallel token prediction. Experiments show that MEDUSA-1 achieves over 2.2× speedup without quality loss, while MEDUSA-2 further improves speedup to 2.3-2.8×.
  - **Keywords:** Large Language Models, Inference Acceleration, Auto-regressive decoding, Tree-based attention mechanism, Speculative decoding, Natural Language Processing, Inference latency, Memory-bandwidth bottleneck, MEDUSA framework, Fine-tuning procedures, Self-distillation, Acceptance scheme


- [Langevin Policy for Safe Reinforcement Learning](https://icml.cc/virtual/2024/poster/32699) (Poster)
  - **Authors:** [Fenghao Lei](http://openreview.net/profile?id=~Fenghao_Lei1), [Long Yang](http://openreview.net/profile?id=~Long_Yang4), [Shiting Wen](http://openreview.net/profile?id=~Shiting_Wen1), [Zhixiong Huang](http://openreview.net/profile?id=~Zhixiong_Huang1), [Zhiwang Zhang](http://openreview.net/profile?id=~Zhiwang_Zhang1), [Chaoyi Pang](http://openreview.net/profile?id=~Chaoyi_Pang2)
  - **Affiliations:** College of Computer Science and Technology, Zhejiang University, Hangzhou, China, School of Artificial Intelligence, Peking University, Beijing, China, School of Computer and Data Engineering, NingboTech University, Ningbo, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, School of Computer and Data Engineering, NingboTech University, Ningbo, China, School of Computer and Data Engineering, NingboTech University, Ningbo, China
  - **TL;DR:** This paper introduces the Langevin policy for safe reinforcement learning and proposes the Langevin Actor-Critic (LAC) method to enhance policy inference. The results demonstrate the effectiveness of LAC in achieving safe policies in various RL tasks, particularly in environments like MuJoCo and Safety Gym.
  - **Keywords:** Safe Reinforcement Learning, Policy Optimization, Langevin Policy, Langevin Actor-Critic (LAC), Monte Carlo Sampling, Autonomous Driving, Robot Control, Safety in Reinforcement Learning, Exploration vs. Exploitation Trade-off, New Policy Learning Approach, Empirical Results on MuJoCo and Safety Gym, MuJoCo, Safety Gym


- [Sample-specific Masks for Visual Reprogramming-based Prompting](https://icml.cc/virtual/2024/poster/35002) (Spotlight Poster)
  - **Authors:** [Chengyi Cai](http://openreview.net/profile?id=~Chengyi_Cai2), [Zesheng Ye](http://openreview.net/profile?id=~Zesheng_Ye1), [Lei Feng](http://openreview.net/profile?id=~Lei_Feng1), [Jianzhong Qi](http://openreview.net/profile?id=~Jianzhong_Qi1), [Feng Liu](http://openreview.net/profile?id=~Feng_Liu2)
  - **Affiliations:** School of Computing and Information Systems, The University of Melbourne, School of Computing and Information Systems, The University of Melbourne, Information Systems Technology and Design Pillar, Singapore University of Technology and Design, School of Computing and Information Systems, The University of Melbourne, School of Computing and Information Systems, The University of Melbourne; Information Systems Technology and Design Pillar, Singapore University of Technology and Design
  - **TL;DR:** This paper introduces a new framework for visual reprogramming called sample-specific multi-channel masks (SMM), which generates individual masks for each sample to improve generalization and reduce approximation error. The proposed method demonstrates performance gains over existing state-of-the-art visual reprogramming techniques on various tasks.
  - **Keywords:** Visual Reprogramming (VR), Model Re-purposing, Sample-specific multi-channel masks (SMM), ConvNet, Patch-wise interpolation, Image classification, Medical data prediction, Generalization capability, Approximation error, Sample-level adaptation, Reduction of approximation error, Performance gain on ResNet and ViT, ImageNet, OxfordPets dataset


- [How Spurious Features are Memorized: Precise Analysis for Random and NTK Features](https://icml.cc/virtual/2024/poster/33096) (Poster)
  - **Authors:** [Simone Bombari](http://openreview.net/profile?id=~Simone_Bombari1), [Marco Mondelli](http://openreview.net/profile?id=~Marco_Mondelli1)
  - **Affiliations:** Institute of Science and Technology, Austria, Institute of Science and Technology, Austria
  - **TL;DR:** This study provides a theoretical framework to understand how deep learning models memorize spurious features that are uncorrelated with the learning task. It characterizes the memorization process through model stability and feature alignment, demonstrating that increased generalization capability reduces the memorization of spurious features.
  - **Keywords:** deep learning, spurious features, overfitting, random features (RF), neural tangent kernel (NTK) regression, memorization of spurious features, generalization error, feature alignment characterization, stability of models, MNIST, CIFAR-10


- [Random matrix theory improved Fréchet mean of symmetric positive definite matrices](https://icml.cc/virtual/2024/poster/32828) (Poster)
  - **Authors:** [Florent Bouchard](http://openreview.net/profile?id=~Florent_Bouchard1), [Ammar Mian](http://openreview.net/profile?id=~Ammar_Mian1), [Malik TIOMOKO](http://openreview.net/profile?id=~Malik_Tiomoko1), [Guillaume GINOLHAC](http://openreview.net/profile?id=~Guillaume_Ginolhac1), [Frederic Pascal](http://openreview.net/profile?id=~Frederic_Pascal1)
  - **Affiliations:** Université Paris Saclay, CNRS, Centrale-Supélec, L2S, Université Savoie Mont Blanc, LISTIC, Huawei Paris Research Center, Université Savoie Mont Blanc, LISTIC, Université Paris Saclay, CNRS, Centrale-Supélec, L2S
  - **TL;DR:** This study introduces a random matrix theory-based method for estimating Fréchet means of symmetric positive definite matrices, which is particularly effective in scenarios with low sample support and high dimensionality. Experimental results demonstrate significant improvements over existing state-of-the-art methods in various machine learning applications.
  - **Keywords:** covariance matrices, symmetric positive definite matrices, Fréchet means, machine learning, random matrix theory, iterative algorithms, Riemannian gradient, EEG, remote sensing, deep learning networks, metric learning, domain adaptation, low sample support, high intra-class variability, numerical unfeasibility, singular matrices, improved estimation of Fréchet means, regularization techniques, synthetic datasets, real-world EEG datasets, hyperspectral datasets, Karcher mean, nearest centroid, Bures-Wasserstein geometry, log-Euclidean geometry


- [Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion](https://icml.cc/virtual/2024/poster/34588) (Poster)
  - **Authors:** [Yang Cai](http://openreview.net/profile?id=~Yang_Cai1), [Argyris Oikonomou](http://openreview.net/profile?id=~Argyris_Oikonomou1), [Weiqiang Zheng](http://openreview.net/profile?id=~Weiqiang_Zheng1)
  - **Affiliations:** Department of Computer Science, Yale University, New Haven, USA, Department of Computer Science, Yale University, New Haven, USA, Department of Computer Science, Yale University, New Haven, USA
  - **TL;DR:** This paper presents accelerated algorithms for constrained comonotone min-max optimization, achieving an optimal convergence rate of O(1/T) and ensuring point convergence to the solution set. The study extends existing algorithms to address limitations in previous research, contributing to the understanding of nonconvex-nonconcave optimization problems.
  - **Keywords:** nonconvex-nonconcave min-max optimization, comonotone optimization, Extra Anchored Gradient (EAG), Fast Extra Gradient (FEG), game theory, optimization, online learning, finding first-order stationary points, convergence rates, optimal convergence rate of O(1/T), point convergence guarantees, GANs (Generative Adversarial Networks), robust optimization, reinforcement learning


- [On dimensionality of feature vectors in MPNNs](https://icml.cc/virtual/2024/poster/33934) (Poster)
  - **Authors:** [César Bravo](http://openreview.net/profile?id=~C%C3%A9sar_Bravo1), [Alexander Kozachinskiy](http://openreview.net/profile?id=~Alexander_Kozachinskiy1), [Cristobal Rojas](http://openreview.net/profile?id=~Cristobal_Rojas1)
  - **Affiliations:** Instituto de Ingeniería Matemática y Computacional, Universidad Católica de Chile; Centro Nacional de Inteligencia Artificial, Chile, Centro Nacional de Inteligencia Artificial, Chile; Instituto Milenio Fundamentos de los Datos, Chile, Instituto de Ingeniería Matemática y Computacional, Universidad Católica de Chile; Centro Nacional de Inteligencia Artificial, Chile
  - **TL;DR:** This paper demonstrates that for message-passing graph neural networks (MPNNs), feature vectors of dimension d = 1 are sufficient to ensure equivalence to the Weisfeiler-Leman (WL) isomorphism test, regardless of graph size. This finding addresses the gap between theoretical guarantees and practical architectures in graph representation learning.
  - **Keywords:** message-passing graph neural networks (MPNNs), expressive power, graph isomorphism, Weisfeiler-Leman (WL) test, non-polynomial analytic activation functions, graph data representation, deep learning, distinguishing non-isomorphic graphs, dimensionality of feature vectors, reduced dimensionality of feature vectors to d = 1 for MPNNs


- [Can a Few Decide for Many? The Metric Distortion of Sortition](https://icml.cc/virtual/2024/poster/33104) (Poster)
  - **Authors:** [Ioannis Caragiannis](http://openreview.net/profile?id=~Ioannis_Caragiannis1), [Evi Micha](http://openreview.net/profile?id=~Evi_Micha1), [Jannik Peters](http://openreview.net/profile?id=~Jannik_Peters1)
  - **Affiliations:** Department of Computer Science, Aarhus University, Aarhus, Denmark, Computer Science, Harvard University, Cambridge, USA, Research Group Efficient Algorithms, Faculty IV – Electrical Engineering and Computer Science, TU Berlin, Berlin, Germany
  - **TL;DR:** The study investigates whether randomly selected sortition panels accurately reflect the opinions of the entire population. It finds that uniform selection can achieve almost optimal decision alignment with minimal panel sizes, and introduces Fair Greedy Capture as a method that maintains these guarantees while ensuring fairness.
  - **Keywords:** sortition, citizens' assemblies, representation, fairness, metric distortion, uniform selection, Fair Greedy Capture, societal issues, climate change, AI threats, representation of the population, decision alignment with population preferences, almost optimal distortion, constant ex-post distortion


- [Enhancing Cross-Modal Fine-Tuning with Gradually Intermediate Modality Generation](https://icml.cc/virtual/2024/poster/33763) (Poster)
  - **Authors:** [Lincan Cai](http://openreview.net/profile?id=~Lincan_Cai1), [Shuang Li](http://openreview.net/profile?id=~Shuang_Li6), [Wenxuan Ma](http://openreview.net/profile?id=~Wenxuan_Ma2), [Jingxuan Kang](http://openreview.net/profile?id=~Jingxuan_Kang1), [Binhui Xie](http://openreview.net/profile?id=~Binhui_Xie1), [Zixun Sun](http://openreview.net/profile?id=~Zixun_Sun1), [Chengwei Zhu](http://openreview.net/profile?id=~Chengwei_Zhu1)
  - **Affiliations:** Beijing Institute of Technology, Beijing Institute of Technology, Beijing Institute of Technology, University of Illinois Urbana-Champaign, Beijing Institute of Technology, Interactive Entertainment Group, Tencent, Interactive Entertainment Group, Tencent
  - **TL;DR:** This paper presents PaRe, an end-to-end method for enhancing cross-modal fine-tuning by generating intermediate modalities to bridge the modality gap and address data scarcity. The proposed approach demonstrates superior performance across multiple benchmarks compared to existing methods.
  - **Keywords:** Cross-modal fine-tuning, Multimodal perception, Gating mechanism, Patch Replacement scheme, Protein sequence analysis, Cosmic ray data analysis, Modality gap, Data scarcity, Enhanced stability and transferability of fine-tuning, Intermediate modality generation, Large-scale pretrained models, Multimodal large language models


- [How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers](https://icml.cc/virtual/2024/poster/35051) (Spotlight Poster)
  - **Authors:** [Gon Buzaglo](http://openreview.net/profile?id=~Gon_Buzaglo1), [Itamar Harel](http://openreview.net/profile?id=~Itamar_Harel1), [Mor Shpigel Nacson](http://openreview.net/profile?id=~Mor_Shpigel_Nacson1), [Alon Brutzkus](http://openreview.net/profile?id=~Alon_Brutzkus1), [Nati Srebro](http://openreview.net/profile?id=~Nathan_Srebro1), [Daniel Soudry](http://openreview.net/profile?id=~Daniel_Soudry1)
  - **Affiliations:** Technion Institute of Technology, Haifa, Israel, Technion Institute of Technology, Haifa, Israel, Technion Institute of Technology, Haifa, Israel, Technion Institute of Technology, Haifa, Israel, Toyota Technological Institute at Chicago, Chicago IL, USA, Technion Institute of Technology, Haifa, Israel
  - **TL;DR:** This study investigates why over-parameterized Neural Networks generalize well when trained to zero loss, demonstrating that random Neural Networks sampled from a uniform prior can generalize effectively if there exists a narrow teacher network that aligns with the labels. The findings reveal that this sampling method induces a bias towards simpler functions, allowing for efficient learning with reduced sample complexity.
  - **Keywords:** Neural Networks, Generalization, Over-parameterization, Stochastic Gradient Descent (SGD), Uniform Sampling, Generalization of Neural Networks, Implicit Bias, Generalization Guarantees, Sample Complexity, Teacher Neural Network, Interpolating Neural Networks


- [Successor Features for Efficient Multi-Subject Controlled Text Generation](https://icml.cc/virtual/2024/poster/34300) (Poster)
  - **Authors:** [Meng Cao](http://openreview.net/profile?id=~Meng_Cao3), [Mehdi Fatemi](http://openreview.net/profile?id=~Mehdi_Fatemi1), [Jackie Chi Kit Cheung](http://openreview.net/profile?id=~Jackie_CK_Cheung1), [Samira Shabanian](http://openreview.net/profile?id=~Samira_Shabanian1)
  - **Affiliations:** School of Computer Science, McGill University; Mila – Québec AI Institute; Work was done during internship at Microsoft Research, Wand X, School of Computer Science, McGill University; Mila – Québec AI Institute; Canada CIFAR AI Chair, None
  - **TL;DR:** This study introduces SF-GEN, a novel framework for controllable text generation that utilizes successor features to enhance the efficiency of large language models in generating text with specific attributes. The method demonstrates comparable performance to state-of-the-art approaches while being more efficient in handling multiple control subjects.
  - **Keywords:** controllable text generation, large language models (LLMs), reinforcement learning, action-value function, successor features, natural language generation (NLG), controlling generated text for safety, factuality, non-toxicity, efficiency in multi-subject control, SF-GEN method, memory-efficient and computationally efficient training and decoding


- [Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design](https://icml.cc/virtual/2024/poster/33257) (Poster)
  - **Authors:** [Andrew Campbell](http://openreview.net/profile?id=~Andrew_Campbell4), [Jason Yim](http://openreview.net/profile?id=~Jason_Yim1), [Regina Barzilay](http://openreview.net/profile?id=~Regina_Barzilay1), [Tom Rainforth](http://openreview.net/profile?id=~Tom_Rainforth1), [Tommi Jaakkola](http://openreview.net/profile?id=~Tommi_S._Jaakkola1)
  - **Affiliations:** Department of Statistics, University of Oxford, UK, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Massachusetts, USA, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Massachusetts, USA, Department of Statistics, University of Oxford, UK, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Massachusetts, USA
  - **TL;DR:** This study introduces Discrete Flow Models (DFMs), a novel flow-based model for discrete data that enhances the capabilities of generative models to handle multimodal data, specifically in protein co-design. The approach achieves state-of-the-art performance by enabling flexible generation of protein structures and sequences.
  - **Keywords:** multimodal generative models, discrete and continuous data, Discrete Flow Models (DFMs), Continuous Time Markov Chains (CTMCs), denoising neural network, protein co-design, combining discrete and continuous data, sampling flexibility, multimodal problems, improved performance over existing diffusion-based approaches, state-of-the-art co-design performance


- [Feasibility Consistent Representation Learning for Safe Reinforcement Learning](https://icml.cc/virtual/2024/poster/34383) (Poster)
  - **Authors:** [Zhepeng Cen](http://openreview.net/profile?id=~Zhepeng_Cen1), [Yihang Yao](http://openreview.net/profile?id=~Yihang_Yao1), [Zuxin Liu](http://openreview.net/profile?id=~Zuxin_Liu1), [Ding Zhao](http://openreview.net/profile?id=~Ding_Zhao1)
  - **Affiliations:** Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University
  - **TL;DR:** This study introduces Feasibility Consistent Safe Reinforcement Learning (FCSRL), a framework that integrates representation learning with feasibility-oriented objectives to enhance safety constraint estimation in reinforcement learning. The proposed method demonstrates improved performance in safety-aware embedding and policy learning across various tasks compared to existing baselines.
  - **Keywords:** Safe Reinforcement Learning, Representation Learning, Self-supervised Learning, Feasibility Score, Healthcare, Finance, Autonomous Systems, Safety Constraints, Cost Estimation, Sparse Cost Signals, Feasibility Consistent Safe Reinforcement Learning (FCSRL), Safety-aware Embedding


- [Simple Ingredients for Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/33295) (Poster)
  - **Authors:** [Edoardo Cetin](http://openreview.net/profile?id=~Edoardo_Cetin1), [Andrea Tirinzoni](http://openreview.net/profile?id=~Andrea_Tirinzoni2), [Matteo Pirotta](http://openreview.net/profile?id=~Matteo_Pirotta1), [Alessandro Lazaric](http://openreview.net/profile?id=~Alessandro_Lazaric2), [Yann Ollivier](http://openreview.net/profile?id=~Yann_Ollivier2), [Ahmed Touati](http://openreview.net/profile?id=~Ahmed_Touati1)
  - **Affiliations:** Sakana AI, Tokyo, Japan; Meta, FAIR at Meta, Paris, France, Meta, FAIR at Meta, Paris, France, Meta, FAIR at Meta, Paris, France, Meta, FAIR at Meta, Paris, France, Meta, FAIR at Meta, Paris, France, Meta, FAIR at Meta, Paris, France
  - **TL;DR:** This study investigates the challenges of offline reinforcement learning when using heterogeneous datasets, revealing that performance declines when combining data from different tasks. The authors introduce a new testbed (MOOD) and demonstrate that simple methods like AWAC and IQL can achieve better results with increased policy size, outperforming existing state-of-the-art algorithms.
  - **Keywords:** Offline reinforcement learning, heterogeneous data, AWAC, IQL, TD3+BC, policy constrained methods, Robotics, control tasks, Performance deterioration with diverse data, data sparsity, extrapolation issues, Introduction of MOOD testbed, improved performance with increased policy size, DeepMind Control suite, D4RL benchmark, Actor-critic algorithms, value function, expectile regression, Gumbel regression


- [Auditing Private Prediction](https://icml.cc/virtual/2024/poster/34531) (Poster)
  - **Authors:** [Karan Chadha](http://openreview.net/profile?id=~Karan_Chadha1), [Matthew Jagielski](http://openreview.net/profile?id=~Matthew_Jagielski1), [Nicolas Papernot](http://openreview.net/profile?id=~Nicolas_Papernot1), [Christopher A. Choquette Choo](http://openreview.net/profile?id=~Christopher_A._Choquette-Choo1), [Milad Nasr](http://openreview.net/profile?id=~Milad_Nasr2)
  - **Affiliations:** Stanford University; Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This paper introduces a novel framework for auditing private prediction algorithms in machine learning, focusing on the privacy leakage associated with different adversarial capabilities. The findings indicate that easier-to-poison algorithms exhibit significantly higher privacy leakage, and that adversaries with limited query control experience lower leakage compared to those with full control.
  - **Keywords:** Differential Privacy, Private Prediction, Renyi Differential Privacy, Auditing Techniques, Machine Learning, Inference Privacy, Privacy Leakage, Adversarial Attacks, Query Control, Auditing Framework, Privacy Analysis Improvements, PATE, CaPC, PromptPATE, Private-kNN


- [Limited Preference Aided Imitation Learning from Imperfect Demonstrations](https://icml.cc/virtual/2024/poster/34137) (Poster)
  - **Authors:** [Xingchen Cao](http://openreview.net/profile?id=~Xingchen_Cao1), [Fan-Ming Luo](http://openreview.net/profile?id=~Fan-Ming_Luo1), [Junyin Ye](http://openreview.net/profile?id=~Junyin_Ye1), [Tian Xu](http://openreview.net/profile?id=~Tian_Xu2), [Zhilong Zhang](http://openreview.net/profile?id=~Zhilong_Zhang2), [Yang Yu](http://openreview.net/profile?id=~Yang_Yu5)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, Nanjing, Jiangsu, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, Nanjing, Jiangsu, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, Nanjing, Jiangsu, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, Nanjing, Jiangsu, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, Nanjing, Jiangsu, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, Nanjing, Jiangsu, China
  - **TL;DR:** This paper introduces Preference Aided Imitation Learning (PAIL), a novel algorithm that enhances imitation learning by leveraging limited human preferences to improve policy performance beyond imperfect demonstrations. Empirical results demonstrate that PAIL significantly outperforms existing methods, achieving a 73.2% improvement in performance.
  - **Keywords:** Imitation Learning, Policy Learning, Preference Aided Imitation Learning (PAIL), Bradley-Terry model, Robotics, Tokamak fusion devices, Imperfect demonstrations, Performance bottleneck, Improved policy performance, Reweighting demonstrations, Human preferences, Expert data


- [Online Learning under Budget and ROI Constraints via Weak Adaptivity](https://icml.cc/virtual/2024/poster/32898) (Poster)
  - **Authors:** [Matteo Castiglioni](http://openreview.net/profile?id=~Matteo_Castiglioni1), [Andrea Celli](http://openreview.net/profile?id=~Andrea_Celli1), [Christian Kroer](http://openreview.net/profile?id=~Christian_Kroer1)
  - **Affiliations:** DEIB, Politecnico di Milano, Milan, Italy, Department of Computing Sciences, Bocconi University, Milan, Italy, IEOR Department, Columbia University, New York, NY
  - **TL;DR:** This paper addresses online learning problems where decision makers must maximize rewards while adhering to budget and ROI constraints, proposing a dual-balancing framework that circumvents traditional assumptions about feasibility. The findings include no-regret guarantees under both stochastic and adversarial conditions, with practical applications in online ad auctions.
  - **Keywords:** online learning, budget constraints, ROI constraints, primal-dual algorithms, weakly adaptive regret minimizers, online ad auctions, decision-making under constraints, adversarial inputs, non-packing constraints, dual-balancing framework, no-regret guarantees, Slater parameters, adversarial bandits, knapsack problem


- [AI Alignment with Changing and Influenceable Reward Functions](https://icml.cc/virtual/2024/poster/33333) (Poster)
  - **Authors:** [Micah Carroll](http://openreview.net/profile?id=~Micah_Carroll1), [Davis Foote](http://openreview.net/profile?id=~Davis_Foote1), [Anand Siththaranjan](http://openreview.net/profile?id=~Anand_Siththaranjan1), [Stuart Russell](http://openreview.net/profile?id=~Stuart_Russell1), [Anca Dragan](http://openreview.net/profile?id=~Anca_Dragan1)
  - **Affiliations:** UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley
  - **TL;DR:** The paper addresses the challenge of aligning AI systems with human preferences that change over time and can be influenced by AI interactions. It introduces Dynamic Reward Markov Decision Processes (DR-MDPs) to model these dynamics and highlights the risks of static-preference assumptions in existing alignment techniques.
  - **Keywords:** AI alignment, changing preferences, influenceable preferences, Dynamic Reward Markov Decision Processes (DR-MDPs), AI systems, human-AI interaction, Static-preference assumption, preference change, AI influence on preferences, New framework for modeling preference changes in AI alignment


- [Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting](https://icml.cc/virtual/2024/poster/34337) (Poster)
  - **Authors:** [Serina Chang](http://openreview.net/profile?id=~Serina_Chang1), [Frederic Koehler](http://openreview.net/profile?id=~Frederic_Koehler1), [Zhaonan Qu](http://openreview.net/profile?id=~Zhaonan_Qu1), [Jure Leskovec](http://openreview.net/profile?id=~Jure_Leskovec1), [Johan Ugander](http://openreview.net/profile?id=~Johan_Ugander1)
  - **Affiliations:** Department of Computer Science, Stanford University, Department of Statistics and Data Science Institute, University of Chicago, Department of Economics, Stanford University; Department of Management Science & Engineering, Stanford University, Department of Computer Science, Stanford University, Department of Management Science & Engineering, Stanford University
  - **TL;DR:** This study addresses the challenge of inferring dynamic networks from time-aggregated adjacency matrices and time-varying marginals using iterative proportional fitting (IPF). The authors establish a generative model that provides a statistical foundation for IPF, introduces a new algorithm to ensure convergence on sparse data, and demonstrates the practical value of their contributions through experiments.
  - **Keywords:** dynamic networks, network inference, time-aggregated adjacency matrix, iterative proportional fitting (IPF), Sinkhorn’s algorithm, epidemic response, transportation planning, mobility networks, data constraints, estimation of dynamic networks, convergence issues in sparse data, generative network model, maximum likelihood estimates, structure-dependent error bounds


- [Feature Importance Disparities for Data Bias Investigations](https://icml.cc/virtual/2024/poster/33445) (Poster)
  - **Authors:** [Peter Chang](http://openreview.net/profile?id=~Peter_W_Chang1), [Leor Fishman](http://openreview.net/profile?id=~Leor_Fishman1), [Seth Neel](http://openreview.net/profile?id=~Seth_Neel2)
  - **Affiliations:** SAFR AI Lab, Harvard Business School & Kempner Institute, Boston, MA, Harvard College, Cambridge, MA, SAFR AI Lab, Harvard Business School & Kempner Institute, Boston, MA
  - **TL;DR:** This paper presents a method for identifying feature importance disparities (FID) in training data to assist in data bias investigations, highlighting subgroups where specific features have disproportionate influence. The findings suggest that these disparities can indicate serious bias issues, providing a new approach to understanding and addressing bias in machine learning models.
  - **Keywords:** Data bias investigations, Feature importance disparity, Machine learning, Fairness in classifiers, Bias in training data, Subgroup bias, Method for identifying feature importance disparities, 4 datasets used in experiments, Feature importance disparity (FID)


- [On the Implicit Bias of Adam](https://icml.cc/virtual/2024/poster/32679) (Poster)
  - **Authors:** [Matias Cattaneo](http://openreview.net/profile?id=~Matias_D._Cattaneo1), [Jason Klusowski](http://openreview.net/profile?id=~Jason_Matthew_Klusowski1), [Boris Shigida](http://openreview.net/profile?id=~Boris_Shigida1)
  - **Affiliations:** Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ, USA, Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ, USA, Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ, USA
  - **TL;DR:** This study investigates the implicit regularization effects of adaptive optimization algorithms like RMSProp and Adam through backward error analysis, revealing that their behavior depends on hyperparameters and training stages. The findings suggest that these algorithms can influence generalization by either penalizing or impeding the reduction of the one-norm of loss gradients.
  - **Keywords:** Implicit regularization, Gradient descent, Ordinary differential equations (ODEs), RMSProp, Adam, Implicit bias in optimization algorithms, Generalization, Modified loss functions, Backward error analysis


- [Scribble-Supervised Semantic Segmentation with Prototype-based Feature Augmentation](https://icml.cc/virtual/2024/poster/34099) (Poster)
  - **Authors:** [Guiyang Chan](http://openreview.net/profile?id=~Guiyang_Chan1), [Pengcheng Zhang](http://openreview.net/profile?id=~Pengcheng_Zhang4), [Hai Dong](http://openreview.net/profile?id=~Hai_Dong1), [Shunhui Ji](http://openreview.net/profile?id=~Shunhui_Ji1), [Bainian Chen](http://openreview.net/profile?id=~Bainian_Chen1)
  - **Affiliations:** Key Laboratory of Water Big Data Technology of Ministry of Water Resources, Hohai University, Nanjing, China; College of Computer Science and Software Engineering, Hohai University, Nanjing, China, Key Laboratory of Water Big Data Technology of Ministry of Water Resources, Hohai University, Nanjing, China; College of Computer Science and Software Engineering, Hohai University, Nanjing, China, School of Computing Technologies, RMIT University, Melbourne, Australia, Key Laboratory of Water Big Data Technology of Ministry of Water Resources, Hohai University, Nanjing, China; College of Computer Science and Software Engineering, Hohai University, Nanjing, China, Key Laboratory of Water Big Data Technology of Ministry of Water Resources, Hohai University, Nanjing, China; College of Computer Science and Software Engineering, Hohai University, Nanjing, China
  - **TL;DR:** This study introduces a prototype-based feature augmentation method for scribble-supervised semantic segmentation, addressing limitations in existing methods by leveraging feature prototypes. The proposed approach achieves state-of-the-art performance on the PASCAL VOC 2012 dataset, demonstrating its effectiveness in reducing annotation costs while enhancing segmentation accuracy.
  - **Keywords:** Scribble-supervised semantic segmentation, weakly supervised learning, Prototype-based feature augmentation, Image semantic segmentation, High annotation costs, feature propagation limitations, State-of-the-art performance on PASCAL VOC 2012 dataset, PASCAL VOC 2012


- [Predictive Dynamic Fusion](https://icml.cc/virtual/2024/poster/34289) (Poster)
  - **Authors:** [Bing Cao](http://openreview.net/profile?id=~Bing_Cao1), [Yinan Xia](http://openreview.net/profile?id=~Yinan_Xia1), [Yi Ding](http://openreview.net/profile?id=~Yi_Ding8), [Changqing Zhang](http://openreview.net/profile?id=~Changqing_Zhang1), [Qinghua Hu](http://openreview.net/profile?id=~Qinghua_Hu1)
  - **Affiliations:** College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Lab of Machine Learning, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Lab of Machine Learning, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Lab of Machine Learning, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Lab of Machine Learning, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Lab of Machine Learning, Tianjin, China
  - **TL;DR:** This study introduces a Predictive Dynamic Fusion (PDF) framework for multimodal learning that addresses the challenges of dynamic data quality in open environments. The proposed method theoretically guarantees improved generalization and reliability in multimodal fusion, validated through extensive experiments.
  - **Keywords:** Multimodal fusion, Dynamic fusion, Predictive Dynamic Fusion (PDF), Collaborative Belief (Co-Belief), Relative calibration, Autonomous driving, Clinical diagnosis, Sentiment analysis, Unreliability and instability in dynamic multimodal fusion, Modality imbalance, High noise, Theoretical guarantees for multimodal fusion, Reduction of generalization error


- [MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion](https://icml.cc/virtual/2024/poster/33301) (Poster)
  - **Authors:** [Di Chang](http://openreview.net/profile?id=~Di_Chang1), [Yichun Shi](http://openreview.net/profile?id=~Yichun_Shi1), [Quankai Gao](http://openreview.net/profile?id=~Quankai_Gao1), [Hongyi Xu](http://openreview.net/profile?id=~Hongyi_Xu1), [Jessica Fu](http://openreview.net/profile?id=~Jessica_Fu1), [Guoxian Song](http://openreview.net/profile?id=~Guoxian_Song1), [Qing Yan](http://openreview.net/profile?id=~Qing_Yan1), [Yizhe Zhu](http://openreview.net/profile?id=~Yizhe_Zhu2), [Xiao Yang](http://openreview.net/profile?id=~Xiao_Yang1), [Mohammad Soleymani](http://openreview.net/profile?id=~Mohammad_Soleymani2)
  - **Affiliations:** University of Southern California; ByteDance Inc., ByteDance Inc., University of Southern California, ByteDance Inc., University of Southern California, ByteDance Inc., ByteDance Inc., ByteDance Inc., ByteDance Inc., University of Southern California
  - **TL;DR:** The study presents MagicPose, a diffusion-based model for retargeting human poses and facial expressions while preserving identity. It introduces a two-stage training strategy that enables robust control over generated images and generalizes well to unseen identities without additional fine-tuning.
  - **Keywords:** human pose retargeting, facial expression retargeting, human motion transfer, diffusion models, appearance-control block, appearance-disentangled pose control, image stylization, digital human synthesis, data generation for training perception models, generalization to unseen identities, interpolation of invisible body parts, dependency on image warping, robust appearance control, zero-shot retargeting, plug-in module for Stable Diffusion, GANs (Generative Adversarial Networks), image diffusion models


- [How Interpretable Are Interpretable Graph Neural Networks?](https://icml.cc/virtual/2024/poster/34550) (Poster)
  - **Authors:** [Yongqiang Chen](http://openreview.net/profile?id=~Yongqiang_Chen1), [Yatao Bian](http://openreview.net/profile?id=~Yatao_Bian1), [Bo Han](http://openreview.net/profile?id=~Bo_Han1), [James Cheng](http://openreview.net/profile?id=~James_Cheng2)
  - **Affiliations:** The Chinese University of Hong Kong; None, Tencent AI Lab, Hong Kong Baptist University, The Chinese University of Hong Kong
  - **TL;DR:** This study presents a theoretical framework for interpretable subgraph learning in graph neural networks, highlighting the limitations of existing methods in approximating the subgraph multilinear extension (SubMT). The proposed Graph Multilinear neT (GMT) architecture significantly improves interpretability and generalizability, outperforming state-of-the-art models by up to 10% across various benchmarks.
  - **Keywords:** Interpretable Graph Neural Networks, Explainability, Attention-based mechanism, Subgraph multilinear extension (SubMT), Graph Multilinear neT (GMT), Scientific applications, Graph-structured data, Approximation failure, Interpretability of extracted subgraphs, Out-of-Distribution (OOD) generalization, Improved interpretability and generalizability, New XGNN architecture (GMT), Graph classification benchmarks, XGNNs (eXplainable Graph Neural Networks), GNNs (Graph Neural Networks), Causal subgraph, Sampling probability


- [Doubly Robust Causal Effect Estimation under Networked Interference via Targeted Learning](https://icml.cc/virtual/2024/poster/34965) (Oral)
  - **Authors:** [Weilin Chen](http://openreview.net/profile?id=~Weilin_Chen1), [Ruichu Cai](http://openreview.net/profile?id=~Ruichu_Cai1), [Zeqin Yang](http://openreview.net/profile?id=~Zeqin_Yang1), [Jie Qiao](http://openreview.net/profile?id=~Jie_Qiao1), [Yuguang Yan](http://openreview.net/profile?id=~Yuguang_Yan1), [Zijian Li](http://openreview.net/profile?id=~Zijian_Li1), [Zhifeng Hao](http://openreview.net/profile?id=~Zhifeng_Hao5)
  - **Affiliations:** School of Computer Science, Guangdong University of Technology, Guangzhou, China; None, School of Computer Science, Guangdong University of Technology, Guangzhou, China; Pazhou Laboratory (Huangpu), Guangzhou, China, School of Computer Science, Guangdong University of Technology, Guangzhou, China, School of Computer Science, Guangdong University of Technology, Guangzhou, China, School of Computer Science, Guangdong University of Technology, Guangzhou, China, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, College of Science, Shantou University, Shantou, China
  - **TL;DR:** This study proposes a novel doubly robust causal effect estimator for networked interference by adapting targeted learning techniques, addressing the challenges of model misspecification. The proposed estimator demonstrates a faster convergence rate and effectiveness through extensive experiments on real-world networks.
  - **Keywords:** Causal effect estimation, Networked interference, Doubly robust estimator, Targeted learning, Neural networks, Epidemiology, Human ecology, Advertisement, Misspecification problems, Confounding bias, Violation of SUTVA, End-to-end causal effect estimator, Faster convergence rate, Stable Unit Treatment Value Assumption (SUTVA), Spillover effects, Main effects, Total effects


- [RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content](https://icml.cc/virtual/2024/poster/34098) (Poster)
  - **Authors:** [Zhuowen Yuan](http://openreview.net/profile?id=~Zhuowen_Yuan1), [Zidi Xiong](http://openreview.net/profile?id=~Zidi_Xiong2), [Yi Zeng](http://openreview.net/profile?id=~Yi_Zeng3), [Ning Yu](http://openreview.net/profile?id=~Ning_Yu2), [Ruoxi Jia](http://openreview.net/profile?id=~Ruoxi_Jia1), [Dawn Song](http://openreview.net/profile?id=~Dawn_Song1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19)
  - **Affiliations:** University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, Virginia Tech, Salesforce Research, Virginia Tech, University of California Berkeley, University of Chicago; University of Illinois Urbana-Champaign
  - **TL;DR:** This paper presents RigorLLM, a novel framework for moderating harmful content in Large Language Models (LLMs) that effectively addresses biases and vulnerabilities to adversarial attacks. Experimental results show that RigorLLM outperforms existing content moderation solutions and sets a new standard for AI safety in LLMs.
  - **Keywords:** Large Language Models, Content Moderation, Energy-based training, Langevin dynamics, Minimax optimization, KNN (K-Nearest Neighbors), AI Safety, AI Alignment, Biases in LLMs, Harmful content generation, Jailbreaking attacks, RigorLLM framework, Robust content moderation solutions, OpenAI API, Perspective API, Nemo Guardrails, LlamaGuard, Resilience to adversarial attacks


- [Probabilistic Forecasting with Stochastic Interpolants and Föllmer Processes](https://icml.cc/virtual/2024/poster/33945) (Poster)
  - **Authors:** [Yifan Chen](http://openreview.net/profile?id=~Yifan_Chen5), [Mark Goldstein](http://openreview.net/profile?id=~Mark_Goldstein1), [Mengjian Hua](http://openreview.net/profile?id=~Mengjian_Hua1), [Michael Albergo](http://openreview.net/profile?id=~Michael_Samuel_Albergo1), [Nicholas Boffi](http://openreview.net/profile?id=~Nicholas_Matthew_Boffi1), [Eric Vanden-Eijnden](http://openreview.net/profile?id=~Eric_Vanden-Eijnden1)
  - **Affiliations:** Courant Institute of Mathematical Sciences, New York University, New York, NY, USA, Courant Institute of Mathematical Sciences, New York University, New York, NY, USA, Courant Institute of Mathematical Sciences, New York University, New York, NY, USA, Courant Institute of Mathematical Sciences, New York University, New York, NY, USA, Courant Institute of Mathematical Sciences, New York University, New York, NY, USA, Courant Institute of Mathematical Sciences, New York University, New York, NY, USA
  - **TL;DR:** This paper presents a framework for probabilistic forecasting of dynamical systems using generative modeling and stochastic interpolants, enabling the sampling of future states from the current state. The approach is validated on complex forecasting problems, demonstrating effective handling of stochastic dynamics and measurement noise.
  - **Keywords:** probabilistic forecasting, dynamical systems, generative modeling, stochastic interpolants, stochastic differential equations (SDEs), square loss regression, climate modeling, fluid dynamics, video prediction, time series data extrapolation, forecasting future states, handling stochastic dynamics, measurement noise, generative modeling approach, adjustment of drift and diffusion coefficients, Föllmer process, KTH dataset, CLEVRER dataset


- [Relational Learning in Pre-Trained Models: A Theory from Hypergraph Recovery Perspective](https://icml.cc/virtual/2024/poster/33018) (Poster)
  - **Authors:** [Yang Chen](http://openreview.net/profile?id=~Yang_Chen17), [Cong Fang](http://openreview.net/profile?id=~Cong_Fang1), [Zhouchen Lin](http://openreview.net/profile?id=~Zhouchen_Lin1), [Bing Liu](http://openreview.net/profile?id=~Bing_Liu1)
  - **Affiliations:** National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Institute for Artificial Intelligence, Peking University, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Institute for Artificial Intelligence, Peking University, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Institute for Artificial Intelligence, Peking University; Pazhou Laboratory (Huangpu), Guangzhou, China, Department of Computer Science, University of Illinois Chicago
  - **TL;DR:** This study introduces a mathematical framework that formalizes relational learning as hypergraph recovery to enhance the understanding of pre-training in Foundation Models. The findings suggest that this approach can effectively capture the complex relationships between entities, offering insights into the capabilities and generalization of pre-trained models.
  - **Keywords:** Relational learning, Foundation Models, Hypergraph recovery, Hypergraph recovery, Minimax near-optimal analysis, Multimodal learning, Entity alignment, Understanding relationships between entities, Data efficiency in pre-training, Novel mathematical framework for relational learning


- [Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation](https://icml.cc/virtual/2024/poster/34398) (Poster)
  - **Authors:** [Xuexin Chen](http://openreview.net/profile?id=~Xuexin_Chen1), [Ruichu Cai](http://openreview.net/profile?id=~Ruichu_Cai1), [Zhengting Huang](http://openreview.net/profile?id=~ZhengTingHuang1), [Yuxuan Zhu](http://openreview.net/profile?id=~Yuxuan_Zhu2), [Julien Horwood](http://openreview.net/profile?id=~Julien_Horwood1), [Zhifeng Hao](http://openreview.net/profile?id=~Zhifeng_Hao4), [Zijian Li](http://openreview.net/profile?id=~Zijian_Li1), [Jose Miguel Hernandez-Lobato](http://openreview.net/profile?id=~Jos%C3%A9_Miguel_Hern%C3%A1ndez-Lobato1)
  - **Affiliations:** School of Computer Science, Guangdong University of Technology, Guangzhou, China; None, School of Computer Science, Guangdong University of Technology, Guangzhou, China; Pazhou Laboratory (Huangpu), Guangzhou, China, School of Computer Science, Guangdong University of Technology, Guangzhou, China, School of Computer Science, Guangdong University of Technology, Guangzhou, China, Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, United Kingdom, Shantou University, Shantou 515063, China, Mohamed bin Zayed University of Artificial Intelligence, Masdar City, Abu Dhabi, Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, United Kingdom
  - **TL;DR:** This study introduces Feature Attribution with Necessity and Sufficiency (FANS) to enhance the discriminative power of feature attribution methods in machine learning by addressing the limitations of standard perturbation tests. The proposed method demonstrates improved performance over existing attribution methods across six benchmarks.
  - **Keywords:** Explainability, Feature Attribution, Feature Attribution Methods (FAMs), Dual-stage Perturbation Test, Counterfactual Reasoning, Machine Learning, Difficulty in distinguishing contributions of different features, Perturbation test limitations, Feature Attribution with Necessity and Sufficiency (FANS), Improved discriminative power of FAMs, Probability of being Necessity and Sufficiency (PNS), Shapley Values


- [Offline Transition Modeling via Contrastive Energy Learning](https://icml.cc/virtual/2024/poster/33532) (Poster)
  - **Authors:** [Ruifeng Chen](http://openreview.net/profile?id=~Ruifeng_Chen1), [Chengxing Jia](http://openreview.net/profile?id=~Chengxing_Jia1), [Zefang Huang](http://openreview.net/profile?id=~Zefang_Huang1), [Tian-Shuo Liu](http://openreview.net/profile?id=~Tian-Shuo_Liu1), [Xu-Hui Liu](http://openreview.net/profile?id=~Xu-Hui_Liu1), [Yang Yu](http://openreview.net/profile?id=~Yang_Yu5)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This study introduces Energy-based Transition Models (ETM) to effectively capture complex transition dynamics in offline decision-making tasks, demonstrating improved evaluation accuracy and generalization to out-of-distribution data. The proposed models significantly outperform existing off-policy evaluation methods and enhance reinforcement learning performance in benchmark tasks.
  - **Keywords:** transition modeling, sequential decision-making, offline settings, Energy-based Transition Models (ETM), Forward Transition Model (FTM), autoregressive dynamics models, Trajectory Transformer (TT), reinforcement learning, offline policy evaluation, complex transition dynamics, discontinuity, model errors, prediction uncertainty, improved evaluation accuracy, better generalization to out-of-distribution transition data, DOPE benchmark, D4RL Gym-Mujoco tasks


- [High-Dimensional Kernel Methods under Covariate Shift: Data-Dependent Implicit Regularization](https://icml.cc/virtual/2024/poster/33648) (Poster)
  - **Authors:** [Yihang Chen](http://openreview.net/profile?id=~Yihang_Chen1), [Fanghui Liu](http://openreview.net/profile?id=~Fanghui_Liu1), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1)
  - **Affiliations:** Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland, Department of Computer Science, University of Warwick, United Kingdom, Department of Mathematical Informatics, The University of Tokyo, Japan; Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan, Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland
  - **TL;DR:** This study investigates kernel ridge regression under covariate shifts, focusing on the impact of importance re-weighting on bias and variance. The findings reveal that the re-weighting strategy can reduce variance while the bias may vary significantly depending on the regularization scale, highlighting the need for refined analyses in high-capacity models.
  - **Keywords:** kernel ridge regression, covariate shift, importance weighting, bias-variance decomposition, high-dimensional data, model overfitting, data-dependent regularization, asymptotic expansion of kernel functions, Radon-Nikodym derivative, spectral decay


- [CogDPM: Diffusion Probabilistic Models via Cognitive Predictive Coding](https://icml.cc/virtual/2024/poster/33359) (Poster)
  - **Authors:** [Kaiyuan Chen](http://openreview.net/profile?id=~Kaiyuan_Chen3), [Xingzhuo Guo](http://openreview.net/profile?id=~Xingzhuo_Guo1), [Yu Zhang](http://openreview.net/profile?id=~Yu_Zhang76), [Jianmin Wang](http://openreview.net/profile?id=~Jianmin_Wang1), [Mingsheng Long](http://openreview.net/profile?id=~Mingsheng_Long5)
  - **Affiliations:** School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University
  - **TL;DR:** This study introduces Cognitive Diffusion Probabilistic Models (CogDPM) that leverage Predictive Coding theory to enhance prediction skills in real-world forecasting tasks. The results demonstrate that CogDPM outperforms existing models by effectively estimating data predictability through a precision weighting mechanism.
  - **Keywords:** Predictive Coding, Cognitive Diffusion Probabilistic Models, Diffusion probabilistic models, hierarchical sampling, Weather forecasting, real-world prediction tasks, Enhancement of prediction skills, precision weighting mechanism, Precision estimation method, improved forecasting capabilities, United Kingdom precipitation dataset, ERA surface wind dataset


- [Towards AutoAI: Optimizing a Machine Learning System with Black-box and Differentiable Components](https://icml.cc/virtual/2024/poster/34370) (Poster)
  - **Authors:** [Zhiliang Chen](http://openreview.net/profile?id=~Zhiliang_Chen1), [Chuan-Sheng Foo](http://openreview.net/profile?id=~Chuan-Sheng_Foo1), [Bryan Kian Hsiang Low](http://openreview.net/profile?id=~Bryan_Kian_Hsiang_Low1)
  - **Affiliations:** Department of Computer Science, National University of Singapore, Singapore; Institute for Infocomm Research, A*STAR, Singapore, Institute for Infocomm Research, A*STAR, Singapore; Centre for Frontier AI Research, A*STAR, Singapore, Department of Computer Science, National University of Singapore, Singapore
  - **TL;DR:** This study introduces A-BAD-BO, an algorithm designed to optimize complex machine learning systems that include both differentiable and black-box components. The results indicate that A-BAD-BO achieves better system optimality and is more sample-efficient compared to traditional gradient-driven methods.
  - **Keywords:** AutoAI, machine learning optimization, complex systems, Bayesian optimization (BO), A-BAD-BO algorithm, healthcare systems, self-driving cars, large language models (LLMs), high-dimensional parameter optimization, lack of analytical form in components, improved system optimality, sample efficiency, differentiable ML components, black-box components


- [InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models](https://icml.cc/virtual/2024/poster/32966) (Poster)
  - **Authors:** [Lichang Chen](http://openreview.net/profile?id=~Lichang_Chen2), [Jiuhai Chen](http://openreview.net/profile?id=~Jiuhai_Chen1), [Tom Goldstein](http://openreview.net/profile?id=~Tom_Goldstein1), [Heng Huang](http://openreview.net/profile?id=~Heng_Huang1), [Tianyi Zhou](http://openreview.net/profile?id=~Tianyi_Zhou1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, Department of Computer Science, University of Maryland, College Park, Department of Computer Science, University of Maryland, College Park, Department of Computer Science, University of Maryland, College Park, Department of Computer Science, University of Maryland, College Park
  - **TL;DR:** This paper presents INSTRUCTZERO, a method for optimizing instructions for black-box large language models by using soft prompts and Bayesian optimization. The approach demonstrates improved performance in generating effective instructions across various tasks compared to existing methods.
  - **Keywords:** Instruction Optimization, Large Language Models, Bayesian Optimization, Soft Prompt Optimization, Zero-shot Learning, Few-shot Learning, Instruction Sensitivity, Combinatorial Optimization, Improved Instruction Generation, Enhanced Zero-shot Performance, Black-box LLMs, Open-source LLMs, Prompt Engineering


- [Accelerated Policy Gradient for s-rectangular Robust MDPs with Large State Spaces](https://icml.cc/virtual/2024/poster/34405) (Poster)
  - **Authors:** [Ziyi Chen](http://openreview.net/profile?id=~Ziyi_Chen2), [Heng Huang](http://openreview.net/profile?id=~Heng_Huang1)
  - **Affiliations:** Department of Computer Science, University of Maryland College Park, Department of Computer Science, University of Maryland College Park
  - **TL;DR:** This study presents an accelerated policy gradient algorithm for s-rectangular robust Markov decision processes, achieving improved iteration complexity in both deterministic and stochastic settings. The proposed methods enhance robustness against environmental perturbations and address the challenges of simulation-to-reality gaps in reinforcement learning applications.
  - **Keywords:** Robust Markov Decision Processes, Reinforcement Learning, Policy Gradient Methods, Accelerated Policy Gradient Algorithm, Entropy Regularization, Robotics, Energy Flow Control, Production Scheduling, Flight Control, Simulation-to-Reality Gap, Environmental Perturbation, NP-Hard Problems, Scalable Policy Gradient Methods, Iteration Complexity Reduction, s-Rectangularity, (s, a)-Rectangularity


- [Improved Communication-Privacy Trade-offs in $L_2$ Mean Estimation under Streaming Differential Privacy](https://icml.cc/virtual/2024/poster/32729) (Poster)
  - **Authors:** [Wei-Ning Chen](http://openreview.net/profile?id=~Wei-Ning_Chen1), [Berivan Isik](http://openreview.net/profile?id=~Berivan_Isik1), [Peter Kairouz](http://openreview.net/profile?id=~Peter_Kairouz1), [Albert No](http://openreview.net/profile?id=~Albert_No1), [Sewoong Oh](http://openreview.net/profile?id=~Sewoong_Oh3), [Zheng Xu](http://openreview.net/profile?id=~Zheng_Xu2)
  - **Affiliations:** Stanford University, Stanford University, Google, Yonsei University, Google; University of Washington, Google
  - **TL;DR:** This paper addresses the challenges of L2 mean estimation under central differential privacy and communication constraints in federated learning. The authors introduce a novel privacy accounting method that significantly improves communication efficiency while maintaining strong privacy guarantees, achieving at least a 100x improvement in compression for DP-SGD across various tasks.
  - **Keywords:** L2 mean estimation, differential privacy, federated learning, Gaussian mechanism, sparsification, matrix factorization, Federated learning (FL), Communication constraints, privacy protection, model update sensitivity, Improved communication-privacy trade-offs, novel privacy accounting method, DP-SGD, DP-FTRL, mean square errors (MSEs)


- [Bagged Deep Image Prior for Recovering Images in the Presence of Speckle Noise](https://icml.cc/virtual/2024/poster/34416) (Poster)
  - **Authors:** [Xi Chen](http://openreview.net/profile?id=~Xi_Chen33), [Zhewen Hou](http://openreview.net/profile?id=~Zhewen_Hou1), [Christopher Metzler](http://openreview.net/profile?id=~Christopher_Metzler1), [Arian Maleki](http://openreview.net/profile?id=~Arian_Maleki1), [Shirin Jalali](http://openreview.net/profile?id=~Shirin_Jalali1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ, USA, Department of Statistics, Columbia University, NY, USA, Department of Computer Science, University of Maryland, College Park, MD, USA, Department of Statistics, Columbia University, NY, USA, Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ, USA
  - **TL;DR:** This study presents a novel method called Bagged Deep Image Prior for recovering images affected by speckle noise, establishing a theoretical upper bound on the Mean Squared Error (MSE) of the maximum likelihood estimator. The proposed method integrates projected gradient descent with the Newton-Schulz algorithm, achieving state-of-the-art performance in image recovery.
  - **Keywords:** Image recovery, Speckle noise, Maximum likelihood estimator, Deep Image Prior, Projected gradient descent, Newton-Schulz algorithm, Coherent imaging systems, Speckle noise, Ill-conditioned measurement matrix, Bagged Deep Image Prior, Theoretical upper bound on Mean Squared Error (MSE)


- [Performative Prediction with Bandit Feedback: Learning through Reparameterization](https://icml.cc/virtual/2024/poster/32675) (Poster)
  - **Authors:** [Yatong Chen](http://openreview.net/profile?id=~Yatong_Chen1), [Wei Tang](http://openreview.net/profile?id=~Wei_Tang1), [Chien-Ju Ho](http://openreview.net/profile?id=~Chien-Ju_Ho1), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu3)
  - **Affiliations:** Department of Computer Science and Engineering, University of California, Santa Cruz, California, United States, Data Science Institute, Columbia University; Department of Decisions, Operations, and Technology, the Chinese University of Hong Kong, Department of Computer Science and Engineering, Washington University in St. Louis, Department of Computer Science and Engineering, University of California, Santa Cruz, California, United States
  - **TL;DR:** This paper introduces a reparameterization framework for performative prediction that addresses the challenges of non-convex objectives and provides a two-level optimization procedure. The approach allows for the transformation of the performative risk into a convex form, achieving sublinear regret guarantees in relation to the number of performative samples.
  - **Keywords:** Performative prediction, social prediction, data distribution changes, Reparameterization framework, zeroth-order optimization, Education, recommendation systems, criminal prediction, Non-convex objective, performative risk, Convex transformation of objectives, provable regret guarantees, Performative risk, empirical risk minimization (ERM)


- [EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism](https://icml.cc/virtual/2024/poster/32721) (Poster)
  - **Authors:** [Yanxi Chen](http://openreview.net/profile?id=~Yanxi_Chen1), [Xuchen Pan](http://openreview.net/profile?id=~Xuchen_Pan1), [Yaliang Li](http://openreview.net/profile?id=~Yaliang_Li1), [Bolin Ding](http://openreview.net/profile?id=~Bolin_Ding3), [Jingren Zhou](http://openreview.net/profile?id=~Jingren_Zhou1)
  - **Affiliations:** Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group
  - **TL;DR:** The study introduces EE-LLM, a framework designed to enhance the training and inference of early-exit large language models using 3D parallelism. It demonstrates significant improvements in training efficiency and inference speed without sacrificing output quality, addressing the challenges of high costs and latency in LLM deployment.
  - **Keywords:** early-exit large language models, large-scale training and inference, 3D parallelism, pipeline parallelism, KV caching, natural language processing, generative models, excessive costs, carbon emissions, inference latency, training efficiency, inference speedup, algorithmic innovations, Megatron-LM


- [MaSS: Multi-attribute Selective Suppression for Utility-preserving Data Transformation from an Information-theoretic Perspective](https://icml.cc/virtual/2024/poster/34947) (Poster)
  - **Authors:** [Yizhuo Chen](http://openreview.net/profile?id=~Yizhuo_Chen2), [Chun-Fu (Richard) Chen](http://openreview.net/profile?id=~Chun-Fu_Chen1), [Hsiang Hsu](http://openreview.net/profile?id=~Hsiang_Hsu1), [Shaohan Hu](http://openreview.net/profile?id=~Shaohan_Hu2), [Marco Pistoia](http://openreview.net/profile?id=~Marco_Pistoia2), [Tarek Abdelzaher](http://openreview.net/profile?id=~Tarek_F._Abdelzaher1)
  - **Affiliations:** Department of Computer Science, University of Illinois Urbana-Champaign, USA; Global Technology Applied Research, JPMorgan Chase, USA, Global Technology Applied Research, JPMorgan Chase, USA, Global Technology Applied Research, JPMorgan Chase, USA, Global Technology Applied Research, JPMorgan Chase, USA, Global Technology Applied Research, JPMorgan Chase, USA, Department of Computer Science, University of Illinois Urbana-Champaign, USA
  - **TL;DR:** This study proposes a formal information-theoretic framework for utility-preserving data transformation that selectively suppresses sensitive attributes while maintaining the utility of other attributes. The method demonstrates effectiveness across various datasets and tasks, addressing significant privacy concerns in data handling.
  - **Keywords:** Data privacy protection, Utility-preserving data transformation, Information-theoretic definition, Data-driven learnable data transformation framework, Facial images, Voice audio clips, Human activity motion sensor signals, Privacy concerns, Sensitive attribute suppression, Data availability degradation, Effective and generalizable data transformation method, ImageNet, UCI HAR dataset, Common Crawl, Differential Privacy, Attribute inference attacks, Membership inference attacks


- [MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models](https://icml.cc/virtual/2024/poster/33459) (Poster)
  - **Authors:** [Justin Chih-Yao Chen](http://openreview.net/profile?id=~Justin_Chen1), [Swarnadeep Saha](http://openreview.net/profile?id=~Swarnadeep_Saha2), [Elias Stengel-Eskin](http://openreview.net/profile?id=~Elias_Stengel-Eskin1), [Mohit Bansal](http://openreview.net/profile?id=~Mohit_Bansal2)
  - **Affiliations:** UNC Chapel Hill, UNC Chapel Hill, UNC Chapel Hill, UNC Chapel Hill
  - **TL;DR:** The study introduces MAGDI, a method for structured distillation of reasoning interactions from multiple Large Language Models into smaller models, significantly improving their reasoning capabilities and efficiency. Experiments demonstrate that MAGDI outperforms existing distillation methods while enhancing generalizability and scalability.
  - **Keywords:** Multi-agent interactions, Reasoning improvement, Language models, Structured distillation, Graph encoder, Next-token prediction, Contrastive loss, Commonsense reasoning, Math reasoning, Long generations, Expensive multi-agent interactions, Lack of a final model for inference, Improved reasoning capabilities, Higher efficiency, Enhanced generalizability, Large Language Models (LLMs), Multi-agent interaction graphs (MAG), Self-consistency


- [Robust Classification via a Single Diffusion Model](https://icml.cc/virtual/2024/poster/32703) (Poster)
  - **Authors:** [Huanran Chen](http://openreview.net/profile?id=~Huanran_Chen1), [Yinpeng Dong](http://openreview.net/profile?id=~Yinpeng_Dong2), [Zhengyi Wang](http://openreview.net/profile?id=~Zhengyi_Wang1), [Xiao Yang](http://openreview.net/profile?id=~Xiao_Yang4), [Chengqi Duan](http://openreview.net/profile?id=~Chengqi_Duan1), [Hang Su](http://openreview.net/profile?id=~Hang_Su3), [Jun Zhu](http://openreview.net/profile?id=~Jun_Zhu2)
  - **Affiliations:** School of Computer Science, Beijing Institute of Technology; Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China; RealAI, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China; Zhongguancun Laboratory, Beijing, 100080, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China; RealAI
  - **TL;DR:** This study introduces the Robust Diffusion Classifier (RDC), leveraging pre-trained diffusion models to enhance adversarial robustness in image classification. RDC demonstrates superior performance against various adaptive attacks, achieving a robust accuracy of 75.67% on CIFAR-10, surpassing previous state-of-the-art models.
  - **Keywords:** Adversarial robustness, Generative classifiers, Diffusion models, Robust Diffusion Classifier (RDC), Bayes’ theorem, Image classification, Adversarial training, Vulnerability to adversarial examples, Limitations of existing methods, Improved robust accuracy, Generalizability against unseen threats, CIFAR-10


- [Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes](https://icml.cc/virtual/2024/poster/35022) (Poster)
  - **Authors:** [Yingyi Chen](http://openreview.net/profile?id=~Yingyi_Chen3), [Qinghua Tao](http://openreview.net/profile?id=~Qinghua_Tao1), [Francesco Tonin](http://openreview.net/profile?id=~Francesco_Tonin1), [Johan Suykens](http://openreview.net/profile?id=~Johan_Suykens1)
  - **Affiliations:** ESAT-STADIUS, KU Leuven, Belgium, ESAT-STADIUS, KU Leuven, Belgium, LIONS, EPFL, Switzerland, ESAT-STADIUS, KU Leuven, Belgium
  - **TL;DR:** This study introduces Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) to address the asymmetry in self-attention kernels and improve uncertainty estimation in Transformers. The proposed method reduces computational complexity while providing reliable predictions across various benchmarks.
  - **Keywords:** Transformers, Uncertainty Estimation, Bayesian Inference, Gaussian Processes (GPs), Sparse Variational Gaussian Processes (SVGP), Kernel Singular Value Decomposition (KSVD), Safety-Critical Applications, Feature Learning, Overconfident Predictions, High Complexity in GP Posteriors, Asymmetry in Attention Kernels, Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP), Evidence Lower Bound for Optimization, Self-Attention, Asymmetric Kernels, Inducing Points


- [DiJiang: Efficient Large Language Models through Compact Kernelization](https://icml.cc/virtual/2024/poster/35174) (Oral)
  - **Authors:** [Hanting Chen](http://openreview.net/profile?id=~Hanting_Chen1), [Liuzhicheng Liuzhicheng](http://openreview.net/profile?id=~Liuzhicheng2), [Xutao Wang](http://openreview.net/profile?id=~Xutao_Wang1), [Yuchuan Tian](http://openreview.net/profile?id=~Yuchuan_Tian1), [Yunhe Wang](http://openreview.net/profile?id=~Yunhe_Wang1)
  - **Affiliations:** Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, Peking University, Huawei Noah’s Ark Lab
  - **TL;DR:** This paper introduces DiJiang, a novel approach for transforming pre-trained Transformers into linear complexity models with minimal retraining costs. The method achieves comparable performance to existing models while significantly reducing training costs and improving inference speeds.
  - **Keywords:** Efficient Transformers, Large Language Models, Frequency Domain Kernelization, Linear Attention, Discrete Cosine Transform (DCT), Quasi-Monte Carlo method, Natural Language Processing (NLP), Speech Recognition, Machine Translation, Document Generation, Computational load reduction, Extensive retraining challenges, Resource constraints, Comparable performance to original Transformers, Reduced training costs, Faster inference speeds, LLaMA2-7B, DiJiang-7B, Transformer, Linear Transformers, Attention Mechanism


- [Subequivariant Reinforcement Learning in 3D Multi-Entity Physical Environments](https://icml.cc/virtual/2024/poster/33396) (Poster)
  - **Authors:** [Runfa Chen](http://openreview.net/profile?id=~Runfa_Chen1), [Ling Wang](http://openreview.net/profile?id=~Ling_Wang4), [Yu Du](http://openreview.net/profile?id=~Yu_Du8), [Tianrui Xue](http://openreview.net/profile?id=~Tianrui_Xue1), [Fuchun Sun](http://openreview.net/profile?id=~Fuchun_Sun1), [Jianwei Zhang](http://openreview.net/profile?id=~Jianwei_Zhang2), [Wenbing Huang](http://openreview.net/profile?id=~Wenbing_Huang1)
  - **Affiliations:** Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua University, Dept. of Info. Eng., Xi’an Research Institute of High-Tech, School of Elec. Eng., Naval University of Engineering, College of Arts & Sci., New York University, Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua University; THU-Bosch JCML Center, TAMS, Dept. of Informatics, University of Hamburg, Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods
  - **TL;DR:** This paper introduces Subequivariant Hierarchical Neural Networks (SHNN) to address the complexities of learning policies in multi-entity systems within 3D environments by dynamically decoupling the global state space into local views. The proposed method shows significant advancements over existing approaches and introduces a new benchmark for multi-entity reinforcement learning.
  - **Keywords:** multi-entity systems, reinforcement learning, 3D environments, Subequivariant Hierarchical Neural Networks (SHNN), subequivariant message passing, exponential complexity, global state space expansion, task assignment, Multi-entity Benchmark (MEBEN), advancements in policy learning, E(3) subequivariance, local reference frames (LRF)


- [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://icml.cc/virtual/2024/poster/34179) (Poster)
  - **Authors:** [Zixiang Chen](http://openreview.net/profile?id=~Zixiang_Chen1), [Yihe Deng](http://openreview.net/profile?id=~Yihe_Deng1), [Huizhuo Yuan](http://openreview.net/profile?id=~Huizhuo_Yuan1), [Kaixuan Ji](http://openreview.net/profile?id=~Kaixuan_Ji2), [Quanquan Gu](http://openreview.net/profile?id=~Quanquan_Gu1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, CA 90095, USA, Department of Computer Science, University of California, Los Angeles, CA 90095, USA, Department of Computer Science, University of California, Los Angeles, CA 90095, USA, Department of Computer Science, University of California, Los Angeles, CA 90095, USA, Department of Computer Science, University of California, Los Angeles, CA 90095, USA
  - **TL;DR:** This study introduces Self-Play Fine-Tuning (SPIN) as a method to enhance weak Large Language Models (LLMs) without additional human-annotated data, demonstrating significant performance improvements across various benchmarks. The findings suggest that self-play can effectively elevate LLM capabilities to human-level performance.
  - **Keywords:** Large Language Models, Self-Play Fine-Tuning, Supervised Fine-Tuning (SFT), Self-Play Fine-Tuning (SPIN), Reinforcement Learning from Human Feedback (RLHF), Need for additional human-annotated data, enhancing weak models, Improved LLM performance, alignment with target data distribution, HuggingFace Open LLM Leaderboard, MT-Bench, Big-Bench


- [GRATH: Gradual Self-Truthifying for Large Language Models](https://icml.cc/virtual/2024/poster/33571) (Poster)
  - **Authors:** [Weixin Chen](http://openreview.net/profile?id=~Weixin_Chen1), [Dawn Song](http://openreview.net/profile?id=~Dawn_Song1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19)
  - **Affiliations:** UIUC, UC Berkeley, UIUC; UChicago
  - **TL;DR:** The study introduces GRATH, a novel post-processing method aimed at enhancing the truthfulness of large language models (LLMs) by utilizing out-of-domain question prompts for training. The results demonstrate significant improvements in truthfulness metrics on the TruthfulQA benchmark, surpassing even larger models.
  - **Keywords:** truthfulness, large language models (LLMs), hallucination, GRAdual self-truTHifying (GRATH), direct preference optimization (DPO), real-world applications, safety-critical applications, generating truthful content, hallucination phenomenon, state-of-the-art performance on TruthfulQA, improvement in model truthfulness, TruthfulQA, ARC-Challenge


- [Recovering Labels from Local Updates in Federated Learning](https://icml.cc/virtual/2024/poster/34600) (Poster)
  - **Authors:** [Huancheng Chen](http://openreview.net/profile?id=~Huancheng_Chen1), [Haris Vikalo](http://openreview.net/profile?id=~Haris_Vikalo1)
  - **Affiliations:** University of Texas at Austin, Texas, USA, University of Texas at Austin, Texas, USA
  - **TL;DR:** This paper introduces a novel label recovery method, Recovering Labels from Local Updates (RLU), which effectively reconstructs labels from local updates in federated learning settings, achieving high accuracy even with multiple local training epochs and heterogeneous data. The proposed method outperforms existing techniques and enhances the quality of reconstructed images in gradient inversion attacks.
  - **Keywords:** Federated Learning, Privacy in Machine Learning, Gradient Inversion, Label Recovery, Least-Square Problem, Healthcare, Finance, Privacy Attacks, Data Reconstruction, Heterogeneous Data, Recovering Labels from Local Updates (RLU), Improved Data Reconstruction Quality


- [Diffusion Model-Augmented Behavioral Cloning](https://icml.cc/virtual/2024/poster/34142) (Poster)
  - **Authors:** [Shang-Fu Chen](http://openreview.net/profile?id=~Shang-Fu_Chen2), [Hsiang-Chun Wang](http://openreview.net/profile?id=~Hsiang-Chun_Wang1), [Ming-Hao Hsu](http://openreview.net/profile?id=~Ming-Hao_Hsu1), [Chun-Mao Lai](http://openreview.net/profile?id=~Chun-Mao_Lai1), [Shao-Hua Sun](http://openreview.net/profile?id=~Shao-Hua_Sun1)
  - **Affiliations:** National Taiwan University, Taipei, Taiwan, National Taiwan University, Taipei, Taiwan, National Taiwan University, Taipei, Taiwan, National Taiwan University, Taipei, Taiwan, National Taiwan University, Taipei, Taiwan
  - **TL;DR:** This study proposes a novel imitation learning framework called Diffusion Model-Augmented Behavioral Cloning (DBC) that combines conditional and joint probability modeling to enhance generalization in behavioral cloning. The results demonstrate that DBC outperforms existing methods in various continuous control tasks, addressing limitations in traditional approaches.
  - **Keywords:** Imitation learning, Behavioral cloning, Diffusion models, Generative models, Continuous control tasks, Navigation, Robot arm manipulation, Dexterous manipulation, Locomotion, Generalization issues, Manifold overfitting, Diffusion Model-Augmented Behavioral Cloning (DBC), Policy optimization, Conditional probability, Joint probability


- [Locally Differentially Private Decentralized Stochastic Bilevel Optimization with Guaranteed Convergence Accuracy](https://icml.cc/virtual/2024/poster/34035) (Poster)
  - **Authors:** [Ziqin Chen](http://openreview.net/profile?id=~Ziqin_Chen1), [Yongqiang Wang](http://openreview.net/profile?id=~Yongqiang_Wang1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, 29634, United States, Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, 29634, United States
  - **TL;DR:** This study presents a new decentralized stochastic bilevel-optimization algorithm that achieves both differential privacy and accurate convergence, addressing significant privacy concerns in decentralized optimization. The proposed method characterizes convergence rates under various conditions and quantifies the impact of differential privacy on these rates.
  - **Keywords:** Decentralized bilevel optimization, Differential privacy, Stochastic bilevel optimization algorithms, Machine learning, Meta-learning, Hyperparameter optimization, Imitation learning, Neural architecture search, Privacy concerns in decentralized optimization, Information exchange challenges, New decentralized stochastic bilevel-optimization algorithm, Convergence rate characterization


- [A General Framework for Learning from Weak Supervision](https://icml.cc/virtual/2024/poster/34867) (Poster)
  - **Authors:** [Hao Chen](http://openreview.net/profile?id=~Hao_Chen15), [Jindong Wang](http://openreview.net/profile?id=~Jindong_Wang1), [Lei Feng](http://openreview.net/profile?id=~Lei_Feng1), [Xiang Li](http://openreview.net/profile?id=~Xiang_Li35), [Yidong Wang](http://openreview.net/profile?id=~Yidong_Wang1), [Xing Xie](http://openreview.net/profile?id=~Xing_Xie3), [Masashi Sugiyama](http://openreview.net/profile?id=~Masashi_Sugiyama1), [Rita Singh](http://openreview.net/profile?id=~Rita_Singh1), [Bhiksha Raj](http://openreview.net/profile?id=~Bhiksha_Raj1)
  - **Affiliations:** Carnegie Mellon University, Microsoft Research; William & Mary, Singapore University of Technology and Design, Carnegie Mellon University, Peking University, Microsoft Research, RIKEN AIP; The University of Tokyo, Carnegie Mellon University; Mohamed bin Zayed University of AI, Carnegie Mellon University; Mohamed bin Zayed University of AI
  - **TL;DR:** This paper presents a general framework for learning from weak supervision (GLWS) that utilizes an Expectation-Maximization formulation to effectively handle various weak supervision sources. The proposed method significantly enhances scalability and demonstrates superior performance across multiple weak supervision scenarios.
  - **Keywords:** weakly supervised learning, scalability, practical deployment, Expectation-Maximization (EM), Non-deterministic Finite Automaton (NFA), forward-backward algorithm, applicability to various scenarios, diverse weak supervision, complexity of existing algorithms, general framework for learning from weak supervision (GLWS), improved performance across weak supervision scenarios


- [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://icml.cc/virtual/2024/poster/32930) (Poster)
  - **Authors:** [Shiqi Chen](http://openreview.net/profile?id=~Shiqi_Chen3), [Miao Xiong](http://openreview.net/profile?id=~Miao_Xiong2), [Junteng Liu](http://openreview.net/profile?id=~Junteng_Liu2), [Zhengxuan Wu](http://openreview.net/profile?id=~Zhengxuan_Wu1), [Teng Xiao](http://openreview.net/profile?id=~Teng_Xiao2), [Siyang Gao](http://openreview.net/profile?id=~Siyang_Gao1), [Junxian He](http://openreview.net/profile?id=~Junxian_He1)
  - **Affiliations:** City University of Hong Kong, National University of Singapore, Shanghai Jiao Tong University, Stanford University, Penn State University, City University of Hong Kong, HKUST
  - **TL;DR:** This study investigates the mechanisms behind hallucinations in large language models by analyzing inner representations and proposes an entropy-based metric to improve decoding accuracy. The approach demonstrates significant effectiveness in mitigating hallucinations, achieving notable improvements on various benchmarks.
  - **Keywords:** Large Language Models, Hallucination Mitigation, Entropy-based metric, Constrained decoding, Knowledge-seeking tasks, Factuality in language models, Hallucinations, Factual errors in LLMs, Improved understanding of hallucinations, Enhanced decoding approach, COUNTERFACT dataset, TruthfulQA, Inner representations, Context activations


- [Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning](https://icml.cc/virtual/2024/poster/33916) (Poster)
  - **Authors:** [Junfeng CHEN](http://openreview.net/profile?id=~Junfeng_CHEN2), [Kailiang Wu](http://openreview.net/profile?id=~Kailiang_Wu1)
  - **Affiliations:** Department of Mathematics, Southern University of Science and Technology, Shenzhen 518055, China; Shenzhen International Center for Mathematics, Southern University of Science and Technology, Shenzhen 518055, China; National Center for Applied Mathematics Shenzhen (NCAMS), Shenzhen 518055, China, Department of Mathematics, Southern University of Science and Technology, Shenzhen 518055, China; Shenzhen International Center for Mathematics, Southern University of Science and Technology, Shenzhen 518055, China; National Center for Applied Mathematics Shenzhen (NCAMS), Shenzhen 518055, China
  - **TL;DR:** This paper introduces the Position-induced Transformer (PiT), a novel approach for operator learning in Partial Differential Equations (PDEs) that utilizes a position-attention mechanism to improve efficiency and performance over traditional self-attention methods. The findings demonstrate PiT's superior capabilities in handling complex operator learning tasks and its enhanced discretization convergence compared to existing models.
  - **Keywords:** Operator learning, Partial Differential Equations (PDEs), Position-induced Transformer (PiT), position-attention mechanism, self-attention, Surrogate modeling, complex systems, High computational demands, limited interpretability, challenges in solving complex nonlinear systems, Enhanced discretization convergence, superior performance over state-of-the-art neural operators, Transformers, numerical methods for PDEs, Fourier neural operator


- [CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process](https://icml.cc/virtual/2024/poster/32914) (Poster)
  - **Authors:** [Guangyi Chen](http://openreview.net/profile?id=~Guangyi_Chen1), [Yifan Shen](http://openreview.net/profile?id=~Yifan_Shen4), [Zhenhao Chen](http://openreview.net/profile?id=~Zhenhao_Chen1), [Xiangchen Song](http://openreview.net/profile?id=~Xiangchen_Song1), [Yuewen Sun](http://openreview.net/profile?id=~Yuewen_Sun1), [Weiran Yao](http://openreview.net/profile?id=~Weiran_Yao1), [Xiao Liu](http://openreview.net/profile?id=~Xiao_Liu23), [Kun Zhang](http://openreview.net/profile?id=~Kun_Zhang1)
  - **Affiliations:** Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; Carnegie Mellon University, Pittsburg, US, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Carnegie Mellon University, Pittsburg, US, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Salesforce, San Francisco, US, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; Carnegie Mellon University, Pittsburg, US
  - **TL;DR:** This study introduces CaRiNG, a method for learning causal representations from sequential data under non-invertible generation processes, addressing challenges in identifying latent causal variables. The approach demonstrates improved temporal understanding and reasoning, validated through experiments on synthetic datasets.
  - **Keywords:** Temporal causal representation, Sequential data analysis, Independent Component Analysis (ICA), Nonlinear ICA, Video analysis, Time series data, Visual perception, Non-invertible generation process, Information loss, Causal dynamics identification, Identifiability theory, CaRiNG method for causal representation, Synthetic datasets


- [Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank](https://icml.cc/virtual/2024/poster/34374) (Poster)
  - **Authors:** [Mouxiang Chen](http://openreview.net/profile?id=~Mouxiang_Chen1), [Chenghao Liu](http://openreview.net/profile?id=~Chenghao_Liu1), [Zemin Liu](http://openreview.net/profile?id=~Zemin_Liu1), [Zhuo Li](http://openreview.net/profile?id=~Zhuo_Li11), [Jianling Sun](http://openreview.net/profile?id=~Jianling_Sun2)
  - **Affiliations:** Zhejiang University, Salesforce Research Asia, National University of Singapore, State Street Technology (Zhejiang) Ltd., Zhejiang University
  - **TL;DR:** This study investigates the conditions under which true relevance can be recovered from biased click data in Unbiased Learning to Rank (ULTR) and introduces methods to restore the connectivity of the identifiability graph. The findings highlight that a disconnected identifiability graph can lead to suboptimal ranking performance, and the proposed methods effectively mitigate data bias.
  - **Keywords:** Unbiased Learning to Rank, click data, ranking models, examination hypothesis, joint optimization, information retrieval systems, data bias, position bias, recoverability of relevance, node intervention, node merging, identifiability graph, simulated dataset, LTR benchmark datasets


- [Efficient Pareto Manifold Learning with Low-Rank Structure](https://icml.cc/virtual/2024/poster/33704) (Spotlight Poster)
  - **Authors:** [Weiyu CHEN](http://openreview.net/profile?id=~Weiyu_Chen1), [James Kwok](http://openreview.net/profile?id=~James_Kwok1)
  - **Affiliations:** Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology
  - **TL;DR:** This study presents a novel approach to multi-task learning that integrates a main network with low-rank matrices to efficiently learn the Pareto manifold, significantly reducing the number of parameters and enhancing performance on large task datasets. The proposed method outperforms existing state-of-the-art algorithms, particularly in scenarios with numerous tasks.
  - **Keywords:** Multi-task learning, Multi-objective optimization, Pareto front, Low-rank matrices, Orthogonal regularization, Scalability issues, Balancing tasks, Parameter reduction, Efficient learning of Pareto manifold, Improved performance on large task datasets, Pareto Manifold Learning (PaMaL), Hypernetwork


- [Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting](https://icml.cc/virtual/2024/poster/33041) (Poster)
  - **Authors:** [Anthony Chen](http://openreview.net/profile?id=~Anthony_Chen3), [Huanrui Yang](http://openreview.net/profile?id=~Huanrui_Yang1), [Yulu Gan](http://openreview.net/profile?id=~Yulu_Gan1), [Denis Gudovskiy](http://openreview.net/profile?id=~Denis_A_Gudovskiy1), [Zhen Dong](http://openreview.net/profile?id=~Zhen_Dong3), [Haofan Wang](http://openreview.net/profile?id=~Haofan_Wang1), [Tomoyuki Okuno](http://openreview.net/profile?id=~Tomoyuki_Okuno1), [Yohei Nakata](http://openreview.net/profile?id=~Yohei_Nakata1), [EECS Kurt Keutzer](http://openreview.net/profile?id=~Kurt_Keutzer1), [Shanghang Zhang](http://openreview.net/profile?id=~Shanghang_Zhang4)
  - **Affiliations:** School of Computer Science, Peking University, University of California, Berkeley, School of Computer Science, Peking University, Panasonic Holdings Corporation, University of California, Berkeley, Carnegie Mellon University, Panasonic Holdings Corporation, Panasonic Holdings Corporation, University of California, Berkeley, School of Computer Science, Peking University
  - **TL;DR:** This study introduces the Split-Ensemble method to enhance uncertainty estimation for deep learning models without requiring additional OOD data or incurring extra computational costs. The proposed method demonstrates significant improvements in accuracy and OOD detection across multiple datasets.
  - **Keywords:** Uncertainty estimation, Out-of-distribution (OOD) detection, Deep learning, Split-Ensemble method, Subtask-splitting ensemble training, Image classification, OOD detection, Uncalibrated uncertainty, High computational costs of ensembles, Improved accuracy, Generalizable uncertainty estimation, CIFAR-10, CIFAR-100, Tiny-ImageNet


- [Compact Optimality Verification for Optimization Proxies](https://icml.cc/virtual/2024/poster/34154) (Poster)
  - **Authors:** [Wenbo Chen](http://openreview.net/profile?id=~Wenbo_Chen2), [Haoruo Zhao](http://openreview.net/profile?id=~Haoruo_Zhao1), [Mathieu Tanneau](http://openreview.net/profile?id=~Mathieu_Tanneau1), [Pascal Van Hentenryck](http://openreview.net/profile?id=~Pascal_Van_Hentenryck2)
  - **Affiliations:** H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, USA; NSF Artificial Intelligence Research Institute for Advances in Optimization (AI4OPT), USA, H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, USA; NSF Artificial Intelligence Research Institute for Advances in Optimization (AI4OPT), USA, H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, USA; NSF Artificial Intelligence Research Institute for Advances in Optimization (AI4OPT), USA, H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, USA; NSF Artificial Intelligence Research Institute for Advances in Optimization (AI4OPT), USA
  - **TL;DR:** This paper presents a compact formulation for verifying the optimality of optimization proxies, addressing both convex and non-convex problems, and introduces a gradient-based heuristic that significantly improves computational efficiency. The proposed methods are validated through applications in DC Optimal Power Flow and knapsack problems, demonstrating their practical benefits.
  - **Keywords:** optimization proxies, parametric optimization problems, optimality verification, gradient-based primal heuristic, Projected Gradient Attack (PGA), Mixed-Integer Programming (MIP), DC Optimal Power Flow (DC-OPF), knapsack problems, power systems operations, supply chain management, manufacturing, worst-case optimality gap, feasibility, constraint violations, compact formulation for optimality verification, computational benefits, quality guarantees for optimization proxies


- [LLaGA: Large Language and Graph Assistant](https://icml.cc/virtual/2024/poster/34739) (Poster)
  - **Authors:** [Runjin Chen](http://openreview.net/profile?id=~Runjin_Chen1), [Tong Zhao](http://openreview.net/profile?id=~Tong_Zhao3), [Ajay Jaiswal](http://openreview.net/profile?id=~AJAY_KUMAR_JAISWAL1), [Neil Shah](http://openreview.net/profile?id=~Neil_Shah2), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1)
  - **Affiliations:** The University of Texas at Austin, Snap Inc., The University of Texas at Austin, Snap Inc., The University of Texas at Austin
  - **TL;DR:** This paper introduces the Large Language and Graph Assistant (LLaGA), a model that integrates large language model capabilities to effectively handle graph-structured data. LLaGA demonstrates superior performance across various datasets and tasks, surpassing existing state-of-the-art graph models in both supervised and zero-shot scenarios.
  - **Keywords:** Graph Neural Networks, Large Language Models, Message Passing, Aggregation Techniques, Graph-structured Data Analysis, Social Networks, Biological Networks, Recommendation Systems, Translating Graph Structures to Language, Performance on Graph Tasks, Large Language and Graph Assistant (LLaGA), Versatile Projector, GNNs (Graph Neural Networks), LLMs (Large Language Models), Explainability, Generalizability, Interpretability


- [Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness](https://icml.cc/virtual/2024/poster/32941) (Poster)
  - **Authors:** [Honghao Chen](http://openreview.net/profile?id=~Honghao_Chen1), [Zhang Yurong](http://openreview.net/profile?id=~Yurong_Zhang1), [xiaokun Feng](http://openreview.net/profile?id=~Xiaokun_Feng1), [Xiangxiang Chu](http://openreview.net/profile?id=~Xiangxiang_Chu1), [Kaiqi Huang](http://openreview.net/profile?id=~Kaiqi_Huang1)
  - **Affiliations:** CRISE, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Shanghai Jiao Tong University, CRISE, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Meituan, CRISE, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences
  - **TL;DR:** This study evaluates the robustness of large kernel convolutional networks compared to traditional small kernel CNNs and vision transformers, revealing that pure CNNs can achieve robustness levels comparable to or exceeding those of ViTs. The findings provide new insights into the factors contributing to this robustness, including occlusion invariance and kernel attention patterns.
  - **Keywords:** robustness, deep learning, large kernel convolutional networks, convolutional neural networks (CNNs), vision transformers (ViTs), image classification, object detection, semantic segmentation, self-supervised learning, robustness evaluation, occlusion invariance, adversarial attacks, exceptional robustness of pure CNNs, insights into kernel attention patterns and frequency characteristics, robustness benchmark datasets


- [Stacking Deep Set Networks and Pooling by Quantiles](https://icml.cc/virtual/2024/poster/34276) (Poster)
  - **Authors:** [Zhuojun Chen](http://openreview.net/profile?id=~Zhuojun_Chen1), [Xinghua Zhu](http://openreview.net/profile?id=~Xinghua_Zhu1), [Dongzhe Su](http://openreview.net/profile?email=dzsu%40astri.org), [Justin CHUANG](http://openreview.net/profile?email=justinchuang%40astri.org)
  - **Affiliations:** ASTRI, Hong Kong, China, None, None, None
  - **TL;DR:** This paper introduces Stacked Deep Sets and Quantile Pooling as a novel approach for learning from set data, combining the strengths of max and average pooling. The proposed methods demonstrate improved performance and efficiency in handling complex data distributions compared to traditional pooling techniques.
  - **Keywords:** Deep Learning, Set Data Processing, Stacked Deep Sets, Quantile Pooling, Max Pooling, Average Pooling, Information Loss in Pooling, Complex Data Distributions, Enhanced Pooling Methods, Improved Learning Efficiency, Permutation-Invariant Functions


- [Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations](https://icml.cc/virtual/2024/poster/34820) (Spotlight Poster)
  - **Authors:** [Yanda Chen](http://openreview.net/profile?id=~Yanda_Chen1), [Ruiqi Zhong](http://openreview.net/profile?id=~Ruiqi_Zhong1), [Narutatsu Ri](http://openreview.net/profile?id=~Narutatsu_Ri1), [Chen Zhao](http://openreview.net/profile?id=~Chen_Zhao2), [He He](http://openreview.net/profile?id=~He_He2), [Jacob Steinhardt](http://openreview.net/profile?id=~Jacob_Steinhardt1), [Zhou Yu](http://openreview.net/profile?id=~Zhou_Yu1), [Kathleen McKeown](http://openreview.net/profile?id=~Kathleen_McKeown1)
  - **Affiliations:** Columbia University, UC Berkeley, Columbia University; NYU Shanghai, NYU Shanghai, New York University, UC Berkeley, Columbia University, Columbia University
  - **TL;DR:** This study evaluates whether large language models can explain their outputs effectively through counterfactual simulatability, revealing that current explanations often mislead users and have low precision. The findings suggest that simply optimizing for human approval may not be sufficient for improving model explanations.
  - **Keywords:** Explainability, Large Language Models, Counterfactual Simulatability, Multi-hop Factual Reasoning, Reward Modeling, Natural Language Processing, Low precision of explanations, Misleading mental models, Evaluation metrics for explanations


- [DRCT: Diffusion Reconstruction Contrastive Training towards Universal Detection of Diffusion Generated Images](https://icml.cc/virtual/2024/poster/33086) (Spotlight Poster)
  - **Authors:** [Baoying Chen](http://openreview.net/profile?id=~Baoying_Chen1), [Jishen Zeng](http://openreview.net/profile?id=~Jishen_Zeng1), [Jianquan Yang](http://openreview.net/profile?id=~Jianquan_Yang1), [Rui Yang](http://openreview.net/profile?id=~Rui_Yang18)
  - **Affiliations:** Alibaba Group, Alibaba Group, School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China, Alibaba Group
  - **TL;DR:** This study introduces Diffusion Reconstruction Contrastive Training (DRCT) to enhance the generalizability of image detection methods for images generated by various diffusion models. The proposed framework significantly improves detection accuracy by over 10% in cross-set tests and includes the creation of a large dataset for evaluation.
  - **Keywords:** Diffusion models, Image detection, Diffusion Reconstruction Contrastive Learning (DRCT), Contrastive training, Digital content generation, Image generation, Generalizability of image detectors, Hard sample classification, Improved accuracy in detecting generated images, Development of a million-scale dataset (DRCT-2M), DRCT-2M, MSCOCO


- [Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation](https://icml.cc/virtual/2024/poster/35170) (Poster)
  - **Authors:** [Yu Chen](http://openreview.net/profile?id=~Yu_Chen19), [XiangCheng Zhang](http://openreview.net/profile?id=~XiangCheng_Zhang1), [Siwei Wang](http://openreview.net/profile?id=~Siwei_Wang2), [Longbo Huang](http://openreview.net/profile?id=~Longbo_Huang2)
  - **Affiliations:** Tsinghua University, Beijing, China, Tsinghua University, Beijing, China, Microsoft Research Asia, Tsinghua University, Beijing, China
  - **TL;DR:** This paper presents a framework for Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL) that incorporates static Lipschitz Risk Measures and general function approximation, addressing the challenges of risk management in RL. The authors introduce two meta-algorithms and derive a novel regret upper bound, contributing to the development of statistically efficient algorithms in this field.
  - **Keywords:** Risk-Sensitive Reinforcement Learning, Distributional Reinforcement Learning, Static Lipschitz Risk Measures, Least Squares Regression, Maximum Likelihood Estimation, Financial investment, Medical treatment, Autonomous driving, Risk management, Sample complexity, Cumulative reward distribution, RS-DisRL-M, RS-DisRL-V, Regret upper bound, Markov Decision Process (MDP), Coherent risk, Conditional Value-at-Risk (CVaR), Entropy risk measures (ERM)


- [Enhancing Implicit Shape Generators Using Topological Regularizations](https://icml.cc/virtual/2024/poster/33834) (Poster)
  - **Authors:** [Liyan Chen](http://openreview.net/profile?id=~Liyan_Chen1), [Yan Zheng](http://openreview.net/profile?id=~Yan_Zheng4), [Yang Li](http://openreview.net/profile?id=~Yang_Li4), [Lohit A. Jagarapu](http://openreview.net/profile?id=~Lohit_Anirudh_Jagarapu1), [Haoxiang Li](http://openreview.net/profile?id=~Haoxiang_Li1), [Hao Kang](http://openreview.net/profile?id=~Hao_Kang1), [Gang Hua](http://openreview.net/profile?id=~Gang_Hua3), [Qixing Huang](http://openreview.net/profile?id=~Qixing_Huang1)
  - **Affiliations:** Department of Computer Science, The University of Texas at Austin, Austin TX, Department of Computer Science, The University of Texas at Austin, Austin TX, Tsinghua Shenzhen International Graduate School, Info Building 1108A, Shenzhen, China, Department of Computer Science, The University of Texas at Austin, Austin TX, Wormpex AI Research, 500 108th Ave NE, Ste 1740, Bellevue WA, Wormpex AI Research, 500 108th Ave NE, Ste 1740, Bellevue WA, Wormpex AI Research, 500 108th Ave NE, Ste 1740, Bellevue WA, Department of Computer Science, The University of Texas at Austin, Austin TX
  - **TL;DR:** This paper presents a method to enhance implicit 3D shape generators by using topological regularization losses to address artifacts in synthetic models. The approach focuses on aligning the persistent diagram distributions of training and synthetic shapes while ensuring smoothness among adjacent synthetic shapes, leading to improved generalization performance.
  - **Keywords:** 3D shape generative models, topological generalization, persistent diagram (PD), generative model, topological regularization losses, visual computing, synthetic 3D model generation, topological artifacts, data sparsity in training, alignment of PD distributions, improved generalization behavior, distribution alignment loss, smoothness regularization, ShapeNet


- [What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks](https://icml.cc/virtual/2024/poster/33776) (Poster)
  - **Authors:** [Xingwu Chen](http://openreview.net/profile?id=~Xingwu_Chen1), [Difan Zou](http://openreview.net/profile?id=~Difan_Zou1)
  - **Affiliations:** Department of Computer Science, The University of Hong Kong, Department of Computer Science and Institute of Data Science, The University of Hong Kong
  - **TL;DR:** This study investigates how the depth of transformer architectures influences their capabilities in memorization, reasoning, and generalization through a novel set of sequence learning tasks. The findings indicate that at least two attention layers are necessary for reasoning and generalization, while three layers may be required for contextual generalization.
  - **Keywords:** Transformer architecture, sequence learning tasks, Attention layers, Natural language processing, deep learning, Memorization, reasoning, generalization, contextual generalization, Evaluation of transformer capabilities, stacking attention layers


- [Diffusive Gibbs Sampling](https://icml.cc/virtual/2024/poster/34092) (Poster)
  - **Authors:** [Wenlin Chen](http://openreview.net/profile?id=~Wenlin_Chen2), [Mingtian Zhang](http://openreview.net/profile?id=~Mingtian_Zhang1), [Brooks Paige](http://openreview.net/profile?id=~Brooks_Paige1), [Jose Miguel Hernandez-Lobato](http://openreview.net/profile?id=~Jos%C3%A9_Miguel_Hern%C3%A1ndez-Lobato1), [David Barber](http://openreview.net/profile?id=~David_Barber2)
  - **Affiliations:** University of Cambridge, Cambridge, UK; Max Planck Institute for Intelligent Systems, Tübingen, Germany, University College London, London, UK, University College London, London, UK, University of Cambridge, Cambridge, UK, University College London, London, UK
  - **TL;DR:** This paper introduces Diffusive Gibbs Sampling (DiGS), a novel sampling method that effectively addresses the challenges of inadequate mixing in conventional MCMC methods for multi-modal distributions. DiGS demonstrates superior performance in sampling tasks across various applications, including Bayesian inference and molecular dynamics.
  - **Keywords:** multi-modal distributions, sampling methods, Diffusive Gibbs Sampling (DiGS), Gaussian convolution, Metropolis-within-Gibbs, Bayesian inference, molecular dynamics, mixtures of Gaussians, Bayesian neural networks, inadequate mixing of MCMC methods, disconnected modes, improved sampling performance, better mixing properties, Markov Chain Monte Carlo (MCMC), Langevin SDE, Metropolis-adjusted Langevin Algorithm (MALA), Hamiltonian Monte Carlo (HMC)


- [FedMBridge: Bridgeable Multimodal Federated Learning](https://icml.cc/virtual/2024/poster/33284) (Oral)
  - **Authors:** [Jiayi Chen](http://openreview.net/profile?id=~Jiayi_Chen4), [Aidong Zhang](http://openreview.net/profile?id=~Aidong_Zhang2)
  - **Affiliations:** Department of Computer Science, University of Virginia, Charlottesville, VA 22903, USA, Department of Computer Science, University of Virginia, Charlottesville, VA 22903, USA
  - **TL;DR:** The study introduces FedMBridge, a novel approach to Multimodal Federated Learning that addresses the challenges of statistical and architecture heterogeneity among clients. The proposed method enhances knowledge sharing and communication efficiency, demonstrating effectiveness in various simulations.
  - **Keywords:** Multimodal Federated Learning, Personalized Federated Learning, Topology-aware hypernetwork, blockwise model aggregation, Statistical heterogeneity, architecture heterogeneity, knowledge sharing among clients, FedMBridge, communication-efficient information sharing


- [GaussianPro: 3D Gaussian Splatting with Progressive Propagation](https://icml.cc/virtual/2024/poster/33215) (Poster)
  - **Authors:** [Kai Cheng](http://openreview.net/profile?id=~Kai_Cheng1), [Xiaoxiao Long](http://openreview.net/profile?id=~Xiaoxiao_Long2), [Kaizhi Yang](http://openreview.net/profile?id=~Kaizhi_Yang1), [Yao Yao](http://openreview.net/profile?id=~Yao_Yao1), [Wei Yin](http://openreview.net/profile?id=~Wei_Yin2), [Yuexin Ma](http://openreview.net/profile?id=~Yuexin_Ma2), [Wenping Wang](http://openreview.net/profile?id=~Wenping_Wang1), [Xuejin Chen](http://openreview.net/profile?id=~Xuejin_Chen1)
  - **Affiliations:** MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, The University of Hong Kong, MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, Nanjing University, The University of Adelaide, ShanghaiTech University, Texas A&M University, MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China
  - **TL;DR:** This paper introduces GaussianPro, a novel method for 3D Gaussian Splatting that addresses the challenges of low-quality renderings in texture-less areas by applying a progressive propagation strategy. The method significantly improves rendering quality, achieving a 1.15dB increase in PSNR compared to existing techniques on the Waymo dataset.
  - **Keywords:** 3D Gaussian Splatting, neural rendering, novel view synthesis, Structure-from-Motion (SfM), multi-view stereo (MVS), patch matching, virtual reality, autonomous driving, 3D content generation, data sparsity, low-quality renderings, optimization challenges, GaussianPro method, improved PSNR, Waymo dataset


- [RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation](https://icml.cc/virtual/2024/poster/34129) (Spotlight Poster)
  - **Authors:** [Zelei Cheng](http://openreview.net/profile?id=~Zelei_Cheng1), [Xian Wu](http://openreview.net/profile?id=~Xian_Wu8), [Jiahao Yu](http://openreview.net/profile?id=~Jiahao_Yu1), [Sabrina Yang](http://openreview.net/profile?id=~Sabrina_Yang1), [Gang Wang](http://openreview.net/profile?id=~Gang_Wang13), [Xinyu Xing](http://openreview.net/profile?id=~Xinyu_Xing3)
  - **Affiliations:** Department of Computer Science, Northwestern University, Evanston, Illinois, USA, Department of Computer Science, Northwestern University, Evanston, Illinois, USA, Department of Computer Science, Northwestern University, Evanston, Illinois, USA, Presentation High School, San Jose, California, USA, Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, Illinois, USA, Department of Computer Science, Northwestern University, Evanston, Illinois, USA
  - **TL;DR:** This paper introduces RICE, a novel refining scheme for deep reinforcement learning that utilizes explanation methods to overcome training bottlenecks, particularly in environments with sparse rewards. The results show that RICE significantly improves agent performance compared to existing methods.
  - **Keywords:** Deep Reinforcement Learning (DRL), Training Bottlenecks, Explanation Methods, RICE (Refining scheme), StateMask, Autonomous Vehicles, Cybersecurity, Simulated Games, Sparse Rewards, Training Bottlenecks, Local Optima, Enhanced Agent Performance, Tighter Sub-optimality Bound


- [Kernel Semi-Implicit Variational Inference](https://icml.cc/virtual/2024/poster/32761) (Poster)
  - **Authors:** [Ziheng Cheng](http://openreview.net/profile?id=~Ziheng_Cheng4), [Longlin Yu](http://openreview.net/profile?id=~Longlin_Yu1), [Tianyu Xie](http://openreview.net/profile?id=~Tianyu_Xie1), [Shiyue Zhang](http://openreview.net/profile?id=~Shiyue_Zhang3), [Cheng Zhang](http://openreview.net/profile?id=~Cheng_Zhang3)
  - **Affiliations:** School of Mathematical Sciences, Peking University, China, School of Mathematical Sciences, Peking University, China, School of Mathematical Sciences, Peking University, China, School of Mathematical Sciences, Peking University, China, School of Mathematical Sciences, Peking University, China; Center for Statistical Science, Peking University, China
  - **TL;DR:** This paper introduces kernel semi-implicit variational inference (KSIVI), which improves upon traditional variational inference methods by eliminating the need for lower-level optimization through kernel tricks. The proposed method demonstrates effectiveness and efficiency in Bayesian inference tasks while providing novel convergence guarantees.
  - **Keywords:** Semi-implicit variational inference, Variational inference, Kernel tricks, Score matching, Stochastic gradient descent, Bayesian inference, Intractable densities, Biases in training, Kernel SIVI (KSIVI), Convergence guarantees, Reproducing kernel Hilbert space (RKHS), Kernel Stein discrepancy (KSD)


- [Hard Tasks First: Multi-Task Reinforcement Learning Through Task Scheduling](https://icml.cc/virtual/2024/poster/33388) (Poster)
  - **Authors:** [MYUNG-SIK CHO](http://openreview.net/profile?id=~Myungsik_Cho1), [Jong Eui Park](http://openreview.net/profile?id=~Jongeui_Park1), [Suyoung Lee](http://openreview.net/profile?id=~Suyoung_Lee4), [Youngchul Sung](http://openreview.net/profile?id=~Youngchul_Sung1)
  - **Affiliations:** School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea
  - **TL;DR:** This study presents a novel Scheduled Multi-Task Training (SMT) algorithm that prioritizes challenging tasks in multi-task reinforcement learning to improve learning efficiency and mitigate negative transfer. The method demonstrates significant performance improvements on the Meta-World benchmark, showcasing its effectiveness in handling varying task difficulties.
  - **Keywords:** Multi-task reinforcement learning, task scheduling, Scheduled Multi-Task Training (SMT), dynamic task prioritization, Robotics, control tasks, Varying task difficulties, negative transfer, simplicity bias, Improved learning efficiency, enhanced adaptability and robustness, Meta-World benchmark, Deep reinforcement learning, deep neural networks (DNNs)


- [Kernel Debiased Plug-in Estimation: Simultaneous, Automated Debiasing without Influence Functions for Many Target Parameters](https://icml.cc/virtual/2024/poster/32771) (Poster)
  - **Authors:** [Brian Cho](http://openreview.net/profile?id=~Brian_M_Cho1), [Yaroslav Mukhin](http://openreview.net/profile?id=~Yaroslav_Mukhin1), [Kyra Gan](http://openreview.net/profile?id=~Kyra_Gan1), [Ivana Malenica](http://openreview.net/profile?id=~Ivana_Malenica1)
  - **Affiliations:** Department of ORIE, Cornell Tech, NY, USA, Massachusetts Institute of Technology, Cambridge, MA, USA, Department of ORIE, Cornell Tech, NY, USA, Department of Statistics, Harvard University, Cambridge, MA, USA
  - **TL;DR:** This paper introduces kernel debiased plug-in estimation (KDPE), a method that simultaneously debiases multiple target parameters in nonparametric models without requiring influence functions. The proposed method enhances efficiency and computational tractability in statistical estimation.
  - **Keywords:** nonparametric models, plug-in bias, targeted maximum likelihood estimation, kernel debiased plug-in estimation (KDPE), regularized likelihood maximization, causal inference, statistical estimation, plug-in bias, bias-variance trade-off, computational challenges, efficient RAL estimators, simultaneous debiasing of target parameters, influence function (IF), efficient influence function (EIF), reproducing kernel Hilbert spaces


- [How Flawed Is ECE? An Analysis via Logit Smoothing](https://icml.cc/virtual/2024/poster/35067) (Poster)
  - **Authors:** [Muthu Chidambaram](http://openreview.net/profile?id=~Muthu_Chidambaram1), [Holden Lee](http://openreview.net/profile?id=~Holden_Lee1), [Colin McSwiggen](http://openreview.net/profile?email=cmcswiggen%40gmail.com), [Semon Rezchikov](http://openreview.net/profile?id=~Semon_Rezchikov1)
  - **Affiliations:** Department of Computer Science, Duke University, Johns Hopkins University, New York University, Princeton University
  - **TL;DR:** This study analyzes the Expected Calibration Error (ECE) and its discontinuities, proposing a new metric called Logit-Smoothed ECE (LS-ECE) to address these issues. Initial experiments suggest that LS-ECE closely tracks binned ECE, indicating potential practical solutions to ECE's theoretical shortcomings.
  - **Keywords:** calibration, expected calibration error (ECE), Logit-Smoothed ECE (LS-ECE), image classification, medical diagnosis, self-driving, model calibration issues, overconfidence in predictions, Logit-Smoothed ECE (LS-ECE), analysis of ECE discontinuities, Polish spaces, predictive uncertainty


- [KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation](https://icml.cc/virtual/2024/poster/34175) (Poster)
  - **Authors:** [Minsik Cho](http://openreview.net/profile?id=~Minsik_Cho1), [Mohammad Rastegari](http://openreview.net/profile?id=~Mohammad_Rastegari2), [Devang Naik](http://openreview.net/profile?id=~Devang_Naik1)
  - **Affiliations:** Apple. USA, Meta. USA; (the work done while being with Apple), Apple. USA
  - **TL;DR:** This paper introduces KV-Runahead, a novel parallelization technique designed to enhance the prompt phase of Large Language Model inference by efficiently populating the key-value cache, thereby minimizing the time-to-first-token (TTFT). Experimental results show that KV-Runahead achieves significant speedups compared to existing parallelization methods.
  - **Keywords:** Large Language Models, Causal Inference, Parallelization, Key-Value Cache (KV-cache), Natural Language Processing, Time-to-first-token (TTFT), Time Per Output Token (TPOT), KV-Runahead, Context-level load-balancing, Llama 7B, Falcon 7B, Generative Pre-trained Transformer (GPT), Causal Attention


- [Listwise Reward Estimation for Offline Preference-based Reinforcement Learning](https://icml.cc/virtual/2024/poster/34419) (Poster)
  - **Authors:** [Heewoong Choi](http://openreview.net/profile?id=~Heewoong_Choi1), [Sangwon Jung](http://openreview.net/profile?id=~Sangwon_Jung1), [Hongjoon Ahn](http://openreview.net/profile?id=~Hongjoon_Ahn2), [Taesup Moon](http://openreview.net/profile?id=~Taesup_Moon1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Seoul National University, Department of Electrical and Computer Engineering, Seoul National University, Department of Electrical and Computer Engineering, Seoul National University, Department of Electrical and Computer Engineering, Seoul National University; ASRI/INMC/IPAI/AIIS, Seoul National University
  - **TL;DR:** This paper introduces Listwise Reward Estimation (LiRE), a novel method for offline Preference-based Reinforcement Learning that utilizes second-order preference information to improve reward estimation. The proposed approach outperforms existing methods even with limited feedback, demonstrating robustness against feedback noise.
  - **Keywords:** Preference-based Reinforcement Learning (PbRL), Reward Estimation, Listwise Reward Estimation (LiRE), Bradley-Terry model, Robotics, Game AI, Autonomous Driving, Designing precise reward functions, Aligning with human intent, Novel reward estimation approach, Improved performance with modest feedback budgets, New offline PbRL dataset, Second-order preference, Ranked List of Trajectories (RLT)


- [Neurodegenerative Brain Network Classification via Adaptive Diffusion with Temporal Regularization](https://icml.cc/virtual/2024/poster/34501) (Poster)
  - **Authors:** [Hyuna Cho](http://openreview.net/profile?id=~Hyuna_Cho1), [Jaeyoon Sim](http://openreview.net/profile?id=~Jaeyoon_Sim1), [Guorong Wu](http://openreview.net/profile?id=~Guorong_Wu1), [Won Hwa Kim](http://openreview.net/profile?id=~Won_Hwa_Kim4)
  - **Affiliations:** Pohang University of Science and Technology (POSTECH), South Korea, Pohang University of Science and Technology (POSTECH), South Korea, University of North Carolina at Chapel Hill, USA, Pohang University of Science and Technology (POSTECH), South Korea
  - **TL;DR:** This study introduces the Adaptive Graph diffusion network with Temporal regularization (AGT) to classify neurodegenerative brain networks, addressing the challenges of disease progression and the complex structures of brain connectomes. The proposed method demonstrates superior interpretability and performance on benchmark datasets for Alzheimer's and Parkinson's diseases.
  - **Keywords:** neurodegenerative diseases, brain connectomes, disease progression, Adaptive Graph diffusion network, node-wise convolution, temporal regularization, neuroimaging, brain network classification, progressive dynamics of diseases, cross-sectional studies, high-dimensional and sparse graphs, homophily and heterophily, interpretable results at node-level and group-level, validation on neurodegenerative disease benchmarks, Alzheimer’s Disease Neuroimaging Initiative (ADNI), Parkinson’s Progression Markers Initiative (PPMI), graph neural networks (GNNs), oversmoothing


- [RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences](https://icml.cc/virtual/2024/poster/34704) (Spotlight Poster)
  - **Authors:** [Jie Cheng](http://openreview.net/profile?id=~Jie_Cheng4), [Gang Xiong](http://openreview.net/profile?id=~Gang_Xiong2), [Xingyuan Dai](http://openreview.net/profile?id=~Xingyuan_Dai1), [Qinghai Miao](http://openreview.net/profile?id=~Qinghai_Miao1), [Yisheng Lv](http://openreview.net/profile?id=~Yisheng_Lv1), [Fei-Yue Wang](http://openreview.net/profile?id=~Fei-Yue_Wang2)
  - **Affiliations:** State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA; School of Artificial Intelligence, the University of Chinese Academy of Sciences, State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA; School of Artificial Intelligence, the University of Chinese Academy of Sciences, State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA; School of Artificial Intelligence, the University of Chinese Academy of Sciences, School of Artificial Intelligence, the University of Chinese Academy of Sciences, State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA; School of Artificial Intelligence, the University of Chinese Academy of Sciences, State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA; School of Artificial Intelligence, the University of Chinese Academy of Sciences
  - **TL;DR:** This paper presents RIME, a robust preference-based reinforcement learning algorithm designed to effectively learn from noisy preferences, addressing the challenges of feedback inefficiency and robustness in reward learning. The proposed method significantly enhances the performance of state-of-the-art PbRL techniques in robotic manipulation and locomotion tasks.
  - **Keywords:** Preference-based Reinforcement Learning (PbRL), Robustness in RL, Denoising discriminator, Sample selection-based discriminator, Robotic manipulation, Locomotion tasks, Noisy preferences, Lack of robustness, Feedback inefficiency, RIME algorithm, Warm start for reward model, Kullback–Leibler (KL) divergence, Human-in-the-loop paradigm


- [Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport](https://icml.cc/virtual/2024/poster/33558) (Poster)
  - **Authors:** [Jaemoo Choi](http://openreview.net/profile?id=~Jaemoo_Choi1), [Jaewoong Choi](http://openreview.net/profile?id=~Jaewoong_Choi1), [Myungjoo Kang](http://openreview.net/profile?id=~Myungjoo_Kang1)
  - **Affiliations:** Seoul National University, Korea Institute for Advanced Study, Seoul National University; Korea Institute for Advanced Study
  - **TL;DR:** This paper introduces a scalable generative model called Semi-dual JKO (S-JKO) that leverages the semi-dual form of the JKO scheme to reduce training complexity from quadratic to linear. The model significantly outperforms existing Wasserstein Gradient Flow-based generative models, achieving competitive FID scores on benchmark datasets.
  - **Keywords:** Generative modeling, Wasserstein Gradient Flow, JKO scheme, Unbalanced Optimal Transport, Image generation, High-resolution image datasets, Scalability challenges, Training complexity, Semi-dual JKO (S-JKO), Reduced training complexity, CIFAR-10, CelebA-HQ-256


- [PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning](https://icml.cc/virtual/2024/poster/32764) (Poster)
  - **Authors:** [Hyeong Kyu Choi](http://openreview.net/profile?id=~Hyeong_Kyu_Choi1), [Sharon Li](http://openreview.net/profile?id=~Yixuan_Li1)
  - **Affiliations:** Department of Computer Sciences, University of Wisconsin–Madison, United States, Department of Computer Sciences, University of Wisconsin–Madison, United States
  - **TL;DR:** This study introduces Persona In-Context Learning (PICLe), a framework for eliciting specific personality traits from Large Language Models (LLMs) using a novel example selection criterion based on likelihood ratios. The effectiveness of PICLe is demonstrated through extensive comparisons with baseline methods across multiple contemporary LLMs.
  - **Keywords:** Persona elicitation, Large Language Models, Persona In-Context Learning (PICLe), Bayesian inference, In-context learning (ICL), Eliciting diverse personas, behavioral preferences of LLMs, Likelihood-ratio-based selection mechanism, effective persona elicitation, Llama-2, Vicuna, GPT-J


- [BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks](https://icml.cc/virtual/2024/poster/32834) (Poster)
  - **Authors:** [Zhiyuan Cheng](http://openreview.net/profile?id=~Zhiyuan_Cheng2), [Zhaoyi Liu](http://openreview.net/profile?id=~Zhaoyi_Liu1), [Tengda Guo](http://openreview.net/profile?id=~Tengda_Guo1), [Shiwei Feng](http://openreview.net/profile?id=~Shiwei_Feng1), [Dongfang Liu](http://openreview.net/profile?id=~Dongfang_Liu1), [Mingjie Tang](http://openreview.net/profile?id=~Mingjie_Tang1), [Xiangyu Zhang](http://openreview.net/profile?id=~Xiangyu_Zhang3)
  - **Affiliations:** Department of Computer Science, Purdue University, West Lafayette, USA, College of Computer Science, Sichuan University, Chengdu, China, College of Computer Science, Sichuan University, Chengdu, China, Department of Computer Science, Purdue University, West Lafayette, USA, Department of Computer Engineering, Rochester Institute of Technology, Rochester, USA, College of Computer Science, Sichuan University, Chengdu, China, Department of Computer Science, Purdue University, West Lafayette, USA
  - **TL;DR:** This study introduces BADPART, a unified black-box adversarial patch attack framework targeting pixel-wise regression tasks, demonstrating significant vulnerabilities in models like monocular depth estimation and optical flow estimation. The framework outperforms existing methods and poses a serious threat to the security of applications in autonomous driving and augmented reality.
  - **Keywords:** Adversarial Patch Attacks, Pixel-wise Regression Tasks, Query-based Black-box Attacks, Adversarial Patch Optimization, Probabilistic Square Sampling, Score-based Gradient Estimation, Monocular Depth Estimation (MDE), Optical Flow Estimation (OFE), Autonomous Driving, Augmented Reality, Video Composition, Adversarial Robustness, Black-box Vulnerabilities, Scalability Issues, BADPART Prototype, Attack Performance, Efficiency Improvements


- [MS-TIP: Imputation Aware Pedestrian Trajectory Prediction](https://icml.cc/virtual/2024/poster/32928) (Poster)
  - **Authors:** [Pranav Singh Chib](http://openreview.net/profile?id=~Pranav_singh_chib1), [Achintya Nath](http://openreview.net/profile?id=~Achintya_Nath1), [Paritosh Kabra](http://openreview.net/profile?id=~Paritosh_Kabra1), [Ishu Gupta](http://openreview.net/profile?id=~Ishu_Gupta1), [Pravendra Singh](http://openreview.net/profile?id=~Pravendra_Singh1)
  - **Affiliations:** Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India, Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India, Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India, Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India, Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India
  - **TL;DR:** This study introduces MS-TIP, a novel approach for pedestrian trajectory prediction that effectively handles missing observations while predicting future trajectories. The method leverages advanced techniques like transformers and multi-scale hypergraphs to enhance prediction accuracy and model complex interactions among pedestrians.
  - **Keywords:** pedestrian trajectory prediction, imputation of missing observations, transformers, diagonal masked self-attention, multi-scale hypergraphs, self-driving cars, human motion pattern analysis, missing values in observed sequences, inaccuracies in predicting final goals, recursive prediction errors, MultiScale hypergraph for Trajectory Imputation and Prediction (MS-TIP), accurate future trajectory inference


- [Creative Text-to-Audio Generation via Synthesizer Programming](https://icml.cc/virtual/2024/poster/34961) (Poster)
  - **Authors:** [Manuel Cherep](http://openreview.net/profile?id=~Manuel_Cherep1), [Nikhil Singh](http://openreview.net/profile?id=~Nikhil_Singh2), [Jessica Shand](http://openreview.net/profile?id=~Jessica_Shand1)
  - **Affiliations:** Media Lab, Massachusetts Institute of Technology, Cambridge MA, USA, Media Lab, Massachusetts Institute of Technology, Cambridge MA, USA, Media Lab, Massachusetts Institute of Technology, Cambridge MA, USA
  - **TL;DR:** This study presents CTAG, a novel text-to-audio generation method that utilizes a virtual modular synthesizer to create high-quality, abstract audio representations from text prompts. The approach allows for easy parameter manipulation, offering a complementary tool to existing neural audio synthesis methods while emphasizing creative sound design over acoustic realism.
  - **Keywords:** text-to-audio generation, sound design, neural audio synthesis, virtual modular synthesizer, procedural sound design, music, film, video games, advertising, product design, difficulty in tweaking neural audio synthesis results, large latent spaces, CTAG method, high-quality audio renderings, abstract sound representation


- [Online bipartite matching with imperfect advice](https://icml.cc/virtual/2024/poster/34946) (Poster)
  - **Authors:** [Davin Choo](http://openreview.net/profile?id=~Davin_Choo1), [Themis Gouleakis](http://openreview.net/profile?id=~Themistoklis_Gouleakis2), [Chun Kai Ling](http://openreview.net/profile?id=~Chun_Kai_Ling2), [Arnab Bhattacharyya](http://openreview.net/profile?id=~Arnab_Bhattacharyya1)
  - **Affiliations:** School of Computing, National University of Singapore, School of Computing, National University of Singapore, Industrial Engineering and Operations Research, Columbia University, School of Computing, National University of Singapore
  - **TL;DR:** This study investigates online bipartite matching with imperfect advice, demonstrating that while traditional algorithms achieve a competitive ratio of 1 - 1/e, no learning-augmented method can be both 1-consistent and better than 1/2-robust under adversarial conditions. The authors propose a new algorithm that leverages external advice to achieve competitive ratios that interpolate between advice-free methods and the optimal ratio of 1, depending on advice quality.
  - **Keywords:** online bipartite matching, learning-augmented algorithms, RANKING algorithm, distribution testing, internet advertising, two-sided markets, competitive ratio, adversarial arrival model, random arrival model, algorithms utilizing external advice, competitive ratio improvement


- [Leveraging (Biased) Information: Multi-armed Bandits with Offline Data](https://icml.cc/virtual/2024/poster/33843) (Spotlight Poster)
  - **Authors:** [Wang Chi Cheung](http://openreview.net/profile?id=~Wang_Chi_Cheung1), [Lixing Lyu](http://openreview.net/profile?id=~Lixing_Lyu1)
  - **Affiliations:** Department of Industrial Systems Engineering and Management, National University of Singapore, Singapore, Institute of Operations Research and Analytics, National University of Singapore, Singapore
  - **TL;DR:** This study explores the use of offline data to enhance online learning in stochastic multi-armed bandits, proposing a new policy called MIN-UCB that outperforms traditional UCB when the offline and online distributions are closely aligned. The findings indicate that leveraging offline data can significantly reduce exploration and improve cumulative rewards in decision-making processes.
  - **Keywords:** multi-armed bandits, online learning, offline data, UCB policy, MIN-UCB, distributional shift, exploration vs. exploitation, regret bounds, adaptive policy


- [Enhancing Trajectory Prediction through Self-Supervised Waypoint Distortion Prediction](https://icml.cc/virtual/2024/poster/34165) (Poster)
  - **Authors:** [Pranav Singh Chib](http://openreview.net/profile?id=~Pranav_singh_chib1), [Pravendra Singh](http://openreview.net/profile?id=~Pravendra_Singh1)
  - **Affiliations:** Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India, Department of Computer Science and Engineering, Indian Institute of Technology, Roorkee, India
  - **TL;DR:** This study introduces a self-supervised approach called SSWDP to enhance trajectory prediction by effectively modeling the distortion in observed trajectories. Experimental results demonstrate significant improvements in prediction accuracy across multiple datasets, highlighting the method's effectiveness in complex environments.
  - **Keywords:** Trajectory prediction, Self-supervised learning, Self-Supervised Waypoint Distortion Prediction (SSWDP), Generative Adversarial Networks (GANs), Conditional Variational Autoencoders (CVAE), Transformers, Autonomous driving, Robotics, Surveillance systems, Drones, Uncertainty in future trajectories, Complex spatio-temporal representations, Interaction between agents, Improvement in representation learning, Significant performance improvements in ADE/FDE metrics, NBA dataset, TrajNet++, ETH-UCY dataset, ADE (Average Displacement Error), FDE (Final Displacement Error)


- [Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning](https://icml.cc/virtual/2024/poster/33406) (Poster)
  - **Authors:** [Chia-Cheng Chiang](http://openreview.net/profile?id=~Chia-Cheng_Chiang1), [Li-Cheng Lan](http://openreview.net/profile?id=~Li-Cheng_Lan1), [Wei-Fang Sun](http://openreview.net/profile?id=~Wei-Fang_Sun1), [Chien Feng](http://openreview.net/profile?id=~Chien_Feng2), [Cho-Jui Hsieh](http://openreview.net/profile?id=~Cho-Jui_Hsieh1), [Chun-Yi Lee](http://openreview.net/profile?id=~Chun-Yi_Lee1)
  - **Affiliations:** ELSA Lab, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, Department of Computer Science, University of California, Los Angeles, CA, USA, NVIDIA AI Technology Center, NVIDIA Corporation, Santa Clara, CA, USA, ELSA Lab, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, Department of Computer Science, University of California, Los Angeles, CA, USA, ELSA Lab, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan
  - **TL;DR:** This study introduces Transition Discriminator-based IL (TDIL) to enhance single-demonstration imitation learning by addressing the challenge of sparse reward signals through a denser surrogate reward function. The proposed method outperforms existing approaches and achieves expert-level performance in various environments.
  - **Keywords:** single-demonstration imitation learning, imitation learning, Transition Discriminator-based IL (TDIL), inverse reinforcement learning (IRL), autonomous robots, surgical robots, vehicle control, sparse reward signals, reward sparsity, high-dimensional environments, denser surrogate reward function, transition discriminator, MuJoCo benchmarks, Adroit Door robotic environment


- [A connection between Tempering and Entropic Mirror Descent](https://icml.cc/virtual/2024/poster/34707) (Poster)
  - **Authors:** [Nicolas Chopin](http://openreview.net/profile?id=~Nicolas_Chopin1), [Francesca R Crucinio](http://openreview.net/profile?id=~Francesca_Romana_Crucinio1), [Anna Korba](http://openreview.net/profile?id=~Anna_Korba2)
  - **Affiliations:** ENSAE, CREST, Institut Polytechnique de Paris, ENSAE, CREST, Institut Polytechnique de Paris, ENSAE, CREST, Institut Polytechnique de Paris
  - **TL;DR:** This paper establishes a connection between tempering in Sequential Monte Carlo and entropic mirror descent, demonstrating that tempering can be viewed as a descent scheme of the KL divergence with respect to Fisher-Rao geometry. The findings include convergence rates for tempering iterates and the development of adaptive tempering rules that enhance existing benchmarks.
  - **Keywords:** Tempering, Entropic Mirror Descent, Sequential Monte Carlo (SMC), Kullback-Leibler (KL) divergence, Fisher-Rao geometry, Wasserstein-2 geometry, Computational statistics, Machine learning, Sampling from target probability distributions, Optimization of probability distributions, Convergence rates for tempering iterates, Adaptive tempering rules


- [Prompt-tuning Latent Diffusion Models for Inverse Problems](https://icml.cc/virtual/2024/poster/33375) (Poster)
  - **Authors:** [Hyungjin Chung](http://openreview.net/profile?id=~Hyungjin_Chung1), [Jong Chul YE](http://openreview.net/profile?id=~Jong_Chul_Ye1), [Peyman Milanfar](http://openreview.net/profile?id=~Peyman_Milanfar1), [Mauricio Delbracio](http://openreview.net/profile?id=~Mauricio_Delbracio1)
  - **Affiliations:** KAIST, Daejeon, Korea, KAIST, Daejeon, Korea, Google Research, Mountain View, US, Google Research, Mountain View, US
  - **TL;DR:** This study introduces a novel method called P2L for solving imaging inverse problems by optimizing text prompts in conjunction with latent diffusion models. The approach significantly reduces image artifacts and outperforms existing methods in tasks like super-resolution, deblurring, and inpainting.
  - **Keywords:** imaging inverse problems, latent diffusion models, prompt tuning, reverse diffusion, alternating minimization, super-resolution, deblurring, inpainting, suboptimal performance, image artifacts, P2L method, optimization framework


- [How Private are DP-SGD Implementations?](https://icml.cc/virtual/2024/poster/32705) (Oral)
  - **Authors:** [Lynn Chua](http://openreview.net/profile?id=~Lynn_Chua1), [Badih Ghazi](http://openreview.net/profile?id=~Badih_Ghazi1), [Pritish Kamath](http://openreview.net/profile?id=~Pritish_Kamath2), [Ravi Kumar](http://openreview.net/profile?id=~Ravi_Kumar1), [Pasin Manurangsi](http://openreview.net/profile?id=~Pasin_Manurangsi2), [Amer Sinha](http://openreview.net/profile?id=~Amer_Sinha1), [Chiyuan Zhang](http://openreview.net/profile?id=~Chiyuan_Zhang1)
  - **Affiliations:** Google Research, Google Research, Google Research, Google Research, Google Research, Google Research, Google Research
  - **TL;DR:** This paper investigates the privacy guarantees of Differentially Private Stochastic Gradient Descent (DP-SGD) under different batch sampling methods, specifically shuffling and Poisson subsampling. The findings reveal a significant gap in privacy analysis between these methods, urging caution in reporting privacy parameters for DP-SGD implementations.
  - **Keywords:** Differential Privacy, Stochastic Gradient Descent, Differentially Private Stochastic Gradient Descent (DP-SGD), Adaptive Batch Linear Queries (ABLQ), Machine Learning, Neural Networks, Privacy guarantees, privacy analysis, batch sampling, Comparison of privacy guarantees between shuffling and Poisson subsampling, TensorFlow Privacy, PyTorch Opacus, JAX Privacy


- [A Provably Effective Method for Pruning Experts in Fine-tuned Sparse Mixture-of-Experts](https://icml.cc/virtual/2024/poster/35133) (Poster)
  - **Authors:** [Mohammed Nowaz Rabbani Chowdhury](http://openreview.net/profile?id=~Mohammed_Nowaz_Rabbani_Chowdhury1), [Meng Wang](http://openreview.net/profile?id=~Meng_Wang4), [Kaoutar El Maghraoui](http://openreview.net/profile?id=~Kaoutar_El_Maghraoui1), [Naigang Wang](http://openreview.net/profile?id=~Naigang_Wang1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1), [Christopher Carothers](http://openreview.net/profile?id=~Christopher_Carothers2)
  - **Affiliations:** Department of Electrical, Computer and Systems Engineering, Rensselaer Polytechnic Institute, NY, USA, Department of Electrical, Computer and Systems Engineering, Rensselaer Polytechnic Institute, NY, USA, IBM Research, Yorktown Heights, NY, USA, IBM Research, Yorktown Heights, NY, USA, IBM Research, Yorktown Heights, NY, USA, Department of Computer Science, Rensselaer Polytechnic Institute, NY, USA
  - **TL;DR:** This paper presents a novel method for pruning experts in fine-tuned sparse mixture-of-experts (MoE) models, demonstrating that prioritizing the pruning of experts with minimal changes to the router's L2 norm preserves test accuracy while significantly reducing model size and computational requirements. The method is validated on large vision MoE models using benchmark datasets like CIFAR-10, CIFAR-100, and ImageNet.
  - **Keywords:** Sparse Mixture-of-Experts (MoE), Model Pruning, Trainable Routers, L2 Norm Analysis, Large Vision Models, Resource-Constrained Environments, High Memory Requirements, Inference Computation Costs, Efficient Expert Pruning Technique, Preservation of Test Accuracy, CIFAR-10, CIFAR-100, ImageNet, Transformer Architecture, Feed-Forward Networks (FFN)


- [$\mathtt{VITS}$ : Variational Inference Thompson Sampling for contextual bandits](https://icml.cc/virtual/2024/poster/33706) (Poster)
  - **Authors:** [Pierre Clavier](http://openreview.net/profile?id=~Pierre_Clavier1), [Tom Huix](http://openreview.net/profile?id=~Tom_Huix1), [Alain Oliviero Durmus](http://openreview.net/profile?id=~Alain_Oliviero_Durmus1)
  - **Affiliations:** CMAP, CNRS, Ecole Polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France; Inria Paris, 75015 Paris, France; Centre de Recherche des Cordeliers, INSERM, Universite de Paris, Sorbonne Universite, 75006 Paris, France, CMAP, CNRS, Ecole Polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France, CMAP, CNRS, Ecole Polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France
  - **TL;DR:** This paper introduces Variational Inference Thompson Sampling (VITS), a new algorithm for contextual bandits that efficiently approximates posterior distributions and achieves a sub-linear regret bound. The effectiveness of VITS is demonstrated through experiments on both synthetic and real-world datasets.
  - **Keywords:** Contextual Bandits, Thompson Sampling, Variational Inference, Gaussian Variational Inference, Recommender Systems, Mobile Health, Finance, Intractable posterior distribution, Exploration vs. Exploitation trade-off, Variational Inference TS (VITS), Sub-linear regret bound, Synthetic datasets, Real-world datasets, Multi-Armed Bandit (MAB), Posterior sampling


- [Studying K-FAC Heuristics by Viewing Adam through a Second-Order Lens](https://icml.cc/virtual/2024/poster/33178) (Poster)
  - **Authors:** [Ross Clarke](http://openreview.net/profile?id=~Ross_M_Clarke1), [Jose Miguel Hernandez-Lobato](http://openreview.net/profile?id=~Jos%C3%A9_Miguel_Hern%C3%A1ndez-Lobato1)
  - **Affiliations:** University of Cambridge, University of Cambridge
  - **TL;DR:** This study investigates the effectiveness of K-FAC's heuristics when applied to the Adam optimizer, introducing AdamQLR, a damped learning rate strategy. The findings suggest that an untuned AdamQLR can achieve performance comparable to tuned benchmarks, highlighting the variable effectiveness of K-FAC's adaptive heuristics.
  - **Keywords:** optimisation for deep learning, first-order methods, second-order methods, Adam, K-FAC, quasi-Newton methods, SGD, regression tasks, classification tasks, computational efficiency, stability issues in optimisation, AdamQLR, damped automatic learning rate strategy, adaptive heuristics


- [Improving Token-Based World Models with Parallel Observation Prediction](https://icml.cc/virtual/2024/poster/34279) (Poster)
  - **Authors:** [Lior Cohen](http://openreview.net/profile?id=~Lior_Cohen1), [Kaixin Wang](http://openreview.net/profile?id=~Kaixin_Wang1), [Bingyi Kang](http://openreview.net/profile?id=~Bingyi_Kang1), [Shie Mannor](http://openreview.net/profile?id=~Shie_Mannor2)
  - **Affiliations:** Technion – Israel Institute of Technology, Technion – Israel Institute of Technology, ByteDance, Technion – Israel Institute of Technology
  - **TL;DR:** This study introduces a novel mechanism called Parallel Observation Prediction (POP) to enhance token-based world models, significantly improving imagination speed and sample efficiency in reinforcement learning. The proposed agent, REM, achieves superhuman performance in multiple Atari games while training in under 12 hours.
  - **Keywords:** token-based world models, reinforcement learning, Parallel Observation Prediction (POP), Retentive Network (RetNet), visual environments, Atari games, sample efficiency, data demands, bottleneck in imagination, REM (Retentive Environment Model), faster imagination, superhuman performance, Atari 100K benchmark, Transformers, discrete tokens, tokenizer


- [Statistical Inference Under Constrained Selection Bias](https://icml.cc/virtual/2024/poster/34666) (Poster)
  - **Authors:** [Santiago Cortes-Gomez](http://openreview.net/profile?id=~Santiago_Cortes-Gomez1), [Mateo Dulce Rubio](http://openreview.net/profile?id=~Mateo_Dulce_Rubio1), [Carlos Miguel Patiño](http://openreview.net/profile?id=~Carlos_Miguel_Pati%C3%B1o1), [Bryan Wilder](http://openreview.net/profile?id=~Bryan_Wilder2)
  - **Affiliations:** Department of Machine Learning, Carnegie Mellon University, Department of Statistics, Carnegie Mellon University, Factored AI, Department of Machine Learning, Carnegie Mellon University
  - **TL;DR:** The study proposes a framework for statistical inference that accounts for selection bias under user-specified constraints, aiming to provide high-probability bounds on estimands for target distributions. The method leverages domain knowledge to partially identify estimands and demonstrates effectiveness through various simulated and real-world applications.
  - **Keywords:** Statistical inference, selection bias, distribution shifts, Public health, policy analysis, epidemiological studies, Selection bias, distributional shifts, confounding, High-probability bounds, estimands


- [Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation](https://icml.cc/virtual/2024/poster/34715) (Poster)
  - **Authors:** [Juntao Dai](http://openreview.net/profile?id=~Juntao_Dai1), [Yaodong Yang](http://openreview.net/profile?id=~Yaodong_Yang1), [Qian Zheng](http://openreview.net/profile?id=~Qian_Zheng5), [Gang Pan](http://openreview.net/profile?id=~Gang_Pan1)
  - **Affiliations:** College of Computer Science and Technology, Zhejiang University, Hangzhou, China; The State Key Lab of Brain-Machine Intelligence, Zhejiang University, Hangzhou, China, Center for AI Safety and Governance, Peking University, Beijing, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; The State Key Lab of Brain-Machine Intelligence, Zhejiang University, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; The State Key Lab of Brain-Machine Intelligence, Zhejiang University, Hangzhou, China
  - **TL;DR:** This study introduces a novel method for estimating constraints in Safe Reinforcement Learning, addressing the limitations of existing approaches that rely on infinite-horizon assumptions. The proposed Gradient-based Estimation method leads to the development of the Constrained Gradient-based Policy Optimization algorithm, which effectively ensures safe and efficient policy updates in finite-horizon scenarios.
  - **Keywords:** Safe Reinforcement Learning, Policy Optimization, Gradient-based Estimation, Advantage-based Estimation, Autonomous Driving, Service Robots, Safety-violation updates, Finite-horizon constraints, Constrained Gradient-based Policy Optimization (CGPO), Estimation of constraint changes, Constrained Markov Decision Process (CMDP), Markov Decision Process (MDP)


- [A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case Graph Clustering](https://icml.cc/virtual/2024/poster/34239) (Poster)
  - **Authors:** [Vincent Cohen-Addad](http://openreview.net/profile?id=~Vincent_Cohen-Addad1), [Tommaso d'Orsi](http://openreview.net/profile?id=~Tommaso_d%27Orsi1), [Aida Mousavifar](http://openreview.net/profile?id=~Aida_Mousavifar2)
  - **Affiliations:** Google Research, Google Research; BIDSA, Bocconi, Google Research
  - **TL;DR:** This paper presents a near-linear time approximation algorithm for the Balanced Cut problem in semi-random graphs, achieving an approximation value of O(α). The findings suggest that the algorithm can be extended to related problems like the Sparsest Cut and hierarchical clustering.
  - **Keywords:** Graph clustering, Graph partitioning, Beyond-worst-case complexity, Near-linear time algorithm, Polynomial time algorithm, Semidefinite programming, Data mining, Unsupervised machine learning, Balanced Cut problem, Sparsest Cut problem, Complexity of graph partitioning, O(α) approximation, O(1)-approximation to Dagupta’s objective function, Semi-random graph model, Bipartite graph, Stochastic block model


- [Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Features Model](https://icml.cc/virtual/2024/poster/33780) (Poster)
  - **Authors:** [Hien Dang](http://openreview.net/profile?id=~Hien_Dang1), [Tho Tran Huu](http://openreview.net/profile?id=~Tho_Tran_Huu1), [Tan Nguyen](http://openreview.net/profile?id=~Tan_Minh_Nguyen1), [Nhat Ho](http://openreview.net/profile?id=~Nhat_Ho1)
  - **Affiliations:** Department of Statistics and Data Sciences, University of Texas at Austin, USA; FPT Software AI Center, Vietnam, FPT Software AI Center, Vietnam, Department of Mathematics, National University of Singapore, Singapore, Department of Statistics and Data Sciences, University of Texas at Austin, USA
  - **TL;DR:** This study generalizes the Neural Collapse phenomenon to class-imbalanced datasets, demonstrating that while within-class features still collapse, class-means converge to a structure of orthogonal vectors influenced by the number of training samples. The findings highlight the alignment of classifier weights with scaled and centered class-means, addressing challenges in training deep neural networks under imbalanced conditions.
  - **Keywords:** Neural Collapse, Class-Imbalanced Learning, Deep Learning, Cross-entropy loss, Unconstrained ReLU features model, Classification tasks, Class imbalance, Training loss minimization, Drop in accuracy for minority classes, Generalization of Neural Collapse to imbalanced datasets, Class-mean convergence structure, Equiangular Tight Frame (ETF), Class-means


- [Position: Beyond Personhood: Agency, Accountability, and the Limits of Anthropomorphic Ethical Analysis](https://icml.cc/virtual/2024/poster/35019) (Oral)
  - **Authors:** [Jessica Dai](http://openreview.net/profile?id=~Jessica_Dai1)
  - **Affiliations:** Department of Computer Science, University of California Berkeley
  - **TL;DR:** This paper explores the concept of agency in AI, contrasting mechanistic and volitional views, and argues that AI should not be considered an ethical agent but rather as a product of political processes. The findings highlight the limitations of viewing AI through a human-like ethical lens and emphasize the need for a broader understanding of accountability in AI systems.
  - **Keywords:** Agency, Ethical AI, Political Processes, Mechanistic Agency, Volitional Agency, Ethical considerations in AI, Accountability, Ethical characteristics of AI, Human-like behavior


- [Generalization Bounds for Causal Regression: Insights, Guarantees and Sensitivity Analysis](https://icml.cc/virtual/2024/poster/33968) (Poster)
  - **Authors:** [Daniel Csillag](http://openreview.net/profile?id=~Daniel_Csillag1), [Claudio Struchiner](http://openreview.net/profile?id=~Claudio_Jose_Struchiner1), [Guilherme Goedert](http://openreview.net/profile?id=~Guilherme_Tegoni_Goedert1)
  - **Affiliations:** School of Applied Mathematics, Fundac¸˜ao Get´ulio Vargas, Rio de Janeiro, Brazil, School of Applied Mathematics, Fundac¸˜ao Get´ulio Vargas, Rio de Janeiro, Brazil, School of Applied Mathematics, Fundac¸˜ao Get´ulio Vargas, Rio de Janeiro, Brazil
  - **TL;DR:** This paper proposes a theory based on generalization bounds for causal machine learning, providing guarantees on model loss even in the presence of hidden confounding and violations of positivity. The authors introduce a novel change-of-measure inequality that allows for tight bounds on causal loss, demonstrating its effectiveness on both semi-synthetic and real data.
  - **Keywords:** Causal machine learning, Generalization bounds, Change-of-measure inequality, Pearson χ2 divergence, Economics, Medicine, Education research, Hidden confounding, Violations of positivity, Treatment propensities, Model loss bounds, Causal loss estimation, Semi-synthetic data, Real data, Outcome regression, Individual treatment effect estimation


- [Weighted distance nearest neighbor condensing](https://icml.cc/virtual/2024/poster/34711) (Poster)
  - **Authors:** [Lee-Ad Gottlieb](http://openreview.net/profile?id=~Lee-Ad_Gottlieb1), [Timor Sharabi](http://openreview.net/profile?id=~Timor_Sharabi1), [Roi Weiss](http://openreview.net/profile?id=~Roi_Weiss1)
  - **Affiliations:** Department of Computer Science, Ariel University, Ariel, Israel, Department of Computer Science, Ariel University, Ariel, Israel, Department of Computer Science, Ariel University, Ariel, Israel
  - **TL;DR:** This paper introduces the weighted distance nearest neighbor condensing problem, which assigns weights to points in a condensed set to improve classification accuracy. The study demonstrates that this approach can achieve significantly better condensing than traditional methods while maintaining similar generalization bounds.
  - **Keywords:** Nearest neighbor condensing, Weighted distance nearest neighbor, Weighted distance function, Condensing heuristic, Classification, Sample compression, Nearest neighbor rule disadvantages, Improved condensing, Generalization bounds


- [Dynamic Correlation Clustering in Sublinear Update Time](https://icml.cc/virtual/2024/poster/35058) (Spotlight Poster)
  - **Authors:** [Vincent Cohen-Addad](http://openreview.net/profile?id=~Vincent_Cohen-Addad1), [Silvio Lattanzi](http://openreview.net/profile?id=~Silvio_Lattanzi1), [Andreas Maggiori](http://openreview.net/profile?id=~Andreas_Maggiori1), [Nikos Parotsidis](http://openreview.net/profile?id=~Nikos_Parotsidis1)
  - **Affiliations:** Google Research, Google Research, Columbia University, Google Research
  - **TL;DR:** This paper presents an algorithm for dynamic correlation clustering in node streams, achieving an O(1)-approximation with O(polylog n) amortized update time. The study addresses the challenge of continuously partitioning nodes to minimize negative edges within clusters and positive edges between clusters.
  - **Keywords:** correlation clustering, dynamic node streams, O(1)-approximation algorithm, O(polylog n) amortized update time, clustering, machine learning, data analysis, minimizing negative edges within clusters, minimizing positive edges crossing clusters, new algorithm for dynamic correlation clustering


- [A2Q+: Improving Accumulator-Aware Weight Quantization](https://icml.cc/virtual/2024/poster/33165) (Poster)
  - **Authors:** [Ian Colbert](http://openreview.net/profile?id=~Ian_Colbert1), [Alessandro Pappalardo](http://openreview.net/profile?id=~Alessandro_Pappalardo1), [Jakoba Petri-Koenig](http://openreview.net/profile?id=~Jakoba_Petri-Koenig1), [Yaman Umuroglu](http://openreview.net/profile?id=~Yaman_Umuroglu1)
  - **Affiliations:** AMD SW Technology Team, San Diego, California, USA, AMD Research and Advanced Development, Dublin, Ireland, AMD Research and Advanced Development, Dublin, Ireland, AMD Research and Advanced Development, Dublin, Ireland
  - **TL;DR:** This study introduces A2Q+, an improved method for accumulator-aware weight quantization that alleviates constraints and enhances model accuracy while avoiding numerical overflow. The results demonstrate that ResNet50 can maintain 95% of its baseline accuracy with 12-bit accumulation, achieving a 17% improvement in test top-1 accuracy over previous methods.
  - **Keywords:** Quantization, Neural Networks, Accumulator-aware quantization (A2Q), Weight normalization, Numerical overflow, Quantization error, A2Q+, Improved ℓ1-norm bound, Weight initialization strategy, ImageNet, ResNet50


- [Multi-View Stochastic Block Models](https://icml.cc/virtual/2024/poster/34731) (Poster)
  - **Authors:** [Vincent Cohen-Addad](http://openreview.net/profile?id=~Vincent_Cohen-Addad1), [Tommaso d'Orsi](http://openreview.net/profile?id=~Tommaso_d%27Orsi1), [Silvio Lattanzi](http://openreview.net/profile?id=~Silvio_Lattanzi1), [Rajai Nasser](http://openreview.net/profile?id=~Rajai_Nasser1)
  - **Affiliations:** Google Research, Google Research; BIDSA, Bocconi, Google Research, Google Research
  - **TL;DR:** This paper introduces a new family of models called multi-view stochastic block models for graph clustering, focusing on leveraging multiple data sources to improve clustering outcomes. The authors present efficient algorithms that outperform previous methods and provide theoretical insights into the model's limitations.
  - **Keywords:** graph clustering, multi-view clustering, multi-view stochastic block models, efficient algorithms, data mining, social sciences, statistics, social network analysis, clustering structure recovery, partial information from multiple graphs, new efficient algorithm, information-theoretic lower bound, stochastic block model


- [Harmonizing Generalization and Personalization in Federated Prompt Learning](https://icml.cc/virtual/2024/poster/33769) (Poster)
  - **Authors:** [Tianyu Cui](http://openreview.net/profile?id=~Tianyu_Cui2), [Hongxia Li](http://openreview.net/profile?id=~Hongxia_Li1), [Jingya Wang](http://openreview.net/profile?id=~Jingya_Wang3), [Ye Shi](http://openreview.net/profile?id=~Ye_Shi1)
  - **Affiliations:** ShanghaiTech University, ShanghaiTech University, ShanghaiTech University, ShanghaiTech University
  - **TL;DR:** This study introduces Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP) to balance personalization and generalization in federated learning. The proposed method demonstrates superior performance in addressing data heterogeneity and improving generalization capabilities across various datasets.
  - **Keywords:** Federated Learning, Vision-Language Models, Personalization, Generalization, Prompt Tuning, CLIP, Low-Rank Adaptation, Contrastive Loss, Data Heterogeneity, Model Overfitting, Generalization to Unseen Domains, Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP), Vision-Language Models (VLM), Federated Prompt Learning (FPL)


- [A decoder-only foundation model for time-series forecasting](https://icml.cc/virtual/2024/poster/33288) (Poster)
  - **Authors:** [Abhimanyu Das](http://openreview.net/profile?id=~Abhimanyu_Das2), [Weihao Kong](http://openreview.net/profile?id=~Weihao_Kong1), [Rajat Sen](http://openreview.net/profile?id=~Rajat_Sen1), [Yichen Zhou](http://openreview.net/profile?id=~Yichen_Zhou3)
  - **Affiliations:** Google Research, Google Research, Google Research, Google Research
  - **TL;DR:** This paper presents TimesFM, a decoder-only foundation model for time-series forecasting that achieves near state-of-the-art zero-shot performance across various datasets. The model leverages a large-scale time-series corpus and demonstrates effectiveness in handling diverse forecasting scenarios without additional training.
  - **Keywords:** time-series forecasting, foundation model, decoder style attention model, input patching, retail, finance, manufacturing, healthcare, natural sciences, zero-shot forecasting, varying history lengths, prediction lengths, time granularities, TimesFM model, state-of-the-art zero-shot accuracy, real-world datasets, synthetic datasets


- [Conformal Prediction Sets Improve Human Decision Making](https://icml.cc/virtual/2024/poster/35030) (Poster)
  - **Authors:** [Jesse Cresswell](http://openreview.net/profile?id=~Jesse_C._Cresswell1), [yi sui](http://openreview.net/profile?id=~Yi_Sui1), [Bhargava Kumar](http://openreview.net/profile?email=bhargava1409%40gmail.com), [Noël Vouitsis](http://openreview.net/profile?id=~No%C3%ABl_Vouitsis1)
  - **Affiliations:** Layer 6 AI, Layer 6 AI, TD Securities, Layer 6 AI
  - **TL;DR:** This study investigates the effectiveness of conformal prediction sets in enhancing human decision-making accuracy. The findings indicate that providing calibrated prediction sets significantly improves task performance compared to fixed-size prediction sets.
  - **Keywords:** Conformal prediction, Human decision making, Conformal prediction, Prediction sets, Human-in-the-loop decision making, Human-AI teams, Uncertainty quantification, Lack of alternative predictions, Improved accuracy with conformal prediction sets


- [Test-Time Degradation Adaptation for Open-Set Image Restoration](https://icml.cc/virtual/2024/poster/33829) (Spotlight Poster)
  - **Authors:** [Yuanbiao Gou](http://openreview.net/profile?id=~Yuanbiao_Gou1), [Haiyu Zhao](http://openreview.net/profile?id=~Haiyu_Zhao2), [Boyun Li](http://openreview.net/profile?id=~Boyun_Li1), [Xinyan Xiao](http://openreview.net/profile?id=~Xinyan_Xiao1), [Xi Peng](http://openreview.net/profile?id=~Xi_Peng3)
  - **Affiliations:** College of Computer Science, Sichuan University, Chengdu, China, College of Computer Science, Sichuan University, Chengdu, China, College of Computer Science, Sichuan University, Chengdu, China, Baidu Inc., Beijing, China, College of Computer Science, Sichuan University, Chengdu, China
  - **TL;DR:** This study introduces a test-time degradation adaptation framework for open-set image restoration, addressing the challenge of handling unknown degradations not seen during pretraining. The proposed method demonstrates comparable or superior performance to task-specific methods through experiments on multiple degradations.
  - **Keywords:** open-set image restoration, image restoration, test-time adaptation, degradation-agnostic diffusion model, image restoration, unknown degradations, distribution shifts, test-time degradation adaptation framework


- [New Bounds on the Cohesion of Complete-link and Other Linkage Methods for Agglomerative Clustering](https://icml.cc/virtual/2024/poster/33432) (Poster)
  - **Authors:** [Sanjoy Dasgupta](http://openreview.net/profile?id=~Sanjoy_Dasgupta3), [Eduardo Laber](http://openreview.net/profile?id=~Eduardo_Sany_Laber1)
  - **Affiliations:** University of California San Diego, USA, Departamento de Informática, PUC-RIO, Brazil
  - **TL;DR:** This study presents new bounds on the maximum diameter of clustering produced by complete-linkage methods in metric spaces, demonstrating that complete-linkage is more effective than single-linkage for creating compact clusters. The findings improve existing bounds and provide insights into the cohesion of various linkage methods, including average-linkage.
  - **Keywords:** hierarchical clustering, agglomerative clustering, complete-linkage, average-linkage, minimax, exploratory analysis, computational resource reduction, clustering quality, maximum diameter, compact clusters, new bounds on clustering diameter, improved analysis


- [High-Order Contrastive Learning with Fine-grained Comparative Levels for Sparse Ordinal Tensor Completion](https://icml.cc/virtual/2024/poster/34135) (Poster)
  - **Authors:** [Yu Dai](http://openreview.net/profile?id=~Yu_Dai4), [Junchen Shen](http://openreview.net/profile?id=~Junchen_Shen2), [Zijie Zhai](http://openreview.net/profile?id=~Zijie_Zhai1), [Danlin Liu](http://openreview.net/profile?id=~Danlin_Liu1), [Jingyang Chen](http://openreview.net/profile?id=~Jingyang_Chen1), [Yu Sun](http://openreview.net/profile?id=~Yu_Sun14), [Ping Li](http://openreview.net/profile?id=~Ping_Li12), [Jie Zhang](http://openreview.net/profile?id=~Jie_Zhang10), [Kai Zhang](http://openreview.net/profile?id=~Kai_Zhang1)
  - **Affiliations:** School of Computer Science and Technology, East China Normal University, Shanghai, China, School of Computer Science and Technology, East China Normal University, Shanghai, China, School of Computer Science and Technology, East China Normal University, Shanghai, China, School of Computer Science and Technology, East China Normal University, Shanghai, China, Institute of Science and Technology for Brain Inspired Intelligence, Fudan University, Shanghai, China, Indeed Inc., Sunnyvale, United States, Southwest Petroleum University, Chengdu, China, Institute of Science and Technology for Brain Inspired Intelligence, Fudan University, Shanghai, China, School of Computer Science and Technology, East China Normal University, Shanghai, China
  - **TL;DR:** The study introduces High-Order Contrastive Tensor Completion (HOCTC), a novel approach to extend contrastive learning for sparse ordinal tensor regression, effectively capturing high-order interactions and improving representation learning. Experiments demonstrate its effectiveness in traffic monitoring and recommendation systems.
  - **Keywords:** Contrastive learning, Sparse ordinal tensor completion, High-Order Contrastive Tensor Completion (HOCTC), Attention-based strategy, Query-expansion, Traffic monitoring, Recommender systems, High-dimensional tensor interactions, Data sparsity, Negative sample growth, Efficient sampling scheme, Self-supervised signals for high-order representation learning


- [Asymptotically Optimal and Computationally Efficient Average Treatment Effect Estimation in A/B testing](https://icml.cc/virtual/2024/poster/33478) (Poster)
  - **Authors:** [VIKAS DEEP](http://openreview.net/profile?id=~VIKAS_DEEP1), [Achal Bassamboo](http://openreview.net/profile?email=a-bassamboo%40kellogg.northwestern.edu), [Sandeep Juneja](http://openreview.net/profile?id=~Sandeep_Kumar_Juneja1)
  - **Affiliations:** Kellogg School of Management, Northwestern University, Evanston, IL 60201, Kellogg School of Management, Northwestern University, Evanston, IL 60201, Ashoka University, Sonipat, Haryana, India
  - **TL;DR:** This study focuses on estimating the average treatment effect (ATE) in A/B testing by developing adaptive policies that minimize expected sample size while ensuring a confidence interval with specified coverage guarantees. The findings reveal that both the proposed asymptotically optimal and an alternative computationally efficient policy perform similarly across practical values, providing effective solutions for A/B testing scenarios.
  - **Keywords:** A/B testing, average treatment effect (ATE), confidence interval (CI), adaptive policy, max-min optimization, clinical trials, online platforms, estimating ATE, minimizing expected sample size, coverage guarantee, asymptotically optimal policies, computationally efficient methods


- [Provably Better Explanations with Optimized Aggregation of Feature Attributions](https://icml.cc/virtual/2024/poster/35061) (Poster)
  - **Authors:** [Thomas Decker](http://openreview.net/profile?id=~Thomas_Decker1), [Ananta Bhattarai](http://openreview.net/profile?id=~Ananta_R._Bhattarai1), [Jindong Gu](http://openreview.net/profile?id=~Jindong_Gu1), [Volker Tresp](http://openreview.net/profile?id=~Volker_Tresp1), [Florian Buettner](http://openreview.net/profile?id=~Florian_Buettner1)
  - **Affiliations:** LMU Munich; Siemens AG, Siemens AG; Technical University of Munich, University of Oxford, LMU Munich; Munich Center for Machine Learning (MCML), Siemens AG; Goethe University Frankfurt; German Cancer Research Center (DKFZ)
  - **TL;DR:** This study proposes a novel approach to improve the quality of feature attributions in machine learning by combining multiple explanation methods through optimal convex combinations. The results demonstrate significant enhancements in robustness and faithfulness compared to individual attribution methods.
  - **Keywords:** Explainability, Feature Attribution, Machine Learning Transparency, Convex Combinations, Feature Attribution Techniques, Reliability of Feature Attributions, Sensitivity to Input Perturbations, Improved Robustness, Faithfulness of Explanations


- [ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback](https://icml.cc/virtual/2024/poster/34726) (Poster)
  - **Authors:** [Ganqu Cui](http://openreview.net/profile?id=~Ganqu_Cui1), [Lifan Yuan](http://openreview.net/profile?id=~Lifan_Yuan1), [Ning Ding](http://openreview.net/profile?id=~Ning_Ding5), [Guanming Yao](http://openreview.net/profile?id=~Guanming_Yao1), [Bingxiang He](http://openreview.net/profile?id=~Bingxiang_He1), [Wei Zhu](http://openreview.net/profile?id=~Wei_Zhu7), [Yuan Ni](http://openreview.net/profile?id=~Yuan_Ni1), [Guotong Xie](http://openreview.net/profile?id=~Guotong_Xie4), [Ruobing Xie](http://openreview.net/profile?id=~Ruobing_Xie2), [Yankai Lin](http://openreview.net/profile?id=~Yankai_Lin1), [Zhiyuan Liu](http://openreview.net/profile?id=~Zhiyuan_Liu1), [Maosong Sun](http://openreview.net/profile?id=~Maosong_Sun1)
  - **Affiliations:** NLP Group, DCST, IAI, BNRIST, Tsinghua University, University of Illinois Urbana-Champaign, NLP Group, DCST, IAI, BNRIST, Tsinghua University, NLP Group, DCST, IAI, BNRIST, Tsinghua University; ModelBest.Inc, NLP Group, DCST, IAI, BNRIST, Tsinghua University, PingAn Technology, PingAn Technology, PingAn Technology, Tencent, Renmin University of China, NLP Group, DCST, IAI, BNRIST, Tsinghua University, Jiangsu Collaborative Innovation Center for Language Ability
  - **TL;DR:** This study introduces ULTRAFEEDBACK, a large-scale AI feedback dataset aimed at enhancing the alignment of large language models with human preferences by automating feedback collection. The findings demonstrate that scaled AI feedback can effectively support the development of robust open-source chat language models.
  - **Keywords:** AI feedback, human feedback, language model alignment, reinforcement learning, best-of-n sampling, open-source chat language models, data scarcity, annotation biases, alignment with human preferences, ULTRAFEEDBACK dataset, scalable AI feedback, ULTRAFEEDBACK, GPT-4, large language models (LLMs), user-assistant interactions


- [Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments](https://icml.cc/virtual/2024/poster/34373) (Poster)
  - **Authors:** [Antoine Dedieu](http://openreview.net/profile?id=~Antoine_Dedieu1), [Wolfgang Lehrach](http://openreview.net/profile?id=~Wolfgang_Lehrach1), [Guangyao Zhou](http://openreview.net/profile?id=~Guangyao_Zhou1), [Dileep George](http://openreview.net/profile?id=~Dileep_George1), [Miguel Lazaro-Gredilla](http://openreview.net/profile?id=~Miguel_Lazaro-Gredilla1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This study introduces a transformer variant that learns cognitive maps from observations in partially observed environments, enabling efficient path planning. The proposed model retains high predictive performance while significantly improving the speed of solving shortest path problems compared to traditional methods.
  - **Keywords:** Partially Observed Environments (POEs), Cognitive Maps, Path Planning, Transformer with discrete bottlenecks (TDB), Next-token prediction, In-context learning (ICL), Navigation, Reinforcement Learning, Natural Language Processing, Path planning in POEs, Perceptual aliasing, Disambiguation of spatial positions, Compressed representation of observations and actions, Interpretable cognitive maps, Efficient path planning, Large Language Models (LLMs), Action-conditioned latent graph


- [Multi-View Clustering by Inter-cluster Connectivity Guided Reward](https://icml.cc/virtual/2024/poster/32835) (Poster)
  - **Authors:** [Hao Dai](http://openreview.net/profile?id=~Hao_Dai2), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu76), [Peng Su](http://openreview.net/profile?id=~Peng_Su2), [Hecheng Cai](http://openreview.net/profile?id=~Hecheng_Cai1), [Shudong Huang](http://openreview.net/profile?id=~Shudong_Huang1), [Jiancheng Lv](http://openreview.net/profile?id=~Jiancheng_Lv2)
  - **Affiliations:** College of Computer Science, Sichuan University, Chengdu 610065, China; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, China, College of Computer Science, Sichuan University, Chengdu 610065, China; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, China, College of Computer Science, Sichuan University, Chengdu 610065, China; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, China, College of Computer Science, Sichuan University, Chengdu 610065, China; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, China, College of Computer Science, Sichuan University, Chengdu 610065, China; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, China, College of Computer Science, Sichuan University, Chengdu 610065, China; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, China
  - **TL;DR:** This paper presents a novel graph-based multi-view clustering algorithm that infers the unknown number of clusters (k) using a graph consistency reward mechanism. The proposed method effectively produces clustering results without requiring a predefined cluster number, demonstrating superior performance on benchmark datasets compared to existing methods.
  - **Keywords:** Multi-view clustering, Graph-based clustering, Reinforcement learning, Graph consistency reward mechanism, Unknown number of clusters (k), Inter-cluster connectivity, Novel clustering algorithm, Effective clustering results without predefined k, Benchmark datasets


- [Boosting Offline Optimizers with Surrogate Sensitivity](https://icml.cc/virtual/2024/poster/33691) (Poster)
  - **Authors:** [Cuong Dao](http://openreview.net/profile?id=~Manh_Cuong_Dao1), [Phi Le Nguyen](http://openreview.net/profile?id=~Phi_Le_Nguyen2), [Thao Nguyen Truong](http://openreview.net/profile?id=~Thao_Nguyen_Truong1), [Nghia Hoang](http://openreview.net/profile?id=~Trong_Nghia_Hoang1)
  - **Affiliations:** School of Information and Communications Technology, Hanoi University of Science and Technology, Hanoi, Vietnam, School of Information and Communications Technology, Hanoi University of Science and Technology, Hanoi, Vietnam, National Institute of Advanced Industrial Science and Technology, Tokyo, Japan, School of Electrical Engineering and Computer Science, Washington State University, Washington, USA
  - **TL;DR:** This study addresses the challenges of offline optimization in material engineering by developing a sensitivity measurement for surrogate models, which helps regulate their sensitivity and improve optimization performance. The findings suggest that conditioning offline optimizers with less sensitive surrogates can lead to more reliable predictions and better outcomes in material design.
  - **Keywords:** Offline optimization, material engineering, Surrogate modeling, sensitivity measurement, sensitivity-informed regularizer, Material design, computational optimization, Model sensitivity, unreliable predictions, out-of-distribution data, Optimizable sensitivity measurement, improved optimization performance, Black-box function, parameterization


- [Global Reinforcement Learning : Beyond Linear and Convex Rewards via Submodular Semi-gradient Methods](https://icml.cc/virtual/2024/poster/35205) (Poster)
  - **Authors:** [Riccardo De Santi](http://openreview.net/profile?id=~Riccardo_De_Santi1), [Manish Prajapat](http://openreview.net/profile?id=~Manish_Prajapat1), [Andreas Krause](http://openreview.net/profile?id=~Andreas_Krause1)
  - **Affiliations:** ETH Zurich; ETH AI Center, ETH Zurich; ETH AI Center, ETH Zurich; ETH AI Center
  - **TL;DR:** This paper introduces Global Reinforcement Learning (GRL) to address the limitations of classic RL in modeling complex interactions between states by defining rewards over trajectories. The authors propose a novel algorithmic scheme that efficiently converts GRL problems into classic RL problems, demonstrating its effectiveness through empirical results.
  - **Keywords:** Global Reinforcement Learning, interactions between states, submodular optimization, semi-gradient methods, experiment design, exploration, imitation learning, risk-averse RL, additive objectives, negative interactions, synergetic effects, curvature-dependent approximation guarantees, hardness of approximation results


- [Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction](https://icml.cc/virtual/2024/poster/35112) (Poster)
  - **Authors:** [Riccardo De Santi](http://openreview.net/profile?id=~Riccardo_De_Santi1), [Federico Arangath Joseph](http://openreview.net/profile?id=~Federico_Arangath_Joseph1), [Noah Liniger](http://openreview.net/profile?id=~Noah_Liniger1), [Mirco Mutti](http://openreview.net/profile?id=~Mirco_Mutti1), [Andreas Krause](http://openreview.net/profile?id=~Andreas_Krause1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, ETH AI Center, Zurich, Switzerland, Department of Computer Science, ETH Zurich, Zurich, Switzerland, Department of Computer Science, ETH Zurich, Zurich, Switzerland, Technion, Haifa, Israel, Department of Computer Science, ETH Zurich, Zurich, Switzerland
  - **TL;DR:** This study proposes the Geometric Active Exploration (GAE) algorithm to enhance the efficiency of Active Exploration in Markov Decision Processes by leveraging geometric structures. The findings indicate that abstraction through MDP homomorphisms significantly improves sample efficiency in experimental design tasks within scientific discovery contexts.
  - **Keywords:** Active Exploration, Optimal Experimental Design, Reinforcement Learning, Convex Reinforcement Learning, MDP Homomorphisms, Scientific Discovery, Environmental Sensing, Scalability of Active Exploration, Uncertainty Minimization, Geometric Active Exploration (GAE) algorithm, Sample Efficiency


- [Prediction-powered Generalization of Causal Inferences](https://icml.cc/virtual/2024/poster/34090) (Poster)
  - **Authors:** [Ilker Demirel](http://openreview.net/profile?id=~Ilker_Demirel1), [Ahmed Alaa](http://openreview.net/profile?id=~Ahmed_Alaa1), [Anthony Philippakis](http://openreview.net/profile?id=~Anthony_Philippakis1), [David Sontag](http://openreview.net/profile?id=~David_Sontag1)
  - **Affiliations:** MIT CSAIL; Eric and Wendy Schmidt Center, Broad Institute of MIT and Harvard, Department of Computational Precision Health, UC Berkeley and UCSF, Eric and Wendy Schmidt Center, Broad Institute of MIT and Harvard, MIT CSAIL
  - **TL;DR:** This study addresses the challenge of generalizing causal inferences from randomized controlled trials to target populations with different distributions of effect modifiers. The authors develop prediction-powered algorithms that enhance generalization by integrating trial data with observational study data, demonstrating improved performance even in the presence of unmeasured confounding.
  - **Keywords:** Causal inference, Generalization of trial results, Generalization algorithms, Prediction models, Randomized controlled trials, Observational studies, Limited external validity, Confounding bias, Estimation of causal effects, Prediction-powered estimators, Improved generalization methods, Randomized controlled trials (RCT), Average treatment effect (ATE), Effect modifiers


- [Predicting Lagrangian Multipliers for Mixed Integer Linear Programs](https://icml.cc/virtual/2024/poster/33682) (Poster)
  - **Authors:** [Francesco Demelas](http://openreview.net/profile?id=~Francesco_Demelas1), [Joseph Roux](http://openreview.net/profile?id=~Joseph_Le_Roux1), [Mathieu Lacroix](http://openreview.net/profile?id=~Mathieu_Lacroix1), [Axel Parmentier](http://openreview.net/profile?id=~Axel_Parmentier1)
  - **Affiliations:** Laboratoire d’Informatique de Paris-Nord, Université Sorbonne Paris Nord — CNRS, France, Laboratoire d’Informatique de Paris-Nord, Université Sorbonne Paris Nord — CNRS, France, Laboratoire d’Informatique de Paris-Nord, Université Sorbonne Paris Nord — CNRS, France, CERMICS, École des Ponts, France
  - **TL;DR:** This study introduces a deep learning approach to predict Lagrangian Multipliers for Mixed Integer Linear Programs, effectively optimizing the computational burden associated with traditional iterative methods. The proposed method significantly narrows the gap between continuous relaxation and the best Lagrangian bound, providing a valuable warm-start for further optimization techniques.
  - **Keywords:** Lagrangian Relaxation, Mixed Integer Linear Programs (MILPs), deep learning, graph neural network, probabilistic encoder, decoder, Multi-Commodity Network Design, Generalized Assignment, difficult constraints, optimization, computational burden, high-dimensional representations, warm-start for descent-based methods, Lagrangian Multipliers (LMs), Continuous Relaxation (CR)


- [Exploring the Low-Pass Filtering Behavior in Image Super-Resolution](https://icml.cc/virtual/2024/poster/35191) (Poster)
  - **Authors:** [Haoyu Deng](http://openreview.net/profile?id=~Haoyu_Deng1), [Zijing Xu](http://openreview.net/profile?id=~Zijing_Xu1), [Yule Duan](http://openreview.net/profile?id=~Yule_Duan1), [Xiao Wu](http://openreview.net/profile?id=~Xiao_Wu6), [Wen-Jie Shu](http://openreview.net/profile?id=~Wenjie_Shu1), [Liang-Jian Deng](http://openreview.net/profile?id=~Liang-Jian_Deng2)
  - **Affiliations:** University of Electronic Science and Technology of China, University of Electronic Science and Technology of China, University of Electronic Science and Technology of China, University of Electronic Science and Technology of China, University of Electronic Science and Technology of China, University of Electronic Science and Technology of China
  - **TL;DR:** This paper investigates the behavior of deep neural networks in image super-resolution (ISR) by interpreting their functionality through signal processing theories, revealing that these networks can be decomposed into linear and non-linear systems. The authors introduce a new metric, Frequency Spectrum Distribution Similarity (FSDS), to quantify high-frequency information injected by the networks, addressing the interpretability issues associated with neural networks in ISR tasks.
  - **Keywords:** Image Super-Resolution (ISR), Deep Neural Networks, Hybrid Response Analysis (HyRA), Low-Pass Filtering, Image Processing, Black Box Nature of Neural Networks, Interpretation of Neural Network Behavior, Frequency Spectrum Distribution Similarity (FSDS), Sinc Phenomenon, Linear System, Non-Linear System, Signal Processing


- [Network Tight Community Detection](https://icml.cc/virtual/2024/poster/33826) (Poster)
  - **Authors:** [Jiayi Deng](http://openreview.net/profile?id=~Jiayi_Deng1), [Xiaodong Yang](http://openreview.net/profile?id=~Xiaodong_Yang7), [Jun Yu](http://openreview.net/profile?id=~Jun_Yu4), [Jun Liu](http://openreview.net/profile?id=~Jun_Liu3), [Zhaiming Shen](http://openreview.net/profile?id=~Zhaiming_Shen1), [Danyang Huang](http://openreview.net/profile?email=dyhuang89%40126.com), [Huimin Cheng](http://openreview.net/profile?id=~Huimin_Cheng1)
  - **Affiliations:** Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing, China, Department of Statistics, Harvard University, Cambridge MA, USA, School of Mathematics and Statistics, Beijing Institute of Technology, Beijing, China, Department of Statistics, Harvard University, Cambridge MA, USA, Department of Mathematics, University of Georgia, Athens GA, USA, Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing, China, Department of Biostatistics, Boston University, Boston MA, USA
  - **TL;DR:** This study introduces a tight community detection (TCD) method that effectively identifies tight communities in networks while excluding noninformative scattered nodes. The proposed method demonstrates strong theoretical guarantees and scalability, addressing biases introduced by conventional community detection methods.
  - **Keywords:** community detection, network analysis, tight community detection (TCD), spectral clustering, biological networks, social networks, scattered nodes, community structure identification, identification accuracy, scalability for large networks, tight nodes, scattered nodes


- [An Unsupervised Approach for Periodic Source Detection in Time Series](https://icml.cc/virtual/2024/poster/33616) (Poster)
  - **Authors:** [Berken Utku Demirel](http://openreview.net/profile?id=~Berken_Utku_Demirel2), [Christian Holz](http://openreview.net/profile?id=~Christian_Holz1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Switzerland, Department of Computer Science, ETH Zurich, Switzerland
  - **TL;DR:** This study presents a novel unsupervised method for detecting periodic patterns in noisy time series data without requiring labeled data or complex augmentations. The proposed approach significantly outperforms existing methods, achieving over 45-50% performance improvements in various tasks.
  - **Keywords:** Periodic pattern detection, Time series analysis, Health monitoring, Behavior analysis, Unsupervised learning, Self-supervised learning, Data representation learning, Health monitoring, Behavior analysis, Time series data analysis, Noisy time series data, Lack of labeled data, Data augmentation challenges, Model collapse, Novel method for periodicity detection, Performance improvements over state-of-the-art methods


- [Going beyond Compositions, DDPMs Can Produce Zero-Shot Interpolations](https://icml.cc/virtual/2024/poster/35132) (Poster)
  - **Authors:** [Justin Deschenaux](http://openreview.net/profile?id=~Justin_Deschenaux1), [Igor Krawczuk](http://openreview.net/profile?id=~Igor_Krawczuk1), [Grigorios Chrysos](http://openreview.net/profile?id=~Grigorios_Chrysos1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1)
  - **Affiliations:** Department of Computer Science, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland, LIONS, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland, Department of Electrical and Computer Engineering, University of Wisconsin-Madison, USA, LIONS, École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland
  - **TL;DR:** This study investigates the capabilities of Denoising Diffusion Probabilistic Models (DDPMs) to generate images in intermediate regions of the data distribution, demonstrating zero-shot interpolation between distinct attributes such as smiling and non-smiling faces. The findings suggest that DDPMs can effectively generalize beyond their training data, which has implications for fairness and bias mitigation in generative models.
  - **Keywords:** Denoising Diffusion Probabilistic Models (DDPMs), image generation, compositionality, Generating images in unexplored, intermediate regions of the distribution, zero-shot interpolation, latent factors, generative models, interpolation, fairness, bias mitigation


- [Collaborative Learning with Different Labeling Functions](https://icml.cc/virtual/2024/poster/33948) (Poster)
  - **Authors:** [yuyang deng](http://openreview.net/profile?id=~Yuyang_Deng3), [Mingda Qiao](http://openreview.net/profile?id=~Mingda_Qiao1)
  - **Affiliations:** Pennsylvania State University, State College, PA, USA, University of California, Berkeley, Berkeley, CA, USA
  - **TL;DR:** This study explores a variant of Collaborative PAC Learning to develop accurate classifiers for multiple data distributions with different labeling functions while minimizing sample usage. The authors present a sample-efficient learning algorithm under a weaker realizability assumption, demonstrating the feasibility of learning despite the challenges posed by heterogeneous data sources.
  - **Keywords:** Collaborative Learning, PAC Learning, Multi-task Learning, Empirical Risk Minimization (ERM), Sample efficiency, Different labeling functions, Data distribution challenges, Learning algorithm with sample complexity O(kd log(n/k) + n log n), (k, ϵ)-realizability, VC dimension


- [Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning](https://icml.cc/virtual/2024/poster/33778) (Poster)
  - **Authors:** [Jannik Deuschel](http://openreview.net/profile?id=~Jannik_Deuschel1), [Caleb Ellington](http://openreview.net/profile?id=~Caleb_Ellington1), [Yingtao Luo](http://openreview.net/profile?id=~Yingtao_Luo1), [Ben Lengerich](http://openreview.net/profile?id=~Ben_Lengerich1), [Pascal Friederich](http://openreview.net/profile?id=~Pascal_Friederich1), [Eric Xing](http://openreview.net/profile?id=~Eric_Xing1)
  - **Affiliations:** Carnegie Mellon University; Karlsruhe Institute of Technology, Carnegie Mellon University, Carnegie Mellon University, Broad Institute of MIT and Harvard; MIT, Karlsruhe Institute of Technology, MBZUAI; Carnegie Mellon University; Petuum, Inc.
  - **TL;DR:** The study introduces Contextualized Policy Recovery (CPR) to improve the interpretability and accuracy of decision-making models in medical contexts. CPR achieves state-of-the-art performance in predicting antibiotic and MRI prescriptions, effectively bridging the gap between interpretable and black-box methods.
  - **Keywords:** Interpretable policy learning, decision-making processes, Contextualized Policy Recovery (CPR), multi-task learning, linear mapping, Medical informatics, antibiotic prescription, MRI prescription, Tradeoff between accuracy and interpretability, dynamic human decision-making, State-of-the-art performance in predicting medical prescriptions, improved interpretability, Inverse reinforcement learning, imitation learning, black-box methods


- [Multicalibration for Confidence Scoring in LLMs](https://icml.cc/virtual/2024/poster/34914) (Poster)
  - **Authors:** [Gianluca Detommaso](http://openreview.net/profile?id=~Gianluca_Detommaso1), [Martin A Bertran](http://openreview.net/profile?id=~Martin_A_Bertran1), [Riccardo Fogliato](http://openreview.net/profile?id=~Riccardo_Fogliato1), [Aaron Roth](http://openreview.net/profile?id=~Aaron_Roth1)
  - **Affiliations:** AWS AI; University of Pennsylvania, AWS AI, AWS AI, University of Pennsylvania
  - **TL;DR:** This paper introduces multicalibration techniques to enhance the reliability of confidence scores for outputs from large language models, addressing the issue of hallucinations. The proposed methods demonstrate significant improvements in calibration and accuracy through systematic benchmarking across various datasets.
  - **Keywords:** Multicalibration, Confidence Scoring, Hallucination Detection, Multicalibration algorithms, Clustering, Self-annotation, Large Language Models (LLMs), Question Answering, Hallucination in LLMs, Calibration of confidence scores, Improved calibration and accuracy of confidence scores, AI Safety, AI Alignment


- [Convex and Bilevel Optimization for Neural-Symbolic Inference and Learning](https://icml.cc/virtual/2024/poster/34926) (Poster)
  - **Authors:** [Charles Dickens](http://openreview.net/profile?id=~Charles_Andrew_Dickens1), [Changyu Gao](http://openreview.net/profile?id=~Changyu_Gao1), [Connor Pryor](http://openreview.net/profile?id=~Connor_Pryor1), [Stephen Wright](http://openreview.net/profile?id=~Stephen_Wright1), [Lise Getoor](http://openreview.net/profile?id=~Lise_Getoor1)
  - **Affiliations:** Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95060, Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI 53706, Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95060, Department of Computer Sciences, University of Wisconsin-Madison, Madison, WI 53706, Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95060
  - **TL;DR:** This paper presents a novel gradient-based learning framework for neural-symbolic systems, specifically NeuPSL, utilizing convex and bilevel optimization techniques. The proposed dual block coordinate descent algorithm significantly enhances learning runtime and prediction performance, achieving over 100× speedup and up to 16% improvement in prediction accuracy across various datasets.
  - **Keywords:** neural-symbolic AI, parameter learning framework, convex optimization, bilevel optimization, dual block coordinate descent, Moreau envelope, augmented Lagrangian method, non-smooth inference, constrained optimization, learning runtime improvements, prediction performance improvements, 8 datasets (specific names not provided), NeSy energy-based models (NeSy-EBMs), NeuPSL


- [Double Stochasticity Gazes Faster: Snap-Shot Decentralized Stochastic Gradient Tracking Methods](https://icml.cc/virtual/2024/poster/33156) (Poster)
  - **Authors:** [Hao Di](http://openreview.net/profile?id=~Hao_Di2), [Haishan Ye](http://openreview.net/profile?id=~Haishan_Ye2), [Xiangyu Chang](http://openreview.net/profile?id=~Xiangyu_Chang1), [Guang Dai](http://openreview.net/profile?id=~Guang_Dai1), [Ivor Tsang](http://openreview.net/profile?id=~Ivor_Tsang1)
  - **Affiliations:** Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China; SGIT AI Lab, State Grid Corporation of China, Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China; SGIT AI Lab, State Grid Corporation of China, Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China, SGIT AI Lab, State Grid Corporation of China, College of Computing and Data Science, NTU, Singapore; CFAR and IHPC, Agency for Science, Technology and Research (A*STAR), Singapore
  - **TL;DR:** This paper proposes two novel algorithms, snap-shot DSGT and accelerated snap-shot DSGT, to improve decentralized stochastic gradient descent methods by addressing the limitations of existing algorithms in terms of iteration complexity and network topology. The proposed methods demonstrate superior performance in decentralized optimization for large-scale machine learning models.
  - **Keywords:** Decentralized optimization, decentralized stochastic gradient descent (SGD), Distributed stochastic gradient tracking (DSGT), snapshot gradient tracking, snap-shot DSGT, accelerated snap-shot DSGT, Large-scale machine learning models, Data ownership, privacy, scalability, data heterogeneity, non-IID data distribution, Lower iteration complexity algorithms, theoretical guarantees for decentralized SGD methods


- [Quality Diversity through Human Feedback: Towards Open-Ended Diversity-Driven Optimization](https://icml.cc/virtual/2024/poster/34789) (Poster)
  - **Authors:** [Li Ding](http://openreview.net/profile?id=~Li_Ding3), [Jenny Zhang](http://openreview.net/profile?id=~Jenny_Zhang1), [Jeff Clune](http://openreview.net/profile?id=~Jeff_Clune3), [Lee Spector](http://openreview.net/profile?id=~Lee_Spector2), [Joel Lehman](http://openreview.net/profile?id=~Joel_Lehman1)
  - **Affiliations:** Manning College of Information & Computer Sciences, University of Massachusetts Amherst, Department of Computer Science, University of British Columbia; Vector Institute, Department of Computer Science, University of British Columbia; Vector Institute; Canada CIFAR AI Chair, Department of Computer Science, Amherst College, Stochastic Labs
  - **TL;DR:** This paper introduces Quality Diversity through Human Feedback (QDHF), a method that enhances Quality Diversity algorithms by inferring diversity metrics from human judgments, significantly improving diversity in generative tasks like text-to-image generation. Empirical results demonstrate QDHF's superiority in automatic diversity discovery and its effectiveness in complex optimization tasks.
  - **Keywords:** Quality Diversity, Human Feedback, Reinforcement Learning, Reinforcement Learning from Human Feedback (RLHF), Quality Diversity (QD) algorithms, Text-to-image generation, Robotics, Lack of diversity in model responses, mode collapse, optimization challenges in open-ended tasks, Quality Diversity through Human Feedback (QDHF), automatic diversity discovery


- [Efficient Algorithms for Sum-Of-Minimum Optimization](https://icml.cc/virtual/2024/poster/33282) (Poster)
  - **Authors:** [Lisang Ding](http://openreview.net/profile?id=~Lisang_Ding1), [Ziang Chen](http://openreview.net/profile?id=~Ziang_Chen1), [Xinshang Wang](http://openreview.net/profile?id=~Xinshang_Wang1), [Wotao Yin](http://openreview.net/profile?id=~Wotao_Yin1)
  - **Affiliations:** Department of Mathematics, University of California, Los Angeles, Los Angeles, CA, USA, Department of Mathematics, Massachusetts Institute of Technology, Cambridge, MA, USA, Decision Intelligence Lab, Alibaba US, Bellevue, WA, USA, Decision Intelligence Lab, Alibaba US, Bellevue, WA, USA
  - **TL;DR:** This paper introduces a novel "sum-of-minimum" optimization model aimed at minimizing the average of N objective functions over k parameters, with applications in clustering. The authors develop efficient algorithms inspired by k-means and demonstrate their effectiveness across various tasks, outperforming simpler optimization reformulations.
  - **Keywords:** sum-of-minimum optimization, clustering, k-means, Lloyd’s algorithm, gradient-descent-like convergence, machine learning, generalized principal component analysis, mixed linear regression, neural network training, optimization problems, clustering applications, efficient algorithms, new tight bound for initialization algorithm, Bregman divergence


- [Consistent Adversarially Robust Linear Classification: Non-Parametric Setting](https://icml.cc/virtual/2024/poster/34236) (Poster)
  - **Authors:** [Elvis Dohmatob](http://openreview.net/profile?id=~Elvis_Dohmatob1)
  - **Affiliations:** Meta FAIR
  - **TL;DR:** This study presents a non-parametric approach to constructing a Bayes-optimal robust linear classifier for binary classification, achieving a minimax excess adversarial risk of (cid:101)O((cid:112)d/n). The proposed estimator effectively addresses adversarial attacks while maintaining consistency in high-dimensional settings.
  - **Keywords:** Adversarial Machine Learning, Robust Linear Classification, Gaussian Smoothing, Robust 0/1 Loss Minimization, Safety-Critical Applications, Autonomous Vehicles, Adversarial Attacks, High-Dimensional Data, Lack of Robustness, Consistent Estimators, Minimax Excess Adversarial Risk


- [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://icml.cc/virtual/2024/poster/34166) (Poster)
  - **Authors:** [Yiran Ding](http://openreview.net/profile?id=~Yiran_Ding1), [Li Lyna Zhang](http://openreview.net/profile?id=~Li_Lyna_Zhang1), [Chengruidong Zhang](http://openreview.net/profile?id=~Chengruidong_Zhang1), [Yuanyuan Xu](http://openreview.net/profile?id=~Yuanyuan_Xu3), [Ning Shang](http://openreview.net/profile?id=~Ning_Shang1), [Jiahang Xu](http://openreview.net/profile?id=~Jiahang_Xu2), [Fan Yang](http://openreview.net/profile?id=~Fan_Yang28), [Mao Yang](http://openreview.net/profile?id=~Mao_Yang1)
  - **Affiliations:** Microsoft Research; Hangzhou Dianzi University, Microsoft Research, Microsoft Research, Microsoft Research; University of Science and Technology of China, Microsoft Research, Microsoft Research, Microsoft Research, Microsoft Research
  - **TL;DR:** This paper presents LongRoPE, a method that extends the context window of pre-trained large language models to 2048k tokens with minimal fine-tuning, addressing challenges related to fine-tuning costs and the scarcity of long texts. The proposed method demonstrates effectiveness through extensive experiments on LLaMA2 and Mistral, while maintaining performance on shorter context windows.
  - **Keywords:** Large Language Models, Context Window Extension, LongRoPE, Positional Interpolation, Fine-tuning, Natural Language Processing, Limited context window size, Fine-tuning costs, Scarcity of long texts, Catastrophic values from new token positions, Extension of context window to 2048k tokens, Improved fine-tuning strategies, LLaMA2, Mistral, RoPE (Rotary Positional Embedding), Transformer


- [Spectral Preconditioning for Gradient Methods on Graded Non-convex Functions](https://icml.cc/virtual/2024/poster/34186) (Poster)
  - **Authors:** [Nikita Doikov](http://openreview.net/profile?id=~Nikita_Doikov1), [Sebastian Stich](http://openreview.net/profile?id=~Sebastian_U_Stich1), [Martin Jaggi](http://openreview.net/profile?id=~Martin_Jaggi1)
  - **Affiliations:** Machine Learning and Optimization Laboratory (MLO), EPFL, Lausanne, Switzerland, CISPA Helmholtz Center for Information Security, Saarbrücken, Germany, Machine Learning and Optimization Laboratory (MLO), EPFL, Lausanne, Switzerland
  - **TL;DR:** This study introduces a novel concept of graded non-convexity to better characterize non-convex optimization problems and proposes spectral preconditioning methods that significantly improve convergence rates for gradient descent, particularly in cases with large gaps between the top eigenvalues of the Hessian. The findings are supported by numerical experiments across various machine learning applications.
  - **Keywords:** optimization methods, graded non-convexity, gradient methods, spectral preconditioning, machine learning, ill-conditioning, slow convergence rates, superior convergence rates, new family of spectral preconditioners, Hessian, eigenvalues, convexity


- [Precise Accuracy / Robustness Tradeoffs in Regression: Case of General Norms](https://icml.cc/virtual/2024/poster/33619) (Poster)
  - **Authors:** [Elvis Dohmatob](http://openreview.net/profile?id=~Elvis_Dohmatob1), [Meyer Scetbon](http://openreview.net/profile?id=~Meyer_Scetbon1)
  - **Affiliations:** Meta FAIR, Microsoft Research
  - **TL;DR:** This study investigates the tradeoffs between adversarial robustness and accuracy in linear regression models, revealing that while some robustness can be achieved without sacrificing accuracy, significant tradeoffs may be unavoidable in certain regimes. The findings provide a theoretical framework for understanding these dynamics, particularly in safety-critical applications.
  - **Keywords:** Adversarial Robustness, Linear Regression, Safety-Critical Applications, Autonomous Vehicles, Health, Adversarial Attacks, Tradeoffs between Standard and Adversarial Risk, Optimal Robustness, Regularized Models, Adversarial Examples, Generalization Error, Covariance Matrices


- [Privacy-Preserving Data Release Leveraging Optimal Transport and Particle Gradient Descent](https://icml.cc/virtual/2024/poster/34998) (Poster)
  - **Authors:** [Konstantin Donhauser](http://openreview.net/profile?id=~Konstantin_Donhauser1), [Javier Abad](http://openreview.net/profile?id=~Javier_Abad1), [Neha Hulkund](http://openreview.net/profile?id=~Neha_Hulkund1), [Fanny Yang](http://openreview.net/profile?id=~Fanny_Yang1)
  - **Affiliations:** Department of Computer Science, ETH, Zurich, Switzerland, Department of Computer Science, ETH, Zurich, Switzerland, MIT CSAIL, Boston, USA, Department of Computer Science, ETH, Zurich, Switzerland
  - **TL;DR:** This paper introduces PrivPGD, a novel method for differentially private data synthesis of protected tabular datasets, leveraging optimal transport and particle gradient descent. The method demonstrates superior performance and scalability compared to existing approaches while preserving the geometric structure of the data.
  - **Keywords:** differential privacy, data synthesis, protected tabular datasets, optimal transport, particle gradient descent, marginal-based approaches, healthcare, government, privacy concerns, data distribution, sensitive datasets, PrivPGD, state-of-the-art performance, scalability, geometry preservation


- [Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models](https://icml.cc/virtual/2024/poster/35128) (Poster)
  - **Authors:** [Peijie Dong](http://openreview.net/profile?id=~Peijie_Dong1), [Lujun Li](http://openreview.net/profile?id=~Lujun_Li1), [Zhenheng Tang](http://openreview.net/profile?id=~Zhenheng_Tang2), [Xiang Liu](http://openreview.net/profile?id=~Xiang_Liu10), [Xinglin Pan](http://openreview.net/profile?id=~Xinglin_Pan1), [Qiang Wang](http://openreview.net/profile?id=~Qiang_Wang14), [Xiaowen Chu](http://openreview.net/profile?id=~Xiaowen_Chu2)
  - **Affiliations:** The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, Hong Kong Baptist University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology, Shenzhen, The Hong Kong University of Science and Technology
  - **TL;DR:** This study presents Pruner-Zero, an automatic framework for evolving symbolic pruning metrics for Large Language Models, addressing the challenges of model deployment without the need for retraining. Extensive experiments demonstrate that Pruner-Zero outperforms state-of-the-art post-training pruning methods, enhancing efficiency in model optimization.
  - **Keywords:** Large Language Models, Pruning Methods, Genetic Programming, Symbolic Pruning Metrics, Language Modeling, Zero-Shot Tasks, Deployment Challenges, Computational Demands, Auto-Generation of Pruning Metrics, Correlation between Pruning Metrics and Performance, LLaMA, LLaMA-2, Post-Training Pruning, Model Compression Techniques


- [Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination](https://icml.cc/virtual/2024/poster/34601) (Poster)
  - **Authors:** [Ilias Diakonikolas](http://openreview.net/profile?id=~Ilias_Diakonikolas1), [Daniel Kane](http://openreview.net/profile?id=~Daniel_Kane1), [Sushrut Karmalkar](http://openreview.net/profile?id=~Sushrut_Karmalkar2), [Ankit Pensia](http://openreview.net/profile?id=~Ankit_Pensia1), [Thanasis Pittas](http://openreview.net/profile?id=~Thanasis_Pittas1)
  - **Affiliations:** University of Wisconsin-Madison, University of California, San Diego, University of Wisconsin-Madison, IBM Research, University of Wisconsin-Madison
  - **TL;DR:** This study presents efficient robust estimators for Gaussian sparse estimation tasks under Huber's contamination model, achieving optimal error guarantees. The proposed algorithms significantly improve upon previous methods by reducing sample complexity and providing accurate mean estimates in the presence of outliers.
  - **Keywords:** Robust statistics, Gaussian sparse estimation, Huber contamination model, Robust estimators, multi-dimensional filtering, Mean estimation, PCA, linear regression, Outlier contamination, sample complexity, k-sparse mean estimation, Optimal error guarantees, efficient algorithms, Huber contamination model, k-sparse


- [Towards Generalization beyond Pointwise Learning: A Unified Information-theoretic Perspective](https://icml.cc/virtual/2024/poster/32665) (Poster)
  - **Authors:** [Yuxin Dong](http://openreview.net/profile?id=~Yuxin_Dong1), [Tieliang Gong](http://openreview.net/profile?id=~Tieliang_Gong2), [Hong Chen](http://openreview.net/profile?id=~Hong_Chen1), [Zhongjiang He](http://openreview.net/profile?id=~Zhongjiang_He1), [Shiquan Wang](http://openreview.net/profile?id=~Mengxiang_Li2), [Shuangyong Song](http://openreview.net/profile?id=~Shuangyong_Song2), [Chen Li](http://openreview.net/profile?id=~Chen_Li19)
  - **Affiliations:** School of Computer Science and Technology, Xi’an Jiaotong University, School of Computer Science and Technology, Xi’an Jiaotong University, College of Science, Huazhong Agriculture University, China Telecom Corporation Limited, China Telecom Corporation Limited, China Telecom Corporation Limited, School of Computer Science and Technology, Xi’an Jiaotong University
  - **TL;DR:** This paper develops a series of information-theoretic bounds that extend beyond pointwise learning scenarios to include pairwise, triplet, quadruplet, and higher-order contexts, addressing challenges like non-i.i.d losses and dimensionality explosion. The proposed bounds effectively capture the generalization dynamics of iterative and noisy learning algorithms, demonstrating their applicability across diverse learning scenarios.
  - **Keywords:** generalization, non-pointwise learning, contrastive learning, information-theoretic bounds, gradient covariance analysis, contrastive representation learning, deep metric learning, AUC maximization, ranking algorithms, non-i.i.d losses, dimensionality explosion, generalization gap, hypothesis-based bounds, prediction-based bounds, computationally tractable information metrics


- [Learning-Rate-Free Stochastic Optimization over Riemannian Manifolds](https://icml.cc/virtual/2024/poster/33494) (Spotlight Poster)
  - **Authors:** [Daniel Dodd](http://openreview.net/profile?id=~Daniel_Dodd1), [Louis Sharrock](http://openreview.net/profile?id=~Louis_Sharrock1), [Chris Nemeth](http://openreview.net/profile?id=~Christopher_Nemeth1)
  - **Affiliations:** Department of Mathematics and Statistics, Lancaster University, UK, Department of Mathematics and Statistics, Lancaster University, UK, Department of Mathematics and Statistics, Lancaster University, UK
  - **TL;DR:** This study presents innovative learning-rate-free algorithms for stochastic optimization over Riemannian manifolds, addressing the challenges of hyperparameter tuning. The proposed methods achieve optimal convergence rates with high probability guarantees, validated through numerical experiments against traditional learning-rate-dependent algorithms.
  - **Keywords:** Riemannian optimization, learning-rate-free optimization, Stochastic optimization algorithms, Distance over Gradients (DoG), Distance over Weighted Gradients (DoWG), Principal component analysis, low-rank matrix completion, tensor factorization, Gaussian mixture models, metric learning, Learning rate tuning, convergence rate challenges, High probability convergence guarantees, dynamic learning-rate-scheduler algorithms, Geodesically convex functions, Riemannian manifolds


- [Trust Regions for Explanations via Black-Box Probabilistic Certification](https://icml.cc/virtual/2024/poster/34549) (Poster)
  - **Authors:** [Amit Dhurandhar](http://openreview.net/profile?id=~Amit_Dhurandhar1), [Swagatam Haldar](http://openreview.net/profile?id=~Swagatam_Haldar1), [Dennis Wei](http://openreview.net/profile?id=~Dennis_Wei1), [Karthikeyan Ramamurthy](http://openreview.net/profile?id=~Karthikeyan_Natesan_Ramamurthy1)
  - **Affiliations:** IBM Research, Yorktown Heights, NY, USA, Eberhard-Karls-Universität Tübingen, Tübingen, Germany, IBM Research, Yorktown Heights, NY, USA, IBM Research, Yorktown Heights, NY, USA
  - **TL;DR:** This paper introduces a novel problem of black-box explanation certification, focusing on finding the largest hypercube around an example where explanations maintain a specified quality metric. The proposed method, Ecertify, provides theoretical guarantees and demonstrates practical efficacy on both synthetic and real data.
  - **Keywords:** Explainability, Black-box models, Explanation certification, ℓ∞ ball, Stability of explanations, Fidelity of explanations, Ecertify approach, Theoretical guarantees, Synthetic data, Real data, Trust region, Quality metric


- [Impact of Decentralized Learning on Player Utilities in Stackelberg Games](https://icml.cc/virtual/2024/poster/32633) (Poster)
  - **Authors:** [Kate Donahue](http://openreview.net/profile?id=~Kate_Donahue1), [Nicole Immorlica](http://openreview.net/profile?id=~Nicole_Immorlica3), [Meena Jagadeesan](http://openreview.net/profile?id=~Meena_Jagadeesan1), [Brendan Lucier](http://openreview.net/profile?id=~Brendan_Lucier1), [Alex Slivkins](http://openreview.net/profile?id=~Aleksandrs_Slivkins1)
  - **Affiliations:** Cornell University, Microsoft Research, University of California, Berkeley, Microsoft Research, Microsoft Research
  - **TL;DR:** This study investigates the learning dynamics in decentralized two-agent systems modeled as Stackelberg games, revealing that standard regret benchmarks can lead to linear regret for at least one player. The authors propose relaxed regret benchmarks and develop algorithms achieving near-optimal regret, enhancing understanding of agent interactions in sequential learning environments.
  - **Keywords:** Decentralized learning, Two-agent systems, Stackelberg games, Learning dynamics, Regret benchmarks, Learning algorithms, Recommender systems, Chatbots, Misalignment of rewards, Learning from noisy feedback, Near-optimal regret algorithms, Relaxed regret benchmarks, Stackelberg equilibrium, Multi-armed bandit


- [Accelerating PDE Data Generation via Differential Operator Action in Solution Space](https://icml.cc/virtual/2024/poster/34521) (Poster)
  - **Authors:** [huanshuo dong](http://openreview.net/profile?id=~huanshuo_dong1), [Hong Wang](http://openreview.net/profile?id=~Hong_Wang14), [Haoyang Liu](http://openreview.net/profile?id=~Haoyang_Liu2), [Jian Luo](http://openreview.net/profile?id=~Jian_Luo5), [Jie Wang](http://openreview.net/profile?id=~Jie_Wang1)
  - **Affiliations:** CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China
  - **TL;DR:** This study introduces a novel algorithm called Differential Operator Action in Solution space (DiffOAS) to accelerate the generation of high-precision datasets for Partial Differential Equations (PDEs). The results demonstrate that DiffOAS can generate large-scale datasets 300 times faster than existing methods while maintaining comparable performance in training Neural Operators.
  - **Keywords:** PDE data generation, data-driven approaches, Neural Operator (NO), Differential Operator Action in Solution space (DiffOAS), Requirement for high-precision training data, computational costs in data generation, Efficient data generation, time complexity reduction, Partial Differential Equations (PDEs), operator action


- [Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference](https://icml.cc/virtual/2024/poster/32813) (Poster)
  - **Authors:** [Harry Dong](http://openreview.net/profile?id=~Harry_Dong1), [Xinyu Yang](http://openreview.net/profile?id=~Xinyu_Yang4), [Zhenyu Zhang](http://openreview.net/profile?id=~Zhenyu_Zhang4), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1), [Yuejie Chi](http://openreview.net/profile?id=~Yuejie_Chi1), [Beidi Chen](http://openreview.net/profile?id=~Beidi_Chen1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Carnegie Mellon University, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, USA, Department of Electrical and Computer Engineering, University of Texas at Austin, USA, Department of Electrical and Computer Engineering, University of Texas at Austin, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, USA; Meta AI (FAIR), USA
  - **TL;DR:** This paper addresses the memory bottleneck in large language models caused by the key-value (KV) cache during decoding. The proposed LESS method integrates a constant-sized cache with eviction-based methods, allowing for efficient information retention and reducing the performance gap associated with traditional caching methods.
  - **Keywords:** Large Language Models, Memory Optimization, KV Cache Compression, Eviction-based Cache Methods, Natural Language Processing, Chatbots, Long Document Tasks, Memory Bottleneck, Computational Burden, LESS Method, Efficient Information Retention, Llama 2 7B, Transformer Architecture, Key-Value (KV) Cache, Sparse Policies


- [Locally Interdependent Multi-Agent MDP: Theoretical Framework for Decentralized Agents with Dynamic Dependencies](https://icml.cc/virtual/2024/poster/33351) (Poster)
  - **Authors:** [Alex DeWeese](http://openreview.net/profile?id=~Alex_DeWeese2), [Guannan Qu](http://openreview.net/profile?id=~Guannan_Qu1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh PA, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh PA, USA
  - **TL;DR:** This paper introduces the Locally Interdependent Multi-Agent MDP, a theoretical framework for decentralized agents with dynamic dependencies, and proposes three near-optimal policies that are scalable and effective in various applications such as cooperative navigation and obstacle avoidance. The findings reveal that the decentralized solution closely approximates the fully observable solution, enhancing the tractability of multi-agent decision-making.
  - **Keywords:** decentralized multi-agent systems, dynamic dependencies, Locally Interdependent Multi-Agent MDP, closed-form policies, cooperative navigation, obstacle avoidance, formation control, decentralized partial observability, dynamic local dependencies, scalability, near-optimal policies, improved tractability


- [Fast Co-Training under Weak Dependence via Stream-Based Active Learning](https://icml.cc/virtual/2024/poster/34486) (Oral)
  - **Authors:** [Ilias Diakonikolas](http://openreview.net/profile?id=~Ilias_Diakonikolas1), [Mingchen Ma](http://openreview.net/profile?id=~Mingchen_Ma1), [Lisheng Ren](http://openreview.net/profile?id=~Lisheng_Ren1), [Christos Tzamos](http://openreview.net/profile?id=~Christos_Tzamos1)
  - **Affiliations:** Department of Computer Sciences, University of Wisconsin-Madison, Madison, USA, Department of Computer Sciences, University of Wisconsin-Madison, Madison, USA, Department of Computer Sciences, University of Wisconsin-Madison, Madison, USA, University of Athens and Archimedes AI, Athens, Greece
  - **TL;DR:** This study investigates co-training in the context of stream-based active learning, demonstrating that various natural concept classes can be efficiently learned with low label complexity. The authors present new algorithms that achieve efficient co-training under weak dependence, significantly improving label efficiency and computational efficiency.
  - **Keywords:** Co-training, Semi-supervised learning, Active learning, Stream-based active learning, Online classification, Webpage classification, Spam detection, Large language models, Data sparsity, Label complexity, Efficient co-training algorithms, Label complexity reductions, VC dimension, Mistake bound online model, Weak dependence


- [Robust Stable Spiking Neural Networks](https://icml.cc/virtual/2024/poster/33217) (Poster)
  - **Authors:** [Ding Jianhao](http://openreview.net/profile?id=~Jianhao_Ding1), [Zhiyu Pan](http://openreview.net/profile?id=~Zhiyu_Pan3), [Yujia Liu](http://openreview.net/profile?id=~Yujia_Liu1), [Zhaofei Yu](http://openreview.net/profile?id=~Zhaofei_Yu1), [Tiejun Huang](http://openreview.net/profile?id=~Tiejun_Huang1)
  - **Affiliations:** School of Computer Science, Peking University, Beijing, China 100871, School of Computer Science, Peking University, Beijing, China 100871, School of Computer Science, Peking University, Beijing, China 100871, Institute for Artificial Intelligence, Peking University, Beijing, China 100871, School of Computer Science, Peking University, Beijing, China 100871
  - **TL;DR:** This paper investigates the robustness of spiking neural networks (SNNs) against adversarial attacks by analyzing the stability of their dynamics and proposing a training framework to enhance robustness. The effectiveness of the framework is validated through experiments involving Gaussian noise training and adversarial training in image classification tasks.
  - **Keywords:** Spiking Neural Networks (SNNs), Robustness, Adversarial Attacks, Leaky Integrate-and-Fire (LIF) neuron model, Membrane potential perturbation dynamics, Autonomous driving, Robotic control, Vulnerability to adversarial attacks, Lack of robustness in safety-critical applications, Training framework for enhanced robustness, Input-output stability of perturbation dynamics


- [Double Variance Reduction: A Smoothing Trick for Composite Optimization Problems without First-Order Gradient](https://icml.cc/virtual/2024/poster/33521) (Spotlight Poster)
  - **Authors:** [Hao Di](http://openreview.net/profile?id=~Hao_Di2), [Haishan Ye](http://openreview.net/profile?id=~Haishan_Ye2), [Yueling Zhang](http://openreview.net/profile?id=~Yueling_Zhang3), [Xiangyu Chang](http://openreview.net/profile?id=~Xiangyu_Chang1), [Guang Dai](http://openreview.net/profile?id=~Guang_Dai1), [Ivor Tsang](http://openreview.net/profile?id=~Ivor_Tsang1)
  - **Affiliations:** Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China; SGIT AI Lab, State Grid Corporation of China, Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China; SGIT AI Lab, State Grid Corporation of China, International Business School, Beijing Foreign Studies University, Beijing, China, Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China, SGIT AI Lab, State Grid Corporation of China, CFAR and IHPC, Agency for Science, Technology and Research (A*STAR), Singapore; College of Computing and Data Science, NTU, Singapore
  - **TL;DR:** This paper introduces the Zeroth-order Proximal Double Variance Reduction (ZPDVR) method to address coordinate-wise variance in composite optimization problems, achieving linear convergence with reduced computational costs. The method relies solely on random gradient estimates and demonstrates superior performance compared to existing techniques.
  - **Keywords:** Composite optimization, Variance reduction, Zeroth-order optimization, Stochastic zeroth-order oracle, Averaging trick, Signal compression, Image recovery, Sparse model training, Coordinate-wise variance, High-dimensional optimization, Zeroth-order Proximal Double Variance Reduction (ZPDVR), Linear convergence, Strongly convex, Smooth functions, Non-smooth functions


- [On the Universality of Volume-Preserving and Coupling-Based Normalizing Flows](https://icml.cc/virtual/2024/poster/32839) (Poster)
  - **Authors:** [Felix Draxler](http://openreview.net/profile?id=~Felix_Draxler1), [Stefan Wahl](http://openreview.net/profile?id=~Stefan_Wahl1), [Christoph Schnörr](http://openreview.net/profile?id=~Christoph_Schnoerr1), [Ullrich Koethe](http://openreview.net/profile?id=~Ullrich_Koethe1)
  - **Affiliations:** Heidelberg University, Germany, Heidelberg University, Germany, Heidelberg University, Germany, Heidelberg University, Germany
  - **TL;DR:** This paper presents a theoretical framework to understand the expressive power of normalizing flows, demonstrating that volume-preserving flows are not universal and proposing a solution to enhance their expressivity. It also establishes a distributional universality theorem for coupling-based normalizing flows, highlighting their efficiency in learning complex distributions.
  - **Keywords:** normalizing flows, generative modeling, density estimation, coupling-based normalizing flows, volume-preserving flows, RealNVP, computer vision, thermodynamic systems, uncertainty quantification, expressivity of normalizing flows, limitations of volume-preserving flows, distributional universality theorem, solutions for restoring universality


- [WorkArena: How Capable are Web Agents at Solving Common Knowledge Work Tasks?](https://icml.cc/virtual/2024/poster/34722) (Poster)
  - **Authors:** [Alexandre Drouin](http://openreview.net/profile?id=~Alexandre_Drouin2), [Maxime Gasse](http://openreview.net/profile?id=~Maxime_Gasse2), [Massimo Caccia](http://openreview.net/profile?id=~Massimo_Caccia1), [Issam Laradji](http://openreview.net/profile?id=~Issam_H._Laradji1), [Manuel Del Verme](http://openreview.net/profile?id=~Manuel_Del_Verme1), [Tom Marty](http://openreview.net/profile?id=~Tom_Marty1), [David Vazquez](http://openreview.net/profile?id=~David_Vazquez1), [Nicolas Chapados](http://openreview.net/profile?id=~Nicolas_Chapados1), [Alexandre Lacoste](http://openreview.net/profile?id=~Alexandre_Lacoste1)
  - **Affiliations:** ServiceNow Research; Mila – Quebec AI Research Institute, ServiceNow Research; Mila – Quebec AI Research Institute, ServiceNow Research, ServiceNow Research, Mila – Quebec AI Research Institute, ServiceNow Research; Mila – Quebec AI Research Institute, ServiceNow Research, ServiceNow Research; Mila – Quebec AI Research Institute, ServiceNow Research
  - **TL;DR:** This study investigates the capabilities of large language model-based web agents in performing common knowledge work tasks within enterprise software systems, introducing the WorkArena benchmark for evaluation. The findings indicate that while these agents show potential, there is a significant gap in achieving full task automation, particularly highlighting disparities between open and closed-source models.
  - **Keywords:** web agents, knowledge work, enterprise software, large language models (LLMs), UI assistants, enterprise software systems, digital workflows, task automation, user experience, accessibility, WorkArena benchmark, BrowserGym environment, ServiceNow platform, performance disparity between open and closed-source LLMs


- [Position: Building Guardrails for Large Language Models Requires Systematic Design](https://icml.cc/virtual/2024/poster/34361) (Poster)
  - **Authors:** [Yi DONG](http://openreview.net/profile?id=~Yi_DONG7), [Ronghui Mu](http://openreview.net/profile?id=~Ronghui_Mu1), [Gaojie Jin](http://openreview.net/profile?id=~Gaojie_Jin1), [Yi Qi](http://openreview.net/profile?id=~Yi_Qi3), [Jinwei Hu](http://openreview.net/profile?id=~Jinwei_Hu1), [Xingyu Zhao](http://openreview.net/profile?id=~Xingyu_Zhao1), [Jie Meng](http://openreview.net/profile?id=~Jie_Meng2), [Wenjie Ruan](http://openreview.net/profile?id=~Wenjie_Ruan2), [Xiaowei Huang](http://openreview.net/profile?id=~Xiaowei_Huang1)
  - **Affiliations:** Department of Computer Science, University of Liverpool, UK, Department of Computer Science, University of Liverpool, UK, Key Laboratory of System Software (Chinese Academy of Sciences); State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Department of Computer Science, University of Liverpool, UK, Department of Computer Science, University of Liverpool, UK, WMG, University of Warwick, Warwick, UK, Institute of Digital Technologies, Loughborough University London, UK, Department of Computer Science, University of Liverpool, UK, Department of Computer Science, University of Liverpool, UK
  - **TL;DR:** This paper discusses the importance of building guardrails for Large Language Models (LLMs) to mitigate risks associated with their use, advocating for a systematic approach that incorporates socio-technical methods and advanced implementations. The authors highlight the challenges in establishing requirements for these guardrails and propose strategies for effective verification and testing.
  - **Keywords:** Large Language Models, AI Safety, Guardrails, Reinforcement Learning from Human Feedback (RLHF), Neural-Symbolic Implementations, Ethical use, Data biases, Privacy, Robustness, Potential misuse, Systematic approach to construct guardrails, Verification and testing methods, Open-source solutions (Llama Guard, Nvidia NeMo, Guardrails AI)


- [TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling](https://icml.cc/virtual/2024/poster/32734) (Poster)
  - **Authors:** [Jiaxiang Dong](http://openreview.net/profile?id=~Jiaxiang_Dong1), [Haixu Wu](http://openreview.net/profile?id=~Haixu_Wu1), [Yuxuan Wang](http://openreview.net/profile?id=~Yuxuan_Wang5), [Yun-Zhong Qiu](http://openreview.net/profile?id=~Yun-Zhong_Qiu1), [Li Zhang](http://openreview.net/profile?id=~Li_Zhang37), [Jianmin Wang](http://openreview.net/profile?id=~Jianmin_Wang1), [Mingsheng Long](http://openreview.net/profile?id=~Mingsheng_Long5)
  - **Affiliations:** School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University
  - **TL;DR:** This paper introduces TimeSiam, a self-supervised pre-training framework for time series modeling that effectively captures temporal correlations using Siamese networks. The proposed method outperforms existing pre-training techniques, demonstrating enhanced forecasting and classification performance across multiple benchmarks.
  - **Keywords:** Time series pre-training, self-supervised learning, Siamese networks, masked modeling, contrastive learning, Energy, traffic, economics, weather, medicine, Temporal correlation modeling, data augmentation challenges, TimeSiam framework, improved forecasting and classification capabilities


- [Position: Compositional Generative Modeling: A Single Model is Not All You Need](https://icml.cc/virtual/2024/poster/33992) (Poster)
  - **Authors:** [Yilun Du](http://openreview.net/profile?id=~Yilun_Du1), [Leslie Kaelbling](http://openreview.net/profile?id=~Leslie_Pack_Kaelbling1)
  - **Affiliations:** MIT, MIT
  - **TL;DR:** This paper advocates for compositional generative modeling as a more efficient alternative to large monolithic models in AI, demonstrating that smaller models can generalize better to unseen data and can be combined to create new generative models for previously unencountered tasks. The findings suggest that this approach can address issues of data efficiency and model adaptability in AI research.
  - **Keywords:** compositional generative modeling, data efficiency, generalization, AI research, natural language processing, computer vision, decision-making, data sparsity, poor reasoning ability, hallucinations, adaptation to new task distributions, new generative models, discovery of compositional components, large generative models, monolithic models


- [Principled Gradient-Based MCMC for Conditional Sampling of Text](https://icml.cc/virtual/2024/poster/34746) (Poster)
  - **Authors:** [Li Du](http://openreview.net/profile?id=~Li_Du2), [Afra Amini](http://openreview.net/profile?id=~Afra_Amini1), [Lucas Torroba Hennigen](http://openreview.net/profile?id=~Lucas_Torroba_Hennigen1), [Xinyan Velocity Yu](http://openreview.net/profile?id=~Xinyan_Velocity_Yu1), [Holden Lee](http://openreview.net/profile?id=~Holden_Lee1), [Jason Eisner](http://openreview.net/profile?id=~Jason_Eisner1), [Ryan Cotterell](http://openreview.net/profile?id=~Ryan_Cotterell1)
  - **Affiliations:** Johns Hopkins University, ETH Zürich, MIT CSAIL, University of Southern California, Johns Hopkins University, Johns Hopkins University, ETH Zürich
  - **TL;DR:** This study proposes principled gradient-based MCMC methods for conditional text sampling from energy-based models, addressing the limitations of previous approaches that failed to converge to the target distribution. The new samplers demonstrate improved fluency in generated text while adhering to control objectives, with theoretical guarantees on their convergence properties.
  - **Keywords:** energy-based models, controlled text generation, Markov Chain Monte Carlo (MCMC), Hamiltonian Monte Carlo (HMC), Langevin dynamics, Gibbs sampling, natural language processing, text generation, sampling from discrete distributions, convergence of sampling methods, novel gradient-based samplers, convergence and mixing properties


- [Spike Distance Function as a Learning Objective for Spike Prediction](https://icml.cc/virtual/2024/poster/33205) (Poster)
  - **Authors:** [Kevin Doran](http://openreview.net/profile?id=~Kevin_Doran1), [Marvin Seifert](http://openreview.net/profile?id=~Marvin_Seifert1), [Carola Yovanovich](http://openreview.net/profile?id=~Carola_AM_Yovanovich1), [Tom Baden](http://openreview.net/profile?id=~Tom_Baden1)
  - **Affiliations:** School of Life Sciences, University of Sussex, UK; School of Engineering and Informatics, University of Sussex, UK, School of Life Sciences, University of Sussex, UK; School of Engineering and Informatics, University of Sussex, UK, School of Life Sciences, University of Sussex, UK; School of Engineering and Informatics, University of Sussex, UK, School of Life Sciences, University of Sussex, UK; Institute of Ophthalmic Research, University of Tübingen, Germany
  - **TL;DR:** This study introduces a novel spike distance function as a learning objective for predicting neuronal spikes, addressing the limitations of traditional Poisson models. The proposed method demonstrates superior performance in inferring spike trains from retinal ganglion cell recordings compared to existing approaches.
  - **Keywords:** spike prediction, neuronal spike responses, spike distance function, neural networks, retinal prosthetics, visual stimuli processing, modeling neuronal responses, limitations of Poisson models, new learning objective, improved spike train inference, recordings of chicken and frog retinal ganglion cells, Poisson process, firing rate


- [AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls](https://icml.cc/virtual/2024/poster/33001) (Poster)
  - **Authors:** [YU DU](http://openreview.net/profile?id=~Yu_Du9), [Fangyun Wei](http://openreview.net/profile?id=~Fangyun_Wei1), [Hongyang Zhang](http://openreview.net/profile?id=~Hongyang_Zhang1)
  - **Affiliations:** Tsinghua University, Microsoft Research Asia, University of Waterloo
  - **TL;DR:** The paper introduces AnyTool, a large language model agent that utilizes over 16,000 APIs to effectively address user queries through a hierarchical structure and self-reflection mechanism. Experiments show that AnyTool significantly outperforms existing models, achieving a 35.4% higher average pass rate on the revised evaluation benchmark, AnyToolBench.
  - **Keywords:** large language models, tool utilization, hierarchical API-retriever, self-reflection mechanism, impractical solutions, evaluation protocol limitations, AnyTool, AnyToolBench, improved pass rate, 16,000 APIs from Rapid API, ToolBench, GPT-4, ToolLLM


- [DE-COP: Detecting Copyrighted Content in Language Models Training Data](https://icml.cc/virtual/2024/poster/34297) (Poster)
  - **Authors:** [André Duarte](http://openreview.net/profile?id=~Andr%C3%A9_Vicente_Duarte1), [Xuandong Zhao](http://openreview.net/profile?id=~Xuandong_Zhao1), [Arlindo Oliveira](http://openreview.net/profile?id=~Arlindo_L._Oliveira1), [Lei Li](http://openreview.net/profile?id=~Lei_Li11)
  - **Affiliations:** INESC-ID / Instituto Superior Técnico, ULisboa, University of California, Santa Barbara, INESC-ID / Instituto Superior Técnico, ULisboa, Carnegie Mellon University
  - **TL;DR:** The study presents DE-COP, a method for detecting copyrighted content in language model training data by probing LLMs with multiple-choice questions. The results demonstrate a significant improvement in detection accuracy, highlighting the importance of addressing copyright issues in AI development.
  - **Keywords:** Copyright detection, Language models, DE-COP, multiple-choice questions, LLM (Large Language Model), AI ethics, Copyright law in AI, Detection of copyrighted content, Ethical and legal standards in data collection, Improved detection performance, Benchmark construction (BookTection), BookTection


- [Learning Iterative Reasoning through Energy Diffusion](https://icml.cc/virtual/2024/poster/34671) (Poster)
  - **Authors:** [Yilun Du](http://openreview.net/profile?id=~Yilun_Du1), [Jiayuan Mao](http://openreview.net/profile?id=~Jiayuan_Mao1), [Josh Tenenbaum](http://openreview.net/profile?id=~Joshua_B._Tenenbaum1)
  - **Affiliations:** MIT, MIT, MIT
  - **TL;DR:** This paper introduces the Iterative Reasoning through Energy Diffusion (IRED) framework, which formulates reasoning and decision-making tasks as energy-based optimization problems. IRED demonstrates strong generalization capabilities and outperforms existing methods in various reasoning tasks, particularly in more complex scenarios.
  - **Keywords:** iterative reasoning, energy-based optimization, decision-making, energy functions, annealed energy landscapes, score function supervision, Sudoku puzzles, matrix completion, path finding, reasoning tasks, optimization challenges, generalization to harder instances, IRED framework, adaptive computation, improved performance on reasoning tasks


- [When and How Does In-Distribution Label Help Out-of-Distribution Detection?](https://icml.cc/virtual/2024/poster/33236) (Poster)
  - **Authors:** [Xuefeng Du](http://openreview.net/profile?id=~Xuefeng_Du1), [Yiyou Sun](http://openreview.net/profile?id=~Yiyou_Sun1), [Sharon Li](http://openreview.net/profile?id=~Yixuan_Li1)
  - **Affiliations:** Department of Computer Sciences, UW-Madison, Department of Computer Sciences, UW-Madison, Department of Computer Sciences, UW-Madison
  - **TL;DR:** This paper investigates the role of in-distribution labels in enhancing out-of-distribution detection through a graph-theoretic approach. It establishes a formal error bound comparing OOD detection performance with and without ID labels, providing insights into the conditions for improved detection.
  - **Keywords:** Out-of-Distribution Detection, Anomaly Detection, Graph-Theoretic Approach, Spectral Decomposition, Machine Learning, Data Deviating from Training Distribution, Error Bound for OOD Detection Performance, In-Distribution Labels, OOD Data


- [Unveiling the Potential of AI for Nanomaterial Morphology Prediction](https://icml.cc/virtual/2024/poster/34963) (Poster)
  - **Authors:** [Ivan Dubrovsky](http://openreview.net/profile?id=~Ivan_Dubrovsky1), [Andrei Dmitrenko](http://openreview.net/profile?id=~Andrei_Dmitrenko1), [Aleksey Dmitrenko](http://openreview.net/profile?id=~Aleksei_Dmitrenko1), [Nikita Serov](http://openreview.net/profile?id=~Nikita_Serov1), [Vladimir Vinogradov](http://openreview.net/profile?id=~Vladimir_Vinogradov1)
  - **Affiliations:** Center for AI in Chemistry, ChemBio Cluster, ITMO University, St. Petersburg, Russia, Center for AI in Chemistry, ChemBio Cluster, ITMO University, St. Petersburg, Russia; D ONE AG, Zurich, Greater Zurich Area, Switzerland, Center for AI in Chemistry, ChemBio Cluster, ITMO University, St. Petersburg, Russia, Center for AI in Chemistry, ChemBio Cluster, ITMO University, St. Petersburg, Russia, Center for AI in Chemistry, ChemBio Cluster, ITMO University, St. Petersburg, Russia
  - **TL;DR:** This study investigates the use of AI to predict the morphology of nanoparticles, addressing the challenges of data availability and the complexity of experimental processes. The authors generated a new multi-modal dataset and evaluated the performance of classical machine learning and large language models, highlighting the potential of AI in nanomaterial design.
  - **Keywords:** AI in nanomaterial morphology prediction, nanomaterials, Classical machine learning, large language models, text-to-image system, Nanomaterial science, drug delivery systems, catalysts, energy storage systems, Complex experimental process, data availability constraints, high demand for predictive models, New multi-modal dataset, evaluation of AI performance in predicting nanomaterial shapes and sizes


- [MF-CLR: Multi-Frequency Contrastive Learning Representation for Time Series](https://icml.cc/virtual/2024/poster/33488) (Poster)
  - **Authors:** [Jufang Duan](http://openreview.net/profile?id=~Jufang_Duan2), [Wei Zheng](http://openreview.net/profile?id=~Wei_Zheng9), [Yangzhou Du](http://openreview.net/profile?id=~Yangzhou_Du2), [Wenfa Wu](http://openreview.net/profile?id=~Wenfa_Wu1), [Haipeng Jiang](http://openreview.net/profile?id=~Haipeng_Jiang1), [Hongsheng Qi](http://openreview.net/profile?id=~Hongsheng_Qi1)
  - **Affiliations:** Lenovo Research, Beijing, China, Lenovo Research, Beijing, China, Lenovo Research, Beijing, China, Lenovo Research, Beijing, China, Lenovo Research, Beijing, China, Lenovo Research, Beijing, China
  - **TL;DR:** This paper introduces Multi-Frequency Contrastive Learning Representation (MF-CLR) for learning representations from multi-frequency time series data in a self-supervised manner, particularly in the financial domain. Experimental results demonstrate that MF-CLR outperforms existing methods across various downstream tasks, maintaining consistent performance regardless of dataset scale.
  - **Keywords:** time series representation learning, self-supervised learning, contrastive learning, hierarchical mechanism, cross-frequency consistency, finance, supply chain, health care, data sparsity, diverse sampling rates, representation learning from unlabeled data, Multi-Frequency Contrastive Learning Representation (MF-CLR), performance validation on downstream tasks


- [Outlier-robust Kalman Filtering through Generalised Bayes](https://icml.cc/virtual/2024/poster/34648) (Poster)
  - **Authors:** [Gerardo Duran-Martin](http://openreview.net/profile?id=~Gerardo_Duran-Martin1), [Matias Altamirano](http://openreview.net/profile?id=~Matias_Altamirano2), [Alex Shestopaloff](http://openreview.net/profile?id=~Alex_Shestopaloff1), [Leandro Sánchez-Betancourt](http://openreview.net/profile?id=~Leandro_S%C3%A1nchez-Betancourt1), [Jeremias Knoblauch](http://openreview.net/profile?id=~Jeremias_Knoblauch1), [Matt Jones](http://openreview.net/profile?id=~Matt_Jones1), [Francois-Xavier Briol](http://openreview.net/profile?id=~Francois-Xavier_Briol1), [Kevin Murphy](http://openreview.net/profile?id=~Kevin_Patrick_Murphy1)
  - **Affiliations:** School of Mathematical Sciences, Queen Mary University, London, UK; Oxford-Man Institute of Quantitative Finance, University of Oxford, UK, Department of Statistical Science, University College London, London, United Kingdom, School of Mathematical Sciences, Queen Mary University, London, UK; Department of Mathematics and Statistics, Memorial University of Newfoundland, St. John’s, NL, Canada, Oxford-Man Institute of Quantitative Finance, University of Oxford, UK; Mathematical Institute, University of Oxford, UK, Department of Statistical Science, University College London, London, United Kingdom, Institute for Cognitive Science, University of Colorado Boulder, US, Department of Statistical Science, University College London, London, United Kingdom, Google DeepMind
  - **TL;DR:** This paper presents a novel Bayesian update rule for robust online filtering in state-space models that effectively handles outliers and measurement model misspecifications. The proposed method demonstrates superior performance and computational efficiency compared to existing robust filtering techniques across various applications, including object tracking and chaotic system state estimation.
  - **Keywords:** Outlier-robust filtering, Bayesian inference, state-space models, Kalman filter (KF), extended Kalman filter (EKF), ensemble Kalman filter (EnKF), generalised Bayesian inference, Object tracking, state estimation, online learning, chaotic systems, Outliers in measurements, model misspecification, online posterior inference, Novel Bayesian update rule, computational efficiency, robust filtering methods


- [Making Old Things New: A Unified Algorithm for Differentially Private Clustering](https://icml.cc/virtual/2024/poster/35054) (Oral)
  - **Authors:** [Max Dupre la Tour](http://openreview.net/profile?id=~Max_Dupre_la_Tour1), [Monika Henzinger](http://openreview.net/profile?id=~Monika_Henzinger1), [David Saulpic](http://openreview.net/profile?id=~David_Saulpic1)
  - **Affiliations:** McGill University, Montreal, Canada, Institute for Science and Technology Austria (ISTA), Klosterneuburg, Austria, CNRS & IRIF, Université Paris Cité, Paris, France
  - **TL;DR:** This paper presents a unified algorithm for differentially private clustering, demonstrating that a 20-year-old algorithm can be adapted to work across various privacy models, including a new continual observation setting. The findings improve upon existing results and address the challenges posed by evolving data in privacy-sensitive contexts.
  - **Keywords:** Differential Privacy, Private Clustering, K-means Clustering, Data Analysis, Unsupervised Learning, Privacy Concerns, Membership-Inference Attacks, Evolving Data, Unified Algorithm for Differentially Private Clustering, Centralized Differential Privacy, Local Model, Continual Observation Model


- [Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation](https://icml.cc/virtual/2024/poster/33510) (Poster)
  - **Authors:** [Benjamin Dupuis](http://openreview.net/profile?id=~Benjamin_Dupuis1), [Umut Simsekli](http://openreview.net/profile?id=~Umut_Simsekli1)
  - **Affiliations:** Inria; Ecole Normale Supérieure, Paris, France; PSL Research University, Paris, France; CNRS, Inria; Ecole Normale Supérieure, Paris, France; PSL Research University, Paris, France; CNRS
  - **TL;DR:** This study investigates the generalization properties of heavy-tailed stochastic optimization algorithms using heavy-tailed stochastic differential equations, proving high-probability generalization bounds without non-computable terms. The findings reveal a phase transition phenomenon indicating that heavy tails can have both beneficial and harmful effects depending on the problem structure.
  - **Keywords:** heavy-tailed stochastic optimization, generalization bounds, stochastic differential equations (SDEs), fractional Fokker-Planck equation, generalization error, population risk, empirical risk, high-probability generalization bounds, phase transition phenomenon, stable Lévy process, tail-index


- [Barrier Algorithms for Constrained Non-Convex Optimization](https://icml.cc/virtual/2024/poster/32868) (Poster)
  - **Authors:** [Pavel Dvurechenskii](http://openreview.net/profile?id=~Pavel_Dvurechensky1), [Mathias Staudigl](http://openreview.net/profile?id=~Mathias_Staudigl1)
  - **Affiliations:** Weierstrass Institute for Applied Analysis and Stochastics, Berlin, Germany, University of Mannheim, Mannheim, Germany
  - **TL;DR:** This paper presents first- and second-order interior-point methods utilizing self-concordant barriers for solving constrained non-convex optimization problems, achieving favorable global complexity similar to unconstrained problems. The proposed methods demonstrate effective complexity guarantees and applicability in machine learning contexts, particularly in training neural networks.
  - **Keywords:** Non-convex optimization, Interior-point methods, Self-concordant barriers, First-order methods, Second-order methods, Machine learning, Neural networks, Constrained optimization problems, Complexity guarantees, Approximate KKT points, Global complexity guarantees, KKT points, Potential function


- [Scalable Pre-training of Large Autoregressive Image Models](https://icml.cc/virtual/2024/poster/33604) (Poster)
  - **Authors:** [Alaaeldin Ali](http://openreview.net/profile?id=~Alaaeldin_El-Nouby1), [Michal Klein](http://openreview.net/profile?id=~Michal_Klein1), [Shuangfei Zhai](http://openreview.net/profile?id=~Shuangfei_Zhai3), [Miguel Angel Bautista Martin](http://openreview.net/profile?id=~Miguel_%C3%81ngel_Bautista1), [Vaishaal Shankar](http://openreview.net/profile?id=~Vaishaal_Shankar1), [Alexander Toshev](http://openreview.net/profile?id=~Alexander_T_Toshev1), [Joshua M Susskind](http://openreview.net/profile?id=~Joshua_M._Susskind1), [Armand Joulin](http://openreview.net/profile?id=~Armand_Joulin2)
  - **Affiliations:** Apple; Google DeepMind, Apple, Apple, Apple, Apple, Apple, Apple, Apple; Google DeepMind
  - **TL;DR:** This paper presents AIM, a collection of vision models pre-trained with an autoregressive objective, demonstrating that performance scales with model capacity and data quantity. The findings suggest that AIM could represent a new frontier for training large-scale vision models, achieving significant accuracy on ImageNet-1k without signs of performance saturation.
  - **Keywords:** Large Autoregressive Image Models, Pre-training, Vision Models, Autoregressive Objective, Scaling Properties, Transformers, Image Recognition, Improved Performance, No Saturation in Performance, ImageNet, Uncurated Web Data, Large Language Models (LLMs)


- [DsDm: Model-Aware Dataset Selection with Datamodels](https://icml.cc/virtual/2024/poster/34510) (Spotlight Poster)
  - **Authors:** [Logan Engstrom](http://openreview.net/profile?id=~Logan_Engstrom1), [Axel Feldmann](http://openreview.net/profile?id=~Axel_Feldmann1), [Aleksander Madry](http://openreview.net/profile?id=~Aleksander_Madry1)
  - **Affiliations:** MIT, MIT, MIT
  - **TL;DR:** The study presents a novel method for dataset selection that optimizes model performance by framing it as an optimization problem, rather than relying on intuitive notions of data quality. The proposed method significantly enhances language model performance on various tasks, achieving a 2× compute multiplier over traditional selection methods.
  - **Keywords:** dataset selection, model performance, data quality, optimization problem, datamodels, language models, machine learning, data quality issues, performance optimization, improved dataset selection methods, performance enhancement, SQuAD, LAMBADA


- [PAC-Bayesian Error Bound, via Rényi Divergence, for a Class of Linear Time-Invariant State-Space Models](https://icml.cc/virtual/2024/poster/33705) (Poster)
  - **Authors:** [Deividas Eringis](http://openreview.net/profile?id=~Deividas_Eringis1), [john leth](http://openreview.net/profile?id=~john_leth1), [Zheng-Hua Tan](http://openreview.net/profile?id=~Zheng-Hua_Tan1), [Rafal Wisniewski](http://openreview.net/profile?id=~Rafal_Wisniewski1), [Mihaly Petreczky](http://openreview.net/profile?id=~Mihaly_Petreczky2)
  - **Affiliations:** Department of Electronic Systems, Aalborg University, Denmark, Department of Electronic Systems, Aalborg University, Denmark, Department of Electronic Systems, Aalborg University, Denmark, Department of Electronic Systems, Aalborg University, Denmark, Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France
  - **TL;DR:** This paper derives a PAC-Bayesian error bound for linear time-invariant stochastic state-space models, addressing the learning problem for these systems with inputs. The findings provide theoretical guarantees for existing learning algorithms and highlight the advantages of state-space models in time-series predictions.
  - **Keywords:** PAC-Bayesian learning, stochastic dynamical systems, linear time-invariant systems, Rényi divergence, error bounds, Control engineering, econometrics, machine learning, Learning problem for stochastic LTI systems, time-series data, PAC-Bayesian error bounds, theoretical guarantees for learning algorithms, State-space models, recurrent neural networks (RNNs), structured state-space models (SSMs)


- [MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving](https://icml.cc/virtual/2024/poster/34064) (Poster)
  - **Authors:** [Jiangfei Duan](http://openreview.net/profile?id=~Jiangfei_Duan1), [Runyu Lu](http://openreview.net/profile?id=~Runyu_Lu2), [Haojie Duanmu](http://openreview.net/profile?id=~Haojie_Duanmu1), [Xiuhong Li](http://openreview.net/profile?id=~Xiuhong_Li1), [Xingcheng ZHANG](http://openreview.net/profile?id=~Xingcheng_ZHANG2), [Dahua Lin](http://openreview.net/profile?id=~Dahua_Lin1), [Ion Stoica](http://openreview.net/profile?id=~Ion_Stoica1), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang2)
  - **Affiliations:** The Chinese University of Hong Kong; Shanghai AI Laboratory, Huazhong University of Science and Technology, Shanghai AI Laboratory; Shanghai Jiao Tong University, Peking University, Shanghai AI Laboratory, The Chinese University of Hong Kong; Shanghai AI Laboratory, UC Berkeley, University of California San Diego
  - **TL;DR:** The study presents MuxServe, a flexible spatial-temporal multiplexing system designed to efficiently serve multiple large language models (LLMs) by optimizing resource utilization based on their varying popularity. Evaluation results indicate that MuxServe can achieve up to 1.8× higher throughput and process 2.9× more requests while maintaining a 99% service level objective (SLO).
  - **Keywords:** Large Language Models (LLMs), Efficient Serving, Spatial-Temporal Multiplexing, Placement Algorithm, Adaptive Batch Scheduling, Chat, Programming, Search, Efficiently serving multiple LLMs, Varying popularity of LLMs, Under-utilization of GPUs, MuxServe system, Higher throughput, Improved request processing


- [Improving Factuality and Reasoning in Language Models through Multiagent Debate](https://icml.cc/virtual/2024/poster/32620) (Poster)
  - **Authors:** [Yilun Du](http://openreview.net/profile?id=~Yilun_Du1), [Shuang Li](http://openreview.net/profile?id=~Shuang_Li5), [Antonio Torralba](http://openreview.net/profile?id=~Antonio_Torralba1), [Josh Tenenbaum](http://openreview.net/profile?id=~Joshua_B._Tenenbaum1), [Igor Mordatch](http://openreview.net/profile?id=~Igor_Mordatch4)
  - **Affiliations:** MIT, MIT, MIT, MIT, Google Deepmind
  - **TL;DR:** This study introduces a multiagent debate approach to enhance the factuality and reasoning capabilities of large language models (LLMs). The findings demonstrate significant improvements in mathematical and strategic reasoning, as well as a reduction in hallucinations and fallacious answers.
  - **Keywords:** Large Language Models, Factuality, Reasoning, Multiagent Debate, Chain-of-Thought, Verification, Self-Consistency, Language Generation, Language Understanding, Hallucination, Factual Validity, Reasoning Errors, Improved Reasoning, Enhanced Factual Accuracy, Society of Mind, Agents


- [Equivariant Frames and the Impossibility of Continuous Canonicalization](https://icml.cc/virtual/2024/poster/35009) (Poster)
  - **Authors:** [Nadav Dym](http://openreview.net/profile?id=~Nadav_Dym1), [Hannah Lawrence](http://openreview.net/profile?id=~Hannah_Lawrence1), [Jonathan Siegel](http://openreview.net/profile?id=~Jonathan_W._Siegel1)
  - **Affiliations:** Faculty of Mathematics, Faculty of Computer Science, Technion, Israel, Department of Electrical Engineering and Computer Science, MIT, MA, USA, Department of Mathematics, Texas A&M University, TX, USA
  - **TL;DR:** This study investigates the limitations of traditional frame-averaging methods in maintaining continuity during canonicalization in equivariant learning. It introduces weighted frames as a solution to preserve continuity and demonstrates their effectiveness for various group actions.
  - **Keywords:** Equivariance, Geometric Deep Learning, Frame-averaging, Canonicalization, Biology, Chemistry, Graphs, Discontinuity in canonicalization, Robustness problem, Weighted frames, Continuity preservation, SO(d), O(d), Sn, Reynolds operator


- [Sharpness-Aware Data Generation for Zero-shot Quantization](https://icml.cc/virtual/2024/poster/34833) (Poster)
  - **Authors:** [Hoang Dung](http://openreview.net/profile?id=~Hoang_Anh_Dung1), [Cuong Pham](http://openreview.net/profile?id=~Cuong_Pham3), [Trung Le](http://openreview.net/profile?id=~Trung_Le2), [Jianfei Cai](http://openreview.net/profile?id=~Jianfei_Cai1), [Thanh-Toan Do](http://openreview.net/profile?id=~Thanh-Toan_Do4)
  - **Affiliations:** Department of Data Science and AI, Monash University, Melbourne, Australia, Department of Data Science and AI, Monash University, Melbourne, Australia, Department of Data Science and AI, Monash University, Melbourne, Australia, Department of Data Science and AI, Monash University, Melbourne, Australia, Department of Data Science and AI, Monash University, Melbourne, Australia
  - **TL;DR:** This paper presents a novel methodology for zero-shot quantization that incorporates sharpness-aware synthetic data generation to improve model generalization. Experimental results on CIFAR-100 and ImageNet datasets show that the proposed method outperforms existing state-of-the-art techniques in low-bit quantization settings.
  - **Keywords:** Zero-shot quantization, model generalization, Synthetic data generation, gradient matching, sharpness minimization, Resource-constrained devices, deep neural networks, Lack of access to original training data, model quantization challenges, Enhanced generalization through sharpness-aware data generation, CIFAR-100, ImageNet


- [Compositional Curvature Bounds for Deep Neural Networks](https://icml.cc/virtual/2024/poster/34036) (Poster)
  - **Authors:** [Taha Entesari](http://openreview.net/profile?id=~Taha_Entesari1), [Sina Sharifi](http://openreview.net/profile?id=~Sina_Sharifi1), [Mahyar Fazlyab](http://openreview.net/profile?id=~Mahyar_Fazlyab1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, United States, Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, United States, Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, United States
  - **TL;DR:** This study investigates the robustness of deep neural networks against adversarial attacks by analyzing the second-order behavior and introducing a novel algorithm to compute curvature bounds. The findings demonstrate that controlling the curvature during training can enhance model robustness, validated through experiments on MNIST and CIFAR-10 datasets.
  - **Keywords:** adversarial robustness, deep neural networks, second-order behavior, curvature constant, Lipschitz constant, safety-critical applications, classification tasks, vulnerability to adversarial attacks, provable upper bounds on second derivative, differentiable regularizer, MNIST, CIFAR-10


- [Model Alignment as Prospect Theoretic Optimization](https://icml.cc/virtual/2024/poster/33352) (Spotlight Poster)
  - **Authors:** [Kawin Ethayarajh](http://openreview.net/profile?id=~Kawin_Ethayarajh1), [Winnie Xu](http://openreview.net/profile?id=~Winnie_Xu1), [Niklas Muennighoff](http://openreview.net/profile?id=~Niklas_Muennighoff1), [Dan Jurafsky](http://openreview.net/profile?id=~Dan_Jurafsky1), [Douwe Kiela](http://openreview.net/profile?id=~Douwe_Kiela1)
  - **Affiliations:** Stanford University; Contextual AI, Contextual AI, Contextual AI, Stanford University, Stanford University; Contextual AI
  - **TL;DR:** This paper explores the alignment of large language models (LLMs) with human feedback through the lens of prospect theory, proposing a new human-aware loss function (KTO) that maximizes utility rather than preferences. The findings indicate that this approach can match or exceed the performance of existing preference-based methods while only requiring binary feedback signals.
  - **Keywords:** AI Alignment, Human Feedback, Large Language Models, Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), Human-Aware Losses (HALOs), Kahneman-Tversky Model, Loss Aversion, Preference Form Feedback, Data Scarcity, New HALO (KTO), Optimization of Generative Models, Prospect Theory, Utility Functions, Inductive Biases


- [DOGE: Domain Reweighting with Generalization Estimation](https://icml.cc/virtual/2024/poster/34869) (Poster)
  - **Authors:** [Simin Fan](http://openreview.net/profile?id=~Simin_Fan1), [Matteo Pagliardini](http://openreview.net/profile?id=~Matteo_Pagliardini1), [Martin Jaggi](http://openreview.net/profile?id=~Martin_Jaggi1)
  - **Affiliations:** EPFL, Switzerland, None, None
  - **TL;DR:** The study introduces DOGE, a method for optimizing domain weights to enhance the generalization ability of Large Language Models. Experimental results show that DOGE improves model performance on various tasks and effectively identifies inter-domain dependencies for out-of-domain generalization.
  - **Keywords:** Large Language Models, Generalization, Domain reweighting, Bi-level optimization, Natural Language Processing (NLP), Generalization ability, Domain composition impact, Improved generalization, Better perplexity and reasoning accuracy, SlimPajama dataset


- [Position: Insights from Survey Methodology can Improve Training Data](https://icml.cc/virtual/2024/poster/33607) (Poster)
  - **Authors:** [Stephanie Eckman](http://openreview.net/profile?id=~Stephanie_Eckman1), [Barbara Plank](http://openreview.net/profile?id=~Barbara_Plank2), [Frauke Kreuter](http://openreview.net/profile?id=~Frauke_Kreuter1)
  - **Affiliations:** Social Data Science Center, University of Maryland, College Park, MD, USA; None, Center for Information and Language Processing (CIS), LMU Munich, Germany; Computer Science Department, IT University of Copenhagen, Denmark; Munich Center for Machine Learning (MCML), LMU Munich, Germany, Institute for Statistics, LMU Munich, Germany; Munich Center for Machine Learning (MCML), LMU Munich, Germany; Joint Program in Survey Methodology, University of Maryland, College Park, MD, USA
  - **TL;DR:** This paper argues that insights from survey methodology can enhance the quality of training data for AI models, addressing challenges in data collection and biases. By integrating social science theories, the authors propose methods to improve data collection processes, ultimately leading to more trustworthy and human-centric AI models.
  - **Keywords:** AI model fairness, data collection, survey methodology, AI/ML model training, human-labeled data, High-quality data collection challenges, biases in data collection, Improved training data quality, human-centric models, Data-centric AI, human feedback for reinforcement learning


- [Revisit the Essence of Distilling Knowledge through Calibration](https://icml.cc/virtual/2024/poster/34199) (Poster)
  - **Authors:** [Wen-Shu Fan](http://openreview.net/profile?id=~Wen-Shu_Fan2), [Su Lu](http://openreview.net/profile?id=~Su_Lu1), [Xin-Chun Li](http://openreview.net/profile?id=~Xin-Chun_Li1), [De-Chuan Zhan](http://openreview.net/profile?id=~De-Chuan_Zhan1), [Le Gan](http://openreview.net/profile?id=~Le_Gan1)
  - **Affiliations:** School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China
  - **TL;DR:** This paper investigates the phenomenon of capacity mismatch in Knowledge Distillation (KD), proposing that the calibration of the teacher model significantly impacts KD performance. The authors recommend using ranking-based loss as a more effective alternative to KL divergence for improving knowledge transfer from larger, poorly calibrated models to smaller students.
  - **Keywords:** Knowledge Distillation, Capacity Mismatch, Model Calibration, KL Divergence, Ranking-based Loss, Resource-constrained Platforms, Mobile Devices, Capacity Mismatch, Calibration Issues, Unifying Analytical Framework, Improved KD Performance, Teacher Model, Student Model


- [Approximate Nearest Neighbor Search with Window Filters](https://icml.cc/virtual/2024/poster/34829) (Poster)
  - **Authors:** [Josh Engels](http://openreview.net/profile?id=~Joshua_Engels1), [Ben Landrum](http://openreview.net/profile?id=~Ben_Landrum1), [Shangdi Yu](http://openreview.net/profile?id=~Shangdi_Yu1), [Laxman Dhulipala](http://openreview.net/profile?id=~Laxman_Dhulipala1), [Julian Shun](http://openreview.net/profile?id=~Julian_Shun1)
  - **Affiliations:** Massachusetts Institute of Technology, CSAIL, University of Maryland, Massachusetts Institute of Technology, CSAIL, University of Maryland, Massachusetts Institute of Technology, CSAIL
  - **TL;DR:** This paper introduces the c-approximate window search problem, a generalization of nearest neighbor search that incorporates numeric label filters. The authors propose a modular tree-based framework that significantly improves search efficiency, achieving up to a 75× speedup over existing methods while maintaining recall.
  - **Keywords:** c-approximate nearest neighbor search, window search, modular tree-based framework, label-space partitioning, image search, document search, product search, large language model retrieval, nearest neighbor search, high-dimensional data, filtering by numeric labels, non-trivial solution for window search, runtime bounds analysis, standard nearest neighbor benchmark datasets, adversarially constructed embeddings


- [Efficient Error Certification for Physics-Informed Neural Networks](https://icml.cc/virtual/2024/poster/34959) (Poster)
  - **Authors:** [Francisco Eiras](http://openreview.net/profile?id=~Francisco_Eiras1), [Adel Bibi](http://openreview.net/profile?id=~Adel_Bibi1), [Rudy Bunel](http://openreview.net/profile?id=~Rudy_R_Bunel1), [Krishnamurthy Dvijotham](http://openreview.net/profile?id=~Krishnamurthy_Dj_Dvijotham1), [Phil Torr](http://openreview.net/profile?id=~Philip_Torr1), [M. Pawan Kumar](http://openreview.net/profile?id=~M._Pawan_Kumar1)
  - **Affiliations:** University of Oxford, University of Oxford, Google DeepMind, Google DeepMind, University of Oxford, Google DeepMind
  - **TL;DR:** This study introduces a framework for certifying the error of Physics-Informed Neural Networks (PINNs) in solving partial differential equations (PDEs), addressing the lack of guaranteed error bounds in previous works. The proposed method, ∂-CROWN, effectively establishes error-based conditions and demonstrates its utility on various PDEs, ensuring reliable performance across the spatio-temporal domain.
  - **Keywords:** Physics-Informed Neural Networks (PINN), Error Certification, Partial Differential Equations (PDE), CROWN framework, Error-based correctness conditions, Groundwater contaminant transport, Complex systems simulation, Residual error estimation, Continuous function approximation, Error bounds, Guaranteed error-based conditions, Efficient error certification methods


- [Bayesian Knowledge Distillation: A Bayesian Perspective of Distillation with Uncertainty Quantification](https://icml.cc/virtual/2024/poster/33237) (Poster)
  - **Authors:** [Luyang Fang](http://openreview.net/profile?id=~Luyang_Fang1), [Yongkai Chen](http://openreview.net/profile?id=~Yongkai_Chen1), [Wenxuan Zhong](http://openreview.net/profile?id=~Wenxuan_Zhong1), [Ping Ma](http://openreview.net/profile?id=~Ping_Ma1)
  - **Affiliations:** Department of Statistics, University of Georgia, Athens, USA, Department of Statistics, University of Georgia, Athens, USA, Department of Statistics, University of Georgia, Athens, USA, Department of Statistics, University of Georgia, Athens, USA
  - **TL;DR:** This study introduces Bayesian Knowledge Distillation (BKD), a method that connects knowledge distillation with Bayesian modeling to enhance model compression and uncertainty quantification. The findings suggest that BKD provides a clearer understanding of the distillation process and improves the performance of student models through effective Bayesian inference techniques.
  - **Keywords:** Knowledge Distillation, Bayesian Inference, Bayesian Knowledge Distillation (BKD), Stochastic Gradient Langevin Monte Carlo, Model Compression, Deployment Acceleration, Uncertainty Quantification, Model Size Challenges, Teacher-informed Prior, Posterior Mode Estimation


- [Revisiting Scalable Hessian Diagonal Approximations for Applications in Reinforcement Learning](https://icml.cc/virtual/2024/poster/32650) (Poster)
  - **Authors:** [Mohamed Elsayed](http://openreview.net/profile?id=~Mohamed_Elsayed2), [Homayoon Farrahi](http://openreview.net/profile?id=~Homayoon_Farrahi1), [Felix Dangel](http://openreview.net/profile?id=~Felix_Dangel1), [Rupam Mahmood](http://openreview.net/profile?id=~A._Rupam_Mahmood1)
  - **Affiliations:** Department of Computing Science, University of Alberta; Alberta Machine Intelligence Institute, Department of Computing Science, University of Alberta; Alberta Machine Intelligence Institute, Vector Institute, Toronto, Canada, Department of Computing Science, University of Alberta; Alberta Machine Intelligence Institute; CIFAR AI Chair
  - **TL;DR:** This study revisits and improves upon an early approximation method for Hessian diagonals, introducing HesScale, which offers a cost-effective and high-quality alternative for second-order optimization in reinforcement learning. The findings indicate that HesScale enhances optimization speed and stability, suggesting potential for scaling second-order methods in larger models.
  - **Keywords:** second-order optimization, Hessian diagonal approximations, reinforcement learning, Hessian-free methods, Hessian-vector product, HesScale, optimization, neural network training, reinforcement learning, computational cost of Hessian entries, approximation quality of Hessian, HesScale method, improved optimization speed and stability, Hessian matrix, scalable second-order methods


- [Exploring Correlations of Self-Supervised Tasks for Graphs](https://icml.cc/virtual/2024/poster/34181) (Poster)
  - **Authors:** [Taoran Fang](http://openreview.net/profile?id=~Taoran_Fang2), [Wei Chow](http://openreview.net/profile?id=~Wei_Chow1), [Yifei Sun](http://openreview.net/profile?id=~Yifei_Sun1), [Kaiqiao Han](http://openreview.net/profile?id=~Kaiqiao_Han1), [Lvbin Ma](http://openreview.net/profile?email=gmmmfly%40163.com), [Yang Yang](http://openreview.net/profile?id=~Yang_Yang35)
  - **Affiliations:** College of Computer Science and Technology, Zhejiang University, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, Zhejiang Huayun Information Technology, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China
  - **TL;DR:** This paper investigates the correlations between various self-supervised tasks in graph learning and introduces Graph Task Correlation Modeling (GraphTCM) to enhance representation learning. The proposed method significantly outperforms existing approaches across multiple downstream tasks, addressing challenges related to data scarcity and model generalization.
  - **Keywords:** Graph self-supervised learning, task correlations, Graph Task Correlation Modeling (GraphTCM), Node classification, link prediction, vertex clustering, recommendation systems, Scarcity of labeled data, low generalization ability, overfitting, Enhanced graph self-supervised training, improved representation learning, Graph neural networks (GNNs)


- [INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer](https://icml.cc/virtual/2024/poster/35157) (Poster)
  - **Authors:** [Han Fang](http://openreview.net/profile?id=~Han_Fang7), [Zhihao Song](http://openreview.net/profile?id=~Zhihao_Song1), [Paul Weng](http://openreview.net/profile?id=~Paul_Weng1), [Yutong Ban](http://openreview.net/profile?id=~Yutong_Ban1)
  - **Affiliations:** Joint Institute of Michigan, Shanghai Jiao Tong University, Shanghai, China, Joint Institute of Michigan, Shanghai Jiao Tong University, Shanghai, China, Duke Kunshan University, Jiangsu, China, Joint Institute of Michigan, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This paper introduces the Invariant Nested View Transformer (INViT), a novel architecture designed to enhance the generalizability of deep reinforcement learning solvers for routing problems. The proposed method demonstrates superior performance in generalizing across different problem scales and distributions, addressing the limitations of existing models.
  - **Keywords:** deep reinforcement learning, routing problems, Invariant Nested View Transformer (INViT), modified policy gradient algorithm, data augmentations, logistics, electronic design automation, bioinformatics, generalization to unseen distributions, cross-size generalization, cross-distribution generalization, fast heuristics, generalization performance, traveling salesman problem (TSP), vehicle routing problem (VRP)


- [Locally Estimated Global Perturbations are Better than Local Perturbations for Federated Sharpness-aware Minimization](https://icml.cc/virtual/2024/poster/34909) (Spotlight Poster)
  - **Authors:** [Ziqing Fan](http://openreview.net/profile?id=~Ziqing_Fan1), [Shengchao Hu](http://openreview.net/profile?id=~Shengchao_Hu1), [Jiangchao Yao](http://openreview.net/profile?id=~Jiangchao_Yao1), [Gang Niu](http://openreview.net/profile?id=~Gang_Niu1), [Ya Zhang](http://openreview.net/profile?id=~Ya_Zhang1), [Masashi Sugiyama](http://openreview.net/profile?id=~Masashi_Sugiyama1), [Yanfeng Wang](http://openreview.net/profile?id=~Yanfeng_Wang1)
  - **Affiliations:** Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, RIKEN AIP, Japan; The University of Tokyo, Japan, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, RIKEN AIP, Japan; The University of Tokyo, Japan, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China
  - **TL;DR:** This study introduces FedLESAM, a novel algorithm that improves federated learning by locally estimating global perturbations to address the challenges of data heterogeneity and sharp minima. The proposed method enhances performance and efficiency in federated sharpness-aware minimization while ensuring consistent perturbation.
  - **Keywords:** Federated Learning, Sharpness-aware Minimization, FedLESAM, Sharpness-aware Minimization (SAM), Medical diagnosis, Autonomous driving, Data heterogeneity, Sharp local minima, Poor generalization ability, Improved federated SAM-based approaches, Consistent perturbation, Federated benchmark datasets


- [TSLANet: Rethinking Transformers for Time Series Representation Learning](https://icml.cc/virtual/2024/poster/34691) (Poster)
  - **Authors:** [Emadeldeen Eldele](http://openreview.net/profile?id=~Emadeldeen_Eldele1), [Mohamed Ragab](http://openreview.net/profile?id=~Mohamed_Ragab1), [Zhenghua Chen](http://openreview.net/profile?id=~Zhenghua_Chen2), [Min Wu](http://openreview.net/profile?id=~Min_Wu2), [Xiaoli Li](http://openreview.net/profile?id=~Xiaoli_Li1)
  - **Affiliations:** Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore, Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore; I2R, Agency for Science, Technology and Research, Singapore, Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore; I2R, Agency for Science, Technology and Research, Singapore, I2R, Agency for Science, Technology and Research, Singapore, Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore; I2R, Agency for Science, Technology and Research, Singapore
  - **TL;DR:** This study introduces TSLANet, a novel convolutional model designed for time series representation learning, which effectively captures both long-term and short-term dependencies while addressing challenges such as noise sensitivity and overfitting. The model demonstrates superior performance across various tasks, including classification, forecasting, and anomaly detection, compared to existing state-of-the-art methods.
  - **Keywords:** Time series representation learning, Convolutional models, Time Series Lightweight Adaptive Network (TSLANet), Adaptive Spectral Block, Interactive Convolution Block, Fourier analysis, Self-supervised learning, Classification, Forecasting, Anomaly detection, Noise sensitivity, Computational efficiency, Overfitting, Long and short-range dependencies, Improved feature representation, Robustness across datasets, Enhanced decoding of temporal patterns, UEA datasets, Transformers, Convolutional Neural Networks (CNNs)


- [Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift](https://icml.cc/virtual/2024/poster/34480) (Poster)
  - **Authors:** [Benjamin Eyre](http://openreview.net/profile?id=~Benjamin_Eyre1), [Elliot Creager](http://openreview.net/profile?id=~Elliot_Creager1), [David Madras](http://openreview.net/profile?id=~David_Madras1), [Vardan Papyan](http://openreview.net/profile?id=~Vardan_Papyan1), [Richard Zemel](http://openreview.net/profile?id=~Richard_Zemel1)
  - **Affiliations:** Columbia University, New York, USA, University of Waterloo, Waterloo, Canada; Vector Institute, Toronto, Canada, Google Research, Vector Institute, Toronto, Canada; University of Toronto, Toronto, Canada, Columbia University, New York, USA
  - **TL;DR:** This paper addresses the challenge of out-of-distribution generalization in regression models by proposing a lightweight method called Spectral Adapted Regresor (SpAR) that adapts the weights of pre-trained neural regression models to better handle covariate shift. The method demonstrates consistent improvements in performance across various datasets, highlighting its potential for enhancing regression tasks in machine learning.
  - **Keywords:** out-of-distribution generalization, regression, covariate shift, Ordinary Least Squares (OLS) regression, Spectral Adapted Regresor (SpAR), machine learning, deep neural networks, synthetic and real-world datasets, distribution shift, poor out-of-distribution performance, spectral adaptation procedure, improved OOD performance, WILDS benchmark


- [Stop Regressing: Training Value Functions via Classification for Scalable Deep RL](https://icml.cc/virtual/2024/poster/33551) (Oral)
  - **Authors:** [Jesse Farebrother](http://openreview.net/profile?id=~Jesse_Farebrother1), [Jordi Orbay](http://openreview.net/profile?id=~Jordi_Orbay1), [Quan Vuong](http://openreview.net/profile?id=~Quan_Vuong2), [Adrien Ali Taiga](http://openreview.net/profile?id=~Adrien_Ali_Taiga1), [Yevgen Chebotar](http://openreview.net/profile?id=~Yevgen_Chebotar1), [Ted Xiao](http://openreview.net/profile?id=~Ted_Xiao1), [Alexander Irpan](http://openreview.net/profile?id=~Alex_Irpan1), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1), [Pablo Samuel Castro](http://openreview.net/profile?id=~Pablo_Samuel_Castro1), [Aleksandra Faust](http://openreview.net/profile?id=~Aleksandra_Faust1), [Aviral Kumar](http://openreview.net/profile?id=~Aviral_Kumar2), [Rishabh Agarwal](http://openreview.net/profile?id=~Rishabh_Agarwal2)
  - **Affiliations:** Google DeepMind; Mila, McGill University, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind; Mila, Université de Montréal, Google DeepMind, Google DeepMind, Google DeepMind; Mila, Université de Montréal
  - **TL;DR:** This study investigates the use of categorical cross-entropy for training value functions in deep reinforcement learning, demonstrating that this approach significantly enhances performance and scalability across various domains. The findings suggest that reframing the training method can mitigate common issues in value-based RL, leading to state-of-the-art results.
  - **Keywords:** deep reinforcement learning, scalability, categorical cross-entropy, value functions, regression, classification, Atari 2600 games, robotic manipulation, Chess, language-agent tasks, noisy targets, non-stationarity, scaling challenges, improved performance, robustness, scalability, state-of-the-art results, Atari, Q-transformers, high-capacity Transformers


- [Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition](https://icml.cc/virtual/2024/poster/33467) (Oral)
  - **Authors:** [Hao Fei](http://openreview.net/profile?id=~Hao_Fei1), [Shengqiong Wu](http://openreview.net/profile?id=~Shengqiong_Wu2), [Wei Ji](http://openreview.net/profile?id=~Wei_Ji1), [Hanwang Zhang](http://openreview.net/profile?id=~Hanwang_Zhang3), [Meishan Zhang](http://openreview.net/profile?id=~Meishan_Zhang1), [Mong-Li Lee](http://openreview.net/profile?id=~Mong-Li_Lee1), [Wynne Hsu](http://openreview.net/profile?id=~Wynne_Hsu1)
  - **Affiliations:** National University of Singapore, Singapore, National University of Singapore, Singapore, National University of Singapore, Singapore, Nanyang Technological University, Singapore, Harbin Institute of Technology (Shenzhen), China, National University of Singapore, Singapore, National University of Singapore, Singapore
  - **TL;DR:** This paper presents a novel framework, Video-of-Thought (VoT), which enhances video reasoning by integrating fine-grained spatial-temporal understanding with cognitive-level comprehension through a Multimodal Large Language Model (MLLM) called MotionEpic. The proposed approach significantly improves the state-of-the-art in complex video question answering and reasoning tasks.
  - **Keywords:** video understanding, reasoning, complex videos, Multimodal Large Language Model (MLLM), MotionEpic, Chain-of-Thought (CoT), video QA, video comprehension, fine-grained spatial-temporal understanding, cognitive-level comprehension, challenges in complex video reasoning, Video-of-Thought (VoT) reasoning framework, improved video reasoning capabilities, spatial-temporal scene graph (STSG), commonsense knowledge


- [Keypoint-based Progressive Chain-of-Thought Distillation for LLMs](https://icml.cc/virtual/2024/poster/32859) (Poster)
  - **Authors:** [Kaituo Feng](http://openreview.net/profile?id=~Kaituo_Feng1), [Changsheng Li](http://openreview.net/profile?id=~Changsheng_Li4), [Xiaolu Zhang](http://openreview.net/profile?id=~Xiaolu_Zhang2), [JUN ZHOU](http://openreview.net/profile?id=~JUN_ZHOU6), [Ye Yuan](http://openreview.net/profile?id=~Ye_Yuan15), [Guoren Wang](http://openreview.net/profile?id=~Guoren_Wang2)
  - **Affiliations:** Beijing Institute of Technology, Beijing Institute of Technology, Ant Group, Ant Group, Beijing Institute of Technology, Beijing Institute of Technology; Hebei Province Key Laboratory of Big Data Science and Intelligent Technology
  - **TL;DR:** This study introduces KPOD, a unified framework for chain-of-thought distillation that enhances the reasoning capabilities of smaller models by focusing on keypoint tokens and employing a progressive learning strategy. The proposed method significantly outperforms existing techniques in reasoning tasks across multiple benchmarks.
  - **Keywords:** Chain-of-thought distillation, reasoning abilities, large language models, Token weighting module, mask learning, in-rationale progressive distillation, Reasoning errors, significance of tokens, learning order of step generation, Unified framework (KPOD), weighted token generation loss, value function for progressive distillation, Large language models (LLMs), keypoint tokens, CoT (chain-of-thought)


- [DSD-DA: Distillation-based Source Debiasing for Domain Adaptive Object Detection](https://icml.cc/virtual/2024/poster/32629) (Poster)
  - **Authors:** [Yongchao Feng](http://openreview.net/profile?id=~Yongchao_Feng1), [Shiwei Li](http://openreview.net/profile?id=~Shiwei_Li5), [Yingjie Gao](http://openreview.net/profile?id=~Yingjie_Gao1), [Ziyue Huang](http://openreview.net/profile?id=~Ziyue_Huang2), [Yanan Zhang](http://openreview.net/profile?id=~Yanan_Zhang4), [Qingjie Liu](http://openreview.net/profile?id=~Qingjie_Liu2), [Yunhong Wang](http://openreview.net/profile?id=~Yunhong_Wang1)
  - **Affiliations:** State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China, Hangzhou Innovation Institute, Beihang University, Hangzhou, China, State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China, State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China, State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China, State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Zhongguancun Laboratory, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China, State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China
  - **TL;DR:** This study introduces a Distillation-based Source Debiasing (DSD) framework to address source bias in Domain Adaptive Object Detection (DAOD), enhancing the model's performance across both source and target domains. The proposed methods improve classification and localization consistency, outperforming existing alignment-based approaches.
  - **Keywords:** Domain Adaptive Object Detection, Source Bias, Generalization, Distillation-based Source Debiasing (DSD), Target-Relevant Object Localization Network (TROLN), Domain-aware Consistency Enhancing (DCE), Object Detection, Source bias issue, Domain shift, Inconsistent classification and localization, Improved detector performance, Harmonization between classification and localization, Domain-agnostic knowledge, Source-specific knowledge


- [UniCorn: A Unified Contrastive Learning Approach for Multi-view Molecular Representation Learning](https://icml.cc/virtual/2024/poster/35109) (Poster)
  - **Authors:** [Shikun Feng](http://openreview.net/profile?id=~Shikun_Feng3), [Yuyan Ni](http://openreview.net/profile?id=~Yuyan_Ni1), [Li](http://openreview.net/profile?id=~Minghao_Li8), [Yanwen Huang](http://openreview.net/profile?id=~Yanwen_Huang2), [Zhiming Ma](http://openreview.net/profile?id=~Zhi-Ming_Ma1), [Wei-Ying Ma](http://openreview.net/profile?id=~Wei-Ying_Ma2), [Yanyan Lan](http://openreview.net/profile?id=~Yanyan_Lan2)
  - **Affiliations:** Institute for AI Industry Research (AIR), Tsinghua University, Academy of Mathematics and Systems Science, Chinese Academy of Sciences; University of Chinese Academy of Sciences, Beijing Institute of Genomics, Chinese Academy of Sciences, Department of Pharmaceutical Science, Peking University, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Institute for AI Industry Research (AIR), Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University; Beijing Academy of Artificial Intelligence
  - **TL;DR:** This study introduces UniCorn, a unified contrastive learning framework for molecular representation that integrates existing pre-training methods to enhance performance across various molecular tasks. The framework demonstrates state-of-the-art results in quantum, physicochemical, and biological property predictions, addressing the limitations of current models.
  - **Keywords:** molecular representation learning, pre-trained foundation models, self-supervised learning (SSL), 2D graph masking, 2D-3D contrastive learning, 3D denoising, drug discovery, property prediction, limited labeled molecular data, unbalanced methods for downstream tasks, novel pre-training framework (UniCorn), SOTA performance across various tasks


- [Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates](https://icml.cc/virtual/2024/poster/35118) (Poster)
  - **Authors:** [Rémi Leluc](http://openreview.net/profile?id=~R%C3%A9mi_Leluc1), [Aymeric Dieuleveut](http://openreview.net/profile?id=~Aymeric_Dieuleveut1), [François Portier](http://openreview.net/profile?id=~Fran%C3%A7ois_Portier1), [Johan Segers](http://openreview.net/profile?id=~Johan_Segers1), [Aigerim Zhuman](http://openreview.net/profile?id=~Aigerim_Zhuman1)
  - **Affiliations:** CMAP, ´Ecole Polytechnique, Institut Polytechnique de Paris, France, CMAP, ´Ecole Polytechnique, Institut Polytechnique de Paris, France, CREST, ENSAI, France, ISBA/LIDAM, UCLouvain, Belgium, ISBA/LIDAM, UCLouvain, Belgium
  - **TL;DR:** This paper introduces a new Monte Carlo method called Spherical Harmonics Control Variates (SHCV) for approximating the Sliced-Wasserstein distance, demonstrating improved convergence rates and theoretical properties, particularly for Gaussian measures. The method effectively reduces variance in the estimation process, outperforming existing techniques in numerical experiments.
  - **Keywords:** Sliced-Wasserstein distance, Monte Carlo methods, Spherical harmonics, Control variates, Generative modeling, Bayesian computation, Image processing, Computational physics, Statistical error, Monte Carlo integration error, High-dimensional data, Spherical Harmonics Control Variates (SHCV), Improved convergence rate, Wasserstein distance, Gaussian measures, Lipschitz property


- [Privacy Backdoors: Stealing Data with Corrupted Pretrained Models](https://icml.cc/virtual/2024/poster/34862) (Poster)
  - **Authors:** [Shanglun Feng](http://openreview.net/profile?id=~Shanglun_Feng1), [Florian Tramer](http://openreview.net/profile?id=~Florian_Tram%C3%A8r1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Zurich, Switzerland, Department of Computer Science, ETH Zurich, Zurich, Switzerland
  - **TL;DR:** This study introduces the concept of privacy backdoors in pretrained machine learning models, demonstrating how attackers can compromise the privacy of finetuning data. The findings reveal significant vulnerabilities in models trained with differential privacy, challenging the assumption that such models provide robust privacy guarantees.
  - **Keywords:** privacy backdoors, machine learning privacy, supply chain attacks, pretrained models, transformers, MLPs (Multi-Layer Perceptrons), privacy vulnerabilities, data traps, model integrity, backdoor design, membership inference attacks, privacy leakage analysis, differential privacy (DP), DP-SGD (Differentially Private Stochastic Gradient Descent)


- [Resisting Stochastic Risks in Diffusion Planners with the Trajectory Aggregation Tree](https://icml.cc/virtual/2024/poster/34197) (Spotlight Poster)
  - **Authors:** [Lang Feng](http://openreview.net/profile?id=~Lang_Feng1), [Pengjie Gu](http://openreview.net/profile?id=~Pengjie_Gu1), [Bo An](http://openreview.net/profile?id=~Bo_An2), [Gang Pan](http://openreview.net/profile?id=~Gang_Pan1)
  - **Affiliations:** Zhejiang University, China, Nanyang Technological University, Singapore; Skywork AI, Singapore, Nanyang Technological University, Singapore; Skywork AI, Singapore, Zhejiang University, China; State Key Laboratory of Brain-Machine Intelligence, China
  - **TL;DR:** This study introduces the Trajectory Aggregation Tree (TAT) to enhance the reliability of diffusion planners by addressing the stochastic risks associated with generating infeasible trajectories. The proposed method significantly improves performance across all tasks and accelerates planning by more than three times.
  - **Keywords:** diffusion planners, trajectory optimization, decision-making, Trajectory Aggregation Tree (TAT), non-autoregressive plan generation, robot control, board games, offline control tasks, stochastic risks, unreliable trajectories, feasibility of plans, performance boosting, risk resistance, acceleration in planning


- [Position: Relational Deep Learning - Graph Representation Learning on Relational Databases](https://icml.cc/virtual/2024/poster/34733) (Poster)
  - **Authors:** [Matthias Fey](http://openreview.net/profile?id=~Matthias_Fey2), [Weihua Hu](http://openreview.net/profile?id=~Weihua_Hu1), [Kexin Huang](http://openreview.net/profile?id=~Kexin_Huang1), [Jan Eric Lenssen](http://openreview.net/profile?id=~Jan_Eric_Lenssen1), [Rishabh Ranjan](http://openreview.net/profile?id=~Rishabh_Ranjan1), [Joshua Robinson](http://openreview.net/profile?id=~Joshua_Robinson4), [ZHITAO YING](http://openreview.net/profile?id=~Zhitao_Ying1), [Jiaxuan You](http://openreview.net/profile?id=~Jiaxuan_You2), [Jure Leskovec](http://openreview.net/profile?id=~Jure_Leskovec1)
  - **Affiliations:** Kumo.AI, Kumo.AI, Stanford University, Kumo.AI; Max Planck Institute for Informatics, Stanford University, Stanford University, Yale University, University of Illinois at Urbana-Champaign, Stanford University
  - **TL;DR:** This paper introduces Relational Deep Learning (RDL) as a method for end-to-end learning on relational databases by representing them as temporal, heterogeneous graphs. It addresses the challenges of manual feature engineering and demonstrates strong initial results with a new benchmarking suite called RELBENCH.
  - **Keywords:** Relational Deep Learning, Graph Representation, Learning on Relational Databases, Graph Neural Networks, E-commerce, Healthcare, Telecommunications, Music Streaming, Feature Engineering, Learning from Multiple Tables, RELBENCH, Benchmarking and Testing Suite


- [Neuroexplicit Diffusion Models for Inpainting of Optical Flow Fields](https://icml.cc/virtual/2024/poster/35074) (Poster)
  - **Authors:** [Tom Fischer](http://openreview.net/profile?id=~Tom_Fischer1), [Pascal Peter](http://openreview.net/profile?id=~Pascal_Peter1), [Joachim Weickert](http://openreview.net/profile?id=~Joachim_Weickert1), [Eddy Ilg](http://openreview.net/profile?id=~Eddy_Ilg3)
  - **Affiliations:** Computer Vision and Machine Perception Lab, Saarland University, Saarbrücken, Germany, Mathematical Image Analysis Group, Saarland University, Saarbrücken, Germany, Mathematical Image Analysis Group, Saarland University, Saarbrücken, Germany, Computer Vision and Machine Perception Lab, Saarland University, Saarbrücken, Germany
  - **TL;DR:** This study presents a novel neuroexplicit architecture that combines PDE-based approaches with convolutional neural networks for inpainting optical flow fields. The proposed method significantly outperforms existing baselines in reconstruction quality and robustness while requiring less training data.
  - **Keywords:** Neuroexplicit modeling, Optical flow inpainting, Partial differential equations (PDEs), Convolutional neural networks, Computer vision, Image processing, Data sparsity, Generalization capabilities, New architecture for optical flow inpainting, Improved reconstruction quality


- [Inverse-Variance Weighting for Estimation of Heterogeneous Treatment Effects](https://icml.cc/virtual/2024/poster/33971) (Poster)
  - **Authors:** [Aaron Fisher](http://openreview.net/profile?id=~Aaron_Fisher1)
  - **Affiliations:** Genentech, Boston, MA, United States
  - **TL;DR:** This study focuses on improving the estimation of conditional average treatment effects (CATEs) by emphasizing the importance of weight selection in pseudo-outcome regressions. The authors demonstrate that using inverse-variance weights leads to more stable estimates and faster convergence rates compared to traditional methods.
  - **Keywords:** Conditional Average Treatment Effects (CATEs), Treatment Effect Estimation, Inverse-Variance Weights (IVWs), Pseudo-Outcome Regression (POR), R-Learning, Instability in Inverse-Propensity Weights, Confounding Bias, Superior performance of IVWs, Convergence rates for IVWs


- [From Geometry to Causality- Ricci Curvature and the Reliability of Causal Inference on Networks](https://icml.cc/virtual/2024/poster/35029) (Poster)
  - **Authors:** [Amirhossein Farzam](http://openreview.net/profile?id=~Amirhossein_Farzam1), [Allen Tannenbaum](http://openreview.net/profile?id=~Allen_Tannenbaum2), [Guillermo Sapiro](http://openreview.net/profile?id=~Guillermo_Sapiro1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Duke University, Durham, NC, United States; None, Department of Computer Science, Stony Brook University, Stony Brook, NY, United States, Department of Electrical and Computer Engineering, Duke University, Durham, NC, United States; Apple, Cupertino, CA, United States
  - **TL;DR:** This study establishes a theoretical connection between network geometry, specifically Ricci curvature, and causal inference, highlighting the challenges posed by negative curvature in estimating causal parameters. The authors propose a method using geometric Ricci flow to enhance the reliability of causal effect estimation in networked data, potentially improving GNN-based causal inference.
  - **Keywords:** Causal inference, Network analysis, Graph neural networks (GNNs), Geometric Ricci flow, Epidemiology, Economics, Political science, Violations of identification assumptions, Dependencies between treatment units, Theoretical link between network geometry and causal inference, Assessment of causal estimates reliability, Ricci curvature, Individual treatment effect (ITE)


- [Hyperbolic Active Learning for Semantic Segmentation under Domain Shift](https://icml.cc/virtual/2024/poster/33399) (Poster)
  - **Authors:** [Luca Franco](http://openreview.net/profile?id=~Luca_Franco1), [Paolo Mandica](http://openreview.net/profile?id=~Paolo_Mandica1), [Konstantinos Kallidromitis](http://openreview.net/profile?id=~Konstantinos_Kallidromitis1), [Devin Guillory](http://openreview.net/profile?id=~Devin_Guillory1), [Yu-Teng Li](http://openreview.net/profile?id=~Yu-Teng_Li1), [Trevor Darrell](http://openreview.net/profile?id=~Trevor_Darrell2), [Fabio Galasso](http://openreview.net/profile?id=~Fabio_Galasso1)
  - **Affiliations:** ITALAI S.R.L.; Sapienza University of Rome, Sapienza University of Rome; ITALAI S.R.L., Panasonic North America, UC Berkeley, UC Berkeley, UC Berkeley, ITALAI S.R.L.; Sapienza University of Rome
  - **TL;DR:** This study introduces HALO, a hyperbolic neural network approach for active learning in semantic segmentation, which utilizes epistemic uncertainty to select data points for labeling. HALO achieves state-of-the-art performance in active learning under domain shift, outperforming traditional supervised domain adaptation methods while using only 1% of labels.
  - **Keywords:** Hyperbolic neural networks, Active learning, Semantic segmentation, Hyperbolic radius, Epistemic uncertainty, Prediction entropy, Self-driving cars, Manufacturing, Medicine, Data scarcity, Domain adaptation, Pixel-wise annotations, Hyperbolic Active Learning Optimization (HALO), State-of-the-art performance in active learning, GTAV, Cityscapes, SYNTHIA, ACDC, Domain shift, Active domain adaptation (ADA)


- [Fast White-Box Adversarial Streaming Without a Random Oracle](https://icml.cc/virtual/2024/poster/32821) (Poster)
  - **Authors:** [Ying Feng](http://openreview.net/profile?id=~Ying_Feng2), [Aayush Jain](http://openreview.net/profile?email=aayushja%40andrew.cmu.edu), [David Woodruff](http://openreview.net/profile?id=~David_Woodruff1)
  - **Affiliations:** Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA, Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA, Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
  - **TL;DR:** This paper presents a solution for the sparse recovery problem in a white-box adversarial streaming model, eliminating the need for a random oracle and achieving polylogarithmic processing time per item. The proposed method is based on the Learning with Errors assumption and extends to other tasks like distinct element estimation and low-rank approximation.
  - **Keywords:** Adversarial Robust Streaming, White-Box Adversarial Model, Sparse Recovery, Homomorphic Encryption, Learning with Errors, Data Streaming, Distributed Models, Adversarial Attacks, Space Complexity, Update Time, Near-Optimal Solutions, Efficient Processing Time


- [Trust the Model Where It Trusts Itself - Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption](https://icml.cc/virtual/2024/poster/34216) (Poster)
  - **Authors:** [Bernd Frauenknecht](http://openreview.net/profile?id=~Bernd_Frauenknecht1), [Artur Eisele](http://openreview.net/profile?id=~Artur_Eisele1), [Devdutt Subhasish](http://openreview.net/profile?id=~Devdutt_Subhasish1), [Friedrich Solowjow](http://openreview.net/profile?id=~Friedrich_Solowjow1), [Sebastian Trimpe](http://openreview.net/profile?id=~Sebastian_Trimpe1)
  - **Affiliations:** Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, 52068 Aachen, Germany, Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, 52068 Aachen, Germany, Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, 52068 Aachen, Germany, Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, 52068 Aachen, Germany, Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, 52068 Aachen, Germany
  - **TL;DR:** This paper introduces the Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption (MACURA) algorithm, which optimizes the use of model-based rollouts in reinforcement learning by adapting rollout lengths based on model uncertainty. The proposed method demonstrates significant improvements in data efficiency and performance over existing deep MBRL techniques on the MuJoCo benchmark.
  - **Keywords:** Model-Based Reinforcement Learning (MBRL), Uncertainty in Models, Model-Based Actor-Critic, Uncertainty-Aware Rollout Adaption (MACURA), Probabilistic Ensemble (PE) models, Soft Actor-Critic (SAC), Robotics, Control Systems, Engineering Problems, Data inefficiency in model-free approaches, Model accuracy, Model exploitation, Improved data efficiency, Performance enhancements in MBRL, MuJoCo benchmark, Dyna-style MBRL, Rollout lengths, Model uncertainty


- [Hyperbolic Geometric Latent Diffusion Model for Graph Generation](https://icml.cc/virtual/2024/poster/34924) (Poster)
  - **Authors:** [Xingcheng Fu](http://openreview.net/profile?id=~Xingcheng_Fu1), [Yisen Gao](http://openreview.net/profile?id=~Yisen_Gao1), [Yuecen Wei](http://openreview.net/profile?id=~Yuecen_Wei1), [Qingyun Sun](http://openreview.net/profile?id=~Qingyun_Sun2), [Hao Peng](http://openreview.net/profile?id=~Hao_Peng7), [Jianxin Li](http://openreview.net/profile?id=~Jianxin_Li3), [Xianxian Li](http://openreview.net/profile?id=~Xianxian_LI2)
  - **Affiliations:** Key Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, Guilin, China, Institute of Artificial Intelligence, Beihang University, Beijing, China, School of Software, Beihang University, Beijing, China, Beijing Advanced Innovation Center for Big Data and Brain Computing, School of Computer Science and Engineering, Beihang University, Beijing, China, Beijing Advanced Innovation Center for Big Data and Brain Computing, School of Computer Science and Engineering, Beihang University, Beijing, China, Beijing Advanced Innovation Center for Big Data and Brain Computing, School of Computer Science and Engineering, Beihang University, Beijing, China, Key Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, Guilin, China
  - **TL;DR:** This study introduces HypDiff, a novel geometrically latent diffusion framework for graph generation that addresses the challenges of computational complexity and topological preservation in existing models. The proposed method effectively captures the anisotropic nature of graph structures, demonstrating superior performance across various topologies.
  - **Keywords:** graph generation, diffusion models, geometrically latent diffusion framework, hyperbolic geometry, computer vision, graph learning, computational complexity, training efficiency, preservation of topological information, anisotropy of non-Euclidean structure, HypDiff framework, preservation of topological properties, Denoising Diffusion Probabilistic Model (DDPM), Variational Graph Auto-Encoder (VGAE), Graph Generative Adversarial Networks (GraphGAN)


- [Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings](https://icml.cc/virtual/2024/poster/33701) (Spotlight Poster)
  - **Authors:** [Kevin Frans](http://openreview.net/profile?id=~Kevin_Frans1), [Seohong Park](http://openreview.net/profile?id=~Seohong_Park1), [Pieter Abbeel](http://openreview.net/profile?id=~Pieter_Abbeel2), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1)
  - **Affiliations:** University of California, Berkeley, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley
  - **TL;DR:** This study presents Functional Reward Encoding (FRE) as a scalable solution for zero-shot reinforcement learning, enabling a generalist agent to adapt to new tasks using unlabeled offline data. The results demonstrate that FRE agents can generalize effectively across various simulated robotic tasks, outperforming existing methods.
  - **Keywords:** zero-shot reinforcement learning, generalist agent, functional reward encoding (FRE), transformer-based variational auto-encoder, simulated robotic benchmarks, discovering task representations without labels, robust to downstream objectives, scalable solution to zero-shot RL, pre-training with unsupervised reward functions


- [Weisfeiler-Leman at the margin: When more expressivity matters](https://icml.cc/virtual/2024/poster/34462) (Poster)
  - **Authors:** [Billy Franks](http://openreview.net/profile?id=~Billy_Joe_Franks1), [Christopher Morris](http://openreview.net/profile?id=~Christopher_Morris1), [Ameya Velingker](http://openreview.net/profile?id=~Ameya_Velingker1), [Floris Geerts](http://openreview.net/profile?id=~Floris_Geerts1)
  - **Affiliations:** Department of Computer Science, University of Kaiserslautern-Landau, Department of Computer Science, RWTH Aachen University, Google Research, Department of Computer Science, University of Antwerp
  - **TL;DR:** This study investigates the limitations of the Weisfeiler–Leman algorithm in distinguishing non-isomorphic graphs and explores how augmenting message-passing graph neural networks with subgraph information can enhance their expressivity and generalization performance. The findings suggest that increased expressivity does not always correlate with improved generalization, highlighting the need for further understanding of these relationships.
  - **Keywords:** graph isomorphism, expressive power of MPNNs, Weisfeiler–Leman algorithm (1-WL), message-passing graph neural networks (MPNNs), bioinformatics, combinatorial optimization, image analysis, social-network analysis, distinguishing non-isomorphic graphs, generalization performance, expressive 1-WL-based kernel and MPNN architectures, provable generalization properties


- [Language-guided Skill Learning with Temporal Variational Inference](https://icml.cc/virtual/2024/poster/33663) (Poster)
  - **Authors:** [Haotian Fu](http://openreview.net/profile?id=~Haotian_Fu3), [Pratyusha Sharma](http://openreview.net/profile?id=~Pratyusha_Sharma1), [Elias Stengel-Eskin](http://openreview.net/profile?id=~Elias_Stengel-Eskin1), [George Konidaris](http://openreview.net/profile?id=~George_Konidaris1), [Nicolas Le Roux](http://openreview.net/profile?id=~Nicolas_Le_Roux2), [Marc-Alexandre Côté](http://openreview.net/profile?id=~Marc-Alexandre_C%C3%B4t%C3%A92), [Xingdi Yuan](http://openreview.net/profile?id=~Xingdi_Yuan2)
  - **Affiliations:** Brown University, MIT, University of North Carolina, Chapel Hill, Brown University, Mila; Microsoft Research, Microsoft Research, Microsoft Research
  - **TL;DR:** This study presents an algorithm for skill discovery from expert demonstrations using Large Language Models and hierarchical variational inference, addressing the challenges of sample inefficiency in Reinforcement Learning. The proposed method enables the discovery of reusable skills that significantly accelerate learning and outperform existing approaches in complex tasks.
  - **Keywords:** Skill discovery, Reinforcement Learning, Long-horizon tasks, Large Language Models (LLMs), Hierarchical variational inference, Minimum Description Length principle, BabyAI (grid world navigation), ALFRED (household simulation), Sample inefficiency in Reinforcement Learning, Balancing compression and reusability, Discovery of reusable skills, Acceleration of learning, Outperformance of baseline skill learning approaches


- [Trustworthy Actionable Perturbations](https://icml.cc/virtual/2024/poster/32617) (Poster)
  - **Authors:** [Jesse Friedbaum](http://openreview.net/profile?id=~Jesse_Friedbaum1), [Sudarshan Adiga](http://openreview.net/profile?id=~Sudarshan_Adiga2), [Ravi Tandon](http://openreview.net/profile?id=~Ravi_Tandon1)
  - **Affiliations:** Program in Applied Mathematics, University of Arizona, Tucson, AZ, USA; Department of Electrical and Computer Engineering, University of Arizona, Tucson, AZ, USA, Department of Electrical and Computer Engineering, University of Arizona, Tucson, AZ, USA, Program in Applied Mathematics, University of Arizona, Tucson, AZ, USA; Department of Electrical and Computer Engineering, University of Arizona, Tucson, AZ, USA
  - **TL;DR:** This paper introduces Trustworthy Actionable Perturbations (TAP), a framework designed to create modified inputs that beneficially change true class probabilities in machine learning classifiers, while ensuring these changes are causally linked to real-world outcomes. The authors present a novel verification procedure and new definitions of cost, reward, and goals to enhance the effectiveness of counterfactuals in practical applications.
  - **Keywords:** Counterfactuals, Machine Learning Classifiers, Trustworthy Actionable Perturbations, Verification Procedure, PAC-learnability, Credit Lending, College Admissions, Healthcare, Undesirable Classifications, Causal Linkage, Adversarial Attacks, New Framework for TAP, Cost and Reward Definitions, Actionable Counterfactuals, Algorithmic Recourses, Improvement-Focused Causal Recourse


- [Towards Theoretical Understandings of Self-Consuming Generative Models](https://icml.cc/virtual/2024/poster/33664) (Poster)
  - **Authors:** [Shi Fu](http://openreview.net/profile?id=~Shi_Fu1), [Sen Zhang](http://openreview.net/profile?id=~Sen_Zhang3), [Yingjie Wang](http://openreview.net/profile?id=~Yingjie_Wang1), [Xinmei Tian](http://openreview.net/profile?id=~Xinmei_Tian1), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** University of Science and Technology of China, Hefei, China, The University of Sydney, Sydney, Australia, College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; Engineering Research Center of Intelligent Technology for Agriculture, Ministry of Education, China, Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China; University of Science and Technology of China, Hefei, China, Nanyang Technological University, Singapore
  - **TL;DR:** This study develops a theoretical framework to analyze the effects of training generative models in a self-consuming loop, revealing that the total variation distance between synthetic and real data distributions can be controlled with sufficient real data. It also identifies a phase transition in the impact of synthetic data amounts on model performance, highlighting the importance of real data in mitigating model collapse.
  - **Keywords:** generative models, self-consuming training loop, diffusion models, one-hidden-layer neural network score function, kernel density estimation, synthetic data generation, data augmentation, medical imaging, model collapse, error propagation, data distribution learning, bounds on total variation distance, phase transition in synthetic data amounts


- [Let Go of Your Labels with Unsupervised Transfer](https://icml.cc/virtual/2024/poster/34048) (Poster)
  - **Authors:** [Artyom Gadetsky](http://openreview.net/profile?id=~Artyom_Gadetsky1), [Yulun Jiang](http://openreview.net/profile?id=~Yulun_Jiang1), [Maria Brbic](http://openreview.net/profile?id=~Maria_Brbic1)
  - **Affiliations:** EPFL, Lausanne, Switzerland, EPFL, Lausanne, Switzerland, EPFL, Lausanne, Switzerland
  - **TL;DR:** This paper introduces TURTLE, a fully unsupervised method for uncovering the underlying labeling of datasets, achieving state-of-the-art performance in unsupervised transfer and outperforming zero-shot transfer baselines across various datasets. The findings suggest that foundation models can be effectively utilized for unsupervised tasks without human guidance.
  - **Keywords:** Unsupervised transfer, Foundation models, Zero-shot transfer, Maximal margin classifiers, Representation learning, Vision-language tasks, Downstream tasks, Need for human guidance in zero-shot transfer, Lack of supervision in labeling, TURTLE method, State-of-the-art unsupervised performance, 26 benchmark datasets, CLIP, Foundation models


- [Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning](https://icml.cc/virtual/2024/poster/33474) (Poster)
  - **Authors:** [Kai Gan](http://openreview.net/profile?id=~Kai_Gan1), [Tong Wei](http://openreview.net/profile?id=~Tong_Wei1)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing 210096, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China
  - **TL;DR:** This paper introduces FINESSL, a novel semi-supervised learning approach that leverages pre-trained foundation models to overcome performance limitations in SSL. The method achieves state-of-the-art results on benchmark datasets while significantly reducing training costs and integrating various fine-tuning techniques.
  - **Keywords:** Semi-supervised learning (SSL), Foundation models, FINESSL, balanced margin softmax, decoupled label smoothing, visual prompt tuning (VPT), full fine-tuning (FFT), linear probing (LP), Aggregated biases, cognitive deviation, subpar performance in SSL methods, New state of the art for SSL, reduced training cost, integration of fine-tuning and modern SSL algorithms, CIFAR-100


- [Positive Concave Deep Equilibrium Models](https://icml.cc/virtual/2024/poster/33522) (Poster)
  - **Authors:** [Mateusz Gabor](http://openreview.net/profile?id=~Mateusz_Gabor1), [Tomasz Piotrowski](http://openreview.net/profile?id=~Tomasz_Jan_Piotrowski1), [Renato L. G. Cavalcante](http://openreview.net/profile?id=~Renato_Luis_Garrido_Cavalcante1)
  - **Affiliations:** Faculty of Electronics, Photonics, and Microsystems, Wrocław University of Science and Technology, Wrocław, Poland, Faculty of Physics, Astronomy and Informatics, Nicolaus Copernicus University, Toruń, Poland, Fraunhofer Heinrich Hertz Institute, Berlin, Germany
  - **TL;DR:** This paper introduces positive concave deep equilibrium (pcDEQ) models, which enhance the stability and convergence of deep equilibrium models by ensuring the existence and uniqueness of fixed points through nonnegative weights and concave activation functions. The proposed models demonstrate competitive performance in language modeling and computer vision tasks while simplifying the training process.
  - **Keywords:** deep equilibrium models, memory efficiency, neural networks, fixed point equations, nonlinear Perron-Frobenius theory, language modeling, computer vision, existence and uniqueness of fixed points, convergence issues, positive concave deep equilibrium models (pcDEQ), geometric convergence guarantees, implicit models, neural ordinary differential equations (Neural ODEs)


- [Stochastic Weakly Convex Optimization beyond Lipschitz Continuity](https://icml.cc/virtual/2024/poster/33053) (Poster)
  - **Authors:** [Wenzhi Gao](http://openreview.net/profile?id=~Wenzhi_Gao1), [Qi Deng](http://openreview.net/profile?id=~Qi_Deng1)
  - **Affiliations:** Institute for Computational and Mathematical Engineering, Stanford University, Antai College of Economics and Management, Shanghai Jiao Tong University
  - **TL;DR:** This paper investigates stochastic weakly convex optimization without the Lipschitz continuity assumption, demonstrating that various stochastic algorithms can maintain an O(1/√K) convergence rate with a constant failure rate. The findings highlight the efficiency and robustness of new stepsize strategies in addressing challenges associated with non-globally Lipschitz functions.
  - **Keywords:** stochastic weakly convex optimization, weak convexity, stochastic subgradient method, robust regularization strategies, phase retrieval, robust PCA, reinforcement learning, non-globally Lipschitz continuity, algorithm divergence, O(1/√K) convergence rate, efficiency and robustness of stepsize policies, Lipschitz parameter, Bregman divergence


- [Multi-Agent Reinforcement Learning Meets Leaf Sequencing in Radiotherapy](https://icml.cc/virtual/2024/poster/32973) (Poster)
  - **Authors:** [Riqiang Gao](http://openreview.net/profile?id=~Riqiang_Gao1), [Florin-Cristian Ghesu](http://openreview.net/profile?id=~Florin-Cristian_Ghesu1), [Simon Arberet](http://openreview.net/profile?id=~Simon_Arberet1), [Shahab Basiri](http://openreview.net/profile?email=shahab.basiri%40varian.com), [Esa Kuusela](http://openreview.net/profile?email=esa.kuusela%40varian.com), [Martin Kraus](http://openreview.net/profile?id=~Martin_Kraus2), [Dorin Comaniciu](http://openreview.net/profile?id=~Dorin_Comaniciu1), [Ali Kamen](http://openreview.net/profile?id=~Ali_Kamen2)
  - **Affiliations:** Digital Technology and Innovation, Siemens Healthineers, Princeton NJ, USA, Digital Technology and Innovation, Siemens Healthineers, Erlangen, Germany, Digital Technology and Innovation, Siemens Healthineers, Princeton NJ, USA, Varian Medical Systems, Siemens Healthineers, Helsinki, Finland, Varian Medical Systems, Siemens Healthineers, Helsinki, Finland, Digital Technology and Innovation, Siemens Healthineers, Erlangen, Germany, Digital Technology and Innovation, Siemens Healthineers, Princeton NJ, USA, Digital Technology and Innovation, Siemens Healthineers, Princeton NJ, USA
  - **TL;DR:** This study introduces a novel deep reinforcement learning model, the Reinforced Leaf Sequencer (RLS), aimed at improving leaf sequencing in radiotherapy planning. The RLS model demonstrates reduced fluence reconstruction errors and potential for faster convergence when integrated into existing optimization planners, suggesting significant advancements in the efficiency of radiotherapy treatment planning.
  - **Keywords:** Radiotherapy Planning (RTP), Deep Reinforcement Learning (DRL), Reinforced Leaf Sequencer (RLS), Multi-Agent Framework, Cancer Treatment, Radiotherapy, Time-consuming optimization steps, Fluence reconstruction errors, Reduced fluence reconstruction errors, Faster convergence in optimization planning, Four datasets used for experiments, Leaf Sequencing, Optimal Fluence Prediction, Planning Target Volume (PTV), Organs at Risk (OARs)


- [A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer](https://icml.cc/virtual/2024/poster/32612) (Poster)
  - **Authors:** [Zhangyang Gao](http://openreview.net/profile?id=~Zhangyang_Gao1), [Daize Dong](http://openreview.net/profile?id=~Daize_Dong1), [Cheng Tan](http://openreview.net/profile?id=~Cheng_Tan1), [Jun Xia](http://openreview.net/profile?id=~Jun_Xia1), [Bozhen Hu](http://openreview.net/profile?id=~Bozhen_Hu1), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** Westlake University, Hangzhou, China; Zhejiang University, Hangzhou, China, Westlake University, Hangzhou, China, Westlake University, Hangzhou, China; Zhejiang University, Hangzhou, China, Westlake University, Hangzhou, China; Zhejiang University, Hangzhou, China, Westlake University, Hangzhou, China; Zhejiang University, Hangzhou, China, Westlake University, Hangzhou, China
  - **TL;DR:** This paper introduces GraphsGPT, a novel framework that transforms Non-Euclidean graphs into learnable Euclidean representations and reconstructs them, achieving state-of-the-art performance in graph representation learning and generation. The findings highlight the effectiveness of the proposed methods in overcoming challenges associated with Non-Euclidean graph modeling.
  - **Keywords:** Non-Euclidean graphs, graph modeling, graph representation learning, Graph2Seq, GraphGPT, pure transformer, edge-centric generation strategy, Molecular design, social network analysis, recommendation systems, 3D surfaces, Challenges in recovering original graphs from Euclidean vectors, limitations of existing graph neural networks and transformers, State-of-the-art results in graph classification and regression tasks, effective graph generation, graph mixup in Euclidean space, 100M molecules dataset, Graph Words, attention maps, auto-regressive generation


- [Reflective Policy Optimization](https://icml.cc/virtual/2024/poster/34656) (Poster)
  - **Authors:** [Yaozhong Gan](http://openreview.net/profile?id=~Yaozhong_Gan1), [yan renye](http://openreview.net/profile?id=~Renye_Yan1), [zhe wu](http://openreview.net/profile?id=~Zhe_Wu6), [Junliang Xing](http://openreview.net/profile?id=~Junliang_Xing1)
  - **Affiliations:** QiYuan Lab, QiYuan Lab, QiYuan Lab, QiYuan Lab
  - **TL;DR:** This paper introduces Reflective Policy Optimization (RPO), an on-policy reinforcement learning algorithm that enhances sample efficiency by integrating past and future state-action information for policy optimization. Theoretical and empirical analyses demonstrate that RPO improves policy performance and accelerates convergence compared to traditional methods.
  - **Keywords:** On-policy reinforcement learning, Policy optimization, Reflective Policy Optimization (RPO), Proximal Policy Optimization (PPO), Trust Region Policy Optimization (TRPO), Continuous control tasks, Robot control, Atari games, Sample inefficiency, Policy evaluation limitations, Improved sample efficiency, Monotonic policy performance improvement, State-action pairs, Value function, Introspection in agents


- [Critical feature learning in deep neural networks](https://icml.cc/virtual/2024/poster/32715) (Poster)
  - **Authors:** [Kirsten Fischer](http://openreview.net/profile?id=~Kirsten_Fischer1), [Javed Lindner](http://openreview.net/profile?id=~Javed_Lindner1), [David Dahmen](http://openreview.net/profile?id=~David_Dahmen1), [Zohar Ringel](http://openreview.net/profile?id=~Zohar_Ringel1), [Michael Krämer](http://openreview.net/profile?id=~Michael_Kr%C3%A4mer1), [Moritz Helias](http://openreview.net/profile?id=~Moritz_Helias1)
  - **Affiliations:** Institute for Advanced Simulation (IAS-6), Computational and Systems Neuroscience, Jülich Research Centre, Jülich, Germany; RWTH Aachen University, Aachen, Germany, Institute for Advanced Simulation (IAS-6), Computational and Systems Neuroscience, Jülich Research Centre, Jülich, Germany; RWTH Aachen University, Aachen, Germany; Department of Physics, RWTH Aachen University, Aachen, Germany, Institute for Advanced Simulation (IAS-6), Computational and Systems Neuroscience, Jülich Research Centre, Jülich, Germany, The Racah Institute of Physics, The Hebrew University of Jerusalem, Jerusalem, Israel, Institute for Theoretical Particle Physics and Cosmology, RWTH Aachen University, Aachen, Germany, Institute for Advanced Simulation (IAS-6), Computational and Systems Neuroscience, Jülich Research Centre, Jülich, Germany; RWTH Aachen University, Aachen, Germany
  - **TL;DR:** This study develops a theoretical framework for understanding feature learning in deep neural networks by analyzing the Bayesian prior as a superposition of Gaussian processes. It reveals how network width influences kernel adaptation and connects feature learning to criticality in hyperparameter space, highlighting the interplay between network architecture and data adaptation.
  - **Keywords:** feature learning, deep neural networks, Bayesian methods, Gaussian processes, network kernels, maximum a posteriori estimation, understanding inductive bias, adaptation to data, data-adaptive kernels, connection between criticality and feature learning, neural network Gaussian process (NNGP), neural tangent kernel (NTK), edge-of-chaos


- [PinNet: Pinpoint Instructive Information for Retrieval Augmented Code-to-Text Generation](https://icml.cc/virtual/2024/poster/33963) (Poster)
  - **Authors:** [Han Fu](http://openreview.net/profile?id=~Han_Fu1), [Jian Tan](http://openreview.net/profile?id=~Jian_Tan2), [Pinhan Zhang](http://openreview.net/profile?id=~Pinhan_Zhang1), [Feifei Li](http://openreview.net/profile?id=~Feifei_Li3), [Jianling Sun](http://openreview.net/profile?id=~Jianling_Sun2)
  - **Affiliations:** Alibaba Group, Hangzhou, China, Alibaba Group, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, Alibaba Group, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China
  - **TL;DR:** The study presents PinNet, a novel framework for code-to-text generation that enhances the quality of generated descriptions by effectively leveraging retrieval augmented methods while addressing the issue of misleading information in retrieved references. Extensive experiments demonstrate that PinNet significantly outperforms existing baselines in code summarization and SQL-to-text generation tasks.
  - **Keywords:** code-to-text generation, retrieval augmented approaches, contrastive learning, discriminator, attention weights, code summarization, SQL-to-text generation, misleading information in retrieved references, semantic matching, new framework (PinNet), improved generation quality


- [Testing the Feasibility of Linear Programs with Bandit Feedback](https://icml.cc/virtual/2024/poster/33966) (Spotlight Poster)
  - **Authors:** [Aditya Gangrade](http://openreview.net/profile?id=~Aditya_Gangrade1), [Aditya Gopalan](http://openreview.net/profile?id=~Aditya_Gopalan1), [Venkatesh Saligrama](http://openreview.net/profile?id=~Venkatesh_Saligrama1), [Clay Scott](http://openreview.net/profile?id=~Clayton_Scott1)
  - **Affiliations:** Department of Electrical Computer Engineering, Boston University; Department of Electrical Engineering and Computer Science, University of Michigan, Department of Electrical Communication Engineering, Indian Institute of Science, Department of Electrical Computer Engineering, Boston University, Department of Electrical Engineering and Computer Science, University of Michigan
  - **TL;DR:** This paper investigates the feasibility of linear programs in the context of constrained bandit problems, proposing a novel testing method that adapts to the signal level and minimizes sample costs. The findings highlight the importance of testing feasibility assumptions in practical applications, providing a framework for reliable and efficient testing.
  - **Keywords:** constrained bandit problems, feasibility testing, linear bandit setting, low-regret algorithms, minimax game, testing feasibility of linear programs, determining existence of solutions under constraints, reliable tests, sample cost analysis, linear programs (LP), binary composite hypothesis testing, signal level


- [Interpretability Illusions in the Generalization of Simplified Models](https://icml.cc/virtual/2024/poster/33777) (Poster)
  - **Authors:** [Dan Friedman](http://openreview.net/profile?id=~Dan_Friedman2), [Andrew Lampinen](http://openreview.net/profile?id=~Andrew_Kyle_Lampinen1), [Lucas Dixon](http://openreview.net/profile?id=~Lucas_Dixon1), [Danqi Chen](http://openreview.net/profile?id=~Danqi_Chen1), [Asma Ghandeharioun](http://openreview.net/profile?id=~Asma_Ghandeharioun1)
  - **Affiliations:** Department of Computer Science, Princeton University; Google DeepMind, Google DeepMind, Google Research, Department of Computer Science, Princeton University; Google DeepMind, Google Research
  - **TL;DR:** This study investigates the limitations of simplified model representations in deep learning, particularly how they may misrepresent a model's behavior when generalizing out-of-distribution. The findings reveal consistent generalization gaps, questioning the reliability of mechanistic interpretations derived from simplifications like SVD.
  - **Keywords:** deep learning, model interpretability, generalization, singular value decomposition (SVD), dimensionality reduction, clustering, out-of-distribution generalization, systematic generalization gaps, Dyck balanced-parenthesis languages, code completion task, Transformer models, mechanistic interpretations


- [Improving Sample Efficiency of Model-Free Algorithms for Zero-Sum Markov Games](https://icml.cc/virtual/2024/poster/33734) (Poster)
  - **Authors:** [Songtao Feng](http://openreview.net/profile?id=~Songtao_Feng1), [Ming Yin](http://openreview.net/profile?id=~Ming_Yin4), [Yu-Xiang Wang](http://openreview.net/profile?id=~Yu-Xiang_Wang1), [Jing Yang](http://openreview.net/profile?id=~Jing_Yang3), [Yingbin LIANG](http://openreview.net/profile?id=~Yingbin_Liang1)
  - **Affiliations:** The University of Florida, Princeton University, The University of California, Santa Barbara, The Pennsylvania State University, The Ohio State University
  - **TL;DR:** This study proposes a model-free stage-based algorithm for two-player zero-sum Markov games that achieves optimal sample complexity, matching the performance of model-based algorithms. The key innovation lies in leveraging a variance reduction technique adapted for the unique challenges of Markov games, enhancing sample efficiency significantly.
  - **Keywords:** Multi-agent reinforcement learning, Zero-sum Markov games, Model-free algorithms, Variance reduction technique, Reference-advantage decomposition, Game theory, Autonomous driving, Real-time strategy games, Sample efficiency, Exploration challenges, Optimality in algorithms, Model-free stage-based algorithm, Sample complexity improvement, Nash Equilibrium, Coarse correlated equilibrium (CCE)


- [Reservoir Computing for Short High-Dimensional Time Series: an Application to SARS-CoV-2 Hospitalization Forecast](https://icml.cc/virtual/2024/poster/34677) (Poster)
  - **Authors:** [Thomas Ferté](http://openreview.net/profile?id=~Thomas_Fert%C3%A91), [Dutartre Dan](http://openreview.net/profile?id=~Dutartre_Dan1), [Boris Hejblum](http://openreview.net/profile?id=~Boris_P_Hejblum1), [Romain Griffier](http://openreview.net/profile?email=romain.griffier%40chu-bordeaux.fr), [Vianney Jouhet](http://openreview.net/profile?email=vianney.jouhet%40chu-bordeaux.fr), [Rodolphe Thiébaut](http://openreview.net/profile?email=rodolphe.thiebaut%40u-bordeaux.fr), [Pierrick Legrand](http://openreview.net/profile?email=pierrick.legrand%40u-bordeaux.fr), [Xavier Hinaut](http://openreview.net/profile?id=~Xavier_Hinaut1)
  - **Affiliations:** Bordeaux Hospital University Center, Pôle de santé publique, Service d’information médicale, F-33000 Bordeaux, France; Inserm Bordeaux Population Health Research Center UMR 1219, Inria centre of Bordeaux University, team SISTM, F-33000 Bordeaux, France; Inria centre of Bordeaux University, F-33000 Bordeaux, France, Inria centre of Bordeaux University, F-33000 Bordeaux, France, Inserm Bordeaux Population Health Research Center UMR 1219, Inria centre of Bordeaux University, team SISTM, F-33000 Bordeaux, France; Inria centre of Bordeaux University, F-33000 Bordeaux, France, Bordeaux Hospital University Center, Pôle de santé publique, Service d’information médicale, F-33000 Bordeaux, France; Inserm Bordeaux Population Health Research Center UMR 1219, team AHeaD, F-33000 Bordeaux, France, Bordeaux Hospital University Center, Pôle de santé publique, Service d’information médicale, F-33000 Bordeaux, France; Inserm Bordeaux Population Health Research Center UMR 1219, team AHeaD, F-33000 Bordeaux, France, Bordeaux Hospital University Center, Pôle de santé publique, Service d’information médicale, F-33000 Bordeaux, France; Inserm Bordeaux Population Health Research Center UMR 1219, Inria centre of Bordeaux University, team SISTM, F-33000 Bordeaux, France, ASTRAL, Centre Inria de l’université de Bordeaux; IMB, Institut de Mathématiques de Bordeaux, UMR CNRS 5251, Inria centre of Bordeaux University, F-33000 Bordeaux, France; LaBRI, Univ. Bordeaux, Bordeaux INP, CNRS UMR 5800; Univ. Bordeaux, CNRS, IMN, UMR 5293, Bordeaux, France
  - **TL;DR:** This study introduces a novel approach combining Reservoir Computing and genetic algorithms to forecast SARS-CoV-2 hospitalizations, significantly improving prediction accuracy in a high-dimensional context. The proposed method outperforms traditional forecasting techniques, highlighting its potential for effective healthcare resource management.
  - **Keywords:** SARS-CoV-2 hospitalization forecasting, time series forecasting, Reservoir Computing (RC), genetic algorithm (GA), LSTM, Transformers, Elastic-Net, XGBoost, Healthcare, epidemic forecasting, High-dimensional data, sample efficiency, dynamic shifts in data, Improved prediction performance, optimal non-linear combinations of inputs, Electronic health records (EHRs), public data


- [Explaining Probabilistic Models with Distributional Values](https://icml.cc/virtual/2024/poster/35080) (Spotlight Poster)
  - **Authors:** [Luca Franceschi](http://openreview.net/profile?id=~Luca_Franceschi1), [Michele Donini](http://openreview.net/profile?id=~Michele_Donini1), [Cedric Archambeau](http://openreview.net/profile?id=~Cedric_Archambeau1), [Matthias Seeger](http://openreview.net/profile?id=~Matthias_Seeger2)
  - **Affiliations:** Amazon Web Services, Berlin, Germany, Amazon Web Services, Berlin, Germany, Helsing, Berlin, Germany, Amazon Web Services, Berlin, Germany
  - **TL;DR:** This paper addresses the limitations of current explainable machine learning methods by introducing distributional values, which provide more nuanced explanations for probabilistic models. The proposed framework enhances the interpretability of model outputs by tracking changes in predictions, thereby bridging the gap between desired explanations and existing methods.
  - **Keywords:** Explainable Machine Learning, Cooperative Game Theory, SHAP, Shapley Values, Distributional Values, Vision Models, Language Models, Mismatch in explanations of model outputs, Limitations of scalar payoffs, Introduction of distributional values for probabilistic models, Game-theoretic XAI, Stochastic Payoffs


- [A Doubly Recursive Stochastic Compositional Gradient Descent Method for Federated Multi-Level Compositional Optimization](https://icml.cc/virtual/2024/poster/34494) (Poster)
  - **Authors:** [Hongchang Gao](http://openreview.net/profile?id=~Hongchang_Gao3)
  - **Affiliations:** Department of Computer and Information Sciences, Temple University, Philadelphia, USA
  - **TL;DR:** This paper presents a novel federated stochastic multi-level compositional optimization algorithm that addresses the challenges of heterogeneity and communication efficiency, achieving a level-independent and linear speedup convergence rate for nonconvex problems. Experimental results validate the effectiveness of the proposed method in federated learning settings.
  - **Keywords:** Federated compositional optimization, Multi-level compositional optimization, Stochastic gradient descent, Jacobian-vector product estimator, Model-agnostic meta-learning, Graph neural networks, Risk-averse portfolio optimization, Heterogeneity issue, Communication efficiency issue, Slow convergence rate, Level-independent convergence rate, Linear speedup convergence rate, Nonconvex problems, Stochastic multi-level compositional optimization (SMCO)


- [Decoupling Learning and Decision-Making: Breaking the $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods](https://icml.cc/virtual/2024/poster/35160) (Poster)
  - **Authors:** [Wenzhi Gao](http://openreview.net/profile?id=~Wenzhi_Gao1), [Chunlin Sun](http://openreview.net/profile?id=~Chunlin_Sun1), [Chenyu Xue](http://openreview.net/profile?id=~Chenyu_Xue1), [Yinyu Ye](http://openreview.net/profile?id=~Yinyu_Ye1)
  - **Affiliations:** Institute for Computational and Mathematical Engineering, Stanford University; Management Science & Engineering, Stanford University, Institute for Computational and Mathematical Engineering, Stanford University, School of Information Management and Engineering, Shanghai University of Finance and Economics, Institute for Computational and Mathematical Engineering, Stanford University; Management Science & Engineering, Stanford University
  - **TL;DR:** This paper introduces a new algorithmic framework for online linear programming that decouples learning from decision-making, allowing first-order methods to achieve a regret of O(T 1/3). This represents a significant improvement over the previously established O(√T) regret bound for first-order methods in this context.
  - **Keywords:** Online linear programming, resource allocation, revenue management, First-order methods, online learning algorithms, Cloud computing, online advertising, Regret minimization, constraint violation, New algorithmic framework, O(T 1/3) regret


- [Causal Customer Churn Analysis with Low-rank Tensor Block Hazard Model](https://icml.cc/virtual/2024/poster/33341) (Poster)
  - **Authors:** [Chenyin Gao](http://openreview.net/profile?id=~Chenyin_Gao1), [ZHIMING ZHANG](http://openreview.net/profile?id=~ZHIMING_ZHANG3), [Shu Yang](http://openreview.net/profile?id=~Shu_Yang4)
  - **Affiliations:** Department of Statistics, North Carolina State University, Raleigh, U.S.A., Independent scholar, Iowa State University, Ames, U.S.A., Department of Statistics, North Carolina State University, Raleigh, U.S.A.
  - **TL;DR:** This study presents a novel tensorized latent factor block hazard model for analyzing customer churn, utilizing tensor completion methods to address the binary nature of churn data. The model enhances the understanding of intervention impacts on customer retention, providing a data-driven approach to improve retention strategies and business value.
  - **Keywords:** Customer churn analysis, Causal analysis, Tensor completion methods, Tensorized latent factor block hazard model, Projected gradient descent algorithm, Spectral clustering, Customer retention strategies, Business value enhancement, Customer churn, Monotone churn pattern, Endogeneity in treatment assignment, New causal model for customer churn, Improved estimation of treatment effects, 1-bit tensor completion, Potential outcomes framework


- [Parameter-Efficient Fine-Tuning with Discrete Fourier Transform](https://icml.cc/virtual/2024/poster/33821) (Poster)
  - **Authors:** [Ziqi Gao](http://openreview.net/profile?id=~Ziqi_Gao1), [Qichao Wang](http://openreview.net/profile?id=~Qichao_Wang1), [Aochuan Chen](http://openreview.net/profile?id=~Aochuan_Chen1), [Zijing Liu](http://openreview.net/profile?id=~Zijing_Liu1), [Bingzhe Wu](http://openreview.net/profile?id=~Bingzhe_Wu1), [Liang Chen](http://openreview.net/profile?id=~Liang_Chen17), [Jia Li](http://openreview.net/profile?id=~Jia_Li4)
  - **Affiliations:** Hong Kong University of Science and Technology (Guangzhou); Hong Kong University of Science and Technology, Sun Yat-sen University, Hong Kong University of Science and Technology (Guangzhou), International Digital Economy Academy, AI Lab, Tencent, Sun Yat-sen University, Hong Kong University of Science and Technology (Guangzhou); Hong Kong University of Science and Technology
  - **TL;DR:** This study introduces FourierFT, a method for parameter-efficient fine-tuning of foundation models using the Discrete Fourier Transform, which significantly reduces the number of trainable parameters compared to LoRA while maintaining or improving performance across various tasks. The findings suggest that FourierFT can effectively address storage challenges associated with large foundation models.
  - **Keywords:** Parameter-efficient fine-tuning, foundation models, Low-rank adaptation (LoRA), Discrete Fourier Transform, FourierFT, Natural language understanding, natural language generation, instruction tuning, image classification, Storage challenges, high IT infrastructure consumption, Compression of trainable parameters, performance comparison with LoRA, LLaMA2-7B model, Vision Transformer (ViT), DTD dataset


- [DMTG: One-Shot Differentiable Multi-Task Grouping](https://icml.cc/virtual/2024/poster/33208) (Poster)
  - **Authors:** [Yuan Gao](http://openreview.net/profile?id=~Yuan_Gao4), [Shuguo Jiang](http://openreview.net/profile?id=~Shuguo_Jiang1), [Moran Li](http://openreview.net/profile?id=~Moran_Li1), [Jin-Gang Yu](http://openreview.net/profile?id=~Jin-Gang_Yu1), [Gui-Song Xia](http://openreview.net/profile?id=~Gui-Song_Xia3)
  - **Affiliations:** School of CS, Wuhan University; School of EI, Wuhan University, School of CS, Wuhan University, Tencent Youtu Lab, School of Automation Science and Engineering, South China University of Technology, School of CS, Wuhan University
  - **TL;DR:** This study presents a novel approach to Multi-Task Learning (MTL) through One-Shot Differentiable Multi-Task Grouping (DMTG), which simultaneously identifies optimal task groups and trains model weights, enhancing training efficiency and reducing bias. The method demonstrates promising performance on CelebA and Taskonomy datasets.
  - **Keywords:** Multi-Task Learning (MTL), Multi-Task Grouping (MTG), Differentiable pruning, Categorical distribution, Autonomous driving, Task grouping, training efficiency, objective bias, Improved training efficiency, unique task categorization, CelebA, Taskonomy


- [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://icml.cc/virtual/2024/poster/34957) (Poster)
  - **Authors:** [Asma Ghandeharioun](http://openreview.net/profile?id=~Asma_Ghandeharioun1), [‪Avi Caciularu‬‏](http://openreview.net/profile?id=~Avi_Caciularu1), [Adam Pearce](http://openreview.net/profile?id=~Adam_Pearce1), [Lucas Dixon](http://openreview.net/profile?id=~Lucas_Dixon1), [Mor Geva](http://openreview.net/profile?id=~Mor_Geva1)
  - **Affiliations:** Google Research; Tel Aviv University, Google Research, Google Research, Google Research, Google Research; Tel Aviv University
  - **TL;DR:** The paper introduces Patchscopes, a framework for inspecting the internal representations of large language models (LLMs) to enhance explainability and address shortcomings of existing interpretability methods. It demonstrates how this framework can facilitate understanding LLM behavior and improve alignment with human values.
  - **Keywords:** Explainability, Internal Representations of Language Models, Large Language Models, Patchscopes, Interpretability Methods, Probing, Representation Projection, Shortcomings of Prior Interpretability Methods, Difficulty in Inspecting Early Layers, New Framework (Patchscopes), Multihop Reasoning Error Correction, Large Language Models (LLMs), Hidden Representations


- [Graph-Triggered Rising Bandits](https://icml.cc/virtual/2024/poster/33642) (Poster)
  - **Authors:** [Gianmarco Genalti](http://openreview.net/profile?id=~Gianmarco_Genalti1), [Marco Mussi](http://openreview.net/profile?id=~Marco_Mussi1), [Nicola Gatti](http://openreview.net/profile?id=~Nicola_Gatti1), [Marcello Restelli](http://openreview.net/profile?id=~Marcello_Restelli1), [Matteo Castiglioni](http://openreview.net/profile?id=~Matteo_Castiglioni1), [Alberto Maria Metelli](http://openreview.net/profile?id=~Alberto_Maria_Metelli2)
  - **Affiliations:** Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy
  - **TL;DR:** This paper introduces Graph-Triggered Rising Bandits (GTRBs), a novel generalization of rested and restless bandits where the expected rewards of arms are influenced by a graph structure. The study addresses the complexity of computing optimal policies and presents regret minimization algorithms, highlighting the model's applicability to various scenarios.
  - **Keywords:** Graph-Triggered Rising Bandits, Multi-Armed Bandits, Regret minimization algorithms, Optimal policy computation, NP-hard problem, Expected reward evolution, Rested and restless bandits, Generalization of bandit models, Closed-form computation of optimal policy, Rested bandits, Restless bandits, Rising bandits, Fully connected sub-graphs (cliques)


- [Learning with 3D rotations, a hitchhiker's guide to SO(3)](https://icml.cc/virtual/2024/poster/34317) (Poster)
  - **Authors:** [Andreas René Geist](http://openreview.net/profile?id=~Andreas_Ren%C3%A9_Geist1), [Jonas Frey](http://openreview.net/profile?id=~Jonas_Frey1), [Mikel Zhobro](http://openreview.net/profile?id=~Mikel_Zhobro1), [Anna Levina](http://openreview.net/profile?id=~Anna_Levina1), [Georg Martius](http://openreview.net/profile?id=~Georg_Martius1)
  - **Affiliations:** Max Planck Institute for Intelligent Systems, Autonomous learning group, Germany; University of Tübingen, Distributed intelligence lab, Germany, Max Planck Institute for Intelligent Systems, Autonomous learning group, Germany; University of Tübingen, Distributed intelligence lab, Germany; Swiss Federal Institute of Technology (ETH Zurich), Robotic systems lab, Switzerland, Max Planck Institute for Intelligent Systems, Autonomous learning group, Germany; University of Tübingen, Distributed intelligence lab, Germany, University of Tübingen, Self-organization of neuronal networks group, Germany, Max Planck Institute for Intelligent Systems, Autonomous learning group, Germany; University of Tübingen, Distributed intelligence lab, Germany
  - **TL;DR:** This paper surveys various rotation representations in machine learning, focusing on their properties and impact on deep learning with gradient-based optimization. It provides guidance on selecting suitable representations based on the context of input and output rotations, emphasizing the preference for high-dimensional representations.
  - **Keywords:** 3D rotations, rotation representations, machine learning, gradient-based optimization, rotation matrices, exponential coordinates, quaternions, control theory, robotics, computer animations, selection of rotation representation, sample-efficient learning, influence of representation on training, comprehensive overview of rotation representations, guidance on representation selection, SO(3), double-cover, sinusoidal transformations


- [Safe and Robust Subgame Exploitation in Imperfect Information Games](https://icml.cc/virtual/2024/poster/34372) (Poster)
  - **Authors:** [Zhenxing Ge](http://openreview.net/profile?id=~Zhenxing_Ge1), [Zheng Xu](http://openreview.net/profile?id=~Zheng_Xu3), [Tianyu Ding](http://openreview.net/profile?id=~Tianyu_Ding1), [Linjian Meng](http://openreview.net/profile?id=~Linjian_Meng1), [Bo An](http://openreview.net/profile?id=~Bo_An2), [Wenbin Li](http://openreview.net/profile?id=~Wenbin_Li5), [Yang Gao](http://openreview.net/profile?id=~Yang_Gao3)
  - **Affiliations:** State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China, Microsoft Corporation, Redmond, Washington, USA, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China, Skywork AI, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Intelligence Science and Technology, Nanjing University, Suzhou Campus, Suzhou, Jiangsu, China
  - **TL;DR:** This paper introduces a novel approach called Adaptation Safety for opponent exploitation in imperfect information games, addressing vulnerabilities to modeling errors and deceptive adversaries. The proposed Opponent eXploitation Search (OX-Search) framework demonstrates superior exploitability and robust exploitation in empirical evaluations of popular poker games.
  - **Keywords:** Opponent exploitation, Adaptation safety, Game theory, Opponent eXploitation Search (OX-Search), Real-time search techniques, Poker games, Imperfect information games, Modeling errors, Deceptive adversaries, Exploitability, Robust exploitation, Theoretical analyses, Empirical evaluations


- [Optimal Eye Surgeon: Finding image priors through sparse generators at initialization](https://icml.cc/virtual/2024/poster/32912) (Poster)
  - **Authors:** [Avrajit Ghosh](http://openreview.net/profile?id=~Avrajit_Ghosh1), [Xitong Zhang](http://openreview.net/profile?id=~Xitong_Zhang1), [Kenneth Sun](http://openreview.net/profile?id=~Kenneth_K._Sun1), [Qing Qu](http://openreview.net/profile?id=~Qing_Qu2), [Saiprasad Ravishankar](http://openreview.net/profile?id=~Saiprasad_Ravishankar1), [Rongrong Wang](http://openreview.net/profile?id=~Rongrong_Wang1)
  - **Affiliations:** Department of Computational Mathematics, Science and Engineering (CMSE), Michigan State University, MI, USA; Department of Biomedical Engineering, Michigan State University, MI, USA, Department of Computational Mathematics, Science and Engineering (CMSE), Michigan State University, MI, USA, Department of Electrical Engineering and Computer Science (EECS), University of Michigan – Ann Arbor, MI, USA, Department of Electrical Engineering and Computer Science (EECS), University of Michigan – Ann Arbor, MI, USA, Department of Computational Mathematics, Science and Engineering (CMSE), Michigan State University, MI, USA; Department of Biomedical Engineering, Michigan State University, MI, USA, Department of Computational Mathematics, Science and Engineering (CMSE), Michigan State University, MI, USA
  - **TL;DR:** The study introduces Optimal Eye Surgeon (OES), a framework for pruning and training deep image generator networks to mitigate overfitting in image restoration tasks. The findings demonstrate that pruned subnetworks, termed Sparse-DIP, effectively capture low-frequency image components and resist noise overfitting, outperforming existing pruning methods.
  - **Keywords:** image restoration, deep learning, network pruning, deep convolutional networks, Sparse-DIP, adaptive pruning, image generation, image recovery, overfitting, overparameterization, noise in images, Optimal Eye Surgeon (OES), transferability of OES-masks, characteristics of sparse-subnetworks


- [LLark: A Multimodal Instruction-Following Language Model for Music](https://icml.cc/virtual/2024/poster/34440) (Poster)
  - **Authors:** [Joshua Gardner](http://openreview.net/profile?id=~Joshua_P_Gardner1), [Simon Durand](http://openreview.net/profile?id=~Simon_Durand1), [Daniel Stoller](http://openreview.net/profile?id=~Daniel_Stoller1), [Rachel Bittner](http://openreview.net/profile?id=~Rachel_M_Bittner1)
  - **Affiliations:** University of Washington, Spotify, Spotify, Spotify
  - **TL;DR:** This paper presents LLARK, a multimodal instruction-tuned model designed for music understanding, which integrates a generative audio encoder with a language model. The model demonstrates superior performance in music understanding tasks and is trained using diverse open-source music datasets.
  - **Keywords:** music understanding, multimodal instruction-following, instruction-tuning, pretrained generative model, pretrained language model, music analysis, audio processing, challenges in music understanding, limitations of existing AI systems for music, LLARK model, end-to-end instruction-tuning approach, open-source music datasets, multimodal architecture


- [Projection-Free Online Convex Optimization with Time-Varying Constraints](https://icml.cc/virtual/2024/poster/34040) (Poster)
  - **Authors:** [Dan Garber](http://openreview.net/profile?id=~Dan_Garber1), [Ben Kretzu](http://openreview.net/profile?id=~Ben_Kretzu1)
  - **Affiliations:** Technion - Israel Institute of Technology, Haifa, Israel, Technion - Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This paper presents projection-free algorithms for online convex optimization under adversarial time-varying constraints, achieving sublinear regret and constraint violation bounds. The proposed methods utilize a linear optimization oracle and extend to bandit feedback settings, demonstrating efficiency in managing complex constraints.
  - **Keywords:** Online Convex Optimization, Time-Varying Constraints, Projection-Free Algorithms, Linear Optimization Oracle (LOO), Adversarial Constraints, Cumulative Constraint Violation, Regret Bounds, Constraint Violation Bounds, Strongly Convex Nonsmooth Function, Euclidean Ball


- [Position: Categorical Deep Learning is an Algebraic Theory of All Architectures](https://icml.cc/virtual/2024/poster/34589) (Poster)
  - **Authors:** [Bruno Gavranović](http://openreview.net/profile?id=~Bruno_Gavranovi%C4%871), [Paul Lessard](http://openreview.net/profile?id=~Paul_Lessard1), [Andrew Dudzik](http://openreview.net/profile?id=~Andrew_Joseph_Dudzik1), [Tamara von Glehn](http://openreview.net/profile?id=~Tamara_von_Glehn1), [João Madeira Araujo](http://openreview.net/profile?id=~Jo%C3%A3o_Guilherme_Madeira_Ara%C3%BAjo1), [Petar Veličković](http://openreview.net/profile?id=~Petar_Veli%C4%8Dkovi%C4%871)
  - **Affiliations:** Symbolica AI; University of Edinburgh, Symbolica AI, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind; University of Cambridge
  - **TL;DR:** The paper proposes a unified framework for deep learning architectures using category theory to bridge the gap between model constraints and implementations. It highlights the effectiveness of this approach in recovering constraints from geometric deep learning and implementing various neural network architectures.
  - **Keywords:** deep learning, neural network architectures, category theory, geometric deep learning, equivariance constraints, monads, lack of coherent framework, bridging top-down and bottom-up approaches, unified theory for neural network design, recovery of constraints, PyG, DGL, Jraph, e3nn, RNNs (Recurrent Neural Networks), symmetry-preserving constraints


- [Individualized Privacy Accounting via Subsampling with Applications in Combinatorial Optimization](https://icml.cc/virtual/2024/poster/32622) (Poster)
  - **Authors:** [Badih Ghazi](http://openreview.net/profile?id=~Badih_Ghazi1), [Pritish Kamath](http://openreview.net/profile?id=~Pritish_Kamath2), [Ravi Kumar](http://openreview.net/profile?id=~Ravi_Kumar1), [Pasin Manurangsi](http://openreview.net/profile?id=~Pasin_Manurangsi2), [Adam Sealfon](http://openreview.net/profile?id=~Adam_Sealfon1)
  - **Affiliations:** Google Research, Mountain View, USA, Google Research, Mountain View, USA, Google Research, Mountain View, USA, Google Research, Thailand; Google Research, New York, USA, Google Research, New York, USA
  - **TL;DR:** This paper presents a novel technique for individualized privacy accounting through subsampling, demonstrating that one-sided add-DP algorithms can be transformed into two-sided DP algorithms. The authors provide improved algorithms for private combinatorial optimization problems, achieving pure-DP guarantees, which were previously only approximated in existing works.
  - **Keywords:** Individualized privacy accounting, Differential privacy, Combinatorial optimization, One-sided add-DP, Two-sided DP, Subsampling, Private combinatorial optimization problems, Submodular maximization, Set cover, Shifting heavy hitter problem, Privacy protection, Sensitive user information, Add-remove neighboring datasets, Improved algorithms for private combinatorial optimization, Pure-DP algorithms, Submodular functions, Cardinality constraint, Matroid constraint


- [Fast-Slow Test-Time Adaptation for Online Vision-and-Language Navigation](https://icml.cc/virtual/2024/poster/33723) (Poster)
  - **Authors:** [JUNYU GAO](http://openreview.net/profile?id=~Junyu_Gao1), [Xuan Yao](http://openreview.net/profile?id=~Xuan_Yao1), [Changsheng Xu](http://openreview.net/profile?id=~Changsheng_Xu1)
  - **Affiliations:** State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA); School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS); Peng Cheng Laboratory, ShenZhen, China, State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA); School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA); School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS); Peng Cheng Laboratory, ShenZhen, China
  - **TL;DR:** This study introduces a Fast-Slow Test-Time Adaptation (FSTTA) approach for online Vision-and-Language Navigation (VLN) to enhance an agent's ability to adapt to dynamically changing environments. The proposed method demonstrates significant performance improvements across multiple benchmarks by effectively managing model updates during instruction execution.
  - **Keywords:** Vision-and-Language Navigation (VLN), online model adaptation, Fast-Slow Test-Time Adaptation (FSTTA), Test-Time Adaptation (TTA), Embodied AI, navigation tasks, Data discrepancies, online instruction execution, model parameter updates, Performance gains on benchmarks, joint decomposition-accumulation analysis, REVERIE


- [Reinforcement Learning within Tree Search for Fast Macro Placement](https://icml.cc/virtual/2024/poster/34772) (Poster)
  - **Authors:** [Zijie Geng](http://openreview.net/profile?id=~Zijie_Geng1), [Jie Wang](http://openreview.net/profile?id=~Jie_Wang1), [Ziyan Liu](http://openreview.net/profile?id=~Ziyan_Liu2), [Siyuan Xu](http://openreview.net/profile?id=~Siyuan_Xu5), [Zhentao Tang](http://openreview.net/profile?id=~Zhentao_Tang1), [Mingxuan Yuan](http://openreview.net/profile?id=~Mingxuan_Yuan1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Yongdong Zhang](http://openreview.net/profile?id=~Yongdong_Zhang2), [Feng Wu](http://openreview.net/profile?id=~Feng_Wu1)
  - **Affiliations:** CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, Noah’s Ark Lab, Huawei, China, Noah’s Ark Lab, Huawei, China, Noah’s Ark Lab, Huawei, China; Tianjin University, Tianjin, China, Noah’s Ark Lab, Huawei, China; Tianjin University, Tianjin, China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China
  - **TL;DR:** This study introduces EfficientPlace, a sample-efficient framework that combines global tree search and reinforcement learning to enhance macro placement in chip design. The proposed method significantly improves placement quality while addressing the challenges of low sample efficiency in existing RL-based techniques.
  - **Keywords:** Macro placement, Chip design, Reinforcement learning, Global tree search algorithm, Low sample efficiency, Large-scale optimization problem, EfficientPlace framework, Improved placement quality, Half-perimeter wirelength (HPWL), Surrogate metrics


- [Non-convex Stochastic Composite Optimization with Polyak Momentum](https://icml.cc/virtual/2024/poster/35123) (Poster)
  - **Authors:** [Yuan Gao](http://openreview.net/profile?id=~Yuan_Gao23), [Anton Rodomanov](http://openreview.net/profile?id=~Anton_Rodomanov1), [Sebastian Stich](http://openreview.net/profile?id=~Sebastian_U_Stich1)
  - **Affiliations:** CISPA, Saarbrücken, Germany; Universität des Saarlandes, CISPA, Saarbrücken, Germany, CISPA, Saarbrücken, Germany
  - **TL;DR:** This paper investigates the stochastic proximal gradient method with Polyak momentum, demonstrating its optimal convergence rate for non-convex composite optimization problems regardless of batch size. The authors also analyze the variance reduction effect of the method and validate their findings through numerical experiments.
  - **Keywords:** Non-convex optimization, Stochastic optimization, Composite optimization, Stochastic proximal gradient method, Polyak momentum, Machine learning, Signal processing, Image processing, Convergence in non-convex settings, Stochastic noise, Variance reduction, Optimal convergence rate, Variance reduction analysis


- [Don't trust your eyes: on the (un)reliability of feature visualizations](https://icml.cc/virtual/2024/poster/32933) (Poster)
  - **Authors:** [Robert Geirhos](http://openreview.net/profile?id=~Robert_Geirhos1), [Roland S. Zimmermann](http://openreview.net/profile?id=~Roland_S._Zimmermann1), [Blair Bilodeau](http://openreview.net/profile?id=~Blair_Bilodeau1), [Wieland Brendel](http://openreview.net/profile?id=~Wieland_Brendel1), [Been Kim](http://openreview.net/profile?id=~Been_Kim1)
  - **Affiliations:** Google DeepMind; None, Max Planck Institute for Intelligent Systems; Tübingen AI Center, Department of Statistical Sciences, University of Toronto, Max Planck Institute for Intelligent Systems; Tübingen AI Center, Google DeepMind
  - **TL;DR:** The study investigates the reliability of feature visualizations in neural networks, revealing that they can produce arbitrary patterns unrelated to normal network behavior. The findings suggest a need for developing networks with structured features to ensure more reliable visualizations.
  - **Keywords:** feature visualizations, mechanistic interpretability, neural networks, activation maximization, reliability of feature visualizations, black-box neural networks


- [Self-Correcting Self-Consuming Loops for Generative Model Training](https://icml.cc/virtual/2024/poster/33370) (Poster)
  - **Authors:** [Nate Gillman](http://openreview.net/profile?id=~Nate_Gillman1), [Michael Freeman](http://openreview.net/profile?id=~Michael_Freeman1), [Daksh Aggarwal](http://openreview.net/profile?id=~Daksh_Aggarwal1), [Chia-Hong HSU](http://openreview.net/profile?id=~Chia-Hong_HSU1), [Calvin Luo](http://openreview.net/profile?id=~Calvin_Luo2), [Yonglong Tian](http://openreview.net/profile?id=~Yonglong_Tian1), [Chen Sun](http://openreview.net/profile?id=~Chen_Sun1)
  - **Affiliations:** Brown University, Brown University, Brown University, Brown University, Brown University, Google DeepMind, Brown University; Google DeepMind
  - **TL;DR:** This paper addresses the challenges of training generative models using synthetic data, particularly the issues of training instability and model collapse. The authors propose self-correcting self-consuming loops that leverage expert knowledge to stabilize the training process, demonstrating effectiveness in human motion synthesis even with a high ratio of synthetic to real data.
  - **Keywords:** generative models, synthetic data, self-consuming loops, self-correction functions, idealized correction function, human motion synthesis, training instability, model collapse, stabilization of generative model training, empirical validation


- [Adaptive-Gradient Policy Optimization: Enhancing Policy Learning in Non-Smooth Differentiable Simulations](https://icml.cc/virtual/2024/poster/34024) (Poster)
  - **Authors:** [Feng Gao](http://openreview.net/profile?id=~Feng_Gao5), [Liangzhi Shi](http://openreview.net/profile?id=~Liangzhi_Shi1), [Shenao Zhang](http://openreview.net/profile?id=~Shenao_Zhang1), [Zhaoran Wang](http://openreview.net/profile?id=~Zhaoran_Wang1), [Yi Wu](http://openreview.net/profile?id=~Yi_Wu1)
  - **Affiliations:** Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China, Northwestern University, Illinois, United States, Northwestern University, Illinois, United States, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China
  - **TL;DR:** This paper introduces the Adaptive-Gradient Policy Optimization (AGPO) algorithm to enhance policy learning in non-smooth differentiable simulations by utilizing an adaptive analytic gradient based on the Q function. The results demonstrate AGPO's effectiveness in mitigating challenges associated with rough simulation transitions, leading to stable performance in policy optimization.
  - **Keywords:** policy optimization, differentiable simulations, reinforcement learning, Adaptive-Gradient Policy Optimization (AGPO), adaptive analytic gradient, Q function, robotics, computer animation, non-smooth dynamics, rough optimization landscape, low sample efficiency, convergence of AGPO, stable performance under non-smooth dynamics, PyTorch, JAX, Bellman equation, zeroth-order sampling, first-order gradients


- [Improving Adversarial Energy-Based Model via Diffusion Process](https://icml.cc/virtual/2024/poster/34079) (Poster)
  - **Authors:** [Cong Geng](http://openreview.net/profile?id=~Cong_Geng1), [Tian Han](http://openreview.net/profile?id=~Tian_Han1), [Peng-Tao Jiang](http://openreview.net/profile?id=~Peng-Tao_Jiang1), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang52), [Jinwei Chen](http://openreview.net/profile?id=~Jinwei_Chen3), [Søren Hauberg](http://openreview.net/profile?id=~S%C3%B8ren_Hauberg1), [Bo Li](http://openreview.net/profile?id=~Bo_Li20)
  - **Affiliations:** vivo Mobile Communication Co., Ltd, China, Department of Computer Science, Stevens Institute of Technology, USA, vivo Mobile Communication Co., Ltd, China, vivo Mobile Communication Co., Ltd, China, vivo Mobile Communication Co., Ltd, China, Technical University of Denmark, Copenhagen, Denmark, vivo Mobile Communication Co., Ltd, China
  - **TL;DR:** This study enhances adversarial energy-based models (EBMs) by integrating a diffusion process to improve generation and density estimation. The proposed method addresses training instability and provides a more efficient approach to modeling complex data distributions.
  - **Keywords:** Energy-based models, Generative models, Adversarial training, Diffusion process, Minimax training, Image classification, Out-of-distribution detection, Semi-supervised learning, Difficulty in training EBMs, Instability in minimax training, Trade-off between generation and density estimation, Improved generation methods, Useful energy function for density estimation, MCMC (Markov Chain Monte Carlo), KL divergence, Gibbs density


- [State-Constrained Zero-Sum Differential Games with One-Sided Information](https://icml.cc/virtual/2024/poster/33631) (Poster)
  - **Authors:** [Mukesh Ghimire](http://openreview.net/profile?id=~Mukesh_Ghimire1), [Lei Zhang](http://openreview.net/profile?id=~Lei_Zhang40), [Zhe Xu](http://openreview.net/profile?id=~Zhe_Xu7), [Yi Ren](http://openreview.net/profile?id=~Yi_Ren3)
  - **Affiliations:** Department of Mechanical and Aerospace Engineering, Arizona State University, Tempe, AZ, USA, Department of Mechanical and Aerospace Engineering, Arizona State University, Tempe, AZ, USA, Department of Mechanical and Aerospace Engineering, Arizona State University, Tempe, AZ, USA, Department of Mechanical and Aerospace Engineering, Arizona State University, Tempe, AZ, USA
  - **TL;DR:** This study investigates zero-sum differential games with state constraints and one-sided information, focusing on how an informed player can manipulate beliefs to minimize their payoff while adhering to constraints. The findings extend previous results to include state constraints and provide insights into strategic behavior in continuous action games.
  - **Keywords:** zero-sum differential games, state constraints, one-sided information, Hamilton-Jacobi equation, behavioral strategies, information asymmetry, state constraint violation, primal and dual subdynamic principles, belief manipulation strategies, categorical payoff type, stochastic state trajectory


- [Position: The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning](https://icml.cc/virtual/2024/poster/34574) (Spotlight Poster)
  - **Authors:** [Micah Goldblum](http://openreview.net/profile?id=~Micah_Goldblum1), [Marc Finzi](http://openreview.net/profile?id=~Marc_Anton_Finzi1), [Keefer Rowan](http://openreview.net/profile?id=~Keefer_Rowan1), [Andrew Wilson](http://openreview.net/profile?id=~Andrew_Gordon_Wilson1)
  - **Affiliations:** New York University, New York University, New York University, New York University
  - **TL;DR:** The study explores the implications of the No Free Lunch Theorem and Kolmogorov Complexity in machine learning, arguing that real-world data often exhibit low complexity, which can be leveraged by neural networks. The authors propose that a unified learning algorithm can automate model selection across various tasks, challenging the need for specialized learners.
  - **Keywords:** No Free Lunch Theorem, Inductive Biases, Machine Learning, Kolmogorov Complexity, Neural Networks, Computer Vision, Language Models, Data Complexity, Inductive Reasoning, Model Selection, Compression of Datasets, Unifying Learning Algorithms


- [Rethinking Specificity in SBDD: Leveraging Delta Score and Energy-Guided Diffusion](https://icml.cc/virtual/2024/poster/34845) (Poster)
  - **Authors:** [Bowen Gao](http://openreview.net/profile?id=~Bowen_Gao1), [Minsi Ren](http://openreview.net/profile?id=~Minsi_Ren1), [Yuyan Ni](http://openreview.net/profile?id=~Yuyan_Ni1), [Yanwen Huang](http://openreview.net/profile?id=~Yanwen_Huang2), [Bo Qiang](http://openreview.net/profile?id=~Bo_Qiang1), [Zhiming Ma](http://openreview.net/profile?id=~Zhi-Ming_Ma1), [Wei-Ying Ma](http://openreview.net/profile?id=~Wei-Ying_Ma2), [Yanyan Lan](http://openreview.net/profile?id=~Yanyan_Lan2)
  - **Affiliations:** Institute for AI Industry Research (AIR), Tsinghua University, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Department of Pharmaceutical Science, Peking University, Department of Pharmaceutical Science, Peking University, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Institute for AI Industry Research (AIR), Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University; Beijing Frontier Research Center for Biological Structure, Tsinghua University; Beijing Academy of Artificial Intelligence (BAAI)
  - **TL;DR:** This study addresses the issue of low specificity in structure-based drug design (SBDD) by introducing the Delta Score, a new metric for evaluating molecular binding specificity, and developing an energy-guided approach that enhances specificity while maintaining traditional docking scores. The findings highlight the need for improved metrics in SBDD to better align with real-world drug efficacy.
  - **Keywords:** Structure-based Drug Design (SBDD), specificity in drug design, deep learning-based generative models, contrastive learning, Delta Score, drug discovery, therapeutic agents, low specificity in molecular binding, promiscuous drugs, pan-assay interference compounds (PAINS), new metric for specificity (Delta Score), energy-guided approach for molecular generation


- [Agnostic Learning of Mixed Linear Regressions with EM and AM Algorithms](https://icml.cc/virtual/2024/poster/33479) (Poster)
  - **Authors:** [Avishek Ghosh](http://openreview.net/profile?id=~Avishek_Ghosh2), [Arya Mazumdar](http://openreview.net/profile?id=~Arya_Mazumdar1)
  - **Affiliations:** Systems and Control Engg. and Centre for Machine Intelligence for Data Sciences, Indian Institute of Technology, Bombay, India, Halıcıoğlu Data Science Institute, University of California, San Diego
  - **TL;DR:** This paper explores agnostic learning of mixed linear regression without assuming a generative model, demonstrating that the EM and AM algorithms can converge to optimal solutions. The findings highlight the effectiveness of these algorithms in minimizing population loss even in the absence of realizable generative models.
  - **Keywords:** Mixed linear regression, Agnostic learning, Expectation Maximization (EM), Alternating Minimization (AM), Estimation of linear functions, Population loss minimization, Convergence to population loss minimizers


- [CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling](https://icml.cc/virtual/2024/poster/33755) (Poster)
  - **Authors:** [JUNCHAO GONG](http://openreview.net/profile?id=~Junchao_Gong1), [LEI BAI](http://openreview.net/profile?id=~LEI_BAI1), [Peng Ye](http://openreview.net/profile?id=~Peng_Ye4), [Wanghan Xu](http://openreview.net/profile?id=~Wanghan_Xu1), [Na Liu](http://openreview.net/profile?id=~Na_Liu1), [Jianhua Dai](http://openreview.net/profile?id=~Jianhua_Dai3), [Xiaokang Yang](http://openreview.net/profile?id=~Xiaokang_Yang1), [Wanli Ouyang](http://openreview.net/profile?id=~Wanli_Ouyang1)
  - **Affiliations:** Shanghai Jiao Tong University; Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, National Meteorological Information Center, Shanghai Meteorological Service, Shanghai Jiao Tong University, Shanghai AI Laboratory
  - **TL;DR:** The study introduces CasCast, a cascaded framework for high-resolution precipitation nowcasting that effectively decouples predictions for different scales of precipitation. It demonstrates significant improvements in forecasting extreme precipitation events, outperforming baseline models by up to 91.8%.
  - **Keywords:** Precipitation nowcasting, Extreme weather prediction, Disaster management, Deep learning, Cascaded framework, Diffusion transformer, Weather forecasting, Disaster mitigation, Modeling complex precipitation system evolutions, Accurate forecasts for extreme precipitation, CasCast framework, Competitive performance in nowcasting, Optimization of extreme events, Benchmark radar precipitation datasets, Mesoscale precipitation, Small-scale patterns, Deterministic models, Probabilistic models


- [Does Label Smoothing Help Deep Partial Label Learning?](https://icml.cc/virtual/2024/poster/33531) (Oral)
  - **Authors:** [Xiuwen Gong](http://openreview.net/profile?id=~Xiuwen_Gong2), [Nitin Bisht](http://openreview.net/profile?id=~Nitin_Bisht1), [Guandong Xu](http://openreview.net/profile?id=~Guandong_Xu2)
  - **Affiliations:** Faculty of Engineering and Information Technology, University of Technology Sydney, NSW, Australia, Faculty of Engineering and Information Technology, University of Technology Sydney, NSW, Australia, Faculty of Engineering and Information Technology, University of Technology Sydney, NSW, Australia; Department of Computing, The Hong Kong Polytechnic University (PolyU), Kowloon, Hong Kong
  - **TL;DR:** This study investigates the effectiveness of label smoothing in deep partial label learning (deep PLL) to mitigate the impact of noisy labels and improve classification performance. The findings demonstrate that label smoothing enhances the learning process, particularly when the empirical smoothing rate aligns with the theoretically optimal rate.
  - **Keywords:** Deep Partial Label Learning, Label Smoothing, Deep Neural Networks, Optimization Algorithms, Automatic Face Recognition, Automatic Object Detection, Noisy Labels, Over-confidence in Predictions, Label Noise, Improved Classification Performance, Learning Distinguishable Representations, Benchmark PLL Datasets


- [Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks](https://icml.cc/virtual/2024/poster/33316) (Oral)
  - **Authors:** [Linyuan Gong](http://openreview.net/profile?id=~Linyuan_Gong1), [Sida Wang](http://openreview.net/profile?id=~Sida_Wang2), [Mostafa Elhoushi](http://openreview.net/profile?id=~Mostafa_Elhoushi1), [Alvin Cheung](http://openreview.net/profile?id=~Alvin_Cheung2)
  - **Affiliations:** Department of EECS, University of California at Berkeley, Berkeley, California, USA, AI at Meta, USA, AI at Meta, USA, Department of EECS, University of California at Berkeley, Berkeley, California, USA
  - **TL;DR:** This study introduces the Syntax-Aware Fill-in-the-Middle (SAFIM) benchmark for evaluating Large Language Models (LLMs) on code completion tasks, revealing that pretraining methods and data quality significantly impact model performance more than size. The findings support the use of FIM as a primary pretraining objective in code LLM development.
  - **Keywords:** Syntax-Aware Fill-in-the-Middle (SAFIM), Large Language Models (LLMs), code completion, Syntax-aware post-processing techniques, prompt designs, execution-based evaluation, Code development, programming languages, Data contamination, model evaluation, pretraining strategies, Benchmark for evaluating LLMs, findings on pretraining methods and data quality, SAFIM dataset, GitHub code submissions, Abstract Syntax Tree (AST), Fill-in-the-Middle (FIM), Left-to-Right (L2R) inference


- [AST-T5: Structure-Aware Pretraining for Code Generation and Understanding](https://icml.cc/virtual/2024/poster/33601) (Poster)
  - **Authors:** [Linyuan Gong](http://openreview.net/profile?id=~Linyuan_Gong1), [Mostafa Elhoushi](http://openreview.net/profile?id=~Mostafa_Elhoushi1), [Alvin Cheung](http://openreview.net/profile?id=~Alvin_Cheung2)
  - **Affiliations:** Department of EECS, University of California at Berkeley, Berkeley, California, USA, AI at Meta, USA, Department of EECS, University of California at Berkeley, Berkeley, California, USA
  - **TL;DR:** The study introduces AST-T5, a novel pretraining paradigm that utilizes the Abstract Syntax Tree (AST) to enhance code generation, transpilation, and understanding without complex program analyses. Evaluations demonstrate that AST-T5 outperforms similar-sized language models in various code-related tasks, particularly in code-to-code tasks.
  - **Keywords:** code generation, code understanding, code transpilation, Abstract Syntax Tree (AST), dynamic programming, AST-Aware Segmentation, AST-Aware Span Corruption, code-related tasks, text-to-code generation, code-to-code transpilation, neglect of code's structured nature, limitations of existing models, scalability issues, AST-T5 model, improved performance on code-related tasks, structure-aware pretraining framework, GitHub corpus, Tree-sitter, Large Language Models (LLMs), encoder-decoder Transformer


- [E$^2$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation](https://icml.cc/virtual/2024/poster/33198) (Poster)
  - **Authors:** [Yifan Gong](http://openreview.net/profile?id=~Yifan_Gong2), [Zheng Zhan](http://openreview.net/profile?id=~Zheng_Zhan3), [Qing Jin](http://openreview.net/profile?id=~Qing_Jin1), [Yanyu Li](http://openreview.net/profile?id=~Yanyu_Li1), [Yerlan Idelbayev](http://openreview.net/profile?id=~Yerlan_Idelbayev1), [Xian Liu](http://openreview.net/profile?id=~Xian_Liu1), [Andrey Zharkov](http://openreview.net/profile?id=~Andrey_Zharkov1), [Kfir Aberman](http://openreview.net/profile?id=~Kfir_Aberman1), [Sergey Tulyakov](http://openreview.net/profile?id=~Sergey_Tulyakov1), [Yanzhi Wang](http://openreview.net/profile?id=~Yanzhi_Wang2), [Jian Ren](http://openreview.net/profile?id=~Jian_Ren2)
  - **Affiliations:** Snap Inc.; Northeastern University, Northeastern University, Snap Inc., Snap Inc.; Northeastern University, Northeastern University, Snap Inc., Snap Inc., Snap Inc., Snap Inc., Northeastern University, Snap Inc.
  - **TL;DR:** This study introduces E2GAN, a novel approach to efficiently train GANs for real-time on-device image editing by leveraging data distillation from diffusion models. The proposed methods significantly reduce training time and storage costs while enabling high-quality image editing across various concepts.
  - **Keywords:** image editing, data distillation, real-time on-device processing, generative adversarial networks (GANs), Low-Rank Adaptation (LoRA), mobile devices, image-to-image translation, high computational requirements, inefficient training processes, efficient training methods, transfer learning, reduced training time, diffusion models, paired datasets


- [Fine-grained Classes and How to Find Them](https://icml.cc/virtual/2024/poster/33651) (Poster)
  - **Authors:** [Matej Grcic](http://openreview.net/profile?id=~Matej_Grcic1), [Artyom Gadetsky](http://openreview.net/profile?id=~Artyom_Gadetsky1), [Maria Brbic](http://openreview.net/profile?id=~Maria_Brbic1)
  - **Affiliations:** EPFL, Lausanne, Switzerland; Faculty of Electrical Engineering and Computing, University of Zagreb, Croatia, EPFL, Lausanne, Switzerland, EPFL, Lausanne, Switzerland
  - **TL;DR:** The study introduces FALCON, a method for discovering fine-grained classes from coarsely labeled data without supervision. FALCON significantly outperforms existing methods, achieving a 22% improvement on the tieredImageNet dataset with over 600 fine-grained classes.
  - **Keywords:** fine-grained classification, coarse labels, unsupervised learning, FALCON (Fine grAined Labels from COarse supervisioN), optimization procedure, weakly-supervised classification, image classification, single-cell classification, lack of fine-grained labels, challenges in precise annotation, subtle differences between classes, discovery of fine-grained classes, improved classification performance, modular learning from multiple datasets, tieredImageNet


- [Evolution-Inspired Loss Functions for Protein Representation Learning](https://icml.cc/virtual/2024/poster/32682) (Poster)
  - **Authors:** [Chengyue Gong](http://openreview.net/profile?id=~Chengyue_Gong1), [Adam Klivans](http://openreview.net/profile?id=~Adam_Klivans1), [James Loy](http://openreview.net/profile?id=~James_Madigan_Loy1), [Tianlong Chen](http://openreview.net/profile?id=~Tianlong_Chen1), [qiang liu](http://openreview.net/profile?id=~qiang_liu4), [Danny Diaz](http://openreview.net/profile?id=~Daniel_Jesus_Diaz1)
  - **Affiliations:** University of Texas at Austin, University of Texas at Austin, Intelligent Proteins, LLC, University of Texas at Austin, University of Texas at Austin, University of Texas at Austin; Intelligent Proteins, LLC
  - **TL;DR:** This study introduces Evolutionary Ranking (EvoRank) as a novel training objective for protein representation learning, which leverages evolutionary information to enhance model performance in suggesting mutations. The results demonstrate significant improvements in zero-shot performance compared to traditional wildtype accuracy methods, highlighting its potential in protein engineering applications.
  - **Keywords:** Protein engineering, self-supervised learning (SSL), Evolutionary Ranking (EvoRank), multiple sequence alignments (MSAs), Biotechnology, enzyme engineering, disease-causing variant identification, Wildtype accuracy, mutation effect prediction, data scarcity for fine-tuning, Improved zero-shot performance, diverse protein representations


- [Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates](https://icml.cc/virtual/2024/poster/33993) (Poster)
  - **Authors:** [Riccardo Grazzi](http://openreview.net/profile?id=~Riccardo_Grazzi2), [Massimiliano Pontil](http://openreview.net/profile?id=~massimiliano_pontil1), [Saverio Salzo](http://openreview.net/profile?id=~Saverio_Salzo1)
  - **Affiliations:** CSML, Istituto Italiano di Tecnologia, Genoa, Italy, Department of Computer Science, University College, London, UK; CSML, Istituto Italiano di Tecnologia, Genoa, Italy, Dipartimento di Ingegneria Informatica, Automatica e Gestionale, Università La Sapienza, Rome, Italy
  - **TL;DR:** This study focuses on efficiently computing the derivative of fixed points of parametric nonsmooth contraction maps, analyzing methods like iterative differentiation and approximate implicit differentiation. The authors establish linear convergence rates for these methods and introduce a new stochastic approach, demonstrating its effectiveness through experiments.
  - **Keywords:** Nonsmooth implicit differentiation, fixed-point computation, machine learning, Iterative differentiation (ITD), approximate implicit differentiation (AID), stochastic methods (NSID), Hyperparameter optimization, meta-learning, data poisoning attacks, bilevel optimization, Non-differentiable contraction maps, piecewise differentiability, convergence rates, Linear convergence rates, new stochastic method for implicit derivatives


- [ACM-MILP: Adaptive Constraint Modification via Grouping and Selection for Hardness-Preserving MILP Instance Generation](https://icml.cc/virtual/2024/poster/33006) (Spotlight Poster)
  - **Authors:** [Ziao Guo](http://openreview.net/profile?id=~Ziao_Guo1), [Yang Li](http://openreview.net/profile?id=~Yang_Li32), [Chang Liu](http://openreview.net/profile?id=~Chang_Liu7), [Wenli Ouyang](http://openreview.net/profile?id=~Wenli_Ouyang1), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, AI Lab, Lenovo Research, Beijing, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This paper presents ACM-MILP, a framework for generating Mixed-Integer Linear Programming instances through adaptive constraint modification and modeling of constraint interrelations, addressing the challenges of data scarcity and quality. The experimental results demonstrate significant improvements in problem-solving hardness similarity, highlighting the framework's efficacy for hyperparameter tuning.
  - **Keywords:** Mixed-Integer Linear Programming (MILP), Instance Generation, Adaptive Constraint Modification, Constraint Interrelation Modeling, Community Detection, Operations Research, Combinatorial Optimization, Data Scarcity, Quality and Solvability of MILP Instances, ACM-MILP Framework, Improved Problem-Solving Hardness Similarity, Hyperparameter Tuning


- [Scaling Down Deep Learning with MNIST-1D](https://icml.cc/virtual/2024/poster/33139) (Poster)
  - **Authors:** [Sam Greydanus](http://openreview.net/profile?id=~Samuel_James_Greydanus1), [Dmitry Kobak](http://openreview.net/profile?id=~Dmitry_Kobak2)
  - **Affiliations:** Oregon State University, The ML Collective, University of Tübingen, Heidelberg University
  - **TL;DR:** This paper introduces MNIST-1D, a low-memory and low-compute alternative to traditional deep learning benchmarks, enabling rapid experimentation and research on deep learning architectures. The study highlights the potential of MNIST-1D for investigating inductive biases and other phenomena in deep learning without the high costs associated with larger datasets.
  - **Keywords:** deep learning, inductive biases, low-memory computing, MNIST-1D, guillotine regularization, metalearning, deep double descent, educational use cases, fast prototyping, research on a low budget, high computational cost, simplicity of benchmarks, new dataset (MNIST-1D), insights into deep learning architectures, MNIST, MNIST-1D


- [Predictive Performance Comparison of Decision Policies Under Confounding](https://icml.cc/virtual/2024/poster/34448) (Poster)
  - **Authors:** [Luke Guerdan](http://openreview.net/profile?id=~Luke_Guerdan1), [Amanda Coston](http://openreview.net/profile?id=~Amanda_Lee_Coston1), [Ken Holstein](http://openreview.net/profile?id=~Ken_Holstein1), [Steven Wu](http://openreview.net/profile?id=~Steven_Wu1)
  - **Affiliations:** Carnegie Mellon University, Microsoft Research, Carnegie Mellon University, Carnegie Mellon University
  - **TL;DR:** This study proposes a method for comparing the predictive performance of decision policies under various identification approaches from causal inference and off-policy evaluation. The findings highlight the ability to estimate regret intervals without strong assumptions about existing policies, with a practical application in healthcare enrollment policy evaluation.
  - **Keywords:** Predictive performance, decision-making policies, causal inference, Instrumental variable, marginal sensitivity model, proximal variable, Healthcare, criminal justice, education, Uncertainty in policy comparison, under-specified decision-making policies, Finite-sample estimation of regret intervals


- [A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models](https://icml.cc/virtual/2024/poster/34065) (Poster)
  - **Authors:** [Sebastian Gregor Gruber](http://openreview.net/profile?id=~Sebastian_Gregor_Gruber1), [Florian Buettner](http://openreview.net/profile?id=~Florian_Buettner1)
  - **Affiliations:** German Cancer Consortium (DKTK), partner site Frankfurt/Mainz; German Cancer Research Center (DKFZ), Heidelberg; Goethe University Frankfurt; Frankfurt Cancer Institute (FCI), German Cancer Consortium (DKTK), partner site Frankfurt/Mainz; German Cancer Research Center (DKFZ), Heidelberg; Goethe University Frankfurt; Frankfurt Cancer Institute (FCI)
  - **TL;DR:** This paper introduces a bias-variance-covariance decomposition for kernel scores to provide a theoretical framework for uncertainty estimation in generative models. The proposed kernel entropy method outperforms existing baselines in predicting performance on question answering datasets and can be applied to closed-source models.
  - **Keywords:** Generative models, Uncertainty estimation, Bias-variance-covariance decomposition, Kernel scores, Kernel-based variance, Kernel entropy, Image generation, Audio generation, Language generation, Uncertainty estimation, Hallucinations in generative models, Unbiased and consistent estimators for uncertainty, Predictive kernel entropy, CoQA, TriviaQA


- [AI Control: Improving Safety Despite Intentional Subversion](https://icml.cc/virtual/2024/poster/34322) (Oral)
  - **Authors:** [Ryan Greenblatt](http://openreview.net/profile?id=~Ryan_Greenblatt1), [Buck Shlegeris](http://openreview.net/profile?id=~Buck_Shlegeris1), [Kshitij Sachan](http://openreview.net/profile?id=~Kshitij_Sachan1), [Fabien Roger](http://openreview.net/profile?id=~Fabien_Roger1)
  - **Affiliations:** Redwood Research, Redwood Research, Anthropic; Redwood Research, Redwood Research
  - **TL;DR:** The study focuses on developing safety protocols to prevent intentional subversion of large language models (LLMs) while solving programming problems. Key findings indicate that utilizing a trusted model to monitor and edit code from an untrusted model significantly enhances safety measures.
  - **Keywords:** AI safety, intentional subversion, large language models, safety techniques, protocols, red-teaming, programming problem solving, preventing harmful outcomes, subverting safety measures, improved safety protocols, use of trusted models for monitoring, LLMs (Large Language Models), GPT-4, GPT-3.5, backdooring, AI control


- [CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution](https://icml.cc/virtual/2024/poster/34526) (Poster)
  - **Authors:** [Alex Gu](http://openreview.net/profile?id=~Alex_Gu1), [Baptiste Roziere](http://openreview.net/profile?id=~Baptiste_Roziere1), [Hugh Leather](http://openreview.net/profile?id=~Hugh_James_Leather1), [Armando Solar-Lezama](http://openreview.net/profile?id=~Armando_Solar-Lezama1), [Gabriel Synnaeve](http://openreview.net/profile?id=~Gabriel_Synnaeve1), [Sida Wang](http://openreview.net/profile?id=~Sida_Wang2)
  - **Affiliations:** AI at Meta; MIT, AI at Meta, AI at Meta, MIT, AI at Meta, AI at Meta
  - **TL;DR:** The paper introduces CRUXEVAL, a benchmark designed to evaluate code reasoning, understanding, and execution capabilities of language models through input and output prediction tasks. The findings reveal that while GPT-4 outperforms other models, significant challenges remain in accurately reasoning about simple Python programs.
  - **Keywords:** Code reasoning, Code understanding, Code execution, Chain of thought (CoT), Benchmarking, Software engineering, Programming, Lack of benchmarks for code understanding and execution, CRUXEVAL benchmark, Performance evaluation of code models, Code LMs (Language Models), HumanEval, MBPP


- [Long Range Propagation on Continuous-Time Dynamic Graphs](https://icml.cc/virtual/2024/poster/33423) (Poster)
  - **Authors:** [Alessio Gravina](http://openreview.net/profile?id=~Alessio_Gravina1), [Giulio Lovisotto](http://openreview.net/profile?id=~Giulio_Lovisotto2), [Claudio Gallicchio](http://openreview.net/profile?id=~Claudio_Gallicchio1), [Davide Bacciu](http://openreview.net/profile?id=~Davide_Bacciu1), [Claas Grohnfeldt](http://openreview.net/profile?id=~Claas_Grohnfeldt1)
  - **Affiliations:** Department of Computer Science, University of Pisa, Pisa, Italy, Huawei Technologies, Munich, Germany, Department of Computer Science, University of Pisa, Pisa, Italy, Department of Computer Science, University of Pisa, Pisa, Italy, Huawei Technologies, Munich, Germany
  - **TL;DR:** This paper introduces the continuous-time graph anti-symmetric network (CTAN) for learning Continuous-Time Dynamic Graphs (C-TDGs), addressing the challenge of long-range information propagation. The proposed method demonstrates superior performance on long-range tasks compared to existing models, supported by both theoretical and empirical evidence.
  - **Keywords:** Continuous-Time Dynamic Graphs, Long-range dependencies, Spatio-temporal modeling, Continuous-time graph anti-symmetric network (CTAN), Ordinary differential equations (ODEs), Social networks, E-commerce platforms, Traffic networks, Long-range information propagation, Over-squashing phenomenon, Long-term dependencies, Scalable long-range propagation of information, Theoretical stability conditions, Deep Graph Networks (DGNs), Recurrent neural networks (RNNs)


- [Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation](https://icml.cc/virtual/2024/poster/35008) (Oral)
  - **Authors:** [Gauthier Guinet](http://openreview.net/profile?id=~Gauthier_Guinet1), [Behrooz Tehrani](http://openreview.net/profile?id=~Behrooz_Omidvar_Tehrani1), [Anoop Deoras](http://openreview.net/profile?id=~Anoop_Deoras1), [Laurent Callot](http://openreview.net/profile?id=~Laurent_Callot1)
  - **Affiliations:** AWS AI Labs, AWS AI Labs, AWS AI Labs, AWS AI Labs
  - **TL;DR:** This paper presents an automated evaluation methodology for measuring the task-specific accuracy of Retrieval-Augmented Language Models (RAG) using synthetic exams. The findings indicate that the choice of retrieval algorithms significantly impacts performance, often more than simply increasing model size.
  - **Keywords:** Retrieval-Augmented Language Models, Task-Specific Evaluation, Item Response Theory (IRT), Automated Exam Generation, Open-ended Question-Answering, AWS DevOps Troubleshooting, Evaluation of Large Language Models, Task-Specific Accuracy, Automated Evaluation Methodology, Insights into RAG Performance Factors, Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, SEC filings, Retrieval-Augmented Models (RAG), Large Language Models (LLM)


- [On the Diminishing Returns of Width for Continual Learning](https://icml.cc/virtual/2024/poster/34281) (Poster)
  - **Authors:** [Etash Guha](http://openreview.net/profile?id=~Etash_Kumar_Guha1), [Vihan Lakshman](http://openreview.net/profile?id=~Vihan_Lakshman1)
  - **Affiliations:** SambaNova Systems, Palo Alto, USA; University of Washington, Seattle, USA, ThirdAI, Houston, USA
  - **TL;DR:** This study investigates the relationship between the width of neural networks and their ability to continually learn, demonstrating that while increasing width can reduce catastrophic forgetting, it yields diminishing returns. The authors establish a theoretical framework to analyze this relationship and empirically verify their findings.
  - **Keywords:** continual learning, catastrophic forgetting, deep neural networks, feed-forward networks (FFN), Stochastic Gradient Descent (SGD), Adam optimizer, catastrophic forgetting, diminishing returns, theoretical framework for continual learning error, empirical verification of width effects


- [SIN: Selective and Interpretable Normalization for Long-Term Time Series Forecasting](https://icml.cc/virtual/2024/poster/33594) (Poster)
  - **Authors:** [Lu Han](http://openreview.net/profile?id=~Lu_Han2), [Han-Jia Ye](http://openreview.net/profile?id=~Han-Jia_Ye1), [De-Chuan Zhan](http://openreview.net/profile?id=~De-Chuan_Zhan1)
  - **Affiliations:** School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China
  - **TL;DR:** This study introduces Selective and Interpretable Normalization (SIN) to address non-stationarity in time series forecasting, enhancing the performance of deep learning models. The proposed method identifies key statistics for normalization, leading to improved accuracy in predictions across various domains.
  - **Keywords:** Time series forecasting, Deep learning, Selective and Interpretable Normalization (SIN), Energy, Transportation, Healthcare, Non-stationarity, Statistical drift, Improved forecasting performance, Learned normalization method


- [Compressing Large Language Models by Joint Sparsification and Quantization](https://icml.cc/virtual/2024/poster/32921) (Poster)
  - **Authors:** [Jinyang Guo](http://openreview.net/profile?id=~Jinyang_Guo1), [Jianyu Wu](http://openreview.net/profile?id=~Jianyu_Wu2), [Zining Wang](http://openreview.net/profile?id=~Zining_Wang3), [Jiaheng Liu](http://openreview.net/profile?id=~Jiaheng_Liu1), [Ge Yang](http://openreview.net/profile?id=~Ge_Yang5), [Yifu Ding](http://openreview.net/profile?id=~Yifu_Ding2), [Ruihao Gong](http://openreview.net/profile?id=~Ruihao_Gong1), [Haotong Qin](http://openreview.net/profile?id=~Haotong_Qin1), [Xianglong Liu](http://openreview.net/profile?id=~Xianglong_Liu3)
  - **Affiliations:** State Key Laboratory of Complex & Critical Software Environment, Beihang University; Institute of Artificial Intelligence, Beihang University, Institute of Artificial Intelligence, Beihang University, State Key Laboratory of Complex & Critical Software Environment, Beihang University, State Key Laboratory of Complex & Critical Software Environment, Beihang University, Institute of Artificial Intelligence, Beihang University, State Key Laboratory of Complex & Critical Software Environment, Beihang University, SenseTime Research, ETH Zurich, State Key Laboratory of Complex & Critical Software Environment, Beihang University
  - **TL;DR:** This paper presents a novel model compression technique called Joint Sparsification and Quantization (JSQ) for large language models, which effectively integrates sparsification and quantization to achieve a 7.96× reduction in computation without performance degradation. The approach addresses the challenges of outlier preservation and high compression ratios, outperforming existing methods.
  - **Keywords:** Model compression, Large language models (LLMs), Joint Sparsification and Quantization (JSQ), sparsification, quantization, Performance degradation at high compression ratios, outlier preservation in LLMs, 7.96× computation reduction, search-based activation editor, LLaMA


- [Prototypical Transformer As Unified Motion Learners](https://icml.cc/virtual/2024/poster/34378) (Poster)
  - **Authors:** [Cheng Han](http://openreview.net/profile?id=~Cheng_Han1), [Yawen Lu](http://openreview.net/profile?id=~Yawen_Lu2), [Guohao Sun](http://openreview.net/profile?id=~Guohao_Sun1), [James Liang](http://openreview.net/profile?id=~James_Chenhao_Liang1), [Zhiwen Cao](http://openreview.net/profile?id=~Zhiwen_Cao1), [Qifan Wang](http://openreview.net/profile?id=~Qifan_Wang2), [Qiang Guan](http://openreview.net/profile?id=~Qiang_Guan1), [Sohail Dianat](http://openreview.net/profile?id=~Sohail_Dianat1), [Raghuveer Rao](http://openreview.net/profile?id=~Raghuveer_Rao1), [Tong Geng](http://openreview.net/profile?id=~Tong_Geng1), [ZHIQIANG TAO](http://openreview.net/profile?id=~ZHIQIANG_TAO2), [Dongfang Liu](http://openreview.net/profile?id=~Dongfang_Liu1)
  - **Affiliations:** University of Missouri – Kansas City; Rochester Institute of Technology, Purdue University, Rochester Institute of Technology, Rochester Institute of Technology, Purdue University, META AI, Kent State University, Rochester Institute of Technology, DEVCOM Army Research Laboratory, University of Rochester, Rochester Institute of Technology, Rochester Institute of Technology
  - **TL;DR:** This study introduces the Prototypical Transformer (ProtoFormer), a unified framework for various motion tasks that integrates prototype learning with Transformer architecture to address motion uncertainty. The approach demonstrates competitive performance in tasks like optical flow and scene depth, showcasing its generality across multiple downstream applications.
  - **Keywords:** motion learning, prototype learning, motion dynamics, Prototypical Transformer, Cross-Attention Prototyping, Latent Synchronization, optical flow, scene depth, object tracking, video stabilization, motion uncertainty, photometric and geometric inconsistencies, competitive performance on motion tasks, effective feature representation learning, Transformer, prototypes


- [Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning](https://icml.cc/virtual/2024/poster/34879) (Poster)
  - **Authors:** [Zijian Guo](http://openreview.net/profile?id=~Zijian_Guo1), [Weichao Zhou](http://openreview.net/profile?id=~Weichao_Zhou1), [Wenchao Li](http://openreview.net/profile?id=~Wenchao_Li1)
  - **Affiliations:** Division of Systems Engineering, Boston University, Department of Electrical and Computer Engineering, Boston University, Department of Electrical and Computer Engineering, Boston University
  - **TL;DR:** This paper introduces the temporal logic Specification-conditioned Decision Transformer (SDT) framework for offline safe reinforcement learning, which effectively combines the expressive power of signal temporal logic with the sequential modeling capabilities of Decision Transformer. Empirical evaluations show that SDT outperforms existing methods in learning safe and high-reward policies while adhering to complex temporal constraints.
  - **Keywords:** Offline safe reinforcement learning, Temporal logic, Signal temporal logic (STL), Decision Transformer (DT), Autonomous driving, Robotics, Healthcare, Constraint satisfaction, Learning from fixed datasets, Specification-conditioned Decision Transformer (SDT), High-reward policies, DSRL benchmarks


- [Pursuing Overall Welfare in Federated Learning through Sequential Decision Making](https://icml.cc/virtual/2024/poster/33455) (Poster)
  - **Authors:** [Seok-Ju Hahn](http://openreview.net/profile?id=~Seok-Ju_Hahn1), [Gi-Soo Kim](http://openreview.net/profile?id=~Gi-Soo_Kim1), [Junghye Lee](http://openreview.net/profile?id=~Junghye_Lee1)
  - **Affiliations:** Department of Industrial Engineering, Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea, Department of Industrial Engineering, Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea; Artificial Intelligence Graduate School, Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea, Technology Management, Economics and Policy Program, Seoul National University (SNU), Seoul, South Korea; Graduate School of Engineering Practice, Seoul National University (SNU), Seoul, South Korea; Institute of Engineering Research, Seoul National University (SNU), Seoul, South Korea
  - **TL;DR:** This study addresses the challenge of client-level fairness in federated learning by proposing an adaptive aggregation strategy called AAggFF, which enhances decision-making through an online convex optimization framework. The method demonstrates improved fairness across clients in both cross-device and cross-silo settings, backed by theoretical guarantees and extensive experimental results.
  - **Keywords:** Federated Learning, Client-level Fairness, Adaptive Aggregation Strategy, Online Convex Optimization, Cross-device Settings, Cross-silo Settings, Statistical Heterogeneity, Client-level Fairness Violation, AAggFF Method, Sublinear Regret Upper Bounds


- [Isometric Representation Learning for Disentangled Latent Space of Diffusion Models](https://icml.cc/virtual/2024/poster/32817) (Poster)
  - **Authors:** [Jaehoon Hahm](http://openreview.net/profile?id=~Jaehoon_Hahm1), [Junho Lee](http://openreview.net/profile?id=~Junho_Lee2), [Sunghyun Kim](http://openreview.net/profile?id=~Sunghyun_Kim4), [Joonseok Lee](http://openreview.net/profile?id=~Joonseok_Lee1)
  - **Affiliations:** Seoul National University, Seoul, Korea, Seoul National University, Seoul, Korea, Seoul National University, Seoul, Korea, Seoul National University, Seoul, Korea; Google Research, Mountain View, California, United States
  - **TL;DR:** This study introduces Isometric Diffusion, a method that enhances diffusion models by incorporating isometric representation learning to create a more disentangled latent space. The approach improves image interpolation, inversion, and editing by addressing the entanglement and distortion issues in the latent space of existing diffusion models.
  - **Keywords:** diffusion models, generative modeling, disentangled latent space, isometric representation learning, geometric regularizer, image interpolation, image inversion, image editing, entangled latent space, distorted mapping, sub-optimal image interpolation, Isometric Diffusion, novel loss for isometry, GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders)


- [Automated Loss function Search for Class-imbalanced Node Classification](https://icml.cc/virtual/2024/poster/34182) (Poster)
  - **Authors:** [Xinyu Guo](http://openreview.net/profile?id=~Xinyu_Guo2), [KAI WU](http://openreview.net/profile?id=~Kai_Wu3), [Xiaoyu Zhang](http://openreview.net/profile?id=~Xiaoyu_Zhang6), [Jing Liu](http://openreview.net/profile?id=~Jing_Liu20)
  - **Affiliations:** School of Artificial Intelligence, Xidian University, Xi’an, China, School of Artificial Intelligence, Xidian University, Xi’an, China, School of Cyber Engineering, Xidian University, Xi’an, China, Guangzhou Institute of Technology, Xidian University, Guangzhou, China
  - **TL;DR:** This paper presents an automated loss function search framework designed to improve class-imbalanced node classification by leveraging graph neural networks. The proposed framework significantly enhances performance across various datasets and demonstrates the importance of homophily in graph-structured data for transferability.
  - **Keywords:** Class-imbalanced node classification, Node representation learning, Graph neural networks (GNNs), Automated loss function search, Real-world graphs, Graph-structured data, Class imbalance, Learning high-quality node representations, High-performance automated loss function search framework, Improved performance over state-of-the-art methods, Homophily in graph-structured data


- [Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming](https://icml.cc/virtual/2024/poster/34444) (Poster)
  - **Authors:** [Hany Hamed](http://openreview.net/profile?id=~Hany_Hamed1), [Subin Kim](http://openreview.net/profile?id=~Subin_Kim4), [Dongyeong Kim](http://openreview.net/profile?id=~Dongyeong_Kim1), [Jaesik Yoon](http://openreview.net/profile?id=~Jaesik_Yoon1), [Sungjin Ahn](http://openreview.net/profile?id=~Sungjin_Ahn1)
  - **Affiliations:** KAIST, KAIST, KAIST, KAIST; SAP, KAIST
  - **TL;DR:** This paper introduces Dr. Strategy, a model-based reinforcement learning agent that employs a novel dreaming strategy inspired by cognitive science to enhance sample efficiency in navigation tasks. The proposed method outperforms existing pixel-based MBRL approaches in complex and partially observable environments.
  - **Keywords:** Model-based reinforcement learning, generalist agents, Dreaming Strategy, divide-and-conquer strategy, landmark-conditioned highway policy, Navigation tasks, exploration tasks, Sample efficiency, high-dimensional observations, partially observable environments, Improved sample efficiency, novel dreaming strategy


- [Data-efficient Large Vision Models through Sequential Autoregression](https://icml.cc/virtual/2024/poster/34328) (Poster)
  - **Authors:** [Zhiwei Hao](http://openreview.net/profile?id=~Zhiwei_Hao1), [Jianyuan Guo](http://openreview.net/profile?id=~Jianyuan_Guo1), [Chengcheng Wang](http://openreview.net/profile?id=~Chengcheng_Wang1), [Yehui Tang](http://openreview.net/profile?id=~Yehui_Tang1), [Han Wu](http://openreview.net/profile?id=~Han_Wu4), [Han Hu](http://openreview.net/profile?id=~Han_Hu6), [Kai Han](http://openreview.net/profile?id=~Kai_Han2), [Chang Xu](http://openreview.net/profile?id=~Chang_Xu4)
  - **Affiliations:** School of Information and Electronics, Beijing Institute of Technology, Beijing, China, School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia, Huawei Noah’s Ark Lab, Beijing, China, Huawei Noah’s Ark Lab, Beijing, China, School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia, School of Information and Electronics, Beijing Institute of Technology, Beijing, China, Huawei Noah’s Ark Lab, Beijing, China, School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia
  - **TL;DR:** This paper presents an efficient autoregression-based vision model designed to operate on limited datasets, achieving proficiency across various visual tasks while significantly reducing the parameter footprint and training data requirements. The findings suggest a promising direction for sustainable advancements in generalist vision models.
  - **Keywords:** data-efficient vision models, visual understanding, generalist models, autoregression, visual sentences, computer vision, out-of-domain tasks, data sparsity, long-tailed distribution of tasks, model performance imbalance, efficient vision model, reduced parameter footprint, decreased training data requirements, SA-1B, COCO


- [MGit: A Model Versioning and Management System](https://icml.cc/virtual/2024/poster/34148) (Poster)
  - **Authors:** [Wei Hao](http://openreview.net/profile?id=~Wei_Hao4), [Daniel Mendoza](http://openreview.net/profile?id=~Daniel_Mendoza1), [Rafael Mendes](http://openreview.net/profile?id=~Rafael_Mendes1), [Deepak Narayanan](http://openreview.net/profile?id=~Deepak_Narayanan2), [Amar Phanishayee](http://openreview.net/profile?id=~Amar_Phanishayee1), [Asaf Cidon](http://openreview.net/profile?id=~Asaf_Cidon1), [Junfeng Yang](http://openreview.net/profile?id=~Junfeng_Yang1)
  - **Affiliations:** Columbia University; Microsoft Research, Stanford University; Microsoft Research, Microsoft Research, NVIDIA; Microsoft Research, Microsoft Research, Columbia University, Columbia University
  - **TL;DR:** The paper presents MGit, a model versioning and management system designed to efficiently manage the relationships between derived machine learning models. MGit significantly reduces model storage requirements and enhances the speed of model updating tasks for practitioners.
  - **Keywords:** model versioning, model management, lineage graph, model fine-tuning, quantization, distillation, machine learning, recommendation systems, self-driving cars, model storage redundancy, debugging challenges, model updating difficulties, collaboration issues, MGit system, reduced model storage footprint, improved model updating speed


- [DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning](https://icml.cc/virtual/2024/poster/34280) (Poster)
  - **Authors:** [Siyuan Guo](http://openreview.net/profile?id=~Siyuan_Guo2), [Cheng Deng](http://openreview.net/profile?id=~Cheng_Deng4), [Ying Wen](http://openreview.net/profile?id=~Ying_Wen1), [Hechang Chen](http://openreview.net/profile?id=~Hechang_Chen2), [Yi Chang](http://openreview.net/profile?id=~Yi_Chang4), [Jun Wang](http://openreview.net/profile?id=~Jun_Wang2)
  - **Affiliations:** School of Artificial Intelligence, Jilin University; Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, Jilin University; International Center of Future Science, Jilin University, Shanghai Jiao Tong University, Shanghai Jiao Tong University, School of Artificial Intelligence, Jilin University; Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, Jilin University; International Center of Future Science, Jilin University, School of Artificial Intelligence, Jilin University; Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, Jilin University; International Center of Future Science, Jilin University, University College London
  - **TL;DR:** This study presents DS-Agent, a framework that utilizes large language models and case-based reasoning to automate data science tasks, achieving a 100% success rate in development and a 36% improvement in deployment performance. The findings highlight the potential of leveraging expert knowledge from Kaggle to enhance the capabilities of LLM agents in data science.
  - **Keywords:** automated data science, large language models (LLMs), case-based reasoning (CBR), machine learning (ML), unreasonable experiment plans, hallucination issues, task completion rate, DS-Agent framework, performance improvement, low-resource deployment, Kaggle


- [Binary Decomposition: A Problem Transformation Perspective for Open-Set Semi-Supervised Learning](https://icml.cc/virtual/2024/poster/34413) (Poster)
  - **Authors:** [Jun-Yi Hang](http://openreview.net/profile?id=~Jun-Yi_Hang1), [Min-Ling Zhang](http://openreview.net/profile?id=~Min-Ling_Zhang2)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China
  - **TL;DR:** This paper introduces a novel binary decomposition strategy for open-set semi-supervised learning, which avoids the pitfalls of outlier detection by transforming the problem into standard binary SSL problems. The proposed method, BDMatch, effectively addresses class imbalance and representation compromise, demonstrating superior performance in experiments.
  - **Keywords:** Semi-supervised learning, Open-set learning, Binary decomposition, Adaptive logit adjustment, Label-specific feature learning, Performance degradation in open-set scenarios, Outlier detection challenges, BDMatch approach, Improved model performance


- [Collaborative Heterogeneous Causal Inference Beyond Meta-analysis](https://icml.cc/virtual/2024/poster/34301) (Poster)
  - **Authors:** [Tianyu Guo](http://openreview.net/profile?id=~Tianyu_Guo4), [Sai Praneeth Karimireddy](http://openreview.net/profile?id=~Sai_Praneeth_Karimireddy1), [Michael Jordan](http://openreview.net/profile?id=~Michael_Jordan1)
  - **Affiliations:** Department of Statistics, UC Berkeley; Department of EECS, University of California, UC Berkeley, Department of EECS, University of California, UC Berkeley, Department of Statistics, UC Berkeley; Department of EECS, University of California, UC Berkeley
  - **TL;DR:** This study introduces a collaborative inverse propensity score weighting estimator for causal inference that effectively addresses heterogeneity across data centers, outperforming traditional meta-analysis methods. The proposed method maintains stability and accuracy even with increasing heterogeneity, facilitating collaboration across diverse domains.
  - **Keywords:** Causal Inference, Federated Learning, Heterogeneous Data, Inverse Propensity Score Weighting, Outcome Regression Models, Medical Sciences, Social Sciences, Heterogeneity across data centers, Distribution shift, Collaborative Estimator, Asymptotic Normality, Average Treatment Effect (ATE), Propensity Score Model, Meta-analysis


- [Riemannian coordinate descent algorithms on matrix manifolds](https://icml.cc/virtual/2024/poster/33630) (Poster)
  - **Authors:** [Andi Han](http://openreview.net/profile?id=~Andi_Han1), [Pratik Kumar Jawanpuria](http://openreview.net/profile?id=~Pratik_Jawanpuria1), [Bamdev Mishra](http://openreview.net/profile?id=~Bamdev_Mishra1)
  - **Affiliations:** Riken AIP, Japan, Microsoft India, Microsoft India
  - **TL;DR:** This study presents a framework for developing computationally efficient coordinate descent algorithms on matrix manifolds, allowing for the update of only a few variables at each iteration while maintaining manifold constraints. The proposed algorithms demonstrate low iteration costs and are applicable to various manifolds, with empirical results showcasing their effectiveness in multiple machine learning applications.
  - **Keywords:** Riemannian optimization, coordinate descent algorithms, Coordinate descent (CD) algorithms, Riemannian gradient, retraction update, Machine learning, PCA, low-rank matrix/tensor completion, computer vision, natural language processing, optimal transport, deep learning, Optimization problems on Riemannian manifolds, maintaining feasibility of variables, computational bottlenecks in retraction, Efficient CD algorithms, first-order approximation of objective function, convergence analysis, Stiefel manifold, Grassmann manifold, hyperbolic manifold, symplectic manifold, symmetric positive (semi)definite


- [EDISON: Enhanced Dictionary-Induced Tensorized Incomplete Multi-View Clustering with Gaussian Error Rank Minimization](https://icml.cc/virtual/2024/poster/33456) (Poster)
  - **Authors:** [Zhibin Gu](http://openreview.net/profile?id=~Zhibin_Gu1), [Zhendong Li](http://openreview.net/profile?id=~Zhendong_Li2), [Songhe Feng](http://openreview.net/profile?id=~Songhe_Feng1)
  - **Affiliations:** Key Laboratory of Big Data & Artificial Intelligence in Transportation (Beijing Jiaotong University), Ministry of Education; School of Computer Science and Technology, Beijing Jiaotong University, Beijing 100044, China, Key Laboratory of Big Data & Artificial Intelligence in Transportation (Beijing Jiaotong University), Ministry of Education; School of Computer Science and Technology, Beijing Jiaotong University, Beijing 100044, China, Key Laboratory of Big Data & Artificial Intelligence in Transportation (Beijing Jiaotong University), Ministry of Education; School of Computer Science and Technology, Beijing Jiaotong University, Beijing 100044, China
  - **TL;DR:** This paper introduces EDISON, an efficient method for incomplete multi-view clustering that utilizes enhanced dictionary representation and Gaussian error rank minimization to effectively handle missing data. The extensive experiments demonstrate that EDISON outperforms state-of-the-art methods in both effectiveness and efficiency.
  - **Keywords:** Incomplete Multi-View Clustering, Multi-View Clustering, Enhanced Dictionary Representation, Gaussian Error Rank Minimization, Hyper-Anchor Graph Laplacian Manifold Regularization, Missing Data, Data Sparsity, EDISON Model, Robustness to Incomplete Data, Tensor Data, Consensus Graph


- [Vectorized Conditional Neural Fields: A Framework for Solving Time-dependent Parametric Partial Differential Equations](https://icml.cc/virtual/2024/poster/32919) (Poster)
  - **Authors:** [Jan Hagnberger](http://openreview.net/profile?id=~Jan_Hagnberger1), [Marimuthu Kalimuthu](http://openreview.net/profile?id=~Marimuthu_Kalimuthu1), [Daniel Musekamp](http://openreview.net/profile?id=~Daniel_Musekamp1), [Mathias Niepert](http://openreview.net/profile?id=~Mathias_Niepert1)
  - **Affiliations:** Machine Learning and Simulation Lab, Institute for Artificial Intelligence, University of Stuttgart, Stuttgart, Germany; Stuttgart Center for Simulation Science (SimTech); International Max Planck Research School for Intelligent Systems (IMPRS-IS), Machine Learning and Simulation Lab, Institute for Artificial Intelligence, University of Stuttgart, Stuttgart, Germany; Stuttgart Center for Simulation Science (SimTech); International Max Planck Research School for Intelligent Systems (IMPRS-IS), Machine Learning and Simulation Lab, Institute for Artificial Intelligence, University of Stuttgart, Stuttgart, Germany; Stuttgart Center for Simulation Science (SimTech); International Max Planck Research School for Intelligent Systems (IMPRS-IS), Machine Learning and Simulation Lab, Institute for Artificial Intelligence, University of Stuttgart, Stuttgart, Germany; Stuttgart Center for Simulation Science (SimTech); International Max Planck Research School for Intelligent Systems (IMPRS-IS)
  - **TL;DR:** This paper introduces Vectorized Conditional Neural Fields (VCNeFs) as a novel approach for solving time-dependent Partial Differential Equations (PDEs) that addresses limitations of existing Transformer-based methods. VCNeFs demonstrate competitive performance and often outperform traditional machine learning surrogate models in simulating physical systems.
  - **Keywords:** Partial Differential Equations (PDEs), Machine Learning-based surrogate models, Transformers, Neural Fields, Vectorized Conditional Neural Fields (VCNeFs), Scientific Machine Learning (SciML), Physical systems simulation, Quadratic memory and time complexity of Transformers, Generalization to unseen PDE parameters, Efficient inference for longer temporal rollouts, VCNeFs outperform existing ML-based surrogate models, Physics-Informed Neural Networks (PINNs), Attention mechanisms


- [DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training](https://icml.cc/virtual/2024/poster/33838) (Poster)
  - **Authors:** [Zhongkai Hao](http://openreview.net/profile?id=~Zhongkai_Hao1), [Chang Su](http://openreview.net/profile?id=~Chang_Su7), [LIU SONGMING](http://openreview.net/profile?id=~Songming_Liu1), [Julius Berner](http://openreview.net/profile?id=~Julius_Berner1), [Chengyang Ying](http://openreview.net/profile?id=~Chengyang_Ying1), [Hang Su](http://openreview.net/profile?id=~Hang_Su3), [Anima Anandkumar](http://openreview.net/profile?id=~Anima_Anandkumar1), [Jian Song](http://openreview.net/profile?id=~Jian_Song3), [Jun Zhu](http://openreview.net/profile?id=~Jun_Zhu2)
  - **Affiliations:** Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University; None, Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University, Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University, Caltech, Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University, Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University, Caltech, Dept. of EE, Tsinghua University; None, Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University; RealAI
  - **TL;DR:** This paper introduces a novel auto-regressive denoising pre-training strategy for neural operators in the context of partial differential equations (PDEs), achieving state-of-the-art performance across multiple datasets. The proposed model architecture, based on Fourier attention, allows for efficient scaling and generalization to various downstream tasks.
  - **Keywords:** Auto-regressive denoising, PDE pre-training, Neural operators, Fourier attention, Weather forecasting, electromagnetism, fluid dynamics, structural optimization, Data scarcity, complexity of PDE data, Large-scale pre-training, SOTA performance, 10+ PDE datasets, 100k trajectories


- [Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning](https://icml.cc/virtual/2024/poster/33464) (Poster)
  - **Authors:** [Sungwon Han](http://openreview.net/profile?id=~Sungwon_Han1), [Jinsung Yoon](http://openreview.net/profile?id=~Jinsung_Yoon1), [Sercan Arik](http://openreview.net/profile?id=~Sercan_O_Arik1), [Tomas Pfister](http://openreview.net/profile?id=~Tomas_Pfister1)
  - **Affiliations:** School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea, Google Cloud AI, Sunnyvale, California, USA, Google Cloud AI, Sunnyvale, California, USA, Google Cloud AI, Sunnyvale, California, USA
  - **TL;DR:** This paper introduces FeatLLM, a novel framework that utilizes Large Language Models for feature engineering in few-shot tabular learning, significantly improving prediction performance while reducing computational costs. The approach eliminates the need for multiple LLM queries during inference and overcomes prompt size limitations, demonstrating effectiveness across various datasets.
  - **Keywords:** Large Language Models, Few-Shot Learning, Tabular Learning, In-Context Learning, Feature Engineering, Disease Prediction, Machine Learning, Computational Expense, Prompt Size Limitations, FeatLLM Framework, High-Quality Rules, Improved Performance


- [Convergence Guarantees for the DeepWalk Embedding on Block Models](https://icml.cc/virtual/2024/poster/32686) (Poster)
  - **Authors:** [Christopher Harker](http://openreview.net/profile?id=~Christopher_Harker1), [Aditya Bhaskara](http://openreview.net/profile?id=~Aditya_Bhaskara1)
  - **Affiliations:** Kahlert School of Computing, University of Utah, Salt Lake City, UT, USA, Kahlert School of Computing, University of Utah, Salt Lake City, UT, USA
  - **TL;DR:** This study investigates the convergence properties of the DeepWalk algorithm on graphs derived from the Stochastic Block Model, demonstrating that it can effectively recover cluster structures with high probability, similar to spectral embeddings. The findings provide theoretical guarantees for the performance of DeepWalk in graph embedding tasks.
  - **Keywords:** Graph embeddings, Stochastic Block Model (SBM), DeepWalk, Node2Vec, nonlinear optimization, random walks, Node classification, link prediction, community detection, Non-convex optimization, theoretical guarantees, Convergence properties, recovery of cluster structure, Spectral embeddings, matrix factorization, point-wise mutual information (PMI)


- [GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks](https://icml.cc/virtual/2024/poster/33884) (Poster)
  - **Authors:** [Shivanshu Gupta](http://openreview.net/profile?id=~Shivanshu_Gupta2), [Clemens Rosenbaum](http://openreview.net/profile?id=~Clemens_Rosenbaum1), [Ethan R. Elenberg](http://openreview.net/profile?id=~Ethan_R._Elenberg2)
  - **Affiliations:** University of California, Irvine, CA, USA, None, Permanence AI, New York, NY, USA
  - **TL;DR:** This study introduces GistScore, a novel approach for selecting informative examples in In-Context Learning (ICL) using supervised finetuning with an attention bottleneck, achieving state-of-the-art performance across multiple tasks and datasets. The proposed method allows for dynamic example selection and generalizes well to new tasks without additional training.
  - **Keywords:** In-Context Learning (ICL), Large Language Models (LLMs), Example Gisting, supervised finetuning, attention bottleneck, Sensitivity to example selection, task generalization, GistScore, state-of-the-art ICL performance, training-free ICL pipeline, 21 datasets, 8 diverse LLMs


- [FedRC: Tackling Diverse Distribution Shifts Challenge in Federated Learning by Robust Clustering](https://icml.cc/virtual/2024/poster/33243) (Poster)
  - **Authors:** [Yongxin Guo](http://openreview.net/profile?id=~Yongxin_Guo1), [Xiaoying Tang](http://openreview.net/profile?id=~Xiaoying_Tang2), [Tao Lin](http://openreview.net/profile?id=~Tao_Lin1)
  - **Affiliations:** School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China; The Shenzhen Institute of Artificial Intelligence and Robotics for Society, School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China; The Shenzhen Institute of Artificial Intelligence and Robotics for Society; The Guangdong Provincial Key Laboratory of Future Networks of Intelligence, Research Center for Industries of the Future, Westlake University; School of Engineering, Westlake University
  - **TL;DR:** This paper addresses the challenges of diverse distribution shifts in Federated Learning by proposing a novel clustering algorithm framework called FedRC, which incorporates a bi-level optimization problem. The extensive experiments show that FedRC significantly outperforms existing state-of-the-art cluster-based Federated Learning methods.
  - **Keywords:** Federated Learning, Privacy-preserving machine learning, Clustering algorithms, Bi-level optimization, Diverse distribution shifts, Non-IID data, Data heterogeneity, FedRC framework, Novel objective function, Concept shift, Label distribution shift, Feature distribution shift, State-of-the-art (SOTA) methods


- [Estimating the Permanent by Nesting Importance Sampling](https://icml.cc/virtual/2024/poster/34371) (Poster)
  - **Authors:** [Juha Harviainen](http://openreview.net/profile?id=~Juha_Harviainen1), [Mikko Koivisto](http://openreview.net/profile?id=~Mikko_Koivisto1)
  - **Affiliations:** Department of Computer Science, University of Helsinki, Helsinki, Finland, Department of Computer Science, University of Helsinki, Helsinki, Finland
  - **TL;DR:** This study presents a variant of sequential importance sampling (SIS) for estimating the permanent of nonnegative matrices, which is more efficient than traditional rejection sampling methods. The proposed method achieves significant speedups in high accuracy regimes while providing accuracy guarantees.
  - **Keywords:** Importance sampling, High-dimensional integrals, Permanent estimation, Sequential importance sampling (SIS), Rejection sampling, Computer science, Statistics, Multi-target tracking, Physics, Constraint satisfaction problems, Estimating the permanent, Accuracy guarantees, Variance bounding, More efficient sampling method, Speedups in estimation, #P-hard problem, Monte Carlo method


- [MAGNOLIA: Matching Algorithms via GNNs for Online Value-to-go Approximation](https://icml.cc/virtual/2024/poster/33807) (Poster)
  - **Authors:** [Alexandre Hayderi](http://openreview.net/profile?id=~Alexandre_Hayderi1), [Amin Saberi](http://openreview.net/profile?id=~Amin_Saberi1), [Ellen Vitercik](http://openreview.net/profile?id=~Ellen_Vitercik1), [Anders Wikum](http://openreview.net/profile?id=~Anders_Wikum1)
  - **Affiliations:** Department of Computer Science, Stanford University, Stanford, CA, USA, Department of Management Science & Engineering, Stanford University, Stanford, CA, USA, Department of Computer Science, Stanford University, Stanford, CA, USA; Department of Management Science & Engineering, Stanford University, Stanford, CA, USA, Department of Management Science & Engineering, Stanford University, Stanford, CA, USA
  - **TL;DR:** This study presents a graph neural network approach to online Bayesian bipartite matching, effectively estimating the value-to-go for actions to achieve high-weight matchings in various digital marketplace applications. The findings demonstrate that the GNN competes well with the optimal online algorithm, providing a theoretical basis for its effectiveness in this complex problem.
  - **Keywords:** Online Bayesian bipartite matching, digital marketplaces, Graph neural networks (GNN), dynamic programming (DP), Advertising, crowdsourcing, ridesharing, kidney exchange, Irrevocable matching decisions, uncertainty in online node arrivals, Estimation of value-to-go (VTG), high-weight matchings, Value-to-go (VTG), optimal online algorithm (OPTon)


- [Position: $C^*$-Algebraic Machine Learning $-$ Moving in a New Direction](https://icml.cc/virtual/2024/poster/32758) (Poster)
  - **Authors:** [Yuka Hashimoto](http://openreview.net/profile?id=~Yuka_Hashimoto2), [Masahiro Ikeda](http://openreview.net/profile?id=~Masahiro_Ikeda1), [Hachem Kadri](http://openreview.net/profile?id=~Hachem_Kadri3)
  - **Affiliations:** NTT corporation, Tokyo, Japan; Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan, Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; Keio University, Yokohama, Japan, Aix-Marseille University, CNRS, LIS, Marseille, France
  - **TL;DR:** This paper proposes a new direction in machine learning by applying C∗-algebra to unify existing methods and address the complexity of structured data. It focuses on kernel methods and neural networks, aiming to develop a framework for more diverse and information-rich data models.
  - **Keywords:** C∗-algebraic machine learning, cross-fertilization between C∗-algebra and machine learning, kernel methods, neural networks, reproducing kernel Hilbert spaces (RKHSs), vector-valued RKHS (vvRKHS), structured data, time-series data, image data, graph data, complexity of machine learning problems, construction of positive definite kernels, C∗-algebra-valued positive definite kernels, unification of existing learning strategies


- [LoRA+: Efficient Low Rank Adaptation of Large Models](https://icml.cc/virtual/2024/poster/34209) (Poster)
  - **Authors:** [Soufiane Hayou](http://openreview.net/profile?id=~Soufiane_Hayou1), [Nikhil Ghosh](http://openreview.net/profile?id=~Nikhil_Ghosh1), [Bin Yu](http://openreview.net/profile?id=~Bin_Yu5)
  - **Affiliations:** Simons Institute, UC Berkeley, Department of Statistics, UC Berkeley, Department of Statistics, UC Berkeley
  - **TL;DR:** This paper introduces LoRA+, an improved version of Low Rank Adaptation that addresses suboptimal finetuning in large models by using different learning rates for adapter matrices. The proposed method demonstrates performance improvements of 1% to 2% and up to 2X speedup in finetuning, while maintaining the same computational cost as LoRA.
  - **Keywords:** Low Rank Adaptation, Finetuning, Large Language Models, LoRA, Learning Rate Adjustment, Suboptimal finetuning, Efficient feature learning, LoRA+, Performance improvement, Finetuning speed enhancement


- [Wasserstein Wormhole: Scalable Optimal Transport Distance with Transformer](https://icml.cc/virtual/2024/poster/33987) (Poster)
  - **Authors:** [Doron Haviv](http://openreview.net/profile?id=~Doron_Haviv1), [Russell Kunes](http://openreview.net/profile?id=~Russell_Kunes1), [Thomas Dougherty](http://openreview.net/profile?id=~Thomas_Dougherty1), [Cassandra Burdziak](http://openreview.net/profile?id=~Cassandra_Burdziak1), [Tal Nawy](http://openreview.net/profile?id=~Tal_Nawy1), [Anna C. Gilbert](http://openreview.net/profile?id=~Anna_Gilbert2), [Dana Pe'er](http://openreview.net/profile?id=~Dana_Pe%27er1)
  - **Affiliations:** Computational and Systems Biology Program, Sloan Kettering Institute, Memorial Sloan Kettering Cancer Center; Tri-Institutional Training Program in Computational Biology and Medicine, Weill Cornell Medicine, Computational and Systems Biology Program, Sloan Kettering Institute, Memorial Sloan Kettering Cancer Center; Department of Statistics, Columbia University, Computational and Systems Biology Program, Sloan Kettering Institute, Memorial Sloan Kettering Cancer Center; Tri-Institutional Training Program in Computational Biology and Medicine, Weill Cornell Medicine, Computational and Systems Biology Program, Sloan Kettering Institute, Memorial Sloan Kettering Cancer Center, Computational and Systems Biology Program, Sloan Kettering Institute, Memorial Sloan Kettering Cancer Center, Department of Mathematics and Statistics, Yale University, Computational and Systems Biology Program, Sloan Kettering Institute, Memorial Sloan Kettering Cancer Center; Howard Hughes Medical Institute
  - **TL;DR:** The study introduces Wasserstein Wormhole, a transformer-based autoencoder that efficiently embeds empirical distributions into a latent space where Euclidean distances approximate Wasserstein distances, significantly improving the scalability of optimal transport methods. This approach enables linear time computation of OT distances and facilitates new data analysis techniques in computational geometry and single-cell biology.
  - **Keywords:** Optimal Transport, Wasserstein Metric, Distribution Comparison, Transformer-based Autoencoder, Multidimensional Scaling (MDS), Sinkhorn’s Algorithm, Computational Biology, Computational Geometry, Single-cell Biology, Intractability of Pairwise Wasserstein Distances, Scalability Issues in OT Algorithms, Wasserstein Wormhole Algorithm, Linear Time Computation of OT Distances, Wasserstein Barycenter Estimation, OT Interpolation, Point Clouds, Embedding Space, Euclidean Distances, Non-Euclidean Distances


- [Deep Neural Room Acoustics Primitive](https://icml.cc/virtual/2024/poster/33608) (Poster)
  - **Authors:** [Yuhang He](http://openreview.net/profile?id=~Yuhang_He3), [Anoop Cherian](http://openreview.net/profile?id=~Anoop_Cherian1), [Gordon Wichern](http://openreview.net/profile?id=~Gordon_Wichern1), [Andrew Markham](http://openreview.net/profile?id=~Andrew_Markham2)
  - **Affiliations:** Department of Computer Science, University of Oxford, Oxford, UK, Mitsubishi Electric Research Labs, Cambridge, MA, US, Mitsubishi Electric Research Labs, Cambridge, MA, US, Department of Computer Science, University of Oxford, Oxford, UK
  - **TL;DR:** This study introduces DeepNeRAP, a self-supervised framework for learning a continuous neural room acoustics field that estimates room impulse responses (RIR) for arbitrary source-receiver positions. The method demonstrates superior RIR estimation quality compared to existing approaches, addressing the challenges of sound propagation dynamics in enclosed spaces.
  - **Keywords:** room acoustics, sound propagation dynamics, neural room acoustics field, self-supervised learning, cooperative acoustic agents, architectural acoustics, audio-based virtual and augmented reality, geometric room structure estimation, measuring room impulse response (RIR), complexity of sound propagation, acoustic primitive estimation, DeepNeRAP framework, improved RIR estimation, synthetic datasets, real-world datasets, room impulse response (RIR), linear time-invariant (LTI) system


- [Domain-wise Data Acquisition to Improve Performance under Distribution Shift](https://icml.cc/virtual/2024/poster/35188) (Poster)
  - **Authors:** [Yue He](http://openreview.net/profile?id=~Yue_He2), [Dongbai Li](http://openreview.net/profile?id=~Dongbai_Li1), [Pengfei Tian](http://openreview.net/profile?id=~Pengfei_Tian2), [Han Yu](http://openreview.net/profile?id=~Han_Yu5), [Jiashuo Liu](http://openreview.net/profile?id=~Jiashuo_Liu1), [Hao Zou](http://openreview.net/profile?id=~Hao_Zou1), [Peng Cui](http://openreview.net/profile?id=~Peng_Cui1)
  - **Affiliations:** Department of Computer Science and Technology, Tsinghua University, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China, Qiuzhen College, Tsinghua University, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China, Zhongguancun Laboratory, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China
  - **TL;DR:** This paper presents a Domain-wise Active Acquisition framework to enhance model performance under distribution shifts by acquiring training samples from various domains on a limited budget. Empirical evidence shows that improved data acquisition significantly benefits model generalization to target test domains.
  - **Keywords:** machine learning, distribution shift, data-centric machine learning, Domain-wise Active Acquisition framework, unsupervised domain adaptation, sample reweighting, self-supervised learning, model generalization, real-world applications, distribution shift, training data quality, cross-distribution generalization, improved model performance, effective data acquisition strategy


- [Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation](https://icml.cc/virtual/2024/poster/33192) (Poster)
  - **Authors:** [Zhenyu He](http://openreview.net/profile?id=~Zhenyu_He3), [Guhao Feng](http://openreview.net/profile?id=~Guhao_Feng1), [Shengjie Luo](http://openreview.net/profile?id=~Shengjie_Luo1), [Kai Yang](http://openreview.net/profile?email=yangkai%40stu.pku.edu.cn), [Liwei Wang](http://openreview.net/profile?id=~Liwei_Wang1), [Jingjing Xu](http://openreview.net/profile?id=~Jingjing_Xu1), [Zhi Zhang](http://openreview.net/profile?id=~Zhi_Zhang4), [Hongxia Yang](http://openreview.net/profile?id=~Hongxia_Yang2), [Di He](http://openreview.net/profile?id=~Di_He1)
  - **Affiliations:** National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, School of EECS, Peking University, National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, School of EECS, Peking University, National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Center for Machine Learning Research, Peking University, ByteDance Inc., ByteDance Inc., ByteDance Inc., National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University
  - **TL;DR:** This study introduces Bilevel Positional Encoding (BiPE), a novel method that combines intra-segment and inter-segment encodings to enhance length extrapolation in language modeling. The findings demonstrate that BiPE significantly improves the model's ability to handle varying sequence lengths across diverse text modalities.
  - **Keywords:** Bilevel Positional Encoding, Length Extrapolation, Positional Encoding, Intra-segment Encoding, Inter-segment Encoding, Natural Language Processing, Language Modeling, Length Extrapolation Problem, Intrinsic Segmentation of Language Data, Improved Length Extrapolation Capabilities, Effective Learning, PG-19 Text Corpus


- [Ambiguity-Aware Abductive Learning](https://icml.cc/virtual/2024/poster/32894) (Poster)
  - **Authors:** [Hao-Yuan He](http://openreview.net/profile?id=~Hao-Yuan_He2), [Hui Sun](http://openreview.net/profile?id=~Hui_Sun1), [Zheng Xie](http://openreview.net/profile?id=~Zheng_Xie1), [Ming Li](http://openreview.net/profile?id=~Ming_Li1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China
  - **TL;DR:** This paper introduces Ambiguity-Aware Abductive Learning (A3BL) to address the ambiguity in the abduction process within Abductive Learning (ABL), enhancing the model's ability to utilize uncertain supervision effectively. The proposed method evaluates all potential candidates and their probabilities, preventing sub-optimal solutions in machine learning tasks that require both perception and logical reasoning.
  - **Keywords:** Abductive Learning, integration of machine learning and logical reasoning, Ambiguity-Aware Abductive Learning (A3BL), Ambiguity in abduction results, uncertainty in knowledge base, Enhanced ABL through efficient exploitation of ambiguous supervision


- [ReDiffuser: Reliable Decision-Making Using a Diffuser with Confidence Estimation](https://icml.cc/virtual/2024/poster/34171) (Poster)
  - **Authors:** [Nantian He](http://openreview.net/profile?id=~Nantian_He1), [Shaohui Li](http://openreview.net/profile?id=~Shaohui_Li3), [Zhi Li](http://openreview.net/profile?id=~Zhi_Li5), [Yu LIU](http://openreview.net/profile?id=~Yu_LIU31), [You He](http://openreview.net/profile?id=~You_He2)
  - **Affiliations:** Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China, Department of Electronics, Tsinghua University, Beijing, China, Department of Electronics, Tsinghua University, Beijing, China
  - **TL;DR:** The study presents ReDiffuser, a diffusion-based offline reinforcement learning method that enhances decision-making reliability through confidence estimation using Random Network Distillation. The proposed method demonstrates state-of-the-art performance on standard offline RL datasets by addressing issues of non-deterministic sampling and planning.
  - **Keywords:** offline reinforcement learning, decision-making, diffusion models, Random Network Distillation, robotic manipulation, medical surgery, non-deterministic sampling, unstable performance, data sparsity, confidence estimation, adaptive-horizon planning, value-embedded planning, D4RL


- [Be Your Own Neighborhood: Detecting Adversarial Examples by the Neighborhood Relations Built on Self-Supervised Learning](https://icml.cc/virtual/2024/poster/34027) (Poster)
  - **Authors:** [Zhiyuan He](http://openreview.net/profile?id=~Zhiyuan_He2), [Yijun Yang](http://openreview.net/profile?id=~Yijun_Yang5), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1), [Qiang Xu](http://openreview.net/profile?id=~Qiang_Xu1), [Tsung-Yi Ho](http://openreview.net/profile?id=~Tsung-Yi_Ho2)
  - **Affiliations:** Department of Computer Science and Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong, IBM Research, New York, USA, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong
  - **TL;DR:** This paper introduces BEYOND, a novel framework for detecting adversarial examples in deep neural networks by analyzing the abnormal relationships between adversarial examples and their augmented neighbors. The framework demonstrates superior detection capabilities and robustness, particularly against adaptive attacks, outperforming existing methods.
  - **Keywords:** Adversarial Examples, Deep Neural Networks, Self-Supervised Learning, BEYOND framework, representation similarity, label consistency, Adversarial Trained Classifier (ATC), Safety-critical systems, autonomous driving, disease diagnosis, Vulnerability to adversarial examples, detection of adversarial examples, adaptive attacks, AE detection framework, state-of-the-art robustness accuracy, improved detection ability and speed, Adversarial Machine Learning, representation learning


- [GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements](https://icml.cc/virtual/2024/poster/34305) (Poster)
  - **Authors:** [Alexander Havrilla](http://openreview.net/profile?id=~Alexander_Havrilla2), [Sharath Chandra Raparthy](http://openreview.net/profile?id=~Sharath_Chandra_Raparthy3), [Christoforos Nalmpantis](http://openreview.net/profile?id=~Christoforos_Nalmpantis1), [Jane Dwivedi-Yu](http://openreview.net/profile?id=~Jane_Dwivedi-Yu1), [Maksym Zhuravinskyi](http://openreview.net/profile?id=~Maksym_Zhuravinskyi1), [Eric Hambro](http://openreview.net/profile?id=~Eric_Hambro1), [Roberta Raileanu](http://openreview.net/profile?id=~Roberta_Raileanu2)
  - **Affiliations:** Meta; Georgia Tech, Meta, Meta, Meta, StabilityAI, Anthropic, Meta
  - **TL;DR:** This paper introduces Stepwise ORMs (SORMs) to enhance the self-refinement capabilities of large language models (LLMs) on reasoning tasks without external feedback. The proposed method significantly improves the accuracy of LLaMA 2-chat models on the GSM8K dataset by effectively detecting and correcting reasoning errors.
  - **Keywords:** reasoning refinement, large language models (LLMs), Stepwise ORMs (SORMs), Outcome Based Reward Models (ORMs), Process Based Reward Models (PRMs), reinforcement learning (RL), math, science, coding tasks, identifying when to refine, where to refine, and how to refine reasoning steps, improved accuracy of LLaMA 2-chat model, step-level feedback for refinement models, GSM8K


- [DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems](https://icml.cc/virtual/2024/poster/32993) (Poster)
  - **Authors:** [Kaibo He](http://openreview.net/profile?id=~Kaibo_He2), [Chenhui Zuo](http://openreview.net/profile?id=~Chenhui_Zuo1), [Chengtian Ma](http://openreview.net/profile?id=~Chengtian_Ma1), [Yanan Sui](http://openreview.net/profile?id=~Yanan_Sui1)
  - **Affiliations:** School of Aerospace Engineering, Tsinghua University, Beijing, China, School of Aerospace Engineering, Tsinghua University, Beijing, China, School of Aerospace Engineering, Tsinghua University, Beijing, China, School of Aerospace Engineering, Tsinghua University, Beijing, China
  - **TL;DR:** The study introduces the Dynamical Synergistic Representation (DynSyn) algorithm to enhance motor control in high-dimensional, overactuated systems by generating synergistic representations from dynamical structures. It demonstrates significant improvements in sample efficiency and robustness across various musculoskeletal models, providing insights into the control mechanisms of vertebrate systems.
  - **Keywords:** overactuated systems, motor control, reinforcement learning, Dynamical Synergistic Representation (DynSyn), deep reinforcement learning (DRL), musculoskeletal systems, embodied intelligence, robotics, high-dimensional action space, control challenges, redundancy in actuation, state-of-the-art sample efficiency, interpretable synergistic representations, muscle synergies, neuro-muscular actuators


- [Riemannian Accelerated Zeroth-order Algorithm: Improved Robustness and Lower Query Complexity](https://icml.cc/virtual/2024/poster/33465) (Poster)
  - **Authors:** [Chang He](http://openreview.net/profile?id=~Chang_He1), [Zhaoye Pan](http://openreview.net/profile?id=~Zhaoye_Pan1), [Xiao Wang](http://openreview.net/profile?id=~Xiao_Wang4), [Bo Jiang](http://openreview.net/profile?id=~Bo_Jiang3)
  - **Affiliations:** School of Information Management and Engineering, Shanghai University of Finance and Economics; Key Laboratory of Interdisciplinary Research of Computation and Economics, Shanghai University of Finance and Economics, Ministry of Education; Dishui Lake Advanced Finance Institute, Shanghai University of Finance and Economics, School of Information Management and Engineering, Shanghai University of Finance and Economics, School of Information Management and Engineering, Shanghai University of Finance and Economics; Key Laboratory of Interdisciplinary Research of Computation and Economics, Shanghai University of Finance and Economics, Ministry of Education, School of Information Management and Engineering, Shanghai University of Finance and Economics; Key Laboratory of Interdisciplinary Research of Computation and Economics, Shanghai University of Finance and Economics, Ministry of Education; Dishui Lake Advanced Finance Institute, Shanghai University of Finance and Economics
  - **TL;DR:** This study presents a Riemannian accelerated zeroth-order algorithm that improves robustness and reduces query complexity for optimization problems on Riemannian manifolds. The proposed method achieves efficient convergence rates and requires larger smoothing parameters, enhancing the performance of zeroth-order algorithms in challenging settings.
  - **Keywords:** Riemannian optimization, zeroth-order algorithms, Riemannian accelerated zeroth-order algorithm, Statistical learning, robot learning, Slow convergence, instability of zeroth-order algorithms, constraints of Riemannian manifolds, Improved robustness, lower query complexity, almost sure convergence, Riemannian manifolds, first-order stationary point, second-order stationary point, Stable Manifold Theorem


- [Understanding Diffusion Models by Feynman's Path Integral](https://icml.cc/virtual/2024/poster/34777) (Poster)
  - **Authors:** [Yuji Hirono](http://openreview.net/profile?id=~Yuji_Hirono1), [Akinori Tanaka](http://openreview.net/profile?id=~Akinori_Tanaka1), [Kenji Fukushima](http://openreview.net/profile?id=~Kenji_Fukushima1)
  - **Affiliations:** Department of Physics, Kyoto University, Kyoto 606-8502, Japan, RIKEN AIP, RIKEN, Nihonbashi 103-0027, Japan; RIKEN iTHEMS, RIKEN, Wako 351-0198, Japan; Department of Mathematics, Keio University, Hiyoshi 223-8522, Japan, Department of Physics, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
  - **TL;DR:** This study introduces a novel formulation of diffusion models using Feynman’s path integral to address the performance disparity between stochastic and deterministic sampling schemes in image generation. The findings highlight the utility of this formulation in deriving backward stochastic differential equations and evaluating negative log-likelihoods, enhancing the understanding of generative models.
  - **Keywords:** diffusion models, image generation, score-based diffusion models, stochastic differential equations (SDEs), ordinary differential equations (ODEs), probability flow ODE, image generation tasks, performance disparity between stochastic and deterministic sampling schemes, novel formulation using Feynman’s path integral, derivation of backward stochastic differential equations, evaluation of negative log-likelihood, Feynman’s path integral, Wentzel–Kramers–Brillouin (WKB) expansion, Fréchet Inception Distance (FID)


- [Randomized Confidence Bounds for Stochastic Partial Monitoring](https://icml.cc/virtual/2024/poster/32731) (Poster)
  - **Authors:** [Maxime Heuillet](http://openreview.net/profile?id=~Maxime_Heuillet1), [Ola Ahmad](http://openreview.net/profile?id=~Ola_Ahmad1), [Audrey Durand](http://openreview.net/profile?id=~Audrey_Durand1)
  - **Affiliations:** Université Laval, Canada, Université Laval, Canada; Thales Research and Technology (cortAIx), Canada, Université Laval, Canada; Canada-CIFAR AI Chair, Mila, Canada
  - **TL;DR:** This paper introduces new strategies for stochastic partial monitoring that utilize randomized confidence bounds to improve performance in learning environments with incomplete feedback. The proposed methods demonstrate competitive results against existing state-of-the-art strategies in various PM games, with implications for real-world applications like monitoring classification system error rates.
  - **Keywords:** Partial Monitoring, Stochastic Outcomes, Sequential Learning, Randomization of Deterministic Confidence Bounds, PM Strategies, Monitoring Classification Systems, Online Learning Problems, Incomplete Feedback, Cumulative Loss Minimization, Regret in Learning, RandCBP, RandCBPside⋆ Strategies, Regret Guarantees


- [Quantum Algorithm for Online Exp-concave Optimization](https://icml.cc/virtual/2024/poster/34394) (Poster)
  - **Authors:** [Jianhao He](http://openreview.net/profile?id=~Jianhao_He1), [Chengchang Liu](http://openreview.net/profile?id=~Chengchang_Liu1), [Xutong Liu](http://openreview.net/profile?id=~Xutong_Liu1), [Lvzhou Li](http://openreview.net/profile?email=lilvzh%40mail.sysu.edu.cn), [John C.S. Lui](http://openreview.net/profile?id=~John_C.S._Lui2)
  - **Affiliations:** Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China
  - **TL;DR:** This paper investigates quantum advantages in zeroth-order feedback online exp-concave optimization, presenting quantum online quasi-Newton methods that achieve O(n log T) regret with O(1) queries per round, significantly improving upon classical algorithms. The findings suggest that quantum methods can enhance performance in online learning scenarios involving exp-concave loss functions.
  - **Keywords:** quantum optimization, online exp-concave optimization, quasi-Newton methods, zeroth-order feedback, online learning, sequential decision making, regret minimization, exp-concave loss functions, O(n log T) regret, improvement over classical algorithms, exp-concave functions, Hessian approximation


- [Learning Surrogates for Offline Black-Box Optimization via Gradient Matching](https://icml.cc/virtual/2024/poster/33149) (Poster)
  - **Authors:** [Minh Hoang](http://openreview.net/profile?id=~Minh_Hoang1), [Azza Fadhel](http://openreview.net/profile?id=~Azza_Fadhel1), [Aryan Deshwal](http://openreview.net/profile?id=~Aryan_Deshwal1), [Jana Doppa](http://openreview.net/profile?id=~Jana_Doppa1), [Nghia Hoang](http://openreview.net/profile?id=~Trong_Nghia_Hoang1)
  - **Affiliations:** Lewis-Sigler Institute of Integrative Genomics, Princeton University, New Jersey, USA, School of Electrical Engineering and Computer Science, Washington State University, Pullman, Washington, USA, School of Electrical Engineering and Computer Science, Washington State University, Pullman, Washington, USA, School of Electrical Engineering and Computer Science, Washington State University, Pullman, Washington, USA, School of Electrical Engineering and Computer Science, Washington State University, Pullman, Washington, USA
  - **TL;DR:** This paper addresses the challenges of offline black-box optimization by developing a theoretical framework that quantifies the performance gap caused by imperfect surrogate models. It introduces a gradient matching algorithm that enhances the effectiveness of surrogate models, demonstrating improvements on real-world benchmarks.
  - **Keywords:** Offline optimization, black-box optimization, surrogate models, Gradient matching algorithm, Material design, chemical design, optimization over design spaces, Performance gap due to imperfect surrogate models, discrepancy in gradient estimation, Theoretical framework for understanding optimization quality, improved surrogate model creation


- [Robust Multi-Task Learning with Excess Risks](https://icml.cc/virtual/2024/poster/34358) (Poster)
  - **Authors:** [Yifei He](http://openreview.net/profile?id=~Yifei_He1), [Shiji Zhou](http://openreview.net/profile?id=~Shiji_Zhou1), [Guojun Zhang](http://openreview.net/profile?id=~Guojun_Zhang1), [Hyokun Yun](http://openreview.net/profile?id=~Hyokun_Yun1), [Yi Xu](http://openreview.net/profile?id=~Yi_Xu10), [Belinda Zeng](http://openreview.net/profile?id=~Belinda_Zeng1), [Trishul Chilimbi](http://openreview.net/profile?id=~Trishul_Chilimbi1), [Han Zhao](http://openreview.net/profile?id=~Han_Zhao1)
  - **Affiliations:** Department of Computer Science, University of Illinois Urbana-Champaign, Urbana, IL, USA, Department of Automation, Tsinghua University, Beijing, China, School of Computer Science, University of Waterloo, Waterloo, ON, Canada, Amazon, Seattle, WA, USA, Amazon, Seattle, WA, USA, Amazon, Seattle, WA, USA, Amazon, Seattle, WA, USA, Department of Computer Science, University of Illinois Urbana-Champaign; Amazon, Seattle, WA, USA
  - **TL;DR:** This study introduces Multi-Task Learning with Excess Risks (ExcessMTL), a robust method for multi-task learning that dynamically adjusts task weights based on their distance to convergence, effectively addressing the challenges posed by label noise. The proposed algorithm demonstrates superior performance over existing methods in the presence of label noise, ensuring better training outcomes across multiple tasks.
  - **Keywords:** Multi-task learning (MTL), Robustness to label noise, Excess risk-based task balancing, Adaptive weight updating, Label noise, Performance drop due to noisy tasks, ExcessMTL algorithm, Convergence guarantees, Pareto stationarity


- [Position: Why We Must Rethink Empirical Research in Machine Learning](https://icml.cc/virtual/2024/poster/34611) (Poster)
  - **Authors:** [Moritz Herrmann](http://openreview.net/profile?id=~Moritz_Herrmann1), [F. Julian D. Lange](http://openreview.net/profile?id=~F._Julian_D._Lange1), [Katharina Eggensperger](http://openreview.net/profile?id=~Katharina_Eggensperger1), [Giuseppe Casalicchio](http://openreview.net/profile?id=~Giuseppe_Casalicchio1), [Marcel Wever](http://openreview.net/profile?id=~Marcel_Wever1), [Matthias Feurer](http://openreview.net/profile?id=~Matthias_Feurer2), [David Rügamer](http://openreview.net/profile?id=~David_R%C3%BCgamer1), [Eyke Hüllermeier](http://openreview.net/profile?id=~Eyke_H%C3%BCllermeier1), [Anne-Laure Boulesteix](http://openreview.net/profile?id=~Anne-Laure_Boulesteix1), [Bernd Bischl](http://openreview.net/profile?id=~Bernd_Bischl1)
  - **Affiliations:** Institute for Medical Information Processing, Biometry, and Epidemiology, Faculty of Medicine, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, Institute for Medical Information Processing, Biometry, and Epidemiology, Faculty of Medicine, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, University of Tübingen, Tübingen, Germany, Department of Statistics, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, Institute of Informatics, LMU Munich, Munich, Germany, Department of Statistics, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, Department of Statistics, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, Institute of Informatics, LMU Munich, Munich, Germany, Institute for Medical Information Processing, Biometry, and Epidemiology, Faculty of Medicine, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, Department of Statistics, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany
  - **TL;DR:** The paper emphasizes the need to rethink empirical research practices in machine learning to address issues of non-replicability and methodological rigor. It warns that the current confirmatory approach undermines the reliability of findings and could hinder progress in the field, particularly in high-stakes applications like medicine.
  - **Keywords:** empirical research in machine learning, replicability, medical domain, non-replicable results, methodological quality, risk of bias, epistemic limitations, exploratory research


- [Estimating Unknown Population Sizes Using the Hypergeometric Distribution](https://icml.cc/virtual/2024/poster/33003) (Spotlight Poster)
  - **Authors:** [Liam Hodgson](http://openreview.net/profile?id=~Liam_Hodgson1), [Danilo Bzdok](http://openreview.net/profile?id=~Danilo_Bzdok2)
  - **Affiliations:** McGill University, Montréal, Canada; Mila - Québec Artificial Intelligence Institute, McGill University, Montréal, Canada; Mila - Québec Artificial Intelligence Institute
  - **TL;DR:** This study presents a novel method for estimating unknown population sizes using the hypergeometric distribution, addressing challenges in discrete distribution estimation under severe under-sampling. The proposed approach demonstrates superior performance in accuracy and latent space learning, with applications in NLP and single-cell genomics.
  - **Keywords:** Hypergeometric distribution, Estimation of population sizes, Hypergeometric likelihood, Variational autoencoder, Natural Language Processing (NLP), Single-cell genomics, Estimating discrete distributions, Under-sampling, Improved accuracy of population size estimates, Informative latent space learning, Mixture of distributions, Collaborative filtering


- [Criterion Collapse and Loss Distribution Control](https://icml.cc/virtual/2024/poster/33862) (Poster)
  - **Authors:** [Matthew J. Holland](http://openreview.net/profile?id=~Matthew_J._Holland1)
  - **Affiliations:** SANKEN, Osaka University, Japan
  - **TL;DR:** This study investigates "criterion collapse," where optimizing one performance metric leads to optimality in another, particularly focusing on conditions that lead to error probability minimizers across various learning criteria. The findings reveal that certain learning criteria, including those involving surrogate losses, can exhibit unexpected relationships in terms of optimality, challenging traditional assumptions in machine learning.
  - **Keywords:** criterion collapse, error probability minimizers, learning criteria, DRO (Distributionally Robust Optimization), OCE (Optimality Criteria), CVaR (Conditional Value at Risk), tilted ERM (Empirical Risk Minimization), optimization of performance metrics, loss distribution challenges, conditions for collapse, implications for surrogate losses, Bernoulli distribution, non-monotonic criteria, monotonic criteria


- [Learning Useful Representations of Recurrent Neural Network Weight Matrices](https://icml.cc/virtual/2024/poster/34097) (Oral)
  - **Authors:** [Vincent Herrmann](http://openreview.net/profile?id=~Vincent_Herrmann1), [Francesco Faccio](http://openreview.net/profile?id=~Francesco_Faccio1), [Jürgen Schmidhuber](http://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1)
  - **Affiliations:** The Swiss AI Lab IDSIA, USI & SUPSI; AI Initiative, KAUST, The Swiss AI Lab IDSIA, USI & SUPSI; AI Initiative, KAUST, The Swiss AI Lab IDSIA, USI & SUPSI; AI Initiative, KAUST
  - **TL;DR:** This paper presents novel techniques for learning useful representations of Recurrent Neural Network (RNN) weight matrices, focusing on mechanistic and functionalist approaches. The findings demonstrate that functionalist methods, particularly interactive probing, significantly enhance the ability to predict RNN behavior and facilitate various downstream tasks.
  - **Keywords:** Recurrent Neural Networks (RNNs), Representation Learning, Mechanistic approaches, Functionalist approaches, Deep Weight Space layer, Probing inputs, RNN analysis, Downstream tasks, Classifiers of sequentially processed MNIST digits, Learning representations of RNN weights, Predicting RNN behavior, Novel techniques for learning RNN representations, Self-supervised learning methods, Model zoo datasets, Generative models of formal languages, MNIST digits


- [Two Tales of Single-Phase Contrastive Hebbian Learning](https://icml.cc/virtual/2024/poster/32812) (Poster)
  - **Authors:** [Rasmus Kjær Høier](http://openreview.net/profile?id=~Rasmus_H%C3%B8ier1), [Christopher Zach](http://openreview.net/profile?id=~Christopher_Zach2)
  - **Affiliations:** Department of Electrical Engineering, Chalmers University of Technology, Sweden, Department of Electrical Engineering, Chalmers University of Technology, Sweden
  - **TL;DR:** This study explores dual propagation, a local learning algorithm that addresses the challenges of synchronization and vanishing error signals in biologically plausible learning systems. It highlights the algorithm's potential for neuromorphic computing while discussing its reliance on symmetric nudging for numerical stability.
  - **Keywords:** biologically plausible learning algorithms, neuromorphic computing, dual propagation, contrastive Hebbian learning, equilibrium propagation, coupled learning, vanishing error signal, synchronization issues, numerical stability, local learning algorithm, error signal representation, CIFAR10, CIFAR100, NGRAD (Neural Gradient Representation by Activity Differences), adversarial robustness


- [Verifying message-passing neural networks via topology-based bounds tightening](https://icml.cc/virtual/2024/poster/33136) (Poster)
  - **Authors:** [Christopher Hojny](http://openreview.net/profile?id=~Christopher_Hojny1), [Shiqiang Zhang](http://openreview.net/profile?id=~Shiqiang_Zhang1), [Juan Campos](http://openreview.net/profile?id=~Juan_S_Campos1), [Ruth Misener](http://openreview.net/profile?id=~Ruth_Misener1)
  - **Affiliations:** Eindhoven University of Technology, Eindhoven, The Netherlands, Department of Computing, Imperial College London, UK, Department of Computing, Imperial College London, UK, Department of Computing, Imperial College London, UK
  - **TL;DR:** This study presents a computationally effective method for certifying the robustness of message-passing neural networks against adversarial attacks by utilizing mixed-integer optimization and topology-based bounds tightening. The findings demonstrate the method's effectiveness in both node and graph classification tasks, addressing vulnerabilities in graph neural networks.
  - **Keywords:** Graph Neural Networks (GNNs), Message-Passing Neural Networks (MPNNs), Robustness, Mixed-Integer Optimization, Rectified Linear Unit (ReLU), Topology-Based Bounds Tightening, Node Classification, Graph Classification, Vulnerability to Adversarial Attacks, Certifiable Robustness, Robust Certificates, Optimization Constraints, SCIP (Solver), Adversarial Attacks, Graph Perturbations


- [Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation](https://icml.cc/virtual/2024/poster/34314) (Poster)
  - **Authors:** [Floris Holstege](http://openreview.net/profile?id=~Floris_Holstege1), [Bram Wouters](http://openreview.net/profile?id=~Bram_Wouters1), [Noud van Giersbergen](http://openreview.net/profile?id=~Noud_Van_Giersbergen1), [Cees Diks](http://openreview.net/profile?id=~Cees_Diks1)
  - **Affiliations:** University of Amsterdam, Department of Quantitative Economics; Tinbergen Institute, University of Amsterdam, Department of Quantitative Economics, University of Amsterdam, Department of Quantitative Economics, University of Amsterdam, Department of Quantitative Economics; Tinbergen Institute
  - **TL;DR:** This study addresses the challenge of ensuring deep neural networks utilize the correct input features by proposing an iterative algorithm that effectively separates spurious concepts from main-task concepts. The algorithm outperforms existing methods in identifying and removing only the spurious concepts, thereby improving model interpretability and performance.
  - **Keywords:** interpretable machine learning, deep neural networks, Joint Subspace Estimation (JSE), concept-removal methods, computer vision, natural language processing, spurious correlations, model interpretability, undesirable concepts, iterative algorithm for separating spurious from main-task concepts, Waterbirds, CelebA, MultiNLI


- [Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression](https://icml.cc/virtual/2024/poster/33520) (Poster)
  - **Authors:** [Junyuan Hong](http://openreview.net/profile?id=~Junyuan_Hong1), [Jinhao Duan](http://openreview.net/profile?id=~Jinhao_Duan1), [Chenhui Zhang](http://openreview.net/profile?id=~Chenhui_Zhang2), [Zhangheng Li](http://openreview.net/profile?id=~Zhangheng_LI2), [Chulin Xie](http://openreview.net/profile?id=~Chulin_Xie1), [Kelsey Lieberman](http://openreview.net/profile?id=~Kelsey_Lieberman1), [James Diffenderfer](http://openreview.net/profile?id=~James_Diffenderfer1), [Brian Bartoldson](http://openreview.net/profile?id=~Brian_R._Bartoldson1), [Ajay Jaiswal](http://openreview.net/profile?id=~AJAY_KUMAR_JAISWAL1), [Kaidi Xu](http://openreview.net/profile?id=~Kaidi_Xu1), [Bhavya Kailkhura](http://openreview.net/profile?id=~Bhavya_Kailkhura1), [Dan Hendrycks](http://openreview.net/profile?id=~Dan_Hendrycks1), [Dawn Song](http://openreview.net/profile?id=~Dawn_Song1), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19)
  - **Affiliations:** University of Texas at Austin, Drexel University, MIT, University of Texas at Austin, UIUC, Duke University, Lawrence Livermore National Laboratory, Lawrence Livermore National Laboratory, University of Texas at Austin, Drexel University, Lawrence Livermore National Laboratory, Center for AI Safety, University of California, Berkeley, University of Texas at Austin, University of Chicago
  - **TL;DR:** This study evaluates the trustworthiness of compressed Large Language Models (LLMs) using various state-of-the-art compression techniques, revealing that quantization is more effective than pruning in maintaining trustworthiness. The findings emphasize the need for comprehensive trustworthiness evaluations beyond benign performance metrics to ensure safety in high-stakes applications.
  - **Keywords:** Trustworthiness, Large Language Models (LLMs), Model Compression, Quantization, Pruning, State-of-the-Art (SoTA) Compression Techniques, Resource-efficient Inferences, AI Safety, Safety Risks, Trustworthiness Degradation, Performance Variability, Evaluation of Trustworthiness Dimensions, Recommendations for Efficiency and Trustworthiness


- [Enhancing Sufficient Dimension Reduction via Hellinger Correlation](https://icml.cc/virtual/2024/poster/34627) (Poster)
  - **Authors:** [Seungbeom Hong](http://openreview.net/profile?id=~SeungBeom_Hong1), [Ilmun Kim](http://openreview.net/profile?id=~Ilmun_Kim1), [Jun Song](http://openreview.net/profile?id=~Jun_Song4)
  - **Affiliations:** Department of Statistics, Korea University, Seoul, South Korea, Department of Applied Statistics, Yonsei University, Seoul, South Korea, Department of Statistics, Korea University, Seoul, South Korea; Department of Applied Statistics, Yonsei University, Seoul, South Korea
  - **TL;DR:** This study introduces a new theory and method for sufficient dimension reduction (SDR) in single-index models, leveraging Hellinger correlation to effectively detect dimension reduction subspaces. The proposed method significantly outperforms existing SDR techniques by enhancing the understanding of data dependencies.
  - **Keywords:** Sufficient Dimension Reduction (SDR), Single-Index Models, Hellinger Correlation, Dimension Reduction Subspace Detection, High-Dimensional Data Analysis, Statistical Analysis, Curse of Dimensionality, Complexity of Modeling and Interpretation, New Theory and Method for SDR, Enhanced Detection of Dimension Reduction Subspace, Conditional Independence, Central Space, Structural Dimension


- [Multi-Sender Persuasion: A Computational Perspective](https://icml.cc/virtual/2024/poster/34852) (Poster)
  - **Authors:** [Safwan Hossain](http://openreview.net/profile?id=~Safwan_Hossain1), [Tonghan Wang](http://openreview.net/profile?id=~Tonghan_Wang1), [Tao Lin](http://openreview.net/profile?id=~Tao_Lin2), [Yiling Chen](http://openreview.net/profile?id=~Yiling_Chen1), [David Parkes](http://openreview.net/profile?id=~David_C._Parkes1), [Haifeng Xu](http://openreview.net/profile?id=~Haifeng_Xu1)
  - **Affiliations:** Harvard University, Harvard University, Harvard University, Harvard University, Harvard University, University of Chicago
  - **TL;DR:** This study explores the multi-sender persuasion problem within the Bayesian persuasion framework, demonstrating that finding Nash equilibria is computationally challenging. The authors propose a novel neural network approach to discover local equilibria that outperform traditional full-revelation strategies.
  - **Keywords:** Multi-sender persuasion, Bayesian persuasion, signaling, Nash equilibrium, differentiable neural network, extra-gradient algorithm, Computational economics, multi-agent learning, machine learning, Finding local Nash equilibria, best response computation, PPAD-Hard problems, Local equilibria that Pareto dominate full-revelation equilibria


- [Equilibrium of Data Markets with Externality](https://icml.cc/virtual/2024/poster/34029) (Poster)
  - **Authors:** [Safwan Hossain](http://openreview.net/profile?id=~Safwan_Hossain1), [Yiling Chen](http://openreview.net/profile?id=~Yiling_Chen1)
  - **Affiliations:** Harvard University, Cambridge, USA, Harvard University, Cambridge, USA
  - **TL;DR:** This study models real-world data markets as a simultaneous game, highlighting the negative externalities buyers impose on each other. It demonstrates that intervention through transaction costs can lead to a pure Nash equilibrium with improved welfare outcomes.
  - **Keywords:** data markets, externality, Nash equilibrium, simultaneous game modeling, transaction cost intervention, negative externality, incomplete information, valuation challenges, pure equilibrium, welfare guarantees, learning algorithms


- [Maestro: Uncovering Low-Rank Structures via Trainable Decomposition](https://icml.cc/virtual/2024/poster/34878) (Poster)
  - **Authors:** [Samuel Horváth](http://openreview.net/profile?id=~Samuel_Horv%C3%A1th1), [Stefanos Laskaridis](http://openreview.net/profile?id=~Stefanos_Laskaridis1), [Shashank Rajput](http://openreview.net/profile?id=~Shashank_Rajput1), [Hongyi Wang](http://openreview.net/profile?id=~Hongyi_Wang1)
  - **Affiliations:** Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE, Brave Software, London, UK, DataBricks, San Francisco, USA, Carnegie Mellon University, Pittsburgh, USA
  - **TL;DR:** The study introduces MAESTRO, a framework for trainable low-rank layers in deep neural networks, addressing the challenges of model size and training costs. It enables efficient low-rank compression while maintaining performance and allows for a trade-off between accuracy and latency for deployment on constrained devices.
  - **Keywords:** Deep Neural Networks (DNNs), Low-Rank Compression, Low-Rank Ordered Decomposition (LOD), Singular Value Decomposition (SVD), Principal Component Analysis (PCA), Efficient Machine Learning, Deployment in Constrained Devices, High computational cost, Model size, Accuracy degradation, Training time overhead, Trainable low-rank layers, Lower footprint models, Accuracy-latency trade-off


- [IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency](https://icml.cc/virtual/2024/poster/33779) (Poster)
  - **Authors:** [Linshan Hou](http://openreview.net/profile?id=~Linshan_Hou1), [Ruili Feng](http://openreview.net/profile?id=~Ruili_Feng1), [Zhongyun Hua](http://openreview.net/profile?id=~Zhongyun_Hua1), [Wei Luo](http://openreview.net/profile?id=~Wei_Luo3), [Leo Yu Zhang](http://openreview.net/profile?id=~Leo_Yu_Zhang1), [Yiming Li](http://openreview.net/profile?id=~Yiming_Li1)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, Alibaba Group, China; University of Science and Technology of China, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Information Technology, Deakin University, Australia, School of Information and Communication Technology, Griffith University, Australia, Nanyang Technological University, Singapore
  - **TL;DR:** This paper introduces IBD-PSC, an effective input-level backdoor detection method for deep neural networks, leveraging parameter-oriented scaling consistency to filter out malicious testing images. The method demonstrates strong effectiveness and efficiency against adaptive attacks, providing a crucial defense mechanism for DNNs.
  - **Keywords:** Backdoor attacks, Deep neural networks (DNNs), Input-level backdoor detection, Parameter-oriented scaling consistency (PSC), Model misclassifications, Malicious inputs, Threats to DNN lifecycle and supply chain, IBD-PSC method, Theoretical analysis of PSC, Adaptive method for detection, Benchmark datasets, Firewall for deployed models


- [A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Linear MDPs](https://icml.cc/virtual/2024/poster/34025) (Poster)
  - **Authors:** [Kihyuk Hong](http://openreview.net/profile?id=~Kihyuk_Hong1), [Ambuj Tewari](http://openreview.net/profile?id=~Ambuj_Tewari1)
  - **Affiliations:** Department of Statistics, University of Michigan, Department of Statistics, University of Michigan
  - **TL;DR:** This paper presents a primal-dual algorithm for offline constrained reinforcement learning with linear MDPs, achieving O(ϵ−2) sample complexity under partial data coverage assumptions. The proposed method addresses challenges related to distribution shift and sample efficiency, making it applicable to real-world safety-critical decision-making scenarios.
  - **Keywords:** offline reinforcement learning, constrained reinforcement learning, primal-dual algorithm, linear MDPs, safety concerns in decision making, distribution shift, sample efficiency, data coverage, computationally efficient algorithm, sample complexity improvement


- [Do Large Code Models Understand Programming Concepts? Counterfactual Analysis for Code Predicates](https://icml.cc/virtual/2024/poster/34779) (Poster)
  - **Authors:** [Ashish Hooda](http://openreview.net/profile?id=~Ashish_Hooda1), [Mihai Christodorescu](http://openreview.net/profile?id=~Mihai_Christodorescu1), [Miltiadis Allamanis](http://openreview.net/profile?id=~Miltiadis_Allamanis1), [Aaron Wilson](http://openreview.net/profile?email=aaroncrwilson%40google.com), [Kassem Fawaz](http://openreview.net/profile?id=~Kassem_Fawaz1), [Somesh Jha](http://openreview.net/profile?id=~Somesh_Jha1)
  - **Affiliations:** UW-Madison; Google Research, Google Research, Google DeepMind, Google Research, UW-Madison, UW-Madison; Google Research
  - **TL;DR:** This study investigates the understanding of programming concepts by large code models using a counterfactual analysis framework. The findings indicate that current models struggle with understanding key concepts such as data flow and control flow, highlighting limitations in their performance on coding tasks.
  - **Keywords:** Large Language Models, Code Generation, Programming Concepts, Counterfactual Analysis, Auto-regressive Models, Code Completion, Code Editing, Code Repair, Code Translation, Understanding of Programming-Concept Predicates, Lack of Understanding in Data Flow and Control Flow, Counterfactual Analysis for Programming Concept Predicates (CACP), HumanEval, MBPP, CodeContests, Programming-Concept Predicates (PCPs), Control Flow, Data Flow, Data Types, Identifier Naming


- [Tripod: Three Complementary Inductive Biases for Disentangled Representation Learning](https://icml.cc/virtual/2024/poster/35189) (Poster)
  - **Authors:** [Kyle Hsu](http://openreview.net/profile?id=~Kyle_Hsu1), [Jubayer Ibn Hamid](http://openreview.net/profile?id=~Jubayer_Ibn_Hamid1), [Kaylee Burns](http://openreview.net/profile?id=~Kaylee_Burns2), [Chelsea Finn](http://openreview.net/profile?id=~Chelsea_Finn1), [Jiajun Wu](http://openreview.net/profile?id=~Jiajun_Wu1)
  - **Affiliations:** Stanford University, Stanford University, Stanford University, Stanford University, Stanford University
  - **TL;DR:** This study introduces Tripod, a model that incorporates three complementary inductive biases to enhance disentangled representation learning in neural networks. The proposed adaptations lead to significant improvements in performance on image disentanglement benchmarks, demonstrating the necessity of all three biases for optimal results.
  - **Keywords:** Disentangled representation learning, Inductive biases, Neural network autoencoder, Quantization, Image disentanglement, Underspecified solution set, Data generation, Tripod model, State-of-the-art results


- [HarmoDT: Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/35117) (Poster)
  - **Authors:** [Shengchao Hu](http://openreview.net/profile?id=~Shengchao_Hu1), [Ziqing Fan](http://openreview.net/profile?id=~Ziqing_Fan1), [Li Shen](http://openreview.net/profile?id=~Li_Shen1), [Ya Zhang](http://openreview.net/profile?id=~Ya_Zhang1), [Yanfeng Wang](http://openreview.net/profile?id=~Yanfeng_Wang1), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Sun Yat-sen University, China; JD Explore Academy, China, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Nanyang Technological University, Singapore
  - **TL;DR:** This study introduces the Harmony Multi-Task Decision Transformer (HarmoDT) for offline multi-task reinforcement learning, aiming to develop a unified policy for diverse tasks without online interaction. The proposed method effectively identifies an optimal harmony subspace of parameters for each task, demonstrating superior performance in empirical evaluations.
  - **Keywords:** offline reinforcement learning, multi-task reinforcement learning, Harmony Multi-Task Decision Transformer, sequence modeling, meta-learning, gradient-based techniques, robotics, diverse task environments, variations in task content and complexity, policy formulation challenges, conflicting gradients management, optimal harmony subspace identification, task-specific mask learning, unified policy performance enhancement, Transformer architecture, bi-level optimization


- [An Information Theoretic Approach to Interaction-Grounded Learning](https://icml.cc/virtual/2024/poster/33990) (Poster)
  - **Authors:** [Xiaoyan Hu](http://openreview.net/profile?id=~Xiaoyan_Hu2), [Farzan Farnia](http://openreview.net/profile?id=~Farzan_Farnia1), [Ho-fung Leung](http://openreview.net/profile?id=~Ho-fung_Leung1)
  - **Affiliations:** Department of Computer Science and Engineering, The Chinese University of Hong Kong, Department of Computer Science and Engineering, The Chinese University of Hong Kong, Independent Researcher
  - **TL;DR:** This study introduces Variational Information-based Interaction-Grounded Learning (VI-IGL), an information-theoretic approach to optimize reinforcement learning by inferring latent binary rewards from feedback variables. The proposed framework demonstrates improved performance in various reinforcement learning settings compared to existing algorithms.
  - **Keywords:** Reinforcement Learning, Interaction-Grounded Learning, Variational Information-based IGL, Conditional Mutual Information, Min-Max Optimization, Brain-Computer Interface, Recommender Systems, Misspecified Reward Variable, Inference of Unseen Rewards, Improved Performance in IGL-based RL Algorithms


- [SceneCraft: An LLM Agent for Synthesizing 3D Scenes as Blender Code](https://icml.cc/virtual/2024/poster/33438) (Oral)
  - **Authors:** [ziniu hu](http://openreview.net/profile?id=~Ziniu_Hu1), [Ahmet Iscen](http://openreview.net/profile?id=~Ahmet_Iscen3), [Aashi Jain](http://openreview.net/profile?id=~Aashi_Jain1), [Thomas Kipf](http://openreview.net/profile?id=~Thomas_Kipf2), [Yisong Yue](http://openreview.net/profile?id=~Yisong_Yue1), [David Ross](http://openreview.net/profile?id=~David_A_Ross1), [Cordelia Schmid](http://openreview.net/profile?id=~Cordelia_Schmid1), [Alireza Fathi](http://openreview.net/profile?id=~Alireza_Fathi1)
  - **Affiliations:** California Institute of Technology; Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, California Institute of Technology, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** The paper presents SceneCraft, an LLM agent that converts text descriptions into Blender-executable Python scripts for rendering complex 3D scenes. It demonstrates superior performance in scene rendering through advanced spatial planning and iterative refinement, with potential applications in various industries.
  - **Keywords:** 3D scene synthesis, Large Language Models (LLMs), Scene graph modeling, Python scripting, library learning, Architectural design, game development, virtual reality, cinematic production, Complex spatial planning, arrangement of multiple 3D assets, SceneCraft agent, iterative refinement of scenes, reusable library for script functions, Blender


- [Improving Interpretation Faithfulness for Vision Transformers](https://icml.cc/virtual/2024/poster/33766) (Spotlight Poster)
  - **Authors:** [Lijie Hu](http://openreview.net/profile?id=~Lijie_Hu1), [Yixin Liu](http://openreview.net/profile?id=~Yixin_Liu4), [Ninghao Liu](http://openreview.net/profile?id=~Ninghao_Liu2), [Mengdi Huai](http://openreview.net/profile?id=~Mengdi_Huai1), [Lichao Sun](http://openreview.net/profile?id=~Lichao_Sun1), [Di Wang](http://openreview.net/profile?id=~Di_Wang1)
  - **Affiliations:** King Abdullah University of Science and Technology (KAUST); Provable Responsible AI and Data Analytics (PRADA) Lab; SDAIA-KAUST AI, Lehigh University, University of Georgia, Iowa State University, Lehigh University, King Abdullah University of Science and Technology (KAUST); Provable Responsible AI and Data Analytics (PRADA) Lab; SDAIA-KAUST AI
  - **TL;DR:** This paper introduces Faithful ViTs (FViTs) to enhance the explanation faithfulness of Vision Transformers by ensuring stable attention under input perturbations and robust prediction distributions. The proposed Denoised Diffusion Smoothing (DDS) method effectively improves robustness against adversarial attacks while maintaining explainability.
  - **Keywords:** Vision Transformers, Explainability, Robustness, Self-attention, Denoised Diffusion Smoothing (DDS), Randomized Smoothing, Image recognition, Objective detection, Image processing, Semantic segmentation, Explanation faithfulness, Sensitivity to perturbations, Adversarial attacks, Faithful ViTs (FViTs), Stability of attention maps, ViTs (Vision Transformers), Attention weights


- [Loss Shaping Constraints for Long-Term Time Series Forecasting](https://icml.cc/virtual/2024/poster/34815) (Poster)
  - **Authors:** [Ignacio Hounie](http://openreview.net/profile?id=~Ignacio_Hounie1), [Javier Porras-Valenzuela](http://openreview.net/profile?id=~Javier_Porras-Valenzuela1), [Alejandro Ribeiro](http://openreview.net/profile?id=~Alejandro_Ribeiro1)
  - **Affiliations:** University of Pennsylvania, University of Pennsylvania, University of Pennsylvania
  - **TL;DR:** This study introduces a Constrained Learning approach for long-term time series forecasting that optimizes average performance while imposing user-defined loss constraints at each time step. The proposed method demonstrates competitive performance across forecasting benchmarks and addresses the issue of error distribution in multi-step predictions.
  - **Keywords:** time series forecasting, multi-step forecasting, constrained learning, loss shaping constraints, Primal-Dual algorithm, transformer architectures, weather forecasting, electricity demand forecasting, price forecasting, passenger demand forecasting, disparate distributions of errors, uneven performance across forecasting steps, competitive average performance, shaping distribution of errors


- [Graph Neural PDE Solvers with Conservation and Similarity-Equivariance](https://icml.cc/virtual/2024/poster/33858) (Poster)
  - **Authors:** [Masanobu Horie](http://openreview.net/profile?id=~Masanobu_Horie1), [NAOTO MITSUME](http://openreview.net/profile?id=~NAOTO_MITSUME1)
  - **Affiliations:** RICOS Co. Ltd., Tokyo, Japan, Graduate School of Science and Technology, University of Tsukuba, Ibaraki, Japan
  - **TL;DR:** This study presents a novel machine-learning architecture based on graph neural networks to effectively solve partial differential equations while adhering to conservation laws and physical symmetries. The proposed model demonstrates significantly enhanced generalizability across unseen spatial domains compared to traditional approaches.
  - **Keywords:** machine learning, partial differential equations (PDEs), graph neural networks (GNNs), finite volume method (FVM), product design, disaster prevention, environmental sciences, generalization, reliability, physical constraints, novel machine-learning architecture, enhanced generalizability, conservation laws, physical symmetries


- [Careful with that Scalpel: Improving Gradient Surgery with an EMA](https://icml.cc/virtual/2024/poster/34418) (Poster)
  - **Authors:** [Yu-Guan Hsieh](http://openreview.net/profile?id=~Yu-Guan_Hsieh1), [James Thornton](http://openreview.net/profile?id=~James_Thornton1), [Eugene Ndiaye](http://openreview.net/profile?id=~Eugene_Ndiaye1), [Michal Klein](http://openreview.net/profile?id=~Michal_Klein1), [Marco Cuturi](http://openreview.net/profile?id=~marco_cuturi2), [Pierre Ablin](http://openreview.net/profile?id=~Pierre_Ablin2)
  - **Affiliations:** Apple, Apple, Apple, Apple, Apple, Apple
  - **TL;DR:** This paper presents a method called Bloop that improves gradient surgery by addressing the trade-off between training and auxiliary losses in deep learning models. The proposed approach demonstrates significantly better performance in NLP and vision tasks compared to existing methods that do not utilize an Exponential Moving Average (EMA).
  - **Keywords:** deep learning, optimization, gradient surgery, constrained minimization, bilevel optimization, natural language processing (NLP), computer vision, optimization trade-offs, model generalization, auxiliary loss, improved performance methods, parameter update direction


- [PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs](https://icml.cc/virtual/2024/poster/35060) (Oral)
  - **Authors:** [Charlie Hou](http://openreview.net/profile?id=~Charlie_Hou1), [Akshat Shrivastava](http://openreview.net/profile?id=~Akshat_Shrivastava1), [Hongyuan Zhan](http://openreview.net/profile?id=~Hongyuan_Zhan2), [Rylan Conway](http://openreview.net/profile?id=~Rylan_Conway1), [Trang Le](http://openreview.net/profile?id=~Trang_Le1), [Adithya Sagar](http://openreview.net/profile?email=adithyasagar%40meta.com), [Giulia Fanti](http://openreview.net/profile?id=~Giulia_Fanti1), [Daniel Lazar](http://openreview.net/profile?id=~Daniel_Lazar1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Carnegie Mellon University, Meta, Meta, Meta, Meta, Meta, Department of Electrical and Computer Engineering, Carnegie Mellon University, Coldrays
  - **TL;DR:** The study proposes Private Evolution-Text (PrE-Text) as a method for generating differentially private synthetic textual data, which outperforms traditional on-device training for small models and enhances the performance of large language models on private data. The findings suggest that training on DP synthetic data is a more efficient alternative to on-device training in terms of communication, computation, and debugging.
  - **Keywords:** Private Federated Learning, Differential Privacy, Language Models, Private Evolution-Text (PrE-Text), Federated Learning (FL), Differential Privacy (DP), Mobile keyboard autocompletion, Instruction-following large language models, Limitations of on-device training, Communication and computation costs, Debugging and deployment challenges, Improved model performance on private data, Efficient training on synthetic data, Large Language Models (LLMs), DP synthetic data


- [Provable Privacy with Non-Private Pre-Processing](https://icml.cc/virtual/2024/poster/34408) (Poster)
  - **Authors:** [Yaxi Hu](http://openreview.net/profile?id=~Yaxi_Hu1), [Amartya Sanyal](http://openreview.net/profile?id=~Amartya_Sanyal1), [Bernhard Schölkopf](http://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1)
  - **Affiliations:** Max Planck Institute for Intelligent Systems, Tübingen, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany, Max Planck Institute for Intelligent Systems, Tübingen, Germany
  - **TL;DR:** This paper proposes a framework to evaluate the privacy costs associated with non-private data-dependent pre-processing in Differentially Private machine learning pipelines. It establishes upper bounds on privacy guarantees and provides explicit privacy assessments for various pre-processing techniques, highlighting the importance of considering data dependencies in privacy accounting.
  - **Keywords:** Differential Privacy, Data Privacy, Machine Learning, Smooth DP, Data Imputation, Quantization, Deduplication, Standard Scaling, PCA, Data Analysis, Privacy-Accuracy Trade-off, Data-Dependent Pre-Processing, Privacy Cost, Independence of Data Points, Privacy Guarantees, Framework for Evaluating Privacy Costs


- [Multigroup Robustness](https://icml.cc/virtual/2024/poster/34187) (Poster)
  - **Authors:** [Lunjia Hu](http://openreview.net/profile?id=~Lunjia_Hu1), [Charlotte Peale](http://openreview.net/profile?id=~Charlotte_Peale1), [Judy Hanwen Shen](http://openreview.net/profile?id=~Judy_Hanwen_Shen1)
  - **Affiliations:** Stanford University, USA, Stanford University, USA, Stanford University, USA
  - **TL;DR:** This study introduces multigroup robust algorithms designed to ensure that the impact of data corruption on specific subpopulations is limited to the corruption within those groups. The findings highlight the importance of addressing localized data corruption to maintain fairness and accuracy in machine learning predictions across diverse populations.
  - **Keywords:** multigroup robustness, data corruption, machine learning, robust learning algorithms, predictions about people, overlapping subpopulations, data corruption, localized data corruption, response bias, multigroup robust algorithms, robustness guarantees


- [Case-Based or Rule-Based: How Do Transformers Do the Math?](https://icml.cc/virtual/2024/poster/35020) (Poster)
  - **Authors:** [Yi Hu](http://openreview.net/profile?id=~Yi_Hu3), [Xiaojuan Tang](http://openreview.net/profile?id=~Xiaojuan_Tang1), [Haotong Yang](http://openreview.net/profile?id=~Haotong_Yang1), [Muhan Zhang](http://openreview.net/profile?id=~Muhan_Zhang1)
  - **Affiliations:** Institute for Artificial Intelligence, Peking University, Institute for Artificial Intelligence, Peking University; National Key Laboratory of General Artificial Intelligence, BIGAI, Institute for Artificial Intelligence, Peking University, Institute for Artificial Intelligence, Peking University; National Key Laboratory of General Artificial Intelligence, BIGAI
  - **TL;DR:** This study investigates whether transformers utilize rule-based or case-based reasoning for math problems, revealing that they predominantly rely on case-based reasoning. The authors propose a Rule-Following Fine-Tuning technique that significantly enhances the generalization of LLMs in addition tasks, achieving over 95% accuracy for larger digit problems.
  - **Keywords:** Large Language Models, Rule-Based Reasoning, Case-Based Reasoning, Rule-Following Fine-Tuning (RFFT), Math Problem Solving, Generalization to longer addition problems, reliance on training corpus, Improved accuracy in addition tasks, systematic generalization ability, Transformers, Subgraph Matching, Shortcut Learning


- [Accelerating Transformer Pre-training with 2:4 Sparsity](https://icml.cc/virtual/2024/poster/33254) (Poster)
  - **Authors:** [Yuezhou Hu](http://openreview.net/profile?id=~Yuezhou_Hu1), [Kang Zhao](http://openreview.net/profile?id=~Kang_Zhao5), [Weiyu Huang](http://openreview.net/profile?id=~Weiyu_Huang2), [Jianfei Chen](http://openreview.net/profile?id=~Jianfei_Chen1), [Jun Zhu](http://openreview.net/profile?id=~Jun_Zhu2)
  - **Affiliations:** Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University, Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University
  - **TL;DR:** This study investigates the acceleration of transformer pre-training using 2:4 sparsity, proposing techniques to maintain accuracy while enhancing training speed. The findings indicate that their 2:4 sparse training algorithm achieves comparable convergence to dense training methods, with observable acceleration benefits.
  - **Keywords:** Transformer pre-training, sparsity-based methods, 2:4 sparse matrix multiplication, feed-forward networks, masked decay, Low accuracy, inefficiency in training, computational cost, Accuracy-preserving techniques, acceleration methods, dense fine-tuning, 2:4 sparsity, tensor core, GPU architecture


- [Task-aware Orthogonal Sparse Network for Exploring Shared Knowledge in Continual Learning](https://icml.cc/virtual/2024/poster/32877) (Poster)
  - **Authors:** [Yusong Hu](http://openreview.net/profile?id=~Yusong_Hu2), [De Cheng](http://openreview.net/profile?id=~De_Cheng3), [Dingwen Zhang](http://openreview.net/profile?id=~Dingwen_Zhang1), [Nannan Wang](http://openreview.net/profile?id=~Nannan_Wang1), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1), [Xinbo Gao](http://openreview.net/profile?id=~Xinbo_Gao5)
  - **Affiliations:** Xidian University, Xi’an, Shaanxi Province, China, Xidian University, Xi’an, Shaanxi Province, China, Northwestern Polytechnical University, Xi’an, Shaanxi Province, China, Xidian University, Xi’an, Shaanxi Province, China, University of Sydney, None, Chongqing University of Post and Telecommunications, Chongqing, China
  - **TL;DR:** This study proposes a Task-aware Orthogonal Sparse Network (OSN) to address catastrophic forgetting in continual learning by partitioning the network into three parts to explore knowledge sharing between old and new tasks. The proposed method achieves minimal interference with past tasks while enhancing model plasticity and capacity, leading to state-of-the-art performance.
  - **Keywords:** Continual Learning, Lifelong Learning, Task-aware Orthogonal Sparse Network (OSN), Lottery Ticket Hypothesis, Catastrophic Forgetting (CF), Stability-Plasticity Trade-off, Shared Knowledge Induced Network Partition, Sharpness-aware Orthogonal Sparse Network Learning


- [Sparse Model Inversion: Efficient Inversion of Vision Transformers for Data-Free Applications](https://icml.cc/virtual/2024/poster/33981) (Poster)
  - **Authors:** [Zixuan Hu](http://openreview.net/profile?id=~Zixuan_Hu1), [Yongxian Wei](http://openreview.net/profile?id=~Yongxian_Wei1), [Li Shen](http://openreview.net/profile?id=~Li_Shen1), [Zhenyi Wang](http://openreview.net/profile?id=~Zhenyi_Wang1), [Lei Li](http://openreview.net/profile?id=~Lei_Li12), [Chun Yuan](http://openreview.net/profile?id=~Chun_Yuan1), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; College of Computing & Data Science, Nanyang Technological University, Singapore, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China, School of Cyber Science and Technology, Sun Yat-sen University, Shenzhen, China; JD Explore Academy, China, University of Maryland, College Park, USA, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China, Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China, College of Computing & Data Science, Nanyang Technological University, Singapore
  - **TL;DR:** This paper presents a novel sparse model inversion strategy to efficiently reconstruct training data from Vision Transformers, addressing inefficiencies in existing dense inversion methods. The proposed approach achieves significant acceleration in inversion while maintaining or enhancing performance in data-free applications.
  - **Keywords:** Model inversion, Vision Transformers, Data-free applications, Sparse model inversion, Dense inversion, Data-free model quantization, Data-free knowledge transfer, Inefficiency in dense inversion, Redundant inversion of noisy backgrounds, Hallucination in model inversion, Significant inversion acceleration, Enhanced downstream performance, Vision Transformers (ViTs), Surrogate data


- [Q-value Regularized Transformer for Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/33072) (Poster)
  - **Authors:** [Shengchao Hu](http://openreview.net/profile?id=~Shengchao_Hu1), [Ziqing Fan](http://openreview.net/profile?id=~Ziqing_Fan1), [Chaoqin Huang](http://openreview.net/profile?id=~Chaoqin_Huang1), [Li Shen](http://openreview.net/profile?id=~Li_Shen1), [Ya Zhang](http://openreview.net/profile?id=~Ya_Zhang1), [Yanfeng Wang](http://openreview.net/profile?id=~Yanfeng_Wang1), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Sun Yat-sen University, China; JD Explore Academy, China, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Shanghai Jiao Tong University, China; Shanghai AI Laboratory, China, Nanyang Technological University, Singapore
  - **TL;DR:** This study introduces the Q-value regularized Transformer (QT) for offline reinforcement learning, which combines trajectory modeling with optimal future return predictability to address challenges in synthesizing optimal trajectories from sub-optimal data. Empirical results demonstrate QT's superiority over traditional methods, enhancing the state-of-the-art in offline RL.
  - **Keywords:** offline reinforcement learning, Conditional Sequence Modeling, Q-learning, Dynamic Programming, Transformer, stitching optimal trajectories, inconsistency in sampled returns, long-horizon learning, sparse-reward scenarios, Q-value regularized Transformer (QT), action-value function, training loss integration, D4RL benchmark datasets


- [High-Performance Temporal Reversible Spiking Neural Networks with $\mathcal{O}(L)$ Training Memory and $\mathcal{O}(1)$ Inference Cost](https://icml.cc/virtual/2024/poster/32927) (Spotlight Poster)
  - **Authors:** [JiaKui Hu](http://openreview.net/profile?id=~JiaKui_Hu1), [Man Yao](http://openreview.net/profile?id=~Man_Yao1), [Xuerui Qiu](http://openreview.net/profile?id=~Xuerui_Qiu1), [Yuhong Chou](http://openreview.net/profile?id=~Yuhong_Chou1), [Yuxuan Cai](http://openreview.net/profile?id=~Yuxuan_Cai1), [Ning Qiao](http://openreview.net/profile?id=~Ning_Qiao1), [Yonghong Tian](http://openreview.net/profile?id=~Yonghong_Tian1), [Bo XU](http://openreview.net/profile?id=~Bo_XU10), [Guoqi Li](http://openreview.net/profile?id=~Guoqi_Li1)
  - **Affiliations:** Peking University, Beijing, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Future Technology, University of Chinese Academy of Sciences, None, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Future Technology, University of Chinese Academy of Sciences, None, Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China, 501.AI, Beijing, China, SynSense AG Corporation, Zurich, Switzerland, Peking University, Beijing, China; Peng Cheng Laboratory, Shenzhen, Guangzhou, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Beijing, China
  - **TL;DR:** This study introduces a novel Temporal Reversible architecture for Spiking Neural Networks (T-RevSNN) that significantly reduces training memory to O(L) and inference energy cost to O(1), while achieving high accuracy on ImageNet. The proposed method addresses the dual challenges of memory efficiency and energy consumption in large-scale SNNs, improving performance metrics by notable factors.
  - **Keywords:** Spiking Neural Networks (SNNs), Temporal Reversible architecture, BackPropagation Through Time (BPTT), surrogate gradient, Image classification, neuromorphic computing, High memory requirements during training, increased inference energy cost, T-RevSNN, O(L) training memory, O(1) inference energy cost, improved memory efficiency, training time acceleration, and inference energy efficiency, ImageNet


- [Pseudo-Calibration: Improving Predictive Uncertainty Estimation in Unsupervised Domain Adaptation](https://icml.cc/virtual/2024/poster/33805) (Poster)
  - **Authors:** [Dapeng Hu](http://openreview.net/profile?id=~Dapeng_Hu2), [Jian Liang](http://openreview.net/profile?id=~Jian_Liang1), [Xinchao Wang](http://openreview.net/profile?id=~Xinchao_Wang1), [Chuan-Sheng Foo](http://openreview.net/profile?id=~Chuan-Sheng_Foo1)
  - **Affiliations:** Centre for Frontier AI Research, A*STAR, Singapore, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, National University of Singapore, Institute for Infocomm Research, A*STAR, Singapore
  - **TL;DR:** This paper introduces Pseudo-Calibration (PseudoCal), a novel framework for improving predictive uncertainty estimation in unsupervised domain adaptation (UDA) by synthesizing a labeled pseudo-target set. The method effectively addresses the challenges of poor calibration and distribution shifts, demonstrating superior performance across various UDA scenarios.
  - **Keywords:** Unsupervised Domain Adaptation (UDA), Predictive Uncertainty Estimation, Pseudo-Calibration (PseudoCal), Temperature Scaling, Safety-Critical Applications, Autonomous Driving, Medical Diagnosis, Poorly Calibrated Predictive Uncertainty, Absence of Labeled Target Data, Severe Distribution Shifts, Novel Post-Hoc Calibration Framework, Inference-Stage Mixup


- [InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks](https://icml.cc/virtual/2024/poster/33569) (Poster)
  - **Authors:** [Xueyu Hu](http://openreview.net/profile?id=~Xueyu_Hu1), [Ziyu Zhao](http://openreview.net/profile?id=~Ziyu_Zhao3), [Shuang Wei](http://openreview.net/profile?id=~Shuang_Wei1), [Ziwei Chai](http://openreview.net/profile?id=~Ziwei_Chai1), [Qianli Ma](http://openreview.net/profile?id=~Qianli_Ma4), [Guoyin Wang](http://openreview.net/profile?id=~Guoyin_Wang1), [Xuwu Wang](http://openreview.net/profile?id=~Xuwu_Wang2), [Jing Su](http://openreview.net/profile?id=~Jing_Su2), [Jingjing Xu](http://openreview.net/profile?id=~Jingjing_Xu1), [Ming Zhu](http://openreview.net/profile?id=~Ming_Zhu1), [Yao Cheng](http://openreview.net/profile?id=~Yao_Cheng7), [Jianbo Yuan](http://openreview.net/profile?id=~Jianbo_Yuan1), [Jiwei Li](http://openreview.net/profile?id=~Jiwei_Li1), [Kun Kuang](http://openreview.net/profile?id=~Kun_Kuang1), [Yang Yang](http://openreview.net/profile?id=~Yang_Yang35), [Hongxia Yang](http://openreview.net/profile?id=~Hongxia_Yang2), [Fei Wu](http://openreview.net/profile?id=~Fei_Wu1)
  - **Affiliations:** Department of Computer Science and Technology, Zhejiang University, Hangzhou, China, Department of Computer Science and Technology, Zhejiang University, Hangzhou, China, Rochester Institute of Technology, Department of Computer Science and Technology, Zhejiang University, Hangzhou, China, ByteDance Inc., ByteDance Inc., ByteDance Inc., ByteDance Inc., ByteDance Inc., ByteDance Inc., ByteDance Inc., ByteDance Inc., Department of Computer Science and Technology, Zhejiang University, Hangzhou, China, Department of Computer Science and Technology, Zhejiang University, Hangzhou, China, Department of Computer Science and Technology, Zhejiang University, Hangzhou, China, ByteDance Inc., Shanghai Institute for Advanced Study, Zhejiang University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China
  - **TL;DR:** This paper introduces InfiAgent-DABench, a benchmark for evaluating LLM-based agents on data analysis tasks, highlighting the development of a specialized agent, DAAgent, which outperforms GPT-3.5. The study addresses the challenges of open-ended data analysis questions and presents a dataset of 603 questions derived from CSV files for evaluation.
  - **Keywords:** LLM-based agents, data analysis tasks, format-prompting technique, ReAct approach, business intelligence, healthcare, finance, scientific research, open-ended data analysis questions, evaluation challenges, InfiAgent-DABench, DAAgent surpassing GPT-3.5, DAEval, CSV files, Large Language Models (LLMs), Artificial General Intelligence (AGI)


- [Auctionformer: A Unified Deep Learning Algorithm for Solving Equilibrium Strategies in Auction Games](https://icml.cc/virtual/2024/poster/33902) (Poster)
  - **Authors:** [Kexin Huang](http://openreview.net/profile?id=~Kexin_Huang6), [Ziqian Chen](http://openreview.net/profile?id=~Ziqian_Chen1), [xue wang](http://openreview.net/profile?id=~Xue_Wang9), [Chongming Gao](http://openreview.net/profile?id=~Chongming_Gao1), [Jinyang Gao](http://openreview.net/profile?id=~Jinyang_Gao1), [Bolin Ding](http://openreview.net/profile?id=~Bolin_Ding3), [Xiang Wang](http://openreview.net/profile?id=~Xiang_Wang6)
  - **Affiliations:** University of Science and Technology of China, Hefei, China; None, Alibaba Group, Hangzhou, China, Alibaba Group, Hangzhou, China, University of Science and Technology of China, Hefei, China; None, Alibaba Group, Hangzhou, China, Alibaba Group, Hangzhou, China, University of Science and Technology of China, Hefei, China; Institute of Dataspace, Hefei Comprehensive National Science Center
  - **TL;DR:** The study introduces Auctionformer, a transformer-based method designed to efficiently solve equilibrium strategies in diverse auction games. It demonstrates superior performance compared to traditional methods, addressing the complexities of real-world auction scenarios.
  - **Keywords:** auction games, equilibrium strategies, deep learning, transformer-based method, Nash error, few-shot framework, self-supervised fine-tuning, online advertising, real estate, trading environments, complexity of real-world scenarios, bidder asymmetries, limitations of traditional learning approaches, Auctionformer model, efficient training and inference, superior performance over contemporary methods, Transformer, Nash equilibrium


- [Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL](https://icml.cc/virtual/2024/poster/35000) (Poster)
  - **Authors:** [Jiawei Huang](http://openreview.net/profile?id=~Jiawei_Huang3), [Niao He](http://openreview.net/profile?id=~Niao_He3), [Andreas Krause](http://openreview.net/profile?id=~Andreas_Krause1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Department of Computer Science, ETH Zurich, Department of Computer Science, ETH Zurich
  - **TL;DR:** This study investigates the sample complexity of model-based reinforcement learning in Mean-Field Games, introducing the Partial Model-Based Eluder Dimension (P-MBED) to characterize model complexity. The findings indicate that learning Nash Equilibrium in MFGs is statistically comparable to solving a logarithmic number of single-agent RL problems, suggesting broader implications for multi-agent systems.
  - **Keywords:** Mean-Field Games, Reinforcement Learning, Model-Based Function Approximation, Nash Equilibrium, Partial Model-Based Eluder Dimension (P-MBED), Financial Markets, Economics, Energy Management, Sample Complexity, Strategic Exploration, Learning Nash Equilibrium, Model Elimination Algorithm, Exploration Strategy, Multi-Agent Reinforcement Learning (MARL), Markov Games


- [Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning](https://icml.cc/virtual/2024/poster/34263) (Poster)
  - **Authors:** [Zhuo Huang](http://openreview.net/profile?id=~Zhuo_Huang2), [Chang Liu](http://openreview.net/profile?id=~Chang_Liu17), [Yinpeng Dong](http://openreview.net/profile?id=~Yinpeng_Dong2), [Hang Su](http://openreview.net/profile?id=~Hang_Su3), [Shibao Zheng](http://openreview.net/profile?id=~Shibao_Zheng1), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1)
  - **Affiliations:** Sydney AI Centre, The University of Sydney, Sydney, Australia, Institute of Image Communication and Network Engineering, Shanghai JiaoTong University, Shanghai, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Institute of Image Communication and Network Engineering, Shanghai JiaoTong University, Shanghai, China, Sydney AI Centre, The University of Sydney, Sydney, Australia; Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China
  - **TL;DR:** This study introduces Machine Vision Therapy, leveraging Multi-modal Large Language Models to enhance the robustness of vision models against Out-of-Distribution scenarios. The proposed Denoising In-Context Learning strategy effectively rectifies erroneous predictions, demonstrating significant improvements in visual robustness.
  - **Keywords:** Visual robustness, Out-of-Distribution (OOD) scenarios, Multi-modal Large Language Models (MLLMs), Denoising In-Context Learning (DICL), Contrastive Language-Image Pre-Training (CLIP), Computer vision, Image recognition, Task incompatibility, Limited robustness under OOD scenarios, Enhanced visual robustness, Rectification of erroneous predictions, OOD datasets, Machine Vision Therapy


- [An Empirical Examination of Balancing Strategy for Counterfactual Estimation on Time Series](https://icml.cc/virtual/2024/poster/34183) (Poster)
  - **Authors:** [Qiang Huang](http://openreview.net/profile?id=~Qiang_Huang4), [Chuizheng Meng](http://openreview.net/profile?id=~Chuizheng_Meng1), [Defu Cao](http://openreview.net/profile?id=~Defu_Cao1), [Biwei Huang](http://openreview.net/profile?id=~Biwei_Huang1), [Yi Chang](http://openreview.net/profile?id=~Yi_Chang4), [Yan Liu](http://openreview.net/profile?id=~Yan_Liu1)
  - **Affiliations:** School of Artificial Intelligence, Jilin University, Changchun, Jilin, China; International Center of Future Science, Jilin University, Changchun, Jilin, China, Department of Computer Science, University of Southern California, California, Los Angeles, United States, Department of Computer Science, University of Southern California, California, Los Angeles, United States, Halicio˘glu Data Science Institute, University of California San Diego, San Diego, California, United States, School of Artificial Intelligence, Jilin University, Changchun, Jilin, China; International Center of Future Science, Jilin University, Changchun, Jilin, China; Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, Changchun, Jilin, China, Department of Computer Science, University of Southern California, California, Los Angeles, United States
  - **TL;DR:** This paper investigates the effectiveness of balancing strategies for counterfactual estimation in time series data, addressing treatment bias and covariate disparities. The findings highlight the need for a reexamination of these strategies in temporal settings, providing insights for researchers and practitioners in various fields.
  - **Keywords:** Counterfactual estimation, Balancing strategies, Treatment bias, Inverse Probability of Treatment Weighting (IPTW), G-computation, Matching, Stratification, Healthcare, Finance, E-commerce, Treatment bias, Covariate disparities, Confounding variables, Effectiveness of balancing strategies, Empirical examination, Temporal counterfactual outcome estimation


- [Quasi-Monte Carlo Features for Kernel Approximation](https://icml.cc/virtual/2024/poster/33426) (Spotlight Poster)
  - **Authors:** [ZHEN HUANG](http://openreview.net/profile?id=~Zhen_Huang7), [Jiajin Sun](http://openreview.net/profile?id=~Jiajin_Sun1), [Yian Huang](http://openreview.net/profile?id=~Yian_Huang1)
  - **Affiliations:** Department of Statistics, Columbia University, New York, NY 10027, USA, Department of Statistics, Columbia University, New York, NY 10027, USA, Department of Statistics, Columbia University, New York, NY 10027, USA
  - **TL;DR:** This paper explores the use of quasi-Monte Carlo methods to enhance kernel approximation techniques, demonstrating that they can significantly reduce approximation error compared to traditional Monte Carlo methods. The findings indicate that fewer random features are needed for kernel ridge regression while maintaining the same convergence rate of excess risk, showcasing superior performance in practical applications.
  - **Keywords:** kernel methods, kernel approximation, quasi-Monte Carlo (QMC), Monte Carlo (MC), random features, kernel ridge regression, approximation error, convergence rate, computational complexity, improved approximation error, empirical evidence of performance, Gaussian kernels, shift invariant kernels


- [On Which Nodes Does GCN Fail? Enhancing GCN From the Node Perspective](https://icml.cc/virtual/2024/poster/33543) (Poster)
  - **Authors:** [Jincheng Huang](http://openreview.net/profile?id=~Jincheng_Huang1), [Jialie SHEN](http://openreview.net/profile?id=~Jialie_Shen2), [Xiaoshuang Shi](http://openreview.net/profile?id=~Xiaoshuang_Shi1), [Xiaofeng Zhu](http://openreview.net/profile?id=~Xiaofeng_Zhu7)
  - **Affiliations:** School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Sichuan Artificial Intelligence Research Institute, Yibin, China; Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China, Shenzhen, China, Department of Computer Science, City, University of London, London, United Kingdom, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Sichuan Artificial Intelligence Research Institute, Yibin, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China, Shenzhen, China
  - **TL;DR:** This study investigates the limitations of Graph Convolutional Networks (GCNs) in handling nodes that do not conform to the label smoothness assumption, introducing a new framework called DaGCN to enhance representation learning for these out-of-control nodes. The findings indicate that while advanced GCNs improve performance on these problematic nodes, the remaining nodes are already optimally represented by standard GCNs.
  - **Keywords:** Graph Convolutional Networks (GCNs), label smoothness assumption, GCN-smoothing based methods, GCN-smoothing enhanced methods, feature smoothing operation, Graph mining, healthcare, recommender systems, natural language processing, Out of GCN’s control (OOC nodes), label smoothness assumption conflicts, DaGCN framework, algorithm to locate OOC nodes


- [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](https://icml.cc/virtual/2024/poster/32992) (Poster)
  - **Authors:** [Wei Huang](http://openreview.net/profile?id=~Wei_Huang36), [Yangdong Liu](http://openreview.net/profile?id=~Yangdong_Liu2), [Haotong Qin](http://openreview.net/profile?id=~Haotong_Qin1), [Ying Li](http://openreview.net/profile?id=~Ying_Li22), [Shiming Zhang](http://openreview.net/profile?email=szhang%40eee.hku.hk), [Xianglong Liu](http://openreview.net/profile?id=~Xianglong_Liu3), [Michele Magno](http://openreview.net/profile?id=~Michele_Magno1), [XIAOJUAN QI](http://openreview.net/profile?id=~XIAOJUAN_QI2)
  - **Affiliations:** The University of Hong Kong, Beihang University, ETH Zürich, Beihang University, The University of Hong Kong, Beihang University, ETH Zürich, The University of Hong Kong
  - **TL;DR:** This paper introduces BiLLM, a novel 1-bit post-training quantization scheme for large language models that significantly reduces memory and computational requirements while maintaining high accuracy. BiLLM achieves impressive results, outperforming existing quantization methods and enabling efficient binarization of large models.
  - **Keywords:** Large Language Models, Model Compression, Post-Training Quantization, Binarization, Binary Residual Approximation, Natural Language Processing, Memory Constraints, Computational Resource Demands, High-Accuracy Inference, Efficient Binarization Process, LLaMA2-70B, WikiText2, State-of-the-Art (SOTA) Quantization Methods


- [How Universal Polynomial Bases Enhance Spectral Graph Neural Networks: Heterophily, Over-smoothing, and Over-squashing](https://icml.cc/virtual/2024/poster/33747) (Poster)
  - **Authors:** [Keke Huang](http://openreview.net/profile?id=~Keke_Huang1), [Yu Guang Wang](http://openreview.net/profile?id=~Yu_Guang_Wang1), [Ming Li](http://openreview.net/profile?id=~Ming_Li15), [Pietro Lió](http://openreview.net/profile?id=~Pietro_Lio1)
  - **Affiliations:** School of Computing, National University of Singapore, Singapore, Institute of Natural Sciences, School of Mathematical Sciences, Zhangjiang Institute for Advanced Study, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China; School of Mathematics and Statistics, University of New South Wales, Sydney, Australia, Zhejiang Institute of Optoelectronics, Jinhua, China; Zhejiang Key Laboratory of Intelligent Education Technology and Application, Zhejiang Normal University, Jinhua, China, Department of Computer Science and Technology, Cambridge University, Cambridge, UK
  - **TL;DR:** This study introduces a universal polynomial basis (UniBasis) for spectral graph neural networks that adapts to varying degrees of heterophily, effectively addressing issues of over-smoothing and over-squashing. The proposed method demonstrates superior performance and graph explanation capabilities across diverse datasets.
  - **Keywords:** Spectral Graph Neural Networks, Heterophily, Graph Filters, Polynomial Filters, Laplacian Eigendecomposition, Adaptive Heterophily Basis, Universal Polynomial Basis (UniBasis), Graph Neural Networks, Machine Learning, Over-smoothing, Over-squashing, Varying Degrees of Heterophily, UniFilter, Optimization of Convolution and Propagation in GNNs


- [MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation](https://icml.cc/virtual/2024/poster/35159) (Poster)
  - **Authors:** [Qian Huang](http://openreview.net/profile?id=~Qian_Huang2), [Jian Vora](http://openreview.net/profile?id=~Jian_Vora1), [Percy Liang](http://openreview.net/profile?id=~Percy_Liang1), [Jure Leskovec](http://openreview.net/profile?id=~Jure_Leskovec1)
  - **Affiliations:** Stanford University, Stanford University, Stanford University, Stanford University
  - **TL;DR:** This paper introduces MLAgentBench, a benchmark for evaluating language agents in machine learning experimentation, demonstrating that a Claude v3 Opus agent achieves a 37.5% success rate across various tasks. The study highlights challenges such as long-term planning and hallucination in language model-based agents.
  - **Keywords:** machine learning experimentation, automation in ML, ReAct framework, language models, long-term planning, reducing hallucination, MLAgentBench, performance metric evaluation, CIFAR-10, Kaggle


- [Triadic-OCD: Asynchronous Online Change Detection with Provable Robustness, Optimality, and Convergence](https://icml.cc/virtual/2024/poster/34141) (Poster)
  - **Authors:** [Yancheng Huang](http://openreview.net/profile?id=~Yancheng_Huang1), [Kai Yang](http://openreview.net/profile?id=~Kai_Yang3), [Zelin Zhu](http://openreview.net/profile?id=~Zelin_Zhu1), [Leian Chen](http://openreview.net/profile?id=~Leian_Chen1)
  - **Affiliations:** Tongji University, Tongji University, Tongji University, Columbia University; Amazon
  - **TL;DR:** This paper presents the Triadic-OCD framework for online change detection that ensures robustness and optimality in the presence of parameter uncertainties. The proposed asynchronous algorithm effectively addresses challenges such as privacy concerns and straggler issues, enhancing detection efficiency in dynamic systems.
  - **Keywords:** online change detection, data stream analysis, triadic-OCD framework, asynchronous distributed algorithm, security detection in smart grids, intrusion detection in communication networks, parameter uncertainties, estimation errors, straggler issues, certifiable robustness, provable optimality, guaranteed convergence


- [CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks](https://icml.cc/virtual/2024/poster/32664) (Spotlight Poster)
  - **Authors:** [Yulong Huang](http://openreview.net/profile?id=~Yulong_Huang2), [Xiaopeng LIN](http://openreview.net/profile?id=~Xiaopeng_LIN1), [Hongwei Ren](http://openreview.net/profile?id=~Hongwei_Ren2), [Haotian FU](http://openreview.net/profile?id=~Haotian_FU4), [Yue Zhou](http://openreview.net/profile?id=~Yue_Zhou8), [Zunchang LIU](http://openreview.net/profile?id=~Zunchang_LIU2), [biao pan](http://openreview.net/profile?id=~biao_pan1), [Bojun Cheng](http://openreview.net/profile?id=~Bojun_Cheng1)
  - **Affiliations:** Function Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China, Function Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China, Function Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China, Function Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China, Function Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China, Function Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China, School of Integrated Circuit Science and Engineering, Beihang University, Beijing, China, Function Hub, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China
  - **TL;DR:** This study introduces the Complementary Leaky Integrate-and-Fire (CLIF) Neuron to enhance the training of Spiking Neural Networks (SNNs) by addressing the challenges posed by their undifferentiable spiking mechanism. The proposed CLIF demonstrates superior performance compared to traditional neuron models and even slightly outperforms conventional Artificial Neural Networks (ANNs) under identical conditions.
  - **Keywords:** Spiking Neural Networks (SNNs), energy-efficient models, temporal information processing, Leaky Integrate-and-Fire (LIF) Neuron, Complementary Leaky Integrate-and-Fire (CLIF) Neuron, surrogate gradients method, Edge device applications, real-time applications, Training challenges of SNNs, undifferentiable spiking mechanism, accuracy disadvantage, Performance advantage of CLIF over other neuron models, hyperparameter-free design


- [MFTN: A Multi-scale Feature Transfer Network Based on IMatchFormer for Hyperspectral Image Super-Resolution](https://icml.cc/virtual/2024/poster/34390) (Poster)
  - **Authors:** [Shuying Huang](http://openreview.net/profile?id=~Shuying_Huang2), [Mingyang Ren](http://openreview.net/profile?id=~Mingyang_Ren1), [Yong Yang](http://openreview.net/profile?id=~Yong_Yang5), [Xiaozheng Wang](http://openreview.net/profile?id=~Xiaozheng_Wang2), [Yingzhi Wei](http://openreview.net/profile?id=~Yingzhi_Wei1)
  - **Affiliations:** School of Software, Tiangong University, Tianjin, China, School of Computer Science and Technology, Tiangong University, Tianjin, China, School of Computer Science and Technology, Tiangong University, Tianjin, China; School of Control Science and Engineering, Tiangong University, Tianjin, China, School of Control Science and Engineering, Tiangong University, Tianjin, China, School of Computer Science and Technology, Tiangong University, Tianjin, China
  - **TL;DR:** This study presents a multi-scale feature transfer network (MFTN) for hyperspectral image super-resolution (HISR) that effectively fuses low-resolution hyperspectral images with high-resolution multispectral images to produce high-resolution hyperspectral images. The proposed method addresses issues of spectral distortion and spatial blurring, demonstrating superior performance over existing state-of-the-art methods.
  - **Keywords:** Hyperspectral image super-resolution (HISR), Remote sensing, Multi-scale feature transfer network (MFTN), Improved feature matching Transformers (IMatchFormers), Multi-scale feature transfer module (MFTM), Multiscale dynamic aggregation module (MDAM), Spectral aware aggregation modules (SAAMs), Mineral exploration, Plant detection, Land cover analysis, Spectral distortion, Blurring of spatial texture, Limited spatial information from single LR-HSI, Better performance compared to state-of-the-art (SOTA) methods


- [Interaction-based Retrieval-augmented Diffusion Models for Protein-specific 3D Molecule Generation](https://icml.cc/virtual/2024/poster/33484) (Poster)
  - **Authors:** [Zhilin Huang](http://openreview.net/profile?id=~Zhilin_Huang1), [Ling Yang](http://openreview.net/profile?id=~Ling_Yang1), [Xiangxin Zhou](http://openreview.net/profile?id=~Xiangxin_Zhou1), [Chujun Qin](http://openreview.net/profile?id=~Chujun_Qin1), [Yijie Yu](http://openreview.net/profile?id=~Yijie_Yu1), [Xiawu Zheng](http://openreview.net/profile?id=~Xiawu_Zheng1), [Zikun Zhou](http://openreview.net/profile?id=~Zikun_Zhou2), [Wentao Zhang](http://openreview.net/profile?id=~Wentao_Zhang1), [Yu Wang](http://openreview.net/profile?id=~Yu_Wang60), [Wenming Yang](http://openreview.net/profile?id=~Wenming_Yang1)
  - **Affiliations:** Shenzhen International Graduate School, Tsinghua University; Peng Cheng Laboratory, Peking University, School of Artificial Intelligence, University of Chinese Academy of Sciences, China Southern Power Grid, Shenzhen International Graduate School, Tsinghua University; Peng Cheng Laboratory, Peng Cheng Laboratory; Xiamen University, Peng Cheng Laboratory, Peking University, Peng Cheng Laboratory, Shenzhen International Graduate School, Tsinghua University; Peng Cheng Laboratory
  - **TL;DR:** This study introduces IRDIFF, an interaction-based retrieval-augmented diffusion model designed to generate ligand molecules that bind to specific protein targets, enhancing structure-based drug design. The model demonstrates improved binding affinities and realistic 3D structures compared to existing methods.
  - **Keywords:** structure-based drug design (SBDD), ligand generation, interaction-based retrieval-augmented diffusion model, geometric protein-molecule interaction network (PMINet), drug discovery, molecular generation, challenges in model optimization, suboptimal outcomes in ligand generation, IRDIFF model, improved binding affinities, realistic 3D structures, CrossDocked2020 dataset


- [Make-A-Shape: a Ten-Million-scale 3D Shape Model](https://icml.cc/virtual/2024/poster/34835) (Poster)
  - **Authors:** [Ka-Hei Hui](http://openreview.net/profile?id=~Ka-Hei_Hui1), [Aditya Sanghi](http://openreview.net/profile?id=~Aditya_Sanghi1), [Arianna Rampini](http://openreview.net/profile?id=~Arianna_Rampini1), [Kamal Rahimi Malekshan](http://openreview.net/profile?id=~Kamal_Rahimi_Malekshan1), [Zhengzhe Liu](http://openreview.net/profile?id=~Zhengzhe_Liu1), [Hooman Shayani](http://openreview.net/profile?id=~Hooman_Shayani1), [Chi-Wing Fu](http://openreview.net/profile?id=~Chi-Wing_Fu2)
  - **Affiliations:** The Chinese University of Hong Kong, Hong Kong SAR, China, Autodesk Research, Autodesk Research, Autodesk Research, The Chinese University of Hong Kong, Hong Kong SAR, China, Autodesk Research, The Chinese University of Hong Kong, Hong Kong SAR, China; Autodesk Research
  - **TL;DR:** This paper presents Make-A-Shape, a novel 3D generative model trained on 10 million diverse shapes, addressing challenges in resource requirements and representation inefficiencies. The model demonstrates superior performance in generating high-quality 3D shapes quickly and supports various input modalities for diverse applications.
  - **Keywords:** 3D generative models, large-scale training, wavelet-tree representation, subband coefficient filtering, diffusion-based generation, unconditional generation, completion, conditional generation, resource requirements for training, inefficient representations, high-dimensional data, novel 3D generative model, effective training strategies, 10 million publicly-available shapes, SDF (Signed Distance Function), U-Net-based diffusion models


- [Faster Adaptive Decentralized Learning Algorithms](https://icml.cc/virtual/2024/poster/34606) (Spotlight Poster)
  - **Authors:** [Feihu Huang](http://openreview.net/profile?id=~Feihu_Huang1), [jianyu zhao](http://openreview.net/profile?id=~jianyu_zhao4)
  - **Affiliations:** College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, China, College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China
  - **TL;DR:** This study introduces faster adaptive decentralized algorithms (AdaMDOS and AdaMDOF) to address high sample complexity in decentralized learning for nonconvex optimization problems. The proposed methods achieve near-optimal sample complexities and demonstrate efficiency through experimental results.
  - **Keywords:** Decentralized learning, Adaptive gradient methods, AdaMDOS, AdaMDOF, decentralized optimization algorithms, Distributed nonconvex stochastic optimization, distributed finite-sum optimization, High sample complexity, data privacy, communication load, Near-optimal sample complexity, convergence analysis framework, Nonconvex functions, stochastic optimization


- [Residual Quantization with Implicit Neural Codebooks](https://icml.cc/virtual/2024/poster/34212) (Poster)
  - **Authors:** [Iris Huijben](http://openreview.net/profile?id=~Iris_A.M._Huijben1), [Matthijs Douze](http://openreview.net/profile?id=~Matthijs_Douze1), [Matthew Muckley](http://openreview.net/profile?id=~Matthew_J._Muckley1), [Ruud J. G. van Sloun](http://openreview.net/profile?id=~Ruud_Van_Sloun1), [Jakob Verbeek](http://openreview.net/profile?id=~Jakob_Verbeek1)
  - **Affiliations:** Eindhoven University of Technology; FAIR at Meta, FAIR at Meta, FAIR at Meta, Eindhoven University of Technology, FAIR at Meta
  - **TL;DR:** This paper introduces QINCo, a neural variant of residual quantization that constructs specialized codebooks for each quantization step, significantly improving accuracy in vector compression and search tasks. The proposed method outperforms existing state-of-the-art techniques on multiple datasets, demonstrating its effectiveness in reducing mean-squared error.
  - **Keywords:** Vector quantization, Data compression, Vector search, Residual quantization (RQ), Neural networks, Multi-codebook methods, Similarity search, Recommender systems, High distortion in shorter codes, Dependency on previously-selected codewords, QINCo method, Improved nearest-neighbor search accuracy, BigANN1M, Deep1M


- [Nash Incentive-compatible Online Mechanism Learning via Weakly Differentially Private Online Learning](https://icml.cc/virtual/2024/poster/34083) (Poster)
  - **Authors:** [Joon Suk Huh](http://openreview.net/profile?id=~Joon_Suk_Huh1), [Kirthevasan Kandasamy](http://openreview.net/profile?id=~Kirthevasan_Kandasamy1)
  - **Affiliations:** Department of Computer Science, University of Wisconsin–Madison, Madison, WI, USA, Department of Computer Science, University of Wisconsin–Madison, Madison, WI, USA
  - **TL;DR:** This study presents an incentive-compatible online learning scheme for multi-round mechanism design that maximizes application-specific objectives without prior knowledge of agents' type distributions. The proposed method achieves O(T^(1+h/2)) regret in an adversarial setting, addressing the challenges posed by strategic agents who may misreport their types.
  - **Keywords:** mechanism design, online learning, incentive compatibility, weakly differentially private online learning, Hedge algorithm, multi-round mechanism design, strategic agents, type misreporting, O(T^(1+h/2)) regret, no-regret learning procedures, incentive-compatible (IC), commitment mechanism, adversarial setting


- [Near-Linear Time Approximation Algorithms for k-means with Outliers](https://icml.cc/virtual/2024/poster/34591) (Poster)
  - **Authors:** [Junyu Huang](http://openreview.net/profile?id=~Junyu_Huang1), [Qilong Feng](http://openreview.net/profile?id=~Qilong_Feng1), [Ziyun Huang](http://openreview.net/profile?id=~Ziyun_Huang1), [Jinhui Xu](http://openreview.net/profile?id=~Jinhui_Xu1), [Jianxin Wang](http://openreview.net/profile?id=~Jianxin_Wang1)
  - **Affiliations:** School of Computer Science and Engineering, Central South University, Changsha 410083, China; Xiangjiang Laboratory, Changsha 410205, China, School of Computer Science and Engineering, Central South University, Changsha 410083, China; Xiangjiang Laboratory, Changsha 410205, China, Department of Computer Science and Software Engineering, Penn State Erie, The Behrend College, Department of Computer Science and Engineering, State University of New York at Buffalo, NY, USA, School of Computer Science and Engineering, Central South University, Changsha 410083, China; Xiangjiang Laboratory, Changsha 410205, China; The Hunan Provincial Key Lab of Bioinformatics, Central South University, Changsha 410083, China
  - **TL;DR:** This study presents near-linear time approximation algorithms for the k-means clustering problem with outliers, proposing a Fast-Sampling algorithm that effectively identifies inliers and reduces the number of centers opened. The empirical results indicate that these algorithms outperform existing state-of-the-art methods in handling outliers in clustering tasks.
  - **Keywords:** k-means clustering, outlier detection, Fast-Sampling algorithm, sampling-based algorithms, Machine learning, data analysis, Handling outliers in clustering, minimizing clustering cost, Near-linear time approximation algorithms, O(1/ε)-approximation solutions


- [Theoretical Guarantees for Variational Inference with Fixed-Variance Mixture of Gaussians](https://icml.cc/virtual/2024/poster/33380) (Poster)
  - **Authors:** [Tom Huix](http://openreview.net/profile?id=~Tom_Huix1), [Anna Korba](http://openreview.net/profile?id=~Anna_Korba2), [Alain Oliviero Durmus](http://openreview.net/profile?id=~Alain_Oliviero_Durmus1), [Eric Moulines](http://openreview.net/profile?id=~Eric_Moulines1)
  - **Affiliations:** CMAP, Ecole polytechnique, ENSAE, CREST, IP Paris, CMAP, Ecole polytechnique, CMAP, Ecole polytechnique
  - **TL;DR:** This paper investigates the theoretical properties of variational inference in the context of fixed-variance mixtures of Gaussians, focusing on the optimization of the positions of Diracs to minimize the Kullback-Leibler divergence. The study establishes descent results for the optimization process and provides bounds on the approximation error relative to the target distribution.
  - **Keywords:** Variational Inference, Bayesian Inference, Kullback-Leibler Divergence, Mollified Relative Entropy, Gradient Descent, Interacting Particle System, Approximation Error, Optimization Error, High-Dimensional Data, Theoretical Guarantees, Optimization Results, Mixture of Gaussians, Fixed Covariance, Atomic Measure, Diracs


- [In-context Convergence of Transformers](https://icml.cc/virtual/2024/poster/34813) (Poster)
  - **Authors:** [Yu Huang](http://openreview.net/profile?id=~Yu_Huang3), [Yuan Cheng](http://openreview.net/profile?id=~Yuan_Cheng6), [Yingbin LIANG](http://openreview.net/profile?id=~Yingbin_Liang1)
  - **Affiliations:** Department of Statistics and Data Science, Wharton School, University of Pennsylvania, Philadelphia, PA, USA, National University of Singapore, Singapore, Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA
  - **TL;DR:** This study investigates the learning dynamics of a one-layer transformer with softmax attention, focusing on its in-context learning capabilities for linear function classes. The findings reveal that the model converges to near-zero prediction error in a stage-wise manner, particularly highlighting the impact of feature representation balance on learning dynamics.
  - **Keywords:** In-context learning, Transformers, Softmax attention, Gradient descent, Natural language processing, Computer vision, Reinforcement learning, Learning dynamics, Prediction error, Feature imbalance, Finite-time convergence guarantee, Stage-wise convergence process, Linear function classes, Attention map


- [Faster Sampling via Stochastic Gradient Proximal Sampler](https://icml.cc/virtual/2024/poster/34874) (Poster)
  - **Authors:** [Xunpeng Huang](http://openreview.net/profile?id=~Xunpeng_Huang2), [Difan Zou](http://openreview.net/profile?id=~Difan_Zou1), [Hanze Dong](http://openreview.net/profile?id=~Hanze_Dong1), [Yian Ma](http://openreview.net/profile?id=~Yian_Ma1), [Tong Zhang](http://openreview.net/profile?id=~Tong_Zhang2)
  - **Affiliations:** The Hong Kong University of Science and Technology, The University of Hong Kong, Salesforce AI Research, University of California, San Diego, University of Illinois Urbana-Champaign
  - **TL;DR:** This paper introduces Stochastic Proximal Samplers (SPS) for efficient sampling from non-log-concave distributions, demonstrating that SPS-SGLD and SPS-MALA achieve improved convergence rates compared to existing methods. The proposed algorithms significantly enhance sampling efficiency, as validated by empirical studies.
  - **Keywords:** Stochastic sampling, Proximal sampling, Stochastic Proximal Samplers (SPS), Stochastic Gradient Langevin Dynamics (SGLD), Metropolis-adjusted Langevin Algorithm (MALA), Large-scale sampling problems, Non-log-concave distributions, Convergence to target distribution, Sampling efficiency, Enhanced convergence rates, Gradient complexities


- [Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion](https://icml.cc/virtual/2024/poster/33440) (Oral)
  - **Authors:** [Yujia Huang](http://openreview.net/profile?id=~Yujia_Huang1), [Adishree Ghatare](http://openreview.net/profile?id=~Adishree_Ghatare1), [Yuanzhe Liu](http://openreview.net/profile?id=~Yuanzhe_Liu1), [ziniu hu](http://openreview.net/profile?id=~Ziniu_Hu1), [Qinsheng Zhang](http://openreview.net/profile?id=~Qinsheng_Zhang1), [Chandramouli Shama Sastry](http://openreview.net/profile?id=~Chandramouli_Shama_Sastry1), [Siddharth Gururani](http://openreview.net/profile?id=~Siddharth_Gururani1), [Sageev Oore](http://openreview.net/profile?id=~Sageev_Oore1), [Yisong Yue](http://openreview.net/profile?id=~Yisong_Yue1)
  - **Affiliations:** California Institute of Technology, California Institute of Technology, Rensselaer Polytechnic Institute, California Institute of Technology, NVIDIA; Dalhousie University; Vector Institute, Dalhousie University; Vector Institute, NVIDIA; Vector Institute, Dalhousie University; Vector Institute, California Institute of Technology
  - **TL;DR:** This study introduces Stochastic Control Guidance (SCG) for symbolic music generation, addressing the challenges of non-differentiable musical rules. The proposed method enhances music quality and rule-based controllability in generative models, outperforming existing state-of-the-art approaches.
  - **Keywords:** symbolic music generation, rule-based controllability, Stochastic Control Guidance (SCG), diffusion models, music composition, generative modeling, non-differentiable rules, integration of multiple musical rules, training-free guidance, advancements in music quality


- [An Embodied Generalist Agent in 3D World](https://icml.cc/virtual/2024/poster/33925) (Poster)
  - **Authors:** [Jiangyong Huang](http://openreview.net/profile?id=~Jiangyong_Huang1), [Silong Yong](http://openreview.net/profile?id=~Silong_Yong1), [Xiaojian Ma](http://openreview.net/profile?id=~Xiaojian_Ma1), [Xiongkun Linghu](http://openreview.net/profile?id=~Xiongkun_Linghu1), [Puhao Li](http://openreview.net/profile?id=~Puhao_Li1), [Yan Wang](http://openreview.net/profile?id=~Yan_Wang24), [Qing Li](http://openreview.net/profile?id=~Qing_Li1), [Song-Chun Zhu](http://openreview.net/profile?id=~Song-Chun_Zhu1), [Baoxiong Jia](http://openreview.net/profile?id=~Baoxiong_Jia1), [Siyuan Huang](http://openreview.net/profile?id=~Siyuan_Huang2)
  - **Affiliations:** State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); Peking University, State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); Tsinghua University, State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); Tsinghua University, State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI); Peking University; Tsinghua University, State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), State Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)
  - **TL;DR:** The study introduces LEO, an embodied multimodal generalist agent designed to perceive, ground, reason, plan, and act in the 3D world, addressing significant limitations of existing models in 3D understanding. Extensive experiments demonstrate LEO's proficiency across various tasks, providing insights for the development of future embodied generalist agents.
  - **Keywords:** embodied generalist agent, 3D world, general intelligence, 3D vision-language (VL) alignment, 3D vision-language-action (VLA) instruction tuning, multi-modal Transformer, computer vision, robotics, 3D captioning, navigation, manipulation, limited capacity for 3D input, challenges in 3D grounding, embodied reasoning, and acting, LEO agent, high-quality 3D VL data, insights for future embodied generalist agents, large-scale datasets, LLM-assisted pipeline


- [Bayesian Power Steering: An Effective Approach for Domain Adaptation of Diffusion Models](https://icml.cc/virtual/2024/poster/34089) (Poster)
  - **Authors:** [Ding Huang](http://openreview.net/profile?id=~Ding_Huang1), [Ting Li](http://openreview.net/profile?id=~Ting_Li4), [Jian Huang](http://openreview.net/profile?id=~Jian_Huang5)
  - **Affiliations:** Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong, China, Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong, China, Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong, China
  - **TL;DR:** This study introduces a Bayesian framework called Bayesian Power Steering (BPS) for fine-tuning large diffusion models, effectively adapting them from a large to a small probability space. The proposed method demonstrates superior performance with limited data, achieving an FID score of 10.49 on the COCO17 dataset.
  - **Keywords:** Domain adaptation, Bayesian framework, Bayesian Power Steering (BPS), fine-tuning, large diffusion models, Computer vision, generative models, Data sparsity, transition from large to small probability space, Improved FID score, task-specific knowledge extraction, COCO17 dataset, LAION-5B dataset, Diffusion models, probability measures


- [Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control](https://icml.cc/virtual/2024/poster/34652) (Poster)
  - **Authors:** [Dongyoon Hwang](http://openreview.net/profile?id=~Dongyoon_Hwang1), [Byungkun Lee](http://openreview.net/profile?id=~Byungkun_Lee1), [Hojoon Lee](http://openreview.net/profile?id=~Hojoon_Lee1), [Hyunseung Kim](http://openreview.net/profile?id=~Hyunseung_Kim1), [Jaegul Choo](http://openreview.net/profile?id=~Jaegul_Choo1)
  - **Affiliations:** Kim Jaechul Graduate School of AI, KAIST, Kim Jaechul Graduate School of AI, KAIST, Kim Jaechul Graduate School of AI, KAIST, Kim Jaechul Graduate School of AI, KAIST, Kim Jaechul Graduate School of AI, KAIST
  - **TL;DR:** This study introduces the Convolution Injector (CoIn) to enhance the adaptation of pretrained Vision Transformers (ViTs) for visuo-motor control tasks by injecting convolutional inductive biases. The results demonstrate consistent performance improvements across various control tasks and pretrained ViTs, validating the effectiveness of this approach.
  - **Keywords:** visuo-motor control, Vision Transformers (ViT), Convolution Injector (CoIn), pretrained ViTs, robotics, computer vision, adaptation challenges for visuo-motor control, lack of control-centric inductive biases, performance enhancement in control tasks, introduction of convolutional inductive biases, CLIP, MVP, VC-1, Adroit, MetaWorld, DMC, spatial locality, translation equivariance


- [Vanilla Bayesian Optimization Performs Great in High Dimensions](https://icml.cc/virtual/2024/poster/34152) (Poster)
  - **Authors:** [Carl Hvarfner](http://openreview.net/profile?id=~Carl_Hvarfner1), [Erik Hellsten](http://openreview.net/profile?id=~Erik_Orm_Hellsten1), [Luigi Nardi](http://openreview.net/profile?id=~Luigi_Nardi1)
  - **Affiliations:** Lund University, Lund, Sweden; DBTune, Malmö, Sweden, Lund University, Lund, Sweden; DBTune, Malmö, Sweden, Lund University, Lund, Sweden; DBTune, Malmö, Sweden
  - **TL;DR:** This paper addresses the challenges of vanilla Bayesian optimization in high dimensions by proposing a modification to the Gaussian process lengthscale prior, demonstrating that standard Bayesian optimization can perform significantly better than previously believed in high-dimensional tasks, outperforming existing algorithms.
  - **Keywords:** Bayesian optimization, high-dimensional optimization, Gaussian process, model complexity reduction, Curse of dimensionality, high-complexity, low-correlation issues, Enhanced Bayesian optimization algorithm, improved performance in high dimensions


- [Fine-Grained Causal Dynamics Learning with Quantization for Improving Robustness in Reinforcement Learning](https://icml.cc/virtual/2024/poster/33153) (Poster)
  - **Authors:** [Inwoo Hwang](http://openreview.net/profile?id=~Inwoo_Hwang1), [Yunhyeok Kwak](http://openreview.net/profile?id=~Yunhyeok_Kwak1), [Suhyung Choi](http://openreview.net/profile?id=~Suhyung_Choi2), [Byoung-Tak Zhang](http://openreview.net/profile?id=~Byoung-Tak_Zhang1), [Sanghack Lee](http://openreview.net/profile?id=~Sanghack_Lee1)
  - **Affiliations:** AI Institute, Seoul National University, AI Institute, Seoul National University, AI Institute, Seoul National University, AI Institute, Seoul National University; Graduate School of Data Science, Seoul National University, AI Institute, Seoul National University; Graduate School of Data Science, Seoul National University
  - **TL;DR:** This study proposes a novel dynamics model that infers fine-grained causal structures to enhance robustness in reinforcement learning by quantizing the state-action space into subgroups. Experimental results show that the method effectively addresses unseen states and locally spurious correlations, demonstrating its superiority over prior approaches.
  - **Keywords:** Causal dynamics learning, Robustness in reinforcement learning, Dynamics model, Discrete latent variable, Quantization, Reinforcement learning, Autonomous driving, Unseen states, Locally spurious correlations, Fine-grained causal reasoning, Improved robustness, Discovery of fine-grained causal relationships, Model-based reinforcement learning (MBRL), Sparse dependencies


- [InstructSpeech: Following Speech Editing Instructions via Large Language Models](https://icml.cc/virtual/2024/poster/32696) (Poster)
  - **Authors:** [Rongjie Huang](http://openreview.net/profile?id=~Rongjie_Huang1), [Ruofan Hu](http://openreview.net/profile?id=~Ruofan_Hu2), [Yongqi Wang](http://openreview.net/profile?id=~Yongqi_Wang1), [Zehan Wang](http://openreview.net/profile?id=~Zehan_Wang2), [xize cheng](http://openreview.net/profile?id=~Xize_Cheng1), [Ziyue Jiang](http://openreview.net/profile?id=~Ziyue_Jiang1), [Zhenhui Ye](http://openreview.net/profile?id=~Zhenhui_Ye1), [Dongchao Yang](http://openreview.net/profile?id=~Dongchao_Yang1), [Luping Liu](http://openreview.net/profile?id=~Luping_Liu2), [Peng Gao](http://openreview.net/profile?id=~Peng_Gao3), [Zhou Zhao](http://openreview.net/profile?id=~Zhou_Zhao3)
  - **Affiliations:** Zhejiang University, Zhejiang University, Zhejiang University, Zhejiang University, Zhejiang University, Zhejiang University, Zhejiang University, The Chinese University of Hong Kong, Zhejiang University, Shanghai AI Lab, Zhejiang University
  - **TL;DR:** This study introduces InstructSpeech, a novel model for instruction-guided speech editing that manipulates both semantic and acoustic attributes of speech. The model demonstrates state-of-the-art performance across eleven tasks by leveraging triplet paired data and advanced task embeddings to enhance instruction execution.
  - **Keywords:** instruction-guided speech editing, speech manipulation, large language model, Flan-T5-XL, triplet paired data, speech processing, speech quality enhancement, data scarcity, complexity of executing instructions, InstructSpeech model, state-of-the-art results in speech editing tasks


- [Position: The Platonic Representation Hypothesis](https://icml.cc/virtual/2024/poster/34734) (Oral)
  - **Authors:** [Minyoung Huh](http://openreview.net/profile?id=~Minyoung_Huh1), [Brian Cheung](http://openreview.net/profile?id=~Brian_Cheung1), [Tongzhou Wang](http://openreview.net/profile?id=~Tongzhou_Wang1), [Phillip Isola](http://openreview.net/profile?id=~Phillip_Isola1)
  - **Affiliations:** MIT, MIT, MIT, MIT
  - **TL;DR:** The paper posits that AI models, particularly deep networks, are converging towards a shared statistical representation of reality, driven by the scaling of model size and diversity of data and tasks. This convergence suggests a growing alignment in how different neural networks represent data across various modalities.
  - **Keywords:** representational convergence, AI models, deep networks, language processing, robotics, bioinformatics, healthcare, convergence of representations, alignment of neural networks, shared statistical model of reality, platonic representation, large language models (LLMs), foundation models


- [Understanding the Learning Dynamics of Alignment with Human Feedback](https://icml.cc/virtual/2024/poster/34438) (Poster)
  - **Authors:** [Shawn Im](http://openreview.net/profile?id=~Shawn_Im1), [Sharon Li](http://openreview.net/profile?id=~Yixuan_Li1)
  - **Affiliations:** Department of Computer Sciences, University of Wisconsin-Madison, Department of Computer Sciences, University of Wisconsin-Madison
  - **TL;DR:** This study theoretically analyzes the learning dynamics of aligning large language models with human feedback, focusing on how the distribution of preference datasets influences model updates. The findings reveal that higher preference distinguishability accelerates learning but may lead to the deprioritization of less distinguishable yet important behaviors.
  - **Keywords:** AI Alignment, Large Language Models, Reinforcement Learning from Human Preferences (RLHF), Direct Preference Optimization (DPO), Alignment with human preferences, undesirable outputs from models, Learning guarantees, preference distinguishability, Preference distinguishability, reward model


- [ReconBoost: Boosting Can Achieve Modality Reconcilement](https://icml.cc/virtual/2024/poster/34822) (Poster)
  - **Authors:** [Cong Hua](http://openreview.net/profile?id=~Cong_Hua2), [Qianqian Xu](http://openreview.net/profile?id=~Qianqian_Xu2), [Shilong Bao](http://openreview.net/profile?id=~Shilong_Bao1), [Zhiyong Yang](http://openreview.net/profile?id=~Zhiyong_Yang1), [Qingming Huang](http://openreview.net/profile?id=~Qingming_Huang1)
  - **Affiliations:** Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China, Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China, School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China, Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Big Data Mining and Knowledge Management, Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This study introduces ReconBoost, a novel multi-modal alternating learning paradigm aimed at reconciling the exploitation of uni-modal features with cross-modal interactions to mitigate modality competition. The proposed method demonstrates improved performance across multiple benchmarks by dynamically adjusting learning objectives and preserving only the newest model for each modality.
  - **Keywords:** multi-modal learning, reconciliation of modalities, ReconBoost, KL-based reconcilement, Gradient-Boosting, data mining, computer vision, medical diagnosis, modality competition, exploitation of uni-modal features, cross-modal interactions, new learning paradigm, memory consolidation scheme, global rectification scheme, CREMA-D dataset


- [Smooth Min-Max Monotonic Networks](https://icml.cc/virtual/2024/poster/33186) (Poster)
  - **Authors:** [Christian Igel](http://openreview.net/profile?id=~Christian_Igel1)
  - **Affiliations:** Department of Computer Science, University of Copenhagen, Copenhagen, Denmark
  - **TL;DR:** This study introduces a smooth min-max (SMM) network that modifies the traditional min-max neural network to overcome training challenges associated with local optima and zero partial derivatives. The SMM network maintains the ability to approximate monotonic functions while being computationally efficient and effective in generalization performance.
  - **Keywords:** Monotonicity constraints, Statistical modelling, Fairness in decision making, Min-max neural network, Smooth min-max network, Bio- and geophysical models, Data-driven scientific models, Local optima during training, Partial derivatives being zero, Underfitting, Smooth min-max network module, Asymptotic approximation properties


- [Zero-Shot Reinforcement Learning via Function Encoders](https://icml.cc/virtual/2024/poster/32872) (Poster)
  - **Authors:** [Tyler Ingebrand](http://openreview.net/profile?id=~Tyler_Ingebrand1), [Amy Zhang](http://openreview.net/profile?id=~Amy_Zhang1), [Ufuk Topcu](http://openreview.net/profile?id=~ufuk_topcu1)
  - **Affiliations:** University of Texas at Austin, University of Texas at Austin, University of Texas at Austin
  - **TL;DR:** This paper introduces the function encoder, a novel representation learning algorithm that enables zero-shot transfer in reinforcement learning by effectively representing tasks as weighted combinations of learned basis functions. The approach demonstrates significant improvements in data efficiency, performance, and stability across various reinforcement learning applications.
  - **Keywords:** Zero-shot transfer, Reinforcement learning, Function encoder, Representation learning, Autonomous robotics, Sequential decision making, Task representation, Transfer learning challenges, State-of-the-art data efficiency, Asymptotic performance, Training stability


- [PASOA- PArticle baSed Bayesian Optimal Adaptive design](https://icml.cc/virtual/2024/poster/32964) (Poster)
  - **Authors:** [Jacopo Iollo](http://openreview.net/profile?id=~Jacopo_Iollo1), [Christophe Heinkelé](http://openreview.net/profile?id=~Christophe_Heinkel%C3%A91), [Pierre Alliez](http://openreview.net/profile?id=~Pierre_Alliez1), [Florence Forbes](http://openreview.net/profile?id=~Florence_Forbes1)
  - **Affiliations:** Université Grenoble Alpes, Inria, CNRS, G-INP, France; None; None, Cerema, Endsum-Strasbourg, France, Université Côte d’Azur, Inria, France, Université Grenoble Alpes, Inria, CNRS, G-INP, France; None; None
  - **TL;DR:** The study introduces PASOA, a novel Bayesian experimental design procedure that optimizes sequential design while providing accurate posterior estimates. It combines stochastic optimization with tempered Sequential Monte Carlo to enhance performance and consistency in parameter inference.
  - **Keywords:** Bayesian experimental design, sequential design optimization, Stochastic optimization, Sequential Monte Carlo (SMC), Expected Information Gain (EIG), Intractable posterior distributions, SMC performance issues, Optimal design estimators, consistency property, Kullback-Leibler divergence, contrastive estimation, tempering


- [An Independence-promoting Loss for Music Generation with Language Models](https://icml.cc/virtual/2024/poster/34687) (Poster)
  - **Authors:** [Jean-Marie Lemercier](http://openreview.net/profile?id=~Jean-Marie_Lemercier1), [Simon Rouard](http://openreview.net/profile?email=srouard%40sfr.fr), [Jade Copet](http://openreview.net/profile?id=~Jade_Copet1), [Yossi Adi](http://openreview.net/profile?id=~Yossi_Adi1), [Alexandre Defossez](http://openreview.net/profile?id=~Alexandre_D%C3%A9fossez1)
  - **Affiliations:** Universität Hamburg, IRCAM; Meta AI, Meta AI, Meta AI; The Hebrew University of Jerusalem, Kyutai
  - **TL;DR:** This study introduces an independence-promoting loss for training auto-encoders in music generation with language models, which reduces statistical dependence between codebooks. The proposed method enhances music quality and accelerates audio generation compared to traditional joint distribution models.
  - **Keywords:** music generation, language models, auto-encoder, multi-stage quantizers, maximum mean discrepancy, audio processing, text-to-music generation, statistical dependence between codebooks, modeling joint distribution vs. marginal distributions, independence-promoting loss, improved music quality, faster audio generation


- [Learning to Reach Goals via Diffusion](https://icml.cc/virtual/2024/poster/35072) (Poster)
  - **Authors:** [Vineet Jain](http://openreview.net/profile?id=~Vineet_Jain1), [Siamak Ravanbakhsh](http://openreview.net/profile?id=~Siamak_Ravanbakhsh1)
  - **Affiliations:** Department of Computer Science, McGill University; Mila - Quebec Artificial Institute, Montréal, Canada, Department of Computer Science, McGill University; Mila - Quebec Artificial Institute, Montréal, Canada
  - **TL;DR:** This paper introduces Merlin, a novel approach to goal-conditioned reinforcement learning that utilizes denoising diffusion models to efficiently reach specified goals from arbitrary initial states without the need for a separate value function. The method demonstrates significant performance improvements and computational efficiency over existing techniques in offline goal-reaching tasks.
  - **Keywords:** goal-conditioned reinforcement learning, diffusion models, sparse reward function, exploration challenges, value function estimation inaccuracies, Merlin (goal-conditioned policy), computational efficiency improvements


- [SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention](https://icml.cc/virtual/2024/poster/34836) (Oral)
  - **Authors:** [Romain Ilbert](http://openreview.net/profile?id=~Romain_Ilbert1), [Ambroise Odonnat](http://openreview.net/profile?id=~Ambroise_Odonnat1), [Vasilii Feofanov](http://openreview.net/profile?id=~Vasilii_Feofanov1), [Aladin Virmaux](http://openreview.net/profile?id=~Aladin_Virmaux1), [Giuseppe Paolo](http://openreview.net/profile?id=~Giuseppe_Paolo1), [Themis Palpanas](http://openreview.net/profile?id=~Themis_Palpanas1), [Ievgen Redko](http://openreview.net/profile?id=~Ievgen_Redko2)
  - **Affiliations:** Huawei Noah’s Ark Lab, Paris, France; LIPADE, Paris Descartes University, Paris, France, Huawei Noah’s Ark Lab, Paris, France; LIPADE, Paris Descartes University, Paris, France, Huawei Noah’s Ark Lab, Paris, France, Huawei Noah’s Ark Lab, Paris, France, Huawei Noah’s Ark Lab, Paris, France, LIPADE, Paris Descartes University, Paris, France, Huawei Noah’s Ark Lab, Paris, France
  - **TL;DR:** The study introduces SAMformer, a shallow transformer model optimized with sharpness-aware minimization, which effectively addresses the challenges of multivariate long-term forecasting. It outperforms existing state-of-the-art methods while maintaining a smaller parameter size, demonstrating improved generalization capabilities.
  - **Keywords:** Time series forecasting, Multivariate forecasting, Transformer, Sharpness-aware minimization, Medical data, Electricity consumption, Temperature forecasting, Stock prices, Long-term forecasting challenges, Feature correlations, Temporal dependencies, SAMformer model, Optimization techniques


- [Rethinking DP-SGD in Discrete Domain: Exploring Logistic Distribution in the Realm of signSGD](https://icml.cc/virtual/2024/poster/33962) (Poster)
  - **Authors:** [Jonggyu Jang](http://openreview.net/profile?id=~Jonggyu_Jang1), [Seongjin Hwang](http://openreview.net/profile?id=~Seongjin_Hwang1), [Hyun Jong Yang](http://openreview.net/profile?id=~Hyun_Jong_Yang2)
  - **Affiliations:** Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea, Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea, Department of Electrical Engineering, Pohang University of Science and Technology, Pohang, South Korea
  - **TL;DR:** This study investigates the limitations of Gaussian noise in DP-SIGNSGD and proposes a Logistic mechanism as a more efficient alternative for securing privacy in deep neural networks. The findings demonstrate that the new method, DP-SIGNLOSGD, outperforms traditional DP-SIGNSGD in maintaining privacy while enhancing communication efficiency.
  - **Keywords:** Data privacy, Deep neural networks, Differential privacy, DP-SGD, DP-SIGNSGD, Logistic mechanism, Exponential mechanism, Information leakage, Membership inference attacks, Data memorization, DP-SIGNLOSGD, Improved privacy mechanism, Gradient compression, Communication efficiency


- [Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method](https://icml.cc/virtual/2024/poster/35161) (Poster)
  - **Authors:** [Jeeveswaran Kishaan](http://openreview.net/profile?id=~Kishaan_Jeeveswaran1), [Elahe Arani](http://openreview.net/profile?id=~Elahe_Arani1), [Bahram Zonooz](http://openreview.net/profile?id=~Bahram_Zonooz1)
  - **Affiliations:** Dep. of Mathematics and Computer Science, Eindhoven University of Technology, NL, Dep. of Mathematics and Computer Science, Eindhoven University of Technology, NL; Wayve Technologies Ltd, London, UK, Dep. of Mathematics and Computer Science, Eindhoven University of Technology, NL
  - **TL;DR:** This study introduces DARE, a novel method for Domain Incremental Learning that effectively mitigates catastrophic forgetting by gradually adapting representations for new tasks while maintaining performance on previous tasks. The proposed approach demonstrates significant improvements in reducing representation drift across multiple benchmarks.
  - **Keywords:** Domain Incremental Learning (DIL), Continual Learning (CL), DARE (Divergence, Adaptation, REfinement), Buffer Sampling Strategy, Autonomous Driving, Robotics, Catastrophic Forgetting, Representation Drift, Reduction of Representation Drift, Well-Calibrated DIL Model, DN4IL dataset, Deep Neural Networks (DNNs), Task-Specific Decision Boundaries


- [Repeat After Me: Transformers are Better than State Space Models at Copying](https://icml.cc/virtual/2024/poster/33527) (Poster)
  - **Authors:** [Samy Jelassi](http://openreview.net/profile?id=~Samy_Jelassi1), [David Brandfonbrener](http://openreview.net/profile?id=~David_Brandfonbrener1), [Sham Kakade](http://openreview.net/profile?id=~Sham_M._Kakade1), [Eran Malach](http://openreview.net/profile?id=~eran_malach1)
  - **Affiliations:** Harvard University, Center of Mathematical Sciences and Applications; Harvard University, Kempner Institute for the Study of Natural and Artificial Intelligence; Harvard University, Departments of Computer Science and Statistics, Harvard University, Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Kempner Institute for the Study of Natural and Artificial Intelligence; Harvard University, Departments of Computer Science and Statistics, Harvard University, Kempner Institute for the Study of Natural and Artificial Intelligence
  - **TL;DR:** This study demonstrates that while generalized state space models (GSSMs) offer efficiency advantages, transformers significantly outperform them in tasks requiring copying from input context. The findings highlight a fundamental gap in capabilities between these two model types, particularly in their ability to retrieve and repeat information.
  - **Keywords:** Transformers, Generalized State Space Models (GSSMs), Sequence Modeling, Two-layer transformer, Fixed-size latent state, Copying from input context, Limitations of GSSMs, Efficiency and generalization of transformers, Theoretical analysis of string copying, GSSMs, n-grams, FlashAttention


- [Finite Volume Features, Global Geometry Representations, and Residual Training for Deep Learning-based CFD Simulation](https://icml.cc/virtual/2024/poster/33839) (Spotlight Poster)
  - **Authors:** [Loh S.E. Jessica](http://openreview.net/profile?id=~Loh_Sher_En_Jessica1), [Naheed Anjum Arafat](http://openreview.net/profile?id=~Naheed_Anjum_Arafat1), [Wei Xian Lim](http://openreview.net/profile?id=~Wei_Xian_Lim2), [Wai Lee Chan](http://openreview.net/profile?id=~Wai_Lee_Chan1), [Adams Wai Kin Kong](http://openreview.net/profile?id=~Adams_Wai-Kin_Kong1)
  - **Affiliations:** Rolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore, Rolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore, Rolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore, Rolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore; School of Mechanical & Aerospace Engineering, Nanyang Technological University, Singapore, Rolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore; College of Computing and Data Science, Nanyang Technological University, Singapore
  - **TL;DR:** This study proposes novel geometric representations and finite volume features to enhance graph neural network-based simulations in computational fluid dynamics, addressing limitations of traditional methods. Experimental results demonstrate a significant reduction in predictive error, improving flow field predictions even with low-resolution data.
  - **Keywords:** Computational Fluid Dynamics (CFD), Deep Learning, Graph Neural Networks (GNN), Finite Volume Features (FVF), Shortest Vector (SV), Directional Integrated Distance (DID), Residual Training, Engineering Design, Aerodynamic Optimization, Marine Hydrodynamics, Microfluidic Device Evaluation, Computational Expense, Low Resolution Data, Predictive Error in GNN Methods, Reduction of Predictive Error by 41%, Improved Flow Field Prediction, Navier–Stokes Equations, Partial Differential Equations (PDEs)


- [ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages](https://icml.cc/virtual/2024/poster/35043) (Poster)
  - **Authors:** [Andrew Jesson](http://openreview.net/profile?id=~Andrew_Jesson1), [Christopher Lu](http://openreview.net/profile?id=~Chris_Lu1), [Gunshi Gupta](http://openreview.net/profile?id=~Gunshi_Gupta1), [Nicolas Beltran-Velez](http://openreview.net/profile?id=~Nicolas_Beltran-Velez1), [Angelos Filos](http://openreview.net/profile?id=~Angelos_Filos1), [Jakob Foerster](http://openreview.net/profile?id=~Jakob_Nicolaus_Foerster1), [Yarin Gal](http://openreview.net/profile?id=~Yarin_Gal1)
  - **Affiliations:** OATML, Department of Computer Science, University of Oxford, Oxford, UK, FLAIR, Department of Computer Science, University of Oxford, Oxford, UK, OATML, Department of Computer Science, University of Oxford, Oxford, UK, Department of Computer Science, Columbia University, New York, NY, OATML, Department of Computer Science, University of Oxford, Oxford, UK, FLAIR, Department of Computer Science, University of Oxford, Oxford, UK, OATML, Department of Computer Science, University of Oxford, Oxford, UK
  - **TL;DR:** This paper introduces a method to enhance on-policy actor-critic algorithms in deep reinforcement learning by applying positive advantages, spectral normalization, and dropout for approximate Bayesian inference. The proposed approach demonstrates significant performance improvements on various benchmarks, including MuJoCo and ProcGen.
  - **Keywords:** Deep Reinforcement Learning, On-Policy Actor-Critic, Asynchronous Advantage Actor-Critic (A3C), Proximal Policy Optimization (PPO), ReLU function, Spectral normalization, Dropout, Robotics, Autonomous driving, Strategy games, Human-computer interaction, Policy updates, Exploration in dynamic environments, State-aware exploration, Improved performance metrics, Adaptive exploration methods, MuJoCo, ProcGen, Bayesian inference, Thompson sampling, Lipschitz constant


- [Advancing Dynamic Sparse Training by Exploring Optimization Opportunities](https://icml.cc/virtual/2024/poster/32886) (Poster)
  - **Authors:** [Jie Ji](http://openreview.net/profile?id=~Jie_Ji1), [Gen Li](http://openreview.net/profile?id=~Gen_Li4), [Lu Yin](http://openreview.net/profile?id=~Lu_Yin1), [Minghai Qin](http://openreview.net/profile?id=~Minghai_Qin1), [Geng Yuan](http://openreview.net/profile?id=~Geng_Yuan1), [Linke Guo](http://openreview.net/profile?id=~Linke_Guo2), [Shiwei Liu](http://openreview.net/profile?id=~Shiwei_Liu2), [Xiaolong Ma](http://openreview.net/profile?id=~Xiaolong_Ma2)
  - **Affiliations:** Clemson University, USA, Clemson University, USA, University of Aberdeen, Scotland, Clemson University, USA, University of Georgia, USA, Clemson University, USA, University of Oxford, England, Clemson University, USA
  - **TL;DR:** This paper introduces BiDST, a novel framework for Dynamic Sparse Training that optimizes both weights and masks simultaneously, addressing the interdependence between them. The proposed method achieves up to 2.62% higher accuracy and significantly improves execution speed and overhead reduction compared to traditional DST approaches.
  - **Keywords:** Dynamic Sparse Training, Deep Neural Networks, Bi-level optimization, DST algorithms, Training resource requirements, weight and mask interdependence, BiDST framework, improved accuracy, execution speed, and reduced overhead


- [ACE: Off-Policy Actor-Critic with Causality-Aware Entropy Regularization](https://icml.cc/virtual/2024/poster/35131) (Oral)
  - **Authors:** [Tianying Ji](http://openreview.net/profile?id=~Tianying_Ji2), [Yongyuan Liang](http://openreview.net/profile?id=~Yongyuan_Liang1), [Yan Zeng](http://openreview.net/profile?id=~Yan_Zeng2), [Yu Luo](http://openreview.net/profile?id=~Yu_Luo5), [Guowei Xu](http://openreview.net/profile?id=~Guowei_Xu2), [Jiawei Guo](http://openreview.net/profile?id=~Jiawei_Guo4), [Ruijie Zheng](http://openreview.net/profile?id=~Ruijie_Zheng1), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1), [Fuchun Sun](http://openreview.net/profile?id=~Fuchun_Sun1), [Huazhe Xu](http://openreview.net/profile?id=~Huazhe_Xu1)
  - **Affiliations:** Tsinghua University, University of Maryland, Beijing Technology and Businesses University, Tsinghua University, Tsinghua University, Tsinghua University, University of Maryland, University of Maryland, Tsinghua University, Tsinghua University; Shanghai Qi Zhi Institute
  - **TL;DR:** The study introduces ACE, an off-policy actor-critic algorithm that incorporates causality-aware entropy regularization to enhance exploration efficiency in reinforcement learning. The method significantly outperforms existing model-free RL baselines across various continuous control tasks, addressing challenges related to sample complexity and exploration effectiveness.
  - **Keywords:** Reinforcement Learning, Policy Learning, Efficient Exploration, Off-Policy Actor-Critic, Causality-Aware Entropy Regularization, Continuous Control Tasks, Robotic Manipulation, High Sample Complexity, Ineffective Exploration, Causality-Aware Entropy Term, Dormancy-Guided Reset Mechanism, Primitive Behaviors, Gradient Dormancy


- [Towards Efficient Exact Optimization of Language Model Alignment](https://icml.cc/virtual/2024/poster/34940) (Poster)
  - **Authors:** [Haozhe Ji](http://openreview.net/profile?id=~Haozhe_Ji2), [Cheng Lu](http://openreview.net/profile?id=~Cheng_Lu5), [Yilin Niu](http://openreview.net/profile?id=~Yilin_Niu1), [Pei Ke](http://openreview.net/profile?id=~Pei_Ke2), [Hongning Wang](http://openreview.net/profile?id=~Hongning_Wang1), [Jun Zhu](http://openreview.net/profile?id=~Jun_Zhu2), [Jie Tang](http://openreview.net/profile?id=~Jie_Tang1), [Minlie Huang](http://openreview.net/profile?id=~Minlie_Huang1)
  - **Affiliations:** The Conversational AI (CoAI) Group, Tsinghua University, The Tsinghua Statistical Artificial Intelligence & Learning (TSAIL) Group, Tsinghua University, Zhipu AI, The Conversational AI (CoAI) Group, Tsinghua University, The Conversational AI (CoAI) Group, Tsinghua University, The Tsinghua Statistical Artificial Intelligence & Learning (TSAIL) Group, Tsinghua University, The Knowledge Engineering Group (KEG), Tsinghua University, The Conversational AI (CoAI) Group, Tsinghua University
  - **TL;DR:** This paper addresses the alignment of language models with human preferences by proposing an efficient exact optimization (EXO) method that circumvents the complexities of reinforcement learning. The authors demonstrate that EXO optimizes the alignment objective effectively while maintaining the same mode-seeking solution as traditional methods, showing advantages over existing approaches on realistic human preference data.
  - **Keywords:** Language Model Alignment, Human Preferences, Reinforcement Learning, Direct Preference Optimization, Efficient Exact Optimization, AI Systems, Natural Language Processing, High Variance in Policy Updates, Distributional Shift, Preference Data Acquisition, Optimization Algorithms, Policy Improvement Methods, Kullback-Leibler Divergence, Policy Parametrization


- [Discrete Latent Perspective Learning for Segmentation and Detection](https://icml.cc/virtual/2024/poster/33911) (Spotlight Poster)
  - **Authors:** [Deyi Ji](http://openreview.net/profile?id=~Deyi_Ji2), [Feng Zhao](http://openreview.net/profile?id=~Feng_Zhao6), [Lanyun Zhu](http://openreview.net/profile?id=~Lanyun_Zhu1), [Wenwei Jin](http://openreview.net/profile?id=~Wenwei_Jin3), [Hongtao Lu](http://openreview.net/profile?id=~Hongtao_Lu1), [Jieping Ye](http://openreview.net/profile?id=~Jieping_Ye4)
  - **Affiliations:** University of Science and Technology of China; Alibaba Group, University of Science and Technology of China, Singapore University of Technology and Design, Alibaba Group, Dept. of CSE, MOE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Alibaba Group; University of Science and Technology of China
  - **TL;DR:** This paper introduces Discrete Latent Perspective Learning (DLPL), a novel framework that enables networks to achieve perspective-invariant learning using single-view images. The proposed method significantly improves the network's ability to interpret images consistently across various perspectives and tasks, such as segmentation and detection.
  - **Keywords:** Perspective-Invariant Learning, Multi-Perspective Fusion Learning, Discrete Latent Perspective Learning (DLPL), Perspective Discrete Decomposition (PDD), Perspective Homography Transformation (PHT), Perspective Invariant Attention (PIA), Image segmentation, Object detection, Daily photos, UAV, Autonomous driving, Challenges in collecting multi-view images, Consistency in semantic learning across varying perspectives, Enhanced network capacity for depicting images across diverse scenarios and tasks


- [Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic](https://icml.cc/virtual/2024/poster/34805) (Poster)
  - **Authors:** [Tianying Ji](http://openreview.net/profile?id=~Tianying_Ji2), [Yu Luo](http://openreview.net/profile?id=~Yu_Luo5), [Fuchun Sun](http://openreview.net/profile?id=~Fuchun_Sun1), [Xianyuan Zhan](http://openreview.net/profile?id=~Xianyuan_Zhan1), [Jianwei Zhang](http://openreview.net/profile?id=~Jianwei_Zhang2), [Huazhe Xu](http://openreview.net/profile?id=~Huazhe_Xu1)
  - **Affiliations:** Department of Computer Science and Technology, Tsinghua University, Department of Computer Science and Technology, Tsinghua University, Department of Computer Science and Technology, Tsinghua University; Institute for Interdisciplinary Information Sciences, Tsinghua University, Institute for AI Industry Research, Tsinghua University; Shanghai Artificial Intelligence Laboratory, Department of Informatics, University of Hamburg, Shanghai Artificial Intelligence Laboratory; Institute for Interdisciplinary Information Sciences, Tsinghua University; Shanghai Qi Zhi Institute
  - **TL;DR:** This study addresses the issue of Q-value underestimation in off-policy reinforcement learning, proposing the Blended Exploitation and Exploration (BEE) operator to enhance Q-value updates. The resulting BAC algorithm demonstrates superior performance across various continuous control tasks and in challenging real-world robotic scenarios.
  - **Keywords:** Reinforcement Learning, Off-Policy Learning, Actor-Critic, Q-value Functions, Blended Exploitation and Exploration (BEE), BAC Algorithm, Continuous Control Tasks, Robotics, Value Underestimation, Sample Efficiency, Policy Learning, Improved Q-value Updates, Enhanced Performance in Failure-Prone Scenarios


- [Simulation-Based Inference with Quantile Regression](https://icml.cc/virtual/2024/poster/32787) (Poster)
  - **Authors:** [He Jia](http://openreview.net/profile?id=~He_Jia1)
  - **Affiliations:** Department of Astrophysical Sciences, Princeton University, USA
  - **TL;DR:** This paper introduces Neural Quantile Estimation (NQE), a novel method for Simulation-Based Inference that utilizes conditional quantile regression to efficiently estimate posterior distributions. NQE demonstrates state-of-the-art performance across various benchmark problems while addressing challenges such as model mis-specification and limited simulation budgets.
  - **Keywords:** Simulation-Based Inference, Quantile Regression, Neural Quantile Estimation, Monotonic Cubic Hermite Spline, Bayesian Credible Region, Model Mis-specification, Limited Simulation Budget, State-of-the-art Performance, Unbiased Posterior Estimation, Bayesian Inference, Highest Posterior Density Region, Local Cumulative Density Function


- [GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer](https://icml.cc/virtual/2024/poster/33720) (Poster)
  - **Authors:** [Ding Jia](http://openreview.net/profile?id=~Ding_Jia1), [Jianyuan Guo](http://openreview.net/profile?id=~Jianyuan_Guo1), [Kai Han](http://openreview.net/profile?id=~Kai_Han2), [Han Wu](http://openreview.net/profile?id=~Han_Wu4), [Chao Zhang](http://openreview.net/profile?id=~Chao_Zhang10), [Chang Xu](http://openreview.net/profile?id=~Chang_Xu4), [Xinghao Chen](http://openreview.net/profile?id=~Xinghao_Chen1)
  - **Affiliations:** Peking University, The University of Sydney, Huawei Noah’s Ark Lab, The University of Sydney, Peking University, The University of Sydney, Huawei Noah’s Ark Lab
  - **TL;DR:** This paper introduces GeminiFusion, an efficient pixel-wise multimodal fusion approach that integrates intra-modal and inter-modal attentions to enhance performance in various vision tasks. The method addresses computational challenges associated with cross-modal transformers while maintaining linear complexity, demonstrating superior results in multimodal image-to-image translation, 3D object detection, and semantic segmentation.
  - **Keywords:** multimodal fusion, cross-modal transformers, pixel-wise fusion, intra-modal attention, inter-modal attention, layer-adaptive noise, image-to-image translation, 3D object detection, semantic segmentation, computational challenges, token exchange methods, inter-modal interactions, GeminiFusion framework, efficient multimodal integration


- [Projection-Free Variance Reduction Methods for Stochastic Constrained Multi-Level Compositional Optimization](https://icml.cc/virtual/2024/poster/35082) (Poster)
  - **Authors:** [Wei Jiang](http://openreview.net/profile?id=~Wei_Jiang8), [Sifan Yang](http://openreview.net/profile?id=~Sifan_Yang2), [Wenhao Yang](http://openreview.net/profile?id=~Wenhao_Yang3), [Yibo Wang](http://openreview.net/profile?id=~Yibo_Wang2), [Yuanyu Wan](http://openreview.net/profile?id=~Yuanyu_Wan1), [Lijun Zhang](http://openreview.net/profile?id=~Lijun_Zhang1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China, School of Software Technology, Zhejiang University, Ningbo, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China
  - **TL;DR:** This paper presents novel projection-free variance reduction algorithms for stochastic constrained multi-level compositional optimization, addressing limitations of existing methods. The proposed algorithms improve sample complexities and provide theoretical guarantees for both convex and non-convex functions, demonstrating effectiveness through numerical experiments.
  - **Keywords:** Stochastic optimization, Multi-level optimization, Projection-free algorithms, Variance reduction algorithms, Gradient mapping, Frank-Wolfe gap criterion, Reinforcement learning, Risk management, Risk-averse portfolio optimization, Graph neural network training, Stochastic constrained optimization, Non-convex functions, Convex and strongly convex objectives, Improved complexities for projection-free methods, Theoretical guarantees for optimization rates


- [NDOT: Neuronal Dynamics-based Online Training for Spiking Neural Networks](https://icml.cc/virtual/2024/poster/33481) (Poster)
  - **Authors:** [Haiyan Jiang](http://openreview.net/profile?id=~Haiyan_Jiang1), [Giulia De Masi](http://openreview.net/profile?id=~Giulia_De_Masi1), [Huan Xiong](http://openreview.net/profile?id=~Huan_Xiong1), [Bin Gu](http://openreview.net/profile?id=~Bin_Gu1)
  - **Affiliations:** Department of Machine Learning, MBZUAI, Abu Dhabi, UAE, Technology Innovation Institute, Abu Dhabi, UAE; Sant’Anna School of Advanced Studies, Italy, Department of Machine Learning, MBZUAI, Abu Dhabi, UAE; Harbin Institute of Technology, China, Department of Machine Learning, MBZUAI, Abu Dhabi, UAE; School of Artificial Intelligence, Jilin University, China
  - **TL;DR:** This study introduces Neuronal Dynamics-based Online Training (NDOT) for Spiking Neural Networks (SNNs) to address the challenges of efficient training and gradient calculation. The proposed method demonstrates superior performance on large-scale datasets while maintaining low memory consumption.
  - **Keywords:** Spiking Neural Networks (SNNs), Neuromorphic Computing, Surrogate Gradient (SG), Backpropagation Through Time (BPTT), Follow-the-Regularized-Leader (FTRL), Efficient training of deep SNNs, Gradient calculation challenges, Non-differentiability of spike-generating activation functions, Neuronal Dynamics-based Online Training (NDOT), Forward-in-time learning, Temporal and spatial gradient decomposition, CIFAR-10, CIFAR-100, CIFAR10-DVS, Energy-efficient, Fast-inference


- [On the Maximal Local Disparity of Fairness-Aware Classifiers](https://icml.cc/virtual/2024/poster/34971) (Poster)
  - **Authors:** [Jinqiu Jin](http://openreview.net/profile?id=~Jinqiu_Jin1), [Haoxuan Li](http://openreview.net/profile?id=~Haoxuan_Li6), [Fuli Feng](http://openreview.net/profile?id=~Fuli_Feng1)
  - **Affiliations:** University of Science and Technology of China, Peking University, University of Science and Technology of China
  - **TL;DR:** This study introduces a novel fairness metric called Maximal Cumulative ratio Disparity (MCDP) to address the limitations of existing fairness metrics in machine learning, particularly in measuring local disparities. The proposed methods demonstrate improved fairness-accuracy trade-offs through extensive experiments on various datasets.
  - **Keywords:** Fairness in machine learning, demographic parity, Maximal Cumulative ratio Disparity (MCDP), bi-level optimization algorithm, Loan management, job-hiring, recidivism prediction, Discrimination in machine learning algorithms, local disparity in predictions, New fairness metric (MCDP), improved fairness-accuracy trade-offs, Tabular datasets, image datasets


- [Federated Optimization with Doubly Regularized Drift Correction](https://icml.cc/virtual/2024/poster/34391) (Poster)
  - **Authors:** [Xiaowen Jiang](http://openreview.net/profile?id=~Xiaowen_Jiang1), [Anton Rodomanov](http://openreview.net/profile?id=~Anton_Rodomanov1), [Sebastian Stich](http://openreview.net/profile?id=~Sebastian_U_Stich1)
  - **Affiliations:** CISPA Helmholtz Center for Information Security; Universität des Saarlandes, CISPA Helmholtz Center for Information Security, CISPA Helmholtz Center for Information Security
  - **TL;DR:** This paper addresses the challenges of client drift in federated learning by revisiting and extending existing methods like DANE and introducing a new method, FedRed, which improves local computational complexity while maintaining communication efficiency. The findings suggest that these methods can effectively reduce communication costs and enhance performance in decentralized training scenarios.
  - **Keywords:** Federated learning, distributed optimization, FedAvg, DANE, DANE+, FedRed, Client drift, communication bottlenecks, heterogeneous data, Communication reduction, drift correction, local computational complexity, Hessian similarity, stochastic gradient descent


- [An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning](https://icml.cc/virtual/2024/poster/34548) (Poster)
  - **Authors:** [Chen Jin](http://openreview.net/profile?id=~Chen_Jin3), [Ryutaro Tanno](http://openreview.net/profile?id=~Ryutaro_Tanno1), [Amrutha Saseendran](http://openreview.net/profile?id=~Amrutha_Saseendran1), [Tom Diethe](http://openreview.net/profile?id=~Tom_Diethe1), [Philip Teare](http://openreview.net/profile?id=~Philip_Alexander_Teare1)
  - **Affiliations:** Centre for AI, DS&AI, AstraZeneca, UK, Google DeepMind, UK, Centre for AI, DS&AI, AstraZeneca, UK, Centre for AI, DS&AI, AstraZeneca, UK, Centre for AI, DS&AI, AstraZeneca, UK
  - **TL;DR:** This study introduces Multi-Concept Prompt Learning (MCPL), a method for learning multiple unknown object-level concepts from a single image-sentence pair without requiring image annotations. The approach enhances word-concept correlation and enables efficient local editing, demonstrating significant improvements in concept discovery.
  - **Keywords:** Multi-Concept Learning, Object-Level Concept Discovery, Textual Inversion, Multi-Concept Prompt Learning (MCPL), Attention Masking, Prompts Contrastive Loss, Bind Adjective, Image Synthesis, Language-Driven Vision, Identifying multiple unknown object-level concepts, Lack of image annotations, New method for learning semantically disentangled concepts, Efficient storage usage


- [Position: What Can Large Language Models Tell Us about Time Series Analysis](https://icml.cc/virtual/2024/poster/33336) (Poster)
  - **Authors:** [Ming Jin](http://openreview.net/profile?id=~Ming_Jin3), [Yi-Fan Zhang](http://openreview.net/profile?id=~YiFan_Zhang8), [Wei Chen](http://openreview.net/profile?id=~Wei_Chen50), [Kexin Zhang](http://openreview.net/profile?id=~Kexin_Zhang3), [Yuxuan Liang](http://openreview.net/profile?id=~Yuxuan_Liang1), [Bin Yang](http://openreview.net/profile?id=~Bin_Yang4), [Jindong Wang](http://openreview.net/profile?id=~Jindong_Wang1), [Shirui Pan](http://openreview.net/profile?id=~Shirui_Pan1), [Qingsong Wen](http://openreview.net/profile?id=~Qingsong_Wen2)
  - **Affiliations:** Griffith University, Chinese Academy of Sciences, The Hong Kong University of Science and Technology (Guangzhou), Zhejiang University, The Hong Kong University of Science and Technology (Guangzhou), East China Normal University, Microsoft Research Asia, Griffith University, Squirrel AI
  - **TL;DR:** The paper explores the potential of large language models (LLMs) to transform time series analysis, emphasizing their ability to enhance decision-making and integrate various data modalities. It highlights the current limitations of traditional time series models and advocates for the recognition of LLMs in advancing analytical intelligence in this field.
  - **Keywords:** Time series analysis, Large language models (LLMs), Artificial general intelligence (AGI), Financial market analysis, Traffic pattern analysis, Domain knowledge reliance, Model tuning, Prediction tasks, Potential for revolutionizing time series analysis, Integration of LLMs with time series analysis, Multimodal data processing, Complex problem solving


- [Homomorphism Counts for Graph Neural Networks: All About That Basis](https://icml.cc/virtual/2024/poster/32630) (Poster)
  - **Authors:** [Emily Jin](http://openreview.net/profile?id=~Emily_Jin2), [Michael Bronstein](http://openreview.net/profile?id=~Michael_M._Bronstein1), [Ismail Ceylan](http://openreview.net/profile?id=~Ismail_Ilkan_Ceylan2), [Matthias Lanzinger](http://openreview.net/profile?id=~Matthias_Lanzinger1)
  - **Affiliations:** Department of Computer Science, University of Oxford, Oxford, UK, Department of Computer Science, University of Oxford, Oxford, UK, Department of Computer Science, University of Oxford, Oxford, UK, Institute for Logic and Computation, TU Wien, Vienna, AT
  - **TL;DR:** This study addresses the limitations of graph neural networks in counting patterns by proposing a more expressive architecture that incorporates homomorphism counts of all structures in the basis of the target pattern. The findings demonstrate that this approach enhances expressive power without increasing computational complexity, supported by theoretical results and empirical validation.
  - **Keywords:** Graph Neural Networks, Expressive Power, Message Passing Neural Networks (MPNNs), Homomorphism Counts, Subgraph Counts, Limitations of GNNs, Counting Patterns in Graphs, More expressive architectures, Theoretical results on motif parameters, Standard benchmark datasets, Weisfeiler Leman graph isomorphism test (1-WL), Graph Motif Parameters


- [SuDA: Support-based Domain Adaptation for Sim2Real Hinge Joint Tracking with Flexible Sensors](https://icml.cc/virtual/2024/poster/34585) (Poster)
  - **Authors:** [Fang Jiawei](http://openreview.net/profile?id=~Fang_Jiawei1), [Haishan Song](http://openreview.net/profile?email=haishansong%40stu.xmu.edu.cn), [Chengxu Zuo](http://openreview.net/profile?id=~Chengxu_Zuo1), [xiaoxia gao](http://openreview.net/profile?email=gaxxia29%40gmail.com), [Xiaowei Chen](http://openreview.net/profile?email=wdenxwa%40stu.xmu.edu.cn), [Guo Shihui](http://openreview.net/profile?id=~Shihui_Guo1), [Yipeng Qin](http://openreview.net/profile?id=~Yipeng_Qin1)
  - **Affiliations:** School of Informatics, Xiamen University, Fujian, China, School of Informatics, Xiamen University, Fujian, China, School of Informatics, Xiamen University, Fujian, China, Department of Computer Science and Technology, Xidian University, Shanxi, China, School of Informatics, Xiamen University, Fujian, China, School of Informatics, Xiamen University, Fujian, China, School of Computer Science & Informatics, Cardiff University, Wales, UK
  - **TL;DR:** This study presents SuDA, a novel support-based domain adaptation method for hinge joint tracking using flexible sensors, which eliminates the need for labeled data while achieving accuracy comparable to supervised learning. The approach addresses the challenges of data scarcity and high costs associated with traditional motion capture methods.
  - **Keywords:** Human motion capture (MoCap), Flexible sensors, Support-based Domain Adaptation (SuDA), Domain adaptation, Wearable technology, Motion tracking, Data scarcity, High labor costs for data collection, Sim2Real solution for hinge joint tracking, Improved accuracy without labeled data


- [Language Models as Semantic Indexers](https://icml.cc/virtual/2024/poster/32907) (Poster)
  - **Authors:** [Bowen Jin](http://openreview.net/profile?id=~Bowen_Jin1), [Hansi Zeng](http://openreview.net/profile?id=~Hansi_Zeng1), [Guoyin Wang](http://openreview.net/profile?id=~Guoyin_Wang1), [Xiusi Chen](http://openreview.net/profile?id=~Xiusi_Chen1), [Tianxin Wei](http://openreview.net/profile?id=~Tianxin_Wei1), [Ruirui Li](http://openreview.net/profile?id=~Ruirui_Li3), [Zhengyang Wang](http://openreview.net/profile?id=~Zhengyang_Wang1), [Zheng Li](http://openreview.net/profile?id=~Zheng_Li9), [Yang Li](http://openreview.net/profile?id=~Yang_Li80), [Hanqing Lu](http://openreview.net/profile?id=~Hanqing_Lu3), [Suhang Wang](http://openreview.net/profile?id=~Suhang_Wang1), [Jiawei Han](http://openreview.net/profile?id=~Jiawei_Han1), [Xianfeng Tang](http://openreview.net/profile?id=~Xianfeng_Tang1)
  - **Affiliations:** University of Illinois at Urbana-Champaign, University of Massachusetts Amherst, Amazon, University of California, Los Angeles, University of Illinois at Urbana-Champaign, Amazon, Amazon, Amazon, Amazon, Amazon, The Pennsylvania State University, University of Illinois at Urbana-Champaign, Amazon
  - **TL;DR:** This paper introduces LMINDEXER, a self-supervised framework for learning semantic IDs using a generative language model, addressing challenges in semantic indexing and information retrieval. The proposed method demonstrates high-quality ID generation and effectiveness across tasks such as recommendation, product search, and document retrieval.
  - **Keywords:** semantic indexing, information retrieval, self-supervised learning, generative language model, contrastive learning, recommendation, product search, document retrieval, information loss, mismatch in embedding distribution, semantic supervision deficiency, LMINDEXER framework, semantic IDs generation


- [FedSC: Provable Federated Self-supervised Learning with Spectral Contrastive Objective over Non-i.i.d. Data](https://icml.cc/virtual/2024/poster/35182) (Poster)
  - **Authors:** [Shusen Jing](http://openreview.net/profile?id=~Shusen_Jing1), [Anlan Yu](http://openreview.net/profile?id=~Anlan_Yu1), [Shuai Zhang](http://openreview.net/profile?id=~Shuai_Zhang6), [Songyang Zhang](http://openreview.net/profile?id=~Songyang_Zhang2)
  - **Affiliations:** Department of Radiation Oncology, University of California, San Francisco, California, USA, Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, Pennsylvania, USA, Department of Data Science, New Jersey Institute of Technology, Newark, New Jersey, USA, Department of Electrical and Computer Engineering, University of Louisiana at Lafayette, Lafayette, Louisiana, USA
  - **TL;DR:** This paper presents FedSC, a provable federated self-supervised learning algorithm that addresses the challenges of non-i.i.d. data and privacy leakage by enabling inter-client contrast of data representations. The proposed method improves the quality of data representations and provides theoretical analysis on convergence and privacy leakage.
  - **Keywords:** Federated Learning, Self-Supervised Learning, Spectral Contrastive Objective, Federated Averaging (FedAvg), Non-i.i.d. Data, Privacy Leakage, Provable FedSSL Algorithm, Improved Data Representations, Differential Privacy (DP), Correlation Matrices


- [Generalized Neural Collapse for a Large Number of Classes](https://icml.cc/virtual/2024/poster/34646) (Poster)
  - **Authors:** [Jiachen Jiang](http://openreview.net/profile?id=~Jiachen_Jiang1), [Jinxin Zhou](http://openreview.net/profile?id=~Jinxin_Zhou2), [Peng Wang](http://openreview.net/profile?id=~Peng_Wang23), [Qing Qu](http://openreview.net/profile?id=~Qing_Qu2), [Dustin Mixon](http://openreview.net/profile?id=~Dustin_G._Mixon1), [Chong You](http://openreview.net/profile?id=~Chong_You2), [Zhihui Zhu](http://openreview.net/profile?id=~Zhihui_Zhu1)
  - **Affiliations:** Department of Computer Science, The Ohio State University, Columbus, OH, USA, Department of Computer Science, The Ohio State University, Columbus, OH, USA, Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA, Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA, Department of Mathematics, The Ohio State University, Columbus, OH, USA, Google Research, New York City, NY, USA, Department of Computer Science, The Ohio State University, Columbus, OH, USA
  - **TL;DR:** This paper generalizes the concept of Neural Collapse to scenarios with a large number of classes exceeding the feature space dimension, providing both empirical and theoretical insights. The findings highlight the arrangement of class-mean features and the introduction of the softmax code, which can enhance understanding and performance of deep neural networks in various applications.
  - **Keywords:** Neural Collapse, Deep Learning, Classification Models, Softmax Code, Unconstrained Feature Model, Spherical Constraint, Language Models, Information Retrieval Systems, Face Recognition, High-dimensional feature space, Class imbalance, Generalized Neural Collapse, Theoretical evidence for Neural Collapse, Equiangular Tight Frame (ETF), Cross-Entropy Loss, Mean-Square-Error Loss


- [What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement](https://icml.cc/virtual/2024/poster/33612) (Spotlight Poster)
  - **Authors:** [Xisen Jin](http://openreview.net/profile?id=~Xisen_Jin3), [Xiang Ren](http://openreview.net/profile?id=~Xiang_Ren1)
  - **Affiliations:** University of Southern California, University of Southern California
  - **TL;DR:** This study addresses the issue of catastrophic forgetting in language models during refinement by forecasting which upstream examples are likely to be forgotten. The authors propose a forecasting model that improves the controllability of the replay process, demonstrating its effectiveness in reducing forgetting of important examples.
  - **Keywords:** language model refinement, catastrophic forgetting, continual learning, forecasting models, black-box classifier, logit-change transfer, pretrained language models (PTLMs), online learning, catastrophic forgetting, model updates, interpretability, reduced forgetting through targeted replay, improved controllability of replay process, BART, T5, pre-softmax logit scores, example representations


- [Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning](https://icml.cc/virtual/2024/poster/34543) (Poster)
  - **Authors:** [Shibo Jie](http://openreview.net/profile?id=~Shibo_Jie1), [Yehui Tang](http://openreview.net/profile?id=~Yehui_Tang1), [Ning Ding](http://openreview.net/profile?id=~Ning_Ding4), [Zhi-Hong Deng](http://openreview.net/profile?id=~Zhi-Hong_Deng1), [Kai Han](http://openreview.net/profile?id=~Kai_Han2), [Yunhe Wang](http://openreview.net/profile?id=~Yunhe_Wang1)
  - **Affiliations:** School of Intelligence Science and Technology, Peking University, Huawei Noah’s Ark Lab, School of Intelligence Science and Technology, Peking University; Huawei Noah’s Ark Lab, School of Intelligence Science and Technology, Peking University; National Key Laboratory of General Artificial Intelligence, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab
  - **TL;DR:** This paper introduces a novel approach called memory-space visual prompting (MemVP) to enhance the efficiency of vision-language models by integrating visual prompts with the weights of language model feed-forward networks. The proposed method significantly reduces training time and inference latency while outperforming existing parameter-efficient fine-tuning techniques.
  - **Keywords:** Vision-Language Models, Parameter-Efficient Fine-Tuning, Memory-Space Visual Prompting, Feed-Forward Network (FFN), Inefficiency in input length for language models, High training and inference costs, Reduction in training time and inference latency, Improved performance over previous methods, Visual Prompts, Key-Value Memory


- [IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation](https://icml.cc/virtual/2024/poster/34975) (Poster)
  - **Authors:** [Taejong Joo](http://openreview.net/profile?id=~Taejong_Joo1), [Diego Klabjan](http://openreview.net/profile?id=~Diego_Klabjan1)
  - **Affiliations:** Department of Industrial Engineering & Management Sciences, Northwestern University, Evanston, IL, USA, Department of Industrial Engineering & Management Sciences, Northwestern University, Evanston, IL, USA
  - **TL;DR:** This study addresses the challenges of model calibration and selection in unsupervised domain adaptation by developing an importance weighted group accuracy estimator. The proposed method significantly improves state-of-the-art performance by 22% in model calibration and 14% in model selection tasks.
  - **Keywords:** Unsupervised Domain Adaptation (UDA), Model Calibration, Model Selection, Importance Weighted Group Accuracy Estimator, Optimization Problem, Distribution Shifts, Lack of Labeled Data in Target Domain, Improved Calibration Performance, Improved Model Selection Performance


- [Graph Generation with Diffusion Mixture](https://icml.cc/virtual/2024/poster/33587) (Poster)
  - **Authors:** [Jaehyeong Jo](http://openreview.net/profile?id=~Jaehyeong_Jo1), [Dongki Kim](http://openreview.net/profile?id=~Dongki_Kim1), [Sung Ju Hwang](http://openreview.net/profile?id=~Sung_Ju_Hwang1)
  - **Affiliations:** Korea Advanced Institute of Science and Technology (KAIST), Korea Advanced Institute of Science and Technology (KAIST), Korea Advanced Institute of Science and Technology (KAIST); DeepAuto.ai
  - **TL;DR:** This study proposes a novel generative framework for graph generation that explicitly learns the final graph structures through a mixture of endpoint-conditioned diffusion processes. The method demonstrates superior performance in generating graphs with correct topology for both continuous and discrete features, outperforming existing models.
  - **Keywords:** graph generation, non-Euclidean structures, diffusion models, endpoint-conditioned diffusion processes, maximum likelihood training, drug discovery, protein design, program synthesis, 2D/3D molecule generation, modeling topological properties, recovering topological properties, generating valid graphs, generative framework, learning final graph structures, outperforming previous generative models, GANs (Generative Adversarial Networks), RNNs (Recurrent Neural Networks), VAEs (Variational Autoencoders)


- [Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs](https://icml.cc/virtual/2024/poster/34766) (Poster)
  - **Authors:** [Daniel D. Johnson](http://openreview.net/profile?id=~Daniel_D._Johnson1), [Daniel Tarlow](http://openreview.net/profile?id=~Daniel_Tarlow1), [David Duvenaud](http://openreview.net/profile?id=~David_Duvenaud2), [Chris Maddison](http://openreview.net/profile?id=~Chris_J._Maddison1)
  - **Affiliations:** Google DeepMind; University of Toronto, Department of Computer Science, Ontario, Canada, Google DeepMind, University of Toronto, Department of Computer Science, Ontario, Canada, University of Toronto, Department of Computer Science, Ontario, Canada
  - **TL;DR:** The study proposes a method for generative models to estimate their knowledge gaps about real-world processes by predicting pairs of responses, allowing them to "cheat" by observing one response while predicting the other. This approach leads to improved uncertainty quantification and detection of incorrect responses, outperforming existing techniques.
  - **Keywords:** uncertainty quantification, generative models, model calibration, second-order calibration, paired response training, image classification, language modeling, navigation tasks, epistemic uncertainty, aleatoric uncertainty, hallucination, cheat-corrected estimates, frequentist confidence intervals, AI safety, AI alignment


- [On the Origins of Linear Representations in Large Language Models](https://icml.cc/virtual/2024/poster/33064) (Poster)
  - **Authors:** [Yibo Jiang](http://openreview.net/profile?id=~Yibo_Jiang2), [Goutham Rajendran](http://openreview.net/profile?id=~Goutham_Rajendran1), [Pradeep Ravikumar](http://openreview.net/profile?id=~Pradeep_Kumar_Ravikumar1), [Bryon Aragam](http://openreview.net/profile?id=~Bryon_Aragam1), [Victor Veitch](http://openreview.net/profile?id=~Victor_Veitch1)
  - **Affiliations:** Department of Computer Science, University of Chicago, Machine Learning Department, Carnegie Mellon University, Machine Learning Department, Carnegie Mellon University, Booth School of Business, University of Chicago, Department of Statistics, University of Chicago; Data Science Institute, University of Chicago
  - **TL;DR:** This study investigates the origins of linear representations in large language models by introducing a latent variable model for next token prediction. The findings demonstrate that both the prediction objective and the implicit bias of gradient descent promote linear representations of concepts, providing insights into the interpretability of language models.
  - **Keywords:** Linear representations, Large language models, Interpretability, Latent variable model, Next token prediction, Softmax with cross-entropy, Gradient descent, Encoding of high-level semantic concepts, Linearity in representations, Emergence of linear representations, Generalizable insights from LLaMA-2, LLaMA-2


- [Chain-of-Thought Predictive Control](https://icml.cc/virtual/2024/poster/34792) (Poster)
  - **Authors:** [Zhiwei Jia](http://openreview.net/profile?id=~Zhiwei_Jia1), [Vineet Thumuluri](http://openreview.net/profile?id=~Vineet_Thumuluri1), [Fangchen Liu](http://openreview.net/profile?id=~Fangchen_Liu2), [Linghao Chen](http://openreview.net/profile?id=~Linghao_Chen2), [Zhiao Huang](http://openreview.net/profile?id=~Zhiao_Huang1), [Hao Su](http://openreview.net/profile?id=~Hao_Su1)
  - **Affiliations:** UC San Diego, UC San Diego, UC Berkeley, Zhejiang University, UC San Diego, UC San Diego
  - **TL;DR:** This study presents Chain-of-Thought Predictive Control (CoTPC), a hierarchical imitation learning method that effectively utilizes sub-optimal demonstrations for low-level control tasks. The proposed approach enhances generalizability and performance by discovering multi-step subskill decompositions and dynamically generating guidance for action predictions.
  - **Keywords:** hierarchical imitation learning, generalizable policy learning, low-level control, Transformer-based design, unsupervised subskill discovery, multi-step subskill decomposition, contact-rich object manipulations, autonomous agents, sub-optimal demonstrations, non-Markovian properties, noisy data, optimization challenges, Chain-of-Thought Predictive Control (CoTPC), improved feature representation, dynamic subskill guidance, Chain-of-Thought (CoT), hierarchical reinforcement learning (HRL), subskills


- [Conditional Common Entropy for Instrumental Variable Testing and Partial Identification](https://icml.cc/virtual/2024/poster/33850) (Poster)
  - **Authors:** [Ziwei Jiang](http://openreview.net/profile?id=~Ziwei_Jiang1), [Murat Kocaoglu](http://openreview.net/profile?id=~Murat_Kocaoglu1)
  - **Affiliations:** Elmore Family School of Electrical and Computer Engineering, Purdue University, Elmore Family School of Electrical and Computer Engineering, Purdue University
  - **TL;DR:** This paper proposes a method for bounding causal effects using instrumental variables under weak confounding and introduces a new criterion to test the validity of instrumental variables with side information about confounders. The findings highlight the challenges of identifying causal effects and the importance of selecting valid instrumental variables in various domains.
  - **Keywords:** Instrumental Variables, Causal Effects, Weak Confounding, Conditional Common Entropy, Instrumental Inequality, Education, Economy, Public Health, Public Policy, Marketing, Non-identifiable causal queries, Validity of Instrumental Variables, Bounding causal effects, Testable conditions for IV, Simulated datasets, Real-world datasets


- [Pre-Training Protein Bi-level Representation Through Span Mask Strategy On 3D Protein Chains](https://icml.cc/virtual/2024/poster/32986) (Poster)
  - **Authors:** [Jiale Zhao](http://openreview.net/profile?id=~Zhao_jiale1), [Wanru Zhuang](http://openreview.net/profile?id=~Wanru_Zhuang1), [Jia Song](http://openreview.net/profile?id=~Jia_Song5), [Yaqi Li](http://openreview.net/profile?id=~Yaqi_Li2), [Shuqi Lu](http://openreview.net/profile?id=~Shuqi_Lu1)
  - **Affiliations:** DP Technology, Beijing, China; Institute of Computing Technology, UCAS, Beijing, China, DP Technology, Beijing, China; Xiamen University, School of Informatics, Xiamen, China, DP Technology, Beijing, China; Xiamen University, Institute of Artificial Intelligence, Xiamen, China, DP Technology, Beijing, China, DP Technology, Beijing, China
  - **TL;DR:** This study introduces a span mask pre-training strategy for 3D protein chains that effectively learns representations of both residues and atoms, addressing the limitations of existing models that primarily focus on residue-level information. The proposed method significantly outperforms other approaches in binding site and function prediction tasks.
  - **Keywords:** 3D protein modeling, pre-trained protein models, Span mask pre-training strategy, self-prediction techniques, Molecular docking, binding site prediction, function prediction, Information leakage in pre-training, insufficient residue representations, Effective protein representation learning, significant performance improvement in downstream tasks, Alpha carbon atoms, side chain atoms, residue-level modeling


- [Decoupling Feature Extraction and Classification Layers for Calibrated Neural Networks](https://icml.cc/virtual/2024/poster/34553) (Poster)
  - **Authors:** [Mikkel Jordahn](http://openreview.net/profile?id=~Mikkel_Jordahn1), [Pablo Olmos](http://openreview.net/profile?id=~Pablo_M._Olmos1)
  - **Affiliations:** Cognitive Systems, Technical University of Denmark, Kongens Lyngby, Denmark, Signal Processing Group (GTS), Universidad Carlos III de Madrid, Madrid, Spain
  - **TL;DR:** This study demonstrates that decoupling the training of feature extraction and classification layers in deep neural networks significantly enhances model calibration without sacrificing accuracy. The proposed methods, Two-Stage Training and Variational Two-Stage Training, effectively address the issue of overconfidence in predictions, particularly in safety-critical applications like healthcare.
  - **Keywords:** Deep Neural Networks, Model Calibration, Two-Stage Training (TST), Variational Two-Stage Training (V-TST), Gaussian Prior, Medical Diagnostics, Image Classification, Poor Calibration, Over-parameterization, Model Overconfidence, Improved Calibration, Retained Accuracy, Wide Residual Networks (WRN), Vision Transformers (ViT)


- [Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?](https://icml.cc/virtual/2024/poster/33148) (Poster)
  - **Authors:** [Mira Juergens](http://openreview.net/profile?id=~Mira_Juergens1), [Nis Meinert](http://openreview.net/profile?id=~Nis_Meinert1), [Viktor Bengs](http://openreview.net/profile?id=~Viktor_Bengs1), [Eyke Hüllermeier](http://openreview.net/profile?id=~Eyke_H%C3%BCllermeier1), [Willem Waegeman](http://openreview.net/profile?id=~Willem_Waegeman1)
  - **Affiliations:** Department of Data Analysis and Mathematical Modeling, Ghent University, Belgium, Institute of Communications and Navigation, German Aerospace Center (DLR), Neustrelitz, Germany, Department of Informatics, University of Munich (LMU), Germany, Department of Informatics, University of Munich (LMU), Germany, Department of Data Analysis and Mathematical Modeling, Ghent University, Belgium
  - **TL;DR:** This paper investigates the representation of epistemic uncertainty in machine learning through evidential deep learning methods, highlighting challenges in optimizing second-order loss functions and interpreting uncertainty measures. The findings suggest that while evidential deep learning is gaining traction, its effectiveness in providing reliable epistemic uncertainty quantification requires further exploration.
  - **Keywords:** Uncertainty quantification, Evidential deep learning, Epistemic uncertainty, Bayesian methods, Second-order risk minimization, Empirical risk minimization (ERM), Out-of-distribution detection, Robustness to adversarial attacks, Active learning, Incomplete data patterns, Noisy data, Biased predictions, Lack of knowledge about model accuracy, Novel theoretical insights, Optimization of second-order loss functions, Identifiability and convergence issues, Aleatoric uncertainty, Epistemic uncertainty, Conditional probability, Bayesian neural networks


- [Unsupervised Episode Generation for Graph Meta-learning](https://icml.cc/virtual/2024/poster/34790) (Poster)
  - **Authors:** [Jihyeong Jung](http://openreview.net/profile?id=~Jihyeong_Jung1), [Sangwoo Seo](http://openreview.net/profile?id=~Sangwoo_Seo1), [Sungwon Kim](http://openreview.net/profile?id=~Sungwon_Kim3), [Chanyoung Park](http://openreview.net/profile?id=~Chanyoung_Park1)
  - **Affiliations:** Department of Industrial & Systems Engineering, KAIST, Department of Industrial & Systems Engineering, KAIST, Graduate School of Data Science, KAIST, Department of Industrial & Systems Engineering, KAIST; Graduate School of Data Science, KAIST
  - **TL;DR:** This study introduces an unsupervised episode generation method called Neighbors as Queries (NAQ) to enhance Few-Shot Node Classification (FSNC) by leveraging all nodes in a graph, addressing label-scarcity and class imbalance issues. Experimental results show that NAQ can improve the performance of existing supervised graph meta-learning methods without requiring abundant labeled data.
  - **Keywords:** Unsupervised Learning, Graph Meta-learning, Few-Shot Node Classification, Neighbors as Queries (NAQ), Graph Neural Networks (GNNs), Graph Contrastive Learning (GCL), Node Classification, Graph-structured Data, Label-scarcity, Class Imbalance, Poor Generalization, Unsupervised Episode Generation, Model-agnostic Training, Few-Shot Learning, Meta-learning, Episodic Learning Framework


- [Tell, Don't Show: Language Guidance Eases Transfer Across Domains in Images and Videos](https://icml.cc/virtual/2024/poster/32918) (Poster)
  - **Authors:** [Tarun Kalluri](http://openreview.net/profile?id=~Tarun_Kalluri1), [Bodhisattwa Prasad Majumder](http://openreview.net/profile?id=~Bodhisattwa_Prasad_Majumder1), [Manmohan Chandraker](http://openreview.net/profile?id=~Manmohan_Chandraker3)
  - **Affiliations:** UC San Diego, Allen Institute for AI, UC San Diego
  - **TL;DR:** The study introduces LaGTran, a framework that leverages text supervision to enhance the transfer of knowledge across domains in images and videos, significantly outperforming previous methods. It addresses challenges related to distribution shifts and the scarcity of labeled data, demonstrating effectiveness on datasets like GeoNet and DomainNet.
  - **Keywords:** Domain adaptation, Language guidance, Unsupervised domain adaptation (UDA), Text-classifier, Computer vision, Image and video transfer, Distribution shifts, Domain gaps, Lack of labeled data, LaGTran framework, Improved transfer performance, GeoNet, DomainNet, Ego2Exo


- [Position: Benchmarking is Limited in Reinforcement Learning Research](https://icml.cc/virtual/2024/poster/33814) (Poster)
  - **Authors:** [Scott Jordan](http://openreview.net/profile?id=~Scott_M._Jordan1), [Adam White](http://openreview.net/profile?id=~Adam_White1), [Bruno da Silva](http://openreview.net/profile?id=~Bruno_Castro_da_Silva1), [Martha White](http://openreview.net/profile?id=~Martha_White1), [Philip Thomas](http://openreview.net/profile?id=~Philip_S._Thomas1)
  - **Affiliations:** University of Alberta; Canada Cifar AI Chair; Alberta Machine Intelligence Institute, University of Alberta; Canada Cifar AI Chair; Alberta Machine Intelligence Institute, University of Massachusetts, University of Alberta; Canada Cifar AI Chair; Alberta Machine Intelligence Institute, University of Massachusetts
  - **TL;DR:** This paper critiques the current benchmarking practices in reinforcement learning research, highlighting their computational costs and limitations in providing insights into algorithm performance. It advocates for an alternative experimentation paradigm focused on understanding algorithm behavior rather than merely evaluating performance.
  - **Keywords:** Reinforcement Learning, Benchmarking, Misleading claims in experimental practices, computational costs in benchmarking, Need for additional experimentation paradigms, scientific testing


- [Replicable Learning of Large-Margin Halfspaces](https://icml.cc/virtual/2024/poster/34688) (Spotlight Poster)
  - **Authors:** [Alkis Kalavasis](http://openreview.net/profile?id=~Alkis_Kalavasis1), [Amin Karbasi](http://openreview.net/profile?id=~Amin_Karbasi3), [Kasper Green Larsen](http://openreview.net/profile?id=~Kasper_Green_Larsen1), [Grigoris Velegkas](http://openreview.net/profile?id=~Grigoris_Velegkas1), [Felix Zhou](http://openreview.net/profile?id=~Felix_Zhou1)
  - **Affiliations:** Yale University, Yale University; Google Research, Aarhus University, Yale University, Yale University
  - **TL;DR:** This paper presents efficient replicable algorithms for learning large-margin halfspaces, improving upon previous methods in terms of sample complexity and running time. The findings emphasize the importance of replicability in machine learning experiments and propose new algorithms that achieve optimal sample complexity with respect to accuracy.
  - **Keywords:** replicable algorithms, large-margin halfspaces, replicability crisis in AI, SGD-based algorithms, dimension-independent algorithms, machine learning, AI, learning large-margin halfspaces, replicability in experiments, improved sample complexity, polynomial time algorithms, large-margin halfspaces, Perceptron algorithm, SVMs, AdaBoost


- [Beyond the Calibration Point: Mechanism Comparison in Differential Privacy](https://icml.cc/virtual/2024/poster/33354) (Poster)
  - **Authors:** [Georgios Kaissis](http://openreview.net/profile?id=~Georgios_Kaissis1), [Stefan Kolek](http://openreview.net/profile?id=~Stefan_Kolek1), [Borja de Balle Pigem](http://openreview.net/profile?id=~Borja_Balle2), [Jamie Hayes](http://openreview.net/profile?id=~Jamie_Hayes1), [Daniel Rueckert](http://openreview.net/profile?id=~Daniel_Rueckert2)
  - **Affiliations:** AI in Healthcare and Medicine; Institute of Radiology, Technical University of Munich, Germany, Mathematical Foundations of AI, LMU Munich, Google DeepMind, Google DeepMind, AI in Healthcare and Medicine; Institute of Radiology, Technical University of Munich, Germany
  - **TL;DR:** This paper addresses the inadequacies in comparing differential privacy mechanisms by introducing the ∆-divergence, which quantifies the worst-case privacy vulnerabilities associated with different mechanisms. The findings highlight that current practices in reporting privacy guarantees can lead to significant privacy risks, emphasizing the need for more robust comparison methods.
  - **Keywords:** Differential Privacy, Privacy Guarantees, Machine Learning, DP-SGD (Differentially Private Stochastic Gradient Descent), Subsampled Gaussian Mechanism, ∆-divergence, Privacy Vulnerabilities, Membership Inference Attacks, Data Reconstruction Attacks, Privacy Profiles, Decision-Theoretic Foundations, f-DP, Approximate DP, Privacy Budget


- [Think Before You Act: Decision Transformers with Working Memory](https://icml.cc/virtual/2024/poster/34121) (Poster)
  - **Authors:** [Jikun Kang](http://openreview.net/profile?id=~Jikun_Kang1), [Romain Laroche](http://openreview.net/profile?id=~Romain_Laroche1), [Xingdi Yuan](http://openreview.net/profile?id=~Xingdi_Yuan2), [Adam Trischler](http://openreview.net/profile?id=~Adam_Trischler1), [Xue Liu](http://openreview.net/profile?id=~Xue_Liu1), [Jie Fu](http://openreview.net/profile?id=~Jie_Fu2)
  - **Affiliations:** Department of Computer Science, McGill University, Montréal, Canada; Mila - Québec AI Institute, Montréal, Canada, None, Microsoft Research, Montréal, Canada, None, Department of Computer Science, McGill University, Montréal, Canada; Mila - Québec AI Institute, Montréal, Canada, Mila - Québec AI Institute, Montréal, Canada
  - **TL;DR:** This study proposes a working memory module for Decision Transformer-based agents to enhance their ability to generalize across multiple tasks while mitigating the forgetting phenomenon. The results demonstrate improved training efficiency and adaptability in various gaming and manipulation tasks.
  - **Keywords:** Decision Transformers, Working Memory, Generalization, Atari games, Meta-World object manipulation, Forgetting phenomenon, inefficiency in training, performance deterioration on new tasks, Working memory module, improved training efficiency, enhanced adaptability, Large Language Models (LLMs), implicit memory, cognitive psychology


- [C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models](https://icml.cc/virtual/2024/poster/34539) (Poster)
  - **Authors:** [Mintong Kang](http://openreview.net/profile?id=~Mintong_Kang1), [Nezihe Merve Gürel](http://openreview.net/profile?id=~Nezihe_Merve_G%C3%BCrel2), [Ning Yu](http://openreview.net/profile?id=~Ning_Yu2), [Dawn Song](http://openreview.net/profile?id=~Dawn_Song1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19)
  - **Affiliations:** University of Illinois at Urbana-Champaign, USA, Delft University of Technology, Netherlands, Netflix Eyeline Studios, USA, University of California, Berkeley, USA, University of Illinois at Urbana-Champaign, USA; University of Chicago, USA
  - **TL;DR:** This paper introduces C-RAG, a framework for certifying generation risks in retrieval-augmented language models (RAG), demonstrating that RAG can achieve lower generation risks compared to vanilla large language models (LLMs) under certain conditions. The study provides theoretical guarantees and empirical results supporting the effectiveness of the proposed method across various NLP datasets.
  - **Keywords:** Retrieval-Augmented Language Models, Trustworthiness of Language Models, Conformal Risk Analysis, Certified Generation Risks, Natural Language Processing, Safety-Critical Domains, Generation Risks, Hallucinations, Misalignments, C-RAG Framework, Conformal Generation Risk Guarantees, Large Language Models (LLMs), Retrieval-Augmented Models (RAG)


- [Certifiably Byzantine-Robust Federated Conformal Prediction](https://icml.cc/virtual/2024/poster/35015) (Poster)
  - **Authors:** [Mintong Kang](http://openreview.net/profile?id=~Mintong_Kang1), [Zhen Lin](http://openreview.net/profile?id=~Zhen_Lin2), [Jimeng Sun](http://openreview.net/profile?id=~Jimeng_Sun3), [Cao Xiao](http://openreview.net/profile?id=~Cao_Xiao2), [Bo Li](http://openreview.net/profile?id=~Bo_Li19)
  - **Affiliations:** University of Illinois at Urbana-Champaign, USA; Carle’s Illinois College of Medicine, USA, University of Illinois at Urbana-Champaign, USA, University of Illinois at Urbana-Champaign, USA; Carle’s Illinois College of Medicine, USA, GE Healthcare, USA, University of Illinois at Urbana-Champaign, USA; University of Chicago, USA
  - **TL;DR:** This study introduces Rob-FCP, a robust framework for federated conformal prediction that addresses vulnerabilities to Byzantine failures by ensuring reliable coverage guarantees despite malicious client behavior. The framework is theoretically validated and empirically demonstrated to maintain robustness across various attacks on healthcare datasets.
  - **Keywords:** Federated Learning, Conformal Prediction, Byzantine Robustness, Robust Federated Conformal Prediction (Rob-FCP), Conformal Calibration, Healthcare, Clinical Risk Prediction, Byzantine Failures, Coverage Guarantees, Malicious Clients, Conformal Coverage Bound, Malicious Client Number Estimator, Deep Neural Networks (DNNs), Exchangeable Data


- [Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choice](https://icml.cc/virtual/2024/poster/34355) (Oral)
  - **Authors:** [Masahiro Kato](http://openreview.net/profile?id=~Masahiro_Kato1), [Oga Akihiro](http://openreview.net/profile?id=~Oga_Akihiro1), [Wataru Komatsubara](http://openreview.net/profile?id=~Wataru_komatsubara1), [Ryo Inokuchi](http://openreview.net/profile?id=~Ryo_Inokuchi1)
  - **Affiliations:** Mizuho-DL Financial Technology Co., Ltd., Tokyo, Japan, Mizuho-DL Financial Technology Co., Ltd., Tokyo, Japan, Mizuho-DL Financial Technology Co., Ltd., Tokyo, Japan, Mizuho-DL Financial Technology Co., Ltd., Tokyo, Japan
  - **TL;DR:** This study proposes an adaptive experimental design that optimizes both covariate density and propensity score to efficiently estimate average treatment effects (ATEs) with reduced asymptotic variance. The findings indicate that this dual optimization approach is more effective than optimizing only the propensity score, leading to improved estimation accuracy in experimental settings.
  - **Keywords:** Adaptive experimental design, Average treatment effects (ATEs), Propensity score optimization, Covariate density optimization, Epidemiology, Economics, Estimation error reduction, Sample size reduction, Efficient ATE estimator, Minimization of semiparametric efficiency bound, Randomized control trial (RCT), Treatment-assignment probability


- [Neural Tangent Kernels for Axis-Aligned Tree Ensembles](https://icml.cc/virtual/2024/poster/33750) (Poster)
  - **Authors:** [Ryuichi Kanoh](http://openreview.net/profile?id=~Ryuichi_Kanoh1), [Mahito Sugiyama](http://openreview.net/profile?id=~Mahito_Sugiyama1)
  - **Affiliations:** National Institute of Informatics; The Graduate University for Advanced Studies, SOKENDAI, National Institute of Informatics; The Graduate University for Advanced Studies, SOKENDAI
  - **TL;DR:** This paper investigates the theoretical properties of axis-aligned constraints in soft tree ensembles using the Neural Tangent Kernel framework, revealing insights into their training behavior and generalization performance. The findings suggest that various tree architectures can be effectively analyzed and transformed while maintaining the same NTK.
  - **Keywords:** axis-aligned tree ensembles, ensemble learning, soft trees, Neural Tangent Kernel (NTK), gradient-based optimization, Multiple Kernel Learning (MKL), theoretical understanding of learning behavior, impact of axis-aligned constraints on generalization performance, insights into training behavior of soft trees, transformation of non-oblivious to oblivious tree ensembles, PyTorch Tabular, interpretability of soft trees


- [On the Generalization of Equivariant Graph Neural Networks](https://icml.cc/virtual/2024/poster/33757) (Poster)
  - **Authors:** [Rafał Karczewski](http://openreview.net/profile?id=~Rafal_Karczewski1), [Amauri Souza](http://openreview.net/profile?id=~Amauri_H_Souza1), [Vikas Garg](http://openreview.net/profile?id=~Vikas_Garg2)
  - **Affiliations:** Department of Computer Science, Aalto University, Finland, Federal Institute of Ceará (Brazil); YaiYai Ltd., Department of Computer Science, Aalto University, Finland; YaiYai Ltd.
  - **TL;DR:** This study establishes the first generalization bound for E(n)-Equivariant Graph Neural Networks (EGNNs), revealing that the spectral norms of initial layers significantly impact generalization and that ε-normalization is beneficial. The findings are supported by experiments demonstrating a strong correlation between theoretical and empirical generalization gaps.
  - **Keywords:** Equivariant Graph Neural Networks, Generalization, E(n)-Equivariant Graph Neural Networks (EGNNs), Weisfeiler-Leman isomorphism test, Molecular property prediction, Drug binding structure prediction, Generative modeling, Structure-based drug design, Molecular dynamics, Generalization capability, Expressivity of models, Generalization bounds, Spectral norm regularizer


- [Progressive Inference: Explaining Decoder-Only Sequence Classification Models Using Intermediate Predictions](https://icml.cc/virtual/2024/poster/33394) (Poster)
  - **Authors:** [Sanjay Kariyappa](http://openreview.net/profile?id=~Sanjay_Kariyappa1), [Freddy Lecue](http://openreview.net/profile?id=~Freddy_Lecue1), [Saumitra Mishra](http://openreview.net/profile?id=~Saumitra_Mishra1), [Christopher Pond](http://openreview.net/profile?id=~Christopher_Pond1), [Daniele Magazzeni](http://openreview.net/profile?id=~Daniele_Magazzeni1), [Manuela Veloso](http://openreview.net/profile?id=~Manuela_Veloso1)
  - **Affiliations:** JPMorganChase AI Research, None, None, None, None, None
  - **TL;DR:** This paper introduces Progressive Inference, a framework for computing input attributions to explain predictions of decoder-only sequence classification models. The proposed methods, SP-PI and MP-PI, leverage intermediate predictions to provide significantly better attributions, enhancing trust in model predictions in critical applications.
  - **Keywords:** Explainability, Large Language Models, Progressive Inference, Single Pass-Progressive Inference (SP-PI), Multi Pass-Progressive Inference (MP-PI), Kernel SHAP, Text classification, Healthcare, Finance, Input attribution, Model predictions explanation, Improved attribution methods, Intermediate predictions, Decoder-only Transformer, Causal attention mechanism


- [Accelerating Convergence in Bayesian Few-Shot Classification](https://icml.cc/virtual/2024/poster/34808) (Poster)
  - **Authors:** [Tianjun Ke](http://openreview.net/profile?id=~Tianjun_Ke1), [Haoqun Cao](http://openreview.net/profile?id=~Haoqun_Cao1), [Feng Zhou](http://openreview.net/profile?id=~Feng_Zhou9)
  - **Affiliations:** Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing, China; None, Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing, China; None, Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing, China; Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing
  - **TL;DR:** This study integrates mirror descent-based variational inference into Gaussian process-based few-shot classification to address non-conjugate inference challenges, resulting in accelerated convergence and improved uncertainty quantification. The experimental results show competitive classification accuracy compared to baseline models.
  - **Keywords:** Bayesian few-shot classification, few-shot learning, Mirror descent-based variational inference, Gaussian processes, Medical diagnosis, autonomous driving, Epistemic uncertainty, model overfitting, non-conjugate inference, Accelerated convergence, improved uncertainty quantification, competitive classification accuracy


- [Challenges and Considerations in the Evaluation of Bayesian Causal Discovery](https://icml.cc/virtual/2024/poster/33620) (Poster)
  - **Authors:** [Amir Mohammad Karimi Mamaghan](http://openreview.net/profile?id=~Amir_Mohammad_Karimi_Mamaghan1), [Panagiotis Tigas](http://openreview.net/profile?id=~Panagiotis_Tigas1), [Karl Johansson](http://openreview.net/profile?id=~Karl_Henrik_Johansson1), [Yarin Gal](http://openreview.net/profile?id=~Yarin_Gal1), [Yashas Annadani](http://openreview.net/profile?id=~Yashas_Annadani1), [Stefan Bauer](http://openreview.net/profile?id=~Stefan_Bauer1)
  - **Affiliations:** KTH Royal Institute of Technology, OATML, University of Oxford, KTH Royal Institute of Technology; Digital Futures, OATML, University of Oxford, Helmholtz AI; TU Munich, Helmholtz AI; TU Munich
  - **TL;DR:** This paper investigates the challenges in evaluating Bayesian Causal Discovery (BCD) methods, emphasizing the need for better metrics to assess the quality of approximate posteriors, particularly in low sample size scenarios. The findings reveal that existing metrics often do not correlate well with the true posterior quality, highlighting the importance of considering the nature of the true posterior in future evaluations.
  - **Keywords:** Bayesian Causal Discovery, Causal Relationships, Uncertainty Representation, Posterior Distribution, Approximate Inference, Experimental Design, Causal Decision Making, Biological Data Analysis, Evaluation of Causal Discovery Metrics, Low Sample Sizes, Identifiability of Causal Models, Evaluation Metrics for Bayesian Causal Discovery, New Evaluation Procedures, Structural Causal Model (SCM), Epistemic Uncertainty


- [Fair Classification with Partial Feedback: An Exploration-Based Data Collection Approach](https://icml.cc/virtual/2024/poster/33825) (Poster)
  - **Authors:** [Vijay Keswani](http://openreview.net/profile?id=~Vijay_Keswani1), [Anay Mehrotra](http://openreview.net/profile?id=~Anay_Mehrotra1), [L. Elisa Celis](http://openreview.net/profile?id=~L._Elisa_Celis2)
  - **Affiliations:** Duke University, Yale University, Yale University
  - **TL;DR:** This study presents an approach to train classifiers in settings with partial feedback, ensuring exploration of all sub-populations and addressing biases in decision-making. The proposed method improves the quality of outcome data and increases true positive rates across groups while maintaining predictive utility.
  - **Keywords:** Fair classification, Partial feedback, Machine learning, Exploration strategies, Classifier training, Credit lending, Healthcare, Criminal justice, Partial feedback, Data distribution distortion, Classification errors, Bias in decision-making, Improved outcome data collection, Enhanced true positive rates, Fairness properties, Real-world datasets


- [A Universal Transfer Theorem for Convex Optimization Algorithms Using Inexact First-order Oracles](https://icml.cc/virtual/2024/poster/33688) (Poster)
  - **Authors:** [Phillip Kerger](http://openreview.net/profile?id=~Phillip_Kerger1), [Marco Molinaro](http://openreview.net/profile?id=~Marco_Molinaro1), [Hongyi Jiang](http://openreview.net/profile?id=~Hongyi_Jiang1), [Amitabh Basu](http://openreview.net/profile?id=~Amitabh_Basu1)
  - **Affiliations:** Department of Applied Mathematics and Statistics, Johns Hopkins University, Baltimore, USA, Microsoft Research, Redmond, USA; Department of Computer Science, PUC-Rio, Rio de Janeiro, Brazil, Department of Civil and Environmental Engineering, Cornell University, Ithaca, USA, Department of Applied Mathematics and Statistics, Johns Hopkins University, Baltimore, USA
  - **TL;DR:** This paper presents a universal transfer theorem that allows existing convex optimization algorithms using exact first-order information to effectively solve problems with inexact first-order information. The findings extend the applicability of these algorithms to a broader range of methods and structured nonconvexities, enhancing their utility in optimization tasks.
  - **Keywords:** convex optimization, inexact first-order information, first-order oracles, gradient descent, projection-free methods, cutting-plane methods, optimization problems, structured nonconvexities, query complexity, inexact oracle responses, universal transfer theorem, optimization algorithms


- [Off-policy Evaluation Beyond Overlap: Sharp Partial Identification Under Smoothness](https://icml.cc/virtual/2024/poster/33073) (Poster)
  - **Authors:** [Samir Khan](http://openreview.net/profile?id=~Samir_Khan1), [Martin Saveski](http://openreview.net/profile?id=~Martin_Saveski1), [Johan Ugander](http://openreview.net/profile?id=~Johan_Ugander1)
  - **Affiliations:** Department of Statistics, Stanford University, Information School, University of Washington, Department of Management Science and Engineering, Stanford University
  - **TL;DR:** This study develops methods for off-policy evaluation without the overlap assumption, focusing on Lipschitz smoothness to provide sharp bounds on the off-policy value. The proposed methods yield asymptotically optimal estimators and demonstrate effectiveness through semi-synthetic examples.
  - **Keywords:** Off-policy evaluation, Policy learning, Importance weighting, Imputation, Linear programming, Overlap violations, Bias in estimates, Sharp bounds, Asymptotically optimal estimators, Lipschitz smoothness, Non-parametric assumptions


- [An Improved Finite-time Analysis of Temporal Difference Learning with Deep Neural Networks](https://icml.cc/virtual/2024/poster/33533) (Poster)
  - **Authors:** [Zhifa Ke](http://openreview.net/profile?id=~Zhifa_Ke1), [Zaiwen Wen](http://openreview.net/profile?id=~Zaiwen_Wen1), [Junyu Zhang](http://openreview.net/profile?id=~Junyu_Zhang1)
  - **Affiliations:** Center for Data Science, Peking University, China, Beijing International Center for Mathematical Research; Center for Machine Learning Research and Changsha Institute for Computing and Digital Economy, Beijing, China, Department of Industrial Systems Engineering and Management, National University of Singapore, Singapore
  - **TL;DR:** This paper presents an improved finite-time analysis of temporal difference learning using deep neural networks, achieving a new sample complexity of ˜O(ϵ−1) under Markovian sampling. The findings address the challenges of nonlinearity in action-value approximation and contribute to the theoretical understanding of reinforcement learning algorithms.
  - **Keywords:** Temporal Difference Learning, Reinforcement Learning, Neural Network Function Approximation, Non-asymptotic Analysis, Large-scale Reinforcement Learning Tasks, Nonlinearity of Action-value Approximation, Convergence Issues, Improved Sample Complexity, Finite-time Analysis, Q Function, Bellman Operator, Markovian Sampling


- [Breaking through the learning plateaus of in-context learning in Transformer](https://icml.cc/virtual/2024/poster/35111) (Poster)
  - **Authors:** [Jingwen Fu](http://openreview.net/profile?id=~Jingwen_Fu1), [Tao Yang](http://openreview.net/profile?id=~Tao_Yang9), [Yuwang Wang](http://openreview.net/profile?id=~Yuwang_Wang3), [Yan Lu](http://openreview.net/profile?id=~Yan_Lu7), [Nanning Zheng](http://openreview.net/profile?id=~Nanning_Zheng1)
  - **Affiliations:** National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Tsinghua University, Microsoft Research Asia, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University
  - **TL;DR:** This study investigates the learning plateaus in the in-context learning ability of Transformer models and proposes three strategies to enhance this learning without increasing model size, demonstrating effective results in natural language processing tasks. The findings suggest a path toward developing more efficient and environmentally sustainable AI systems.
  - **Keywords:** In-context learning, Transformer models, Natural language processing, Learning plateaus, computational intensity, Strategies to expedite learning, improved in-context learning capability, Weights component, context component, Eco-friendly AI systems


- [DUPLEX: Dual GAT for Complex Embedding of Directed Graphs](https://icml.cc/virtual/2024/poster/34257) (Poster)
  - **Authors:** [Zhaoru Ke](http://openreview.net/profile?id=~Zhaoru_Ke1), [Hang Yu](http://openreview.net/profile?id=~Hang_Yu1), [Jianguo Li](http://openreview.net/profile?id=~Jianguo_Li2), [Haipeng Zhang](http://openreview.net/profile?id=~Haipeng_Zhang3)
  - **Affiliations:** School of Information Science and Technology, ShanghaiTech University, China; Ant Group, China, Ant Group, China, Ant Group, China, School of Information Science and Technology, ShanghaiTech University, China; Ant Group, China
  - **TL;DR:** The study introduces DUPLEX, an inductive framework for directed graph embeddings that effectively captures directed edge information and enhances representation for nodes with low connectivity. It demonstrates superior performance compared to existing models, particularly in terms of inductive learning and adaptability across diverse tasks.
  - **Keywords:** directed graph embedding, graph analytics, Hermitian adjacency matrix decomposition, dual GAT encoder, social networks, recommendation systems, bioinformatics, traffic prediction, financial analysis, suboptimal representations for low in/out-degree nodes, limited inductive ability, narrow generalizability, robust inductive capability, adaptability across various tasks


- [LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging](https://icml.cc/virtual/2024/poster/32836) (Poster)
  - **Authors:** [Jinuk Kim](http://openreview.net/profile?id=~Jinuk_Kim1), [Marwa El Halabi](http://openreview.net/profile?id=~Marwa_El_Halabi2), [Mingi Ji](http://openreview.net/profile?id=~Mingi_Ji1), [Hyun Oh Song](http://openreview.net/profile?id=~Hyun_Oh_Song1)
  - **Affiliations:** Department of Computer Science and Engineering, Seoul National University; Neural Processing Research Center, Samsung - SAIT AI Lab, Montreal, Google, Department of Computer Science and Engineering, Seoul National University; Neural Processing Research Center
  - **TL;DR:** The study introduces LayerMerge, a novel method for compressing neural networks by jointly pruning convolution and activation layers to enhance efficiency while minimizing performance loss. Empirical results show that LayerMerge outperforms existing methods in reducing latency and maintaining model performance across various architectures.
  - **Keywords:** neural network depth compression, efficiency enhancement, layer pruning, layer merging, dynamic programming, image classification, image generation, latency reduction, kernel size increase, LayerMerge method, surrogate optimization problem, convolutional neural networks (CNNs), non-linear activation functions


- [Achieving Lossless Gradient Sparsification via Mapping to Alternative Space in Federated Learning](https://icml.cc/virtual/2024/poster/32781) (Poster)
  - **Authors:** [Do-Yeon Kim](http://openreview.net/profile?id=~Do-Yeon_Kim1), [Dong-Jun Han](http://openreview.net/profile?id=~Dong-Jun_Han1), [Jun Seo](http://openreview.net/profile?id=~Jun_Seo1), [Jaekyun Moon](http://openreview.net/profile?id=~Jaekyun_Moon2)
  - **Affiliations:** Korea Advanced Institute of Science and Technology (KAIST), Purdue University, LG AI Research, Korea Advanced Institute of Science and Technology (KAIST)
  - **TL;DR:** This paper presents a novel approach to lossless gradient sparsification in federated learning by mapping gradients to an alternative space, which enhances compressibility and reduces communication burdens. The proposed method not only achieves more aggressive compression with minimal information loss but also demonstrates higher accuracy than traditional full gradient uploading in certain scenarios.
  - **Keywords:** Federated Learning, Gradient Compression, Lossless Gradient Sparsification, Mapping Function, Sparsification-based Compressors, Healthcare, Financial Services, Autonomous Driving, Communication Burden, Compression Rate Limitations, Improved Compressibility, Higher Accuracies, Theoretical Convergence Rate Confirmation


- [CARTE: Pretraining and Transfer for Tabular Learning](https://icml.cc/virtual/2024/poster/34796) (Poster)
  - **Authors:** [Myung Jun Kim](http://openreview.net/profile?id=~Myung_Jun_Kim1), [Leo Grinsztajn](http://openreview.net/profile?id=~Leo_Grinsztajn1), [Gael Varoquaux](http://openreview.net/profile?id=~Gael_Varoquaux1)
  - **Affiliations:** SODA Team, Inria Saclay, France, SODA Team, Inria Saclay, France; Probabl.ai, France, SODA Team, Inria Saclay, France; Probabl.ai, France
  - **TL;DR:** The study introduces CARTE, a neural architecture designed for pretraining and transfer learning on tabular data without the need for schema or entity matching. It demonstrates significant performance improvements over traditional tree-based models and enables joint learning across multiple tables.
  - **Keywords:** Pretraining, Transfer Learning, Tabular Data, Neural Architecture, Graph Representation, Graph-Attentional Network, Data Integration, Schema Matching, Entity Matching, CARTE (Context Aware Representation of Table Entries), Performance Gain, Joint Learning, Tree-Based Models, Background Data


- [Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model](https://icml.cc/virtual/2024/poster/34846) (Poster)
  - **Authors:** [Mikail Khona](http://openreview.net/profile?id=~Mikail_Khona2), [Maya Okawa](http://openreview.net/profile?id=~Maya_Okawa1), [Jan Hula](http://openreview.net/profile?id=~Jan_Hula1), [Rahul Ramesh](http://openreview.net/profile?id=~Rahul_Ramesh2), [Kento Nishi](http://openreview.net/profile?id=~Kento_Nishi1), [Robert Dick](http://openreview.net/profile?id=~Robert_P._Dick1), [Ekdeep Singh Lubana](http://openreview.net/profile?id=~Ekdeep_Singh_Lubana1), [Hidenori Tanaka](http://openreview.net/profile?id=~Hidenori_Tanaka1)
  - **Affiliations:** Massachusetts Institute of Technology; NTT Physics and Informatics Lab, NTT Physics and Informatics Lab, Czech Technical University in Prague; University of Ostrava, University of Pennsylvania; NTT Physics and Informatics Lab, Center for Brain Science, Harvard University; NTT Physics and Informatics Lab, University of Michigan, Center for Brain Science, Harvard University; University of Michigan, NTT Physics and Informatics Lab; Center for Brain Science, Harvard University
  - **TL;DR:** This study investigates stepwise inference in Transformers through a synthetic graph navigation model, revealing key phenomena such as reasoning gaps and biases in model outputs. The findings provide a framework for understanding the mechanisms behind stepwise inference, which enhances the performance of language models on complex tasks.
  - **Keywords:** Stepwise inference, Transformers, Language models, Autoregressive models, Chain-of-thought, Scratchpads, Reasoning gap, Diversity-accuracy trade-off, Simplicity bias, Compositional generalization, Synthetic framework for studying stepwise inference, Mechanistic hypotheses, Large Language Models (LLMs), Graph navigation


- [Active Label Correction for Semantic Segmentation with Foundation Models](https://icml.cc/virtual/2024/poster/33872) (Poster)
  - **Authors:** [Hoyoung Kim](http://openreview.net/profile?id=~Hoyoung_Kim1), [SEHYUN HWANG](http://openreview.net/profile?id=~Sehyun_Hwang1), [Suha Kwak](http://openreview.net/profile?id=~Suha_Kwak3), [Jungseul Ok](http://openreview.net/profile?id=~Jungseul_Ok2)
  - **Affiliations:** Graduate School of AI, POSTECH, Pohang, Republic of Korea, Department of CSE, POSTECH, Pohang, Republic of Korea, Graduate School of AI, POSTECH, Pohang, Republic of Korea; Department of CSE, POSTECH, Pohang, Republic of Korea, Graduate School of AI, POSTECH, Pohang, Republic of Korea; Department of CSE, POSTECH, Pohang, Republic of Korea
  - **TL;DR:** This study presents an Active Label Correction (ALC) framework that utilizes foundation models to improve the accuracy of pixel-wise annotations in semantic segmentation. The proposed method demonstrates significant efficiency in correcting pseudo labels, resulting in a revised dataset with 2.6 million corrected pixels.
  - **Keywords:** Active Label Correction, Semantic Segmentation, Foundation Models, Zero-shot Predictions, Correction Queries, Image Segmentation, Dataset Annotation, Labor-intensive Annotations, Error-prone Datasets, Cold-start Problem, Annotator-friendly Correction Queries, Revised Dataset, PASCAL, Cityscapes, Kvasir-SEG, Superpixels, Pixel-wise Annotations


- [Neural Tangent Kernels Motivate Cross-Covariance Graphs in Neural Networks](https://icml.cc/virtual/2024/poster/34948) (Poster)
  - **Authors:** [Shervin Khalafi](http://openreview.net/profile?id=~Shervin_Khalafi1), [Saurabh Sihag](http://openreview.net/profile?id=~Saurabh_Sihag1), [Alejandro Ribeiro](http://openreview.net/profile?id=~Alejandro_Ribeiro1)
  - **Affiliations:** Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA
  - **TL;DR:** This study investigates the role of Neural Tangent Kernels in optimizing graph representations within Graph Neural Networks, revealing that enhancing alignment with the NTK can improve convergence rates and generalization. Experimental results demonstrate that models utilizing cross-covariance matrices outperform those using only input data covariance matrices in multi-variate time series prediction tasks.
  - **Keywords:** Neural Tangent Kernels, Graph Neural Networks, Over-parameterized Neural Networks, Gradient Descent, Graph Shift Operator, Multi-variate Time Series Prediction, Generalization to unseen data, Convergence rate of gradient descent, Theoretical guarantees on alignment optimality, Performance comparison of GNN-based models, Cross-Covariance, Covariance Matrix


- [Pluvial Flood Emulation with Hydraulics-informed Message Passing](https://icml.cc/virtual/2024/poster/33265) (Poster)
  - **Authors:** [Arnold Kazadi](http://openreview.net/profile?id=~Arnold_Kazadi1), [James Doss-Gollin](http://openreview.net/profile?id=~James_Doss-Gollin1), [Arlei Silva](http://openreview.net/profile?id=~Arlei_Lopes_da_Silva1)
  - **Affiliations:** Department of Computer Science, Rice University, Houston, TX, USA; Department of Civil and Environmental Engineering, Rice University, Houston, TX, USA, Department of Civil and Environmental Engineering, Rice University, Houston, TX, USA, Department of Computer Science, Rice University, Houston, TX, USA
  - **TL;DR:** This study introduces a hydraulics-informed graph neural network for flood simulation that effectively predicts water depths using precipitation data, addressing challenges related to topography and data sparsity. The model outperforms existing methods, particularly in early flood stages, demonstrating its potential for real-time applications in flood risk management.
  - **Keywords:** flood modeling, machine learning, physics-based simulation, graph neural network, message-passing framework, shallow-water equations, flood risk mitigation, real-time flood warning, complexity of topography, sparsity of precipitation data, calibration failures in numerical models, hydraulics-informed model, improved water flow propagation prediction, dataset covering 9 regions and 7 historical precipitation events, LSTM (Long Short-Term Memory networks), hydrodynamic numerical models


- [ODIM: Outlier Detection via Likelihood of Under-Fitted Generative Models](https://icml.cc/virtual/2024/poster/34059) (Poster)
  - **Authors:** [Dongha Kim](http://openreview.net/profile?id=~Dongha_Kim1), [Jaesung Hwang](http://openreview.net/profile?id=~JaesungHwang1), [Jongjin Lee](http://openreview.net/profile?id=~Jongjin_Lee1), [Kunwoong Kim](http://openreview.net/profile?id=~Kunwoong_Kim1), [Yongdai Kim](http://openreview.net/profile?id=~Yongdai_Kim1)
  - **Affiliations:** Department of Statistics and Data Science Center, Sungshin Women’s University, Seoul, Republic of Korea, SK Telecom, Seoul, Republic of Korea, Samsung Research, Seoul, Republic of Korea, Department of Statistics, Seoul National University, Seoul, Republic of Korea, Department of Statistics, Seoul National University, Seoul, Republic of Korea
  - **TL;DR:** This study introduces ODIM, a novel method for unsupervised outlier detection that leverages the likelihood from carefully under-fitted deep generative models to effectively identify inliers and outliers. The method demonstrates significant computational efficiency and robust performance across various data types, validated through extensive empirical analyses on nearly 60 datasets.
  - **Keywords:** Unsupervised outlier detection (UOD), Deep generative models (DGMs), Likelihood-based approaches, Inlier-memorization (IM) effect, Fraud detection, Fault detection, Defect detection in images, Identifying inliers and outliers without labeled information, Poor performance of fully-trained DGMs in distinguishing inliers from outliers, Outlier detection via the IM effect (ODIM), Computational efficiency improvements


- [Can Machines Learn the True Probabilities?](https://icml.cc/virtual/2024/poster/33957) (Poster)
  - **Authors:** [Jinsook Kim](http://openreview.net/profile?id=~Jinsook_Kim1)
  - **Affiliations:** Underwood International College, Yonsei University, Seoul, Korea
  - **TL;DR:** The paper investigates whether AI machines can learn true objective probabilities from data under uncertainty. It concludes that machines can learn these probabilities only if they are directly observable, establishing a connection between well-calibration and the learning process.
  - **Keywords:** true objective probabilities, probabilistic machine learning, uncertainty, learning true probabilities, empirical frequency, well-calibration, theorem on learning and well-calibration


- [Tuning-Free Stochastic Optimization](https://icml.cc/virtual/2024/poster/34786) (Spotlight Poster)
  - **Authors:** [Ahmed Khaled](http://openreview.net/profile?id=~Ahmed_Khaled1), [Chi Jin](http://openreview.net/profile?id=~Chi_Jin1)
  - **Affiliations:** Electrical and Computer Engineering Department, Princeton University, Princeton, NJ, USA, Electrical and Computer Engineering Department, Princeton University, Princeton, NJ, USA
  - **TL;DR:** The paper introduces the concept of tuning-free algorithms that can self-tune during optimization, particularly focusing on matching the performance of optimally-tuned Stochastic Gradient Descent (SGD). It discusses the conditions under which tuning-free optimization is possible and presents both achievable results and impossibility results in this context.
  - **Keywords:** tuning-free algorithms, stochastic optimization, Stochastic Gradient Descent (SGD), DoG algorithm, DoWG algorithm, hyperparameter tuning, optimization over bounded and unbounded domains, matching performance of optimally-tuned algorithms, impossibility results for tuning-free optimization, convex functions, Lipschitz functions, stationary points


- [Gaussian Plane-Wave Neural Operator for Electron Density Estimation](https://icml.cc/virtual/2024/poster/33959) (Poster)
  - **Authors:** [Seongsu Kim](http://openreview.net/profile?id=~Seongsu_Kim2), [Sungsoo Ahn](http://openreview.net/profile?id=~Sungsoo_Ahn1)
  - **Affiliations:** Pohang University of Science and Technology (POSTECH), Pohang, South Korea, Pohang University of Science and Technology (POSTECH), Pohang, South Korea
  - **TL;DR:** This study introduces the Gaussian plane-wave neural operator (GPWNO) for predicting electron density, addressing the computational challenges of density functional theory (DFT). The proposed method demonstrates superior performance in representing both high- and low-frequency components of electron density across various datasets.
  - **Keywords:** electron density prediction, machine learning, density functional theory (DFT), Gaussian plane-wave neural operator (GPWNO), neural operators, deep learning, chemical systems, battery cathodes, solar cell materials, drug design, computational complexity of DFT, high-dimensional data representation, superior performance over baselines, effective representation of density components, QM9, MD, material project datasets, plane-wave basis, Gaussian-type orbital, Kohn-Sham density functional theory


- [Clustered Federated Learning via Gradient-based Partitioning](https://icml.cc/virtual/2024/poster/34402) (Poster)
  - **Authors:** [Heasung Kim](http://openreview.net/profile?id=~Heasung_Kim1), [Hyeji Kim](http://openreview.net/profile?id=~Hyeji_Kim1), [Gustavo De Veciana](http://openreview.net/profile?id=~Gustavo_De_Veciana2)
  - **Affiliations:** Department of Electrical and Computer Engineering, The University of Texas at Austin, TX, USA, Department of Electrical and Computer Engineering, The University of Texas at Austin, TX, USA, Department of Electrical and Computer Engineering, The University of Texas at Austin, TX, USA
  - **TL;DR:** This paper presents a novel algorithm for Clustered Federated Learning (CFL) that effectively groups clients based on the similarity of their model updates, addressing challenges related to data heterogeneity and privacy. The proposed method demonstrates improved clustering accuracy and convergence speed, achieving near-optimal performance in various benchmarks.
  - **Keywords:** Clustered Federated Learning, Distributed Learning, Data Heterogeneity, Gradient-based Partitioning, Clustering Algorithms, Data Heterogeneity, Sub-optimal Client Groupings, Privacy-preserving Federated Learning, Robust Clustering, Near-optimal Error Rate, Improved Convergence Speed, Clustering Accuracy, Federated Learning (FL), Model Personalization, Lightweight Models


- [Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs](https://icml.cc/virtual/2024/poster/34866) (Poster)
  - **Authors:** [Mingyu Kim](http://openreview.net/profile?id=~Mingyu_Kim2), [Kim Jun-Seong](http://openreview.net/profile?id=~Kim_Jun-Seong1), [Se-Young Yun](http://openreview.net/profile?id=~Se-Young_Yun1), [Jin-Hwa Kim](http://openreview.net/profile?id=~Jin-Hwa_Kim1)
  - **Affiliations:** KAIST AI, POSTECH EE, KAIST AI, NAVER AI Lab & SNU AIIS
  - **TL;DR:** This study proposes a method that integrates multi-plane representation with a coordinate-based MLP network to improve the performance of Neural Radiance Fields from sparse inputs. The approach effectively captures both low-frequency and fine-grained details, demonstrating superior results with fewer parameters compared to baseline models.
  - **Keywords:** Neural Radiance Fields, multi-plane representation, coordinate-based MLP network, MLP networks, sinusoidal encoding, residual connections, Static and dynamic scenes, 3D rendering, Data sparsity, low-frequency detail capture, model overfitting, Integration of multi-plane representation and coordinate-based networks, progressive training scheme


- [Improving Robustness to Multiple Spurious Correlations by Multi-Objective Optimization](https://icml.cc/virtual/2024/poster/34672) (Poster)
  - **Authors:** [Nayeong Kim](http://openreview.net/profile?id=~Nayeong_Kim1), [Juwon Kang](http://openreview.net/profile?id=~Juwon_Kang1), [Sungsoo Ahn](http://openreview.net/profile?id=~Sungsoo_Ahn1), [Jungseul Ok](http://openreview.net/profile?id=~Jungseul_Ok2), [Suha Kwak](http://openreview.net/profile?id=~Suha_Kwak3)
  - **Affiliations:** Department of Computer Science and Engineering, POSTECH, Pohang, Korea; Graduate School of Artificial Intelligence, POSTECH, Pohang, Korea, Department of Computer Science and Engineering, POSTECH, Pohang, Korea; Graduate School of Artificial Intelligence, POSTECH, Pohang, Korea, Department of Computer Science and Engineering, POSTECH, Pohang, Korea; Graduate School of Artificial Intelligence, POSTECH, Pohang, Korea, Department of Computer Science and Engineering, POSTECH, Pohang, Korea; Graduate School of Artificial Intelligence, POSTECH, Pohang, Korea, Department of Computer Science and Engineering, POSTECH, Pohang, Korea; Graduate School of Artificial Intelligence, POSTECH, Pohang, Korea
  - **TL;DR:** This study proposes a novel training method to improve model robustness against multiple spurious correlations by dynamically optimizing group-wise losses. The method outperforms existing approaches on datasets with multiple biases and introduces a new benchmark, MultiCelebA, for evaluating debiased training methods.
  - **Keywords:** debiased training, spurious correlations, multi-objective optimization, linear combination of group-wise losses, mini-max Pareto solution, image classification, bias mitigation, multiple biases, undesirable shortcuts, subgroup imbalance, novel training method, new benchmark (MultiCelebA), MultiCelebA, empirical risk minimization (ERM)


- [Learning to Explore for Stochastic Gradient MCMC](https://icml.cc/virtual/2024/poster/33694) (Poster)
  - **Authors:** [SeungHyun Kim](http://openreview.net/profile?id=~SeungHyun_Kim3), [Seohyeon Jung](http://openreview.net/profile?id=~Seohyeon_Jung1), [SeongHyeon Kim](http://openreview.net/profile?id=~SeongHyeon_Kim4), [Juho Lee](http://openreview.net/profile?id=~Juho_Lee2)
  - **Affiliations:** Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, Nexon Korea, Seongnam, South Korea; Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; AITRICS, Seoul, South Korea
  - **TL;DR:** This paper proposes a meta-learning strategy to enhance Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) for efficient exploration of multi-modal posterior distributions in Bayesian Neural Networks. The method significantly improves sampling efficiency and performance on various tasks without incurring substantial computational overhead.
  - **Keywords:** Bayesian Neural Networks, Stochastic Gradient Markov Chain Monte Carlo, SGMCMC, cyclical learning rate scheduling, meta-learning, autonomous driving, medical diagnosis, finance, image classification, high-dimensional parameters, multi-modality of posterior distributions, computational expense, improved sampling efficiency, exploration of high-density regions in posterior landscape


- [Variational Partial Group Convolutions for Input-Aware Partial Equivariance of Rotations and Color-Shifts](https://icml.cc/virtual/2024/poster/32678) (Poster)
  - **Authors:** [Hyunsu Kim](http://openreview.net/profile?id=~Hyunsu_Kim2), [Ye Gon Kim](http://openreview.net/profile?id=~Yegon_Kim1), [Hongseok Yang](http://openreview.net/profile?id=~Hongseok_Yang2), [Juho Lee](http://openreview.net/profile?id=~Juho_Lee2)
  - **Affiliations:** Kim Jaechul Graduate School of AI, KAIST, Daejeon, South Korea; AITRICS, Seoul, South Korea, Kim Jaechul Graduate School of AI, KAIST, Daejeon, South Korea, School of Computing, KAIST, Daejeon, South Korea, Kim Jaechul Graduate School of AI, KAIST, Daejeon, South Korea; AITRICS, Seoul, South Korea
  - **TL;DR:** This paper introduces Variational Partial G-CNN (VP G-CNN), a novel approach that captures varying levels of partial equivariance tailored to individual data instances, addressing the limitations of fixed equivariance in existing models. The proposed method demonstrates robust performance across various datasets, effectively adapting to the specific needs of the data.
  - **Keywords:** Group Equivariant CNNs, Partial Equivariance, Variational Partial G-CNN, Variational Inference, Image Classification, Computer Vision, Fixed Equivariance, Adaptability to Partial Symmetries, Training Instability, Adjusting Equivariance Levels, Robust Performance Metrics, MNIST67-180, CIFAR10, ColorMNIST, Flowers102


- [EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning](https://icml.cc/virtual/2024/poster/34832) (Poster)
  - **Authors:** [Jongsuk Kim](http://openreview.net/profile?id=~Jongsuk_Kim1), [Hyeongkeun Lee](http://openreview.net/profile?id=~Hyeongkeun_Lee1), [Kyeongha Rho](http://openreview.net/profile?id=~Kyeongha_Rho1), [Junmo Kim](http://openreview.net/profile?id=~Junmo_Kim1), [Joon Son Chung](http://openreview.net/profile?id=~Joon_Son_Chung1)
  - **Affiliations:** Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea
  - **TL;DR:** The study introduces EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning, addressing the challenge of maintaining correspondence between audio and visual modalities during data augmentation. The method demonstrates improved performance across various benchmarks with minimal computational overhead.
  - **Keywords:** audio-visual representation learning, self-supervised learning, contrastive learning, equivariance, attention-based transformation predictor, correspondence disruption due to data augmentation, representational capability enhancement, EquiAV framework, robust supervision, effective feature aggregation


- [Demystifying SGD with Doubly Stochastic Gradients](https://icml.cc/virtual/2024/poster/32646) (Poster)
  - **Authors:** [Kyurae Kim](http://openreview.net/profile?id=~Kyurae_Kim1), [Joohwan Ko](http://openreview.net/profile?id=~Joohwan_Ko2), [Yian Ma](http://openreview.net/profile?id=~Yian_Ma1), [Jacob Gardner](http://openreview.net/profile?id=~Jacob_R._Gardner1)
  - **Affiliations:** Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA, U.S.A., KAIST, Daejeon, South Korea, Republic of, Halıcıo˘glu Data Science Institute, University of California San Diego, San Diego, CA, U.S.A., Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA, U.S.A.
  - **TL;DR:** This study investigates the convergence properties of Stochastic Gradient Descent (SGD) with doubly stochastic gradients in the context of optimization problems characterized by intractable expectations. The findings highlight the importance of independent minibatching and random reshuffling, providing insights into effective budget allocation for computational resources.
  - **Keywords:** Stochastic Gradient Descent (SGD), Optimization, Intractable Expectations, Doubly Stochastic Gradients, Monte Carlo Sampling, Random Reshuffling, Diffusion Models, Variational Autoencoders, Empirical Risk Minimization, Finite Sum with Infinite Data, Convergence Properties, Subsampling Noise, Convergence Analysis, Budget Allocation for Computational Resources


- [Risk-Sensitive Policy Optimization via Predictive CVaR Policy Gradient](https://icml.cc/virtual/2024/poster/35120) (Poster)
  - **Authors:** [Ju-Hyun Kim](http://openreview.net/profile?id=~Ju-Hyun_Kim1), [Seungki Min](http://openreview.net/profile?id=~Seungki_Min2)
  - **Affiliations:** Department of Industrial and Systems Engineering, KAIST, Daejeon, South Korea, Department of Industrial and Systems Engineering, KAIST, Daejeon, South Korea
  - **TL;DR:** This paper introduces a predictive CVaR policy gradient method for optimizing policies in risk-sensitive reinforcement learning, enhancing sample efficiency by reweighting individual cost terms based on their predicted contributions. The proposed approach addresses the limitations of existing CVaR policy gradient algorithms, leading to improved convergence rates.
  - **Keywords:** Risk-sensitive reinforcement learning, Policy optimization, Conditional value-at-risk (CVaR), Predictive CVaR policy gradient, Autonomous driving, Robotic surgery, Finance, Low sample efficiency, Slow convergence, Reweighting strategy, Predictive tail probabilities


- [Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning](https://icml.cc/virtual/2024/poster/32710) (Poster)
  - **Authors:** [Dongkwan Kim](http://openreview.net/profile?id=~Dongkwan_Kim1), [Alice Oh](http://openreview.net/profile?id=~Alice_Oh1)
  - **Affiliations:** School of Computing, KAIST, South Korea, School of Computing, KAIST, South Korea
  - **TL;DR:** This paper introduces Subgraph-To-Node (S2N) translation as a novel method for efficient subgraph representation learning, significantly reducing memory and computational costs while outperforming existing models. The proposed method allows for effective processing of subgraphs, even in data-scarce settings, demonstrating the ability to handle a much larger number of subgraph samples.
  - **Keywords:** Subgraph representation learning, Graph neural networks, Subgraph-To-Node (S2N) translation, Graph coarsening methods, Memory and computational costs, Hierarchical structures of subgraphs, Data scarcity, Efficient subgraph representation learning, Improved processing of subgraph samples


- [A Unified Linear Programming Framework for Offline Reward Learning from Human Demonstrations and Feedback](https://icml.cc/virtual/2024/poster/34146) (Poster)
  - **Authors:** [Kihyun Kim](http://openreview.net/profile?id=~Kihyun_Kim1), [Jiawei Zhang](http://openreview.net/profile?id=~Jiawei_Zhang6), [Asuman Ozdaglar](http://openreview.net/profile?id=~Asuman_E._Ozdaglar1), [Pablo A. Parrilo](http://openreview.net/profile?id=~Pablo_Parrilo1)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, USA, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, USA, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, USA, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, USA
  - **TL;DR:** This paper introduces a novel linear programming framework for offline reward learning that estimates a feasible reward set from human demonstrations and feedback, addressing robustness issues in existing methodologies. The proposed framework offers optimality guarantees and improved performance compared to conventional maximum likelihood estimation approaches.
  - **Keywords:** Reward Learning, Inverse Reinforcement Learning (IRL), Reinforcement Learning from Human Feedback (RLHF), Linear Programming (LP), Maximum Likelihood Estimation (MLE), Sequential Decision-Making, Robotics, Language Models, Robustness Issues, Theoretical Analysis Gap, Optimality Guarantee, Sample Efficiency


- [An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural Network](https://icml.cc/virtual/2024/poster/34855) (Poster)
  - **Authors:** [Taeyoung Kim](http://openreview.net/profile?id=~Taeyoung_Kim2), [Hongseok Yang](http://openreview.net/profile?id=~Hongseok_Yang2)
  - **Affiliations:** School of Computing, KAIST, Daejeon, South Korea, School of Computing, KAIST, Daejeon, South Korea
  - **TL;DR:** This study extends the infinite-width analysis of deep neural networks to their Jacobians, demonstrating that a multilayer perceptron and its Jacobian converge to a Gaussian process at initialization. It also characterizes the training dynamics under Jacobian regularization using a new kernel, the Jacobian Neural Tangent Kernel (JNTK), which becomes deterministic in the infinite-width limit.
  - **Keywords:** deep neural networks, infinite-width analysis, Jacobian regularisation, multilayer perceptron (MLP), Gaussian process (GP), Neural Tangent Kernel (NTK), Jacobian Neural Tangent Kernel (JNTK), robustness against noise, training dynamics, analytical posterior computation, kernel regression solution, robust training, regulariser


- [One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for Continual Learning](https://icml.cc/virtual/2024/poster/33864) (Poster)
  - **Authors:** [Doyoung Kim](http://openreview.net/profile?id=~Doyoung_Kim2), [Susik Yoon](http://openreview.net/profile?id=~Susik_Yoon1), [Dongmin Park](http://openreview.net/profile?id=~Dongmin_Park1), [Youngjun Lee](http://openreview.net/profile?id=~Youngjun_Lee1), [Hwanjun Song](http://openreview.net/profile?id=~Hwanjun_Song2), [Jihwan Bang](http://openreview.net/profile?id=~Jihwan_Bang1), [Jae-Gil Lee](http://openreview.net/profile?id=~Jae-Gil_Lee1)
  - **Affiliations:** KAIST, Daejeon, Republic of Korea, Korea University, Seoul, Republic of Korea, KRAFTON, Seoul, Republic of Korea, KAIST, Daejeon, Republic of Korea, KAIST, Daejeon, Republic of Korea, KAIST, Daejeon, Republic of Korea, KAIST, Daejeon, Republic of Korea
  - **TL;DR:** This paper introduces AdaPromptCL, an adaptive prompting approach for continual learning that effectively manages varying degrees of semantic shifts between tasks. The method outperforms existing prompting strategies by up to 21.3% in scenarios with diverse semantic shifts.
  - **Keywords:** Continual Learning, Semantic Shifts, Adaptive Prompting, Prompt Management, Catastrophic Forgetting, Semantic Shift Accommodation, AdaPromptCL, Enhanced Prompt Grouping, Benchmark Datasets


- [Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time](https://icml.cc/virtual/2024/poster/33726) (Spotlight Poster)
  - **Authors:** [Sungyoon Kim](http://openreview.net/profile?id=~Sungyoon_Kim3), [Mert Pilanci](http://openreview.net/profile?id=~Mert_Pilanci3)
  - **Affiliations:** Department of Electrical Engineering, Stanford University, California, United States, Department of Electrical Engineering, Stanford University, California, United States
  - **TL;DR:** This study investigates the optimality gap between two-layer ReLU networks and their convex relaxations, demonstrating that a polynomial-time algorithm can effectively solve the original non-convex problem with a logarithmic factor. The findings provide insights into the convergence of local gradient methods in deep learning contexts.
  - **Keywords:** ReLU neural networks, convex relaxations, deep learning, Stochastic gradient descent (SGD), ADAM, local gradient methods, polynomial-time algorithms, Optimality gap, non-convex optimization, NP-Hard problems, Tractable algorithms, convergence results, understanding of local gradient methods, Two-layer networks, weight decay, hyperplane arrangement patterns


- [DistiLLM: Towards Streamlined Distillation for Large Language Models](https://icml.cc/virtual/2024/poster/33197) (Poster)
  - **Authors:** [Jongwoo Ko](http://openreview.net/profile?id=~Jongwoo_Ko1), [Sungnyun Kim](http://openreview.net/profile?id=~Sungnyun_Kim1), [Tianyi Chen](http://openreview.net/profile?id=~Tianyi_Chen3), [Se-Young Yun](http://openreview.net/profile?id=~Se-Young_Yun1)
  - **Affiliations:** KAIST AI, Seoul, Republic of Korea, KAIST AI, Seoul, Republic of Korea, Microsoft, Redmond, Washington, USA, KAIST AI, Seoul, Republic of Korea; Microsoft, Redmond, Washington, USA
  - **TL;DR:** The study introduces DISTILLM, an efficient knowledge distillation framework for large language models that addresses the shortcomings of existing methods by utilizing a novel skew Kullback-Leibler divergence loss and an adaptive off-policy approach. The framework demonstrates significant improvements in model performance and computational efficiency, achieving up to 4.3× speedup compared to recent methods.
  - **Keywords:** Knowledge Distillation, Large Language Models, Kullback-Leibler Divergence, Adaptive Off-Policy Approach, Auto-Regressive Language Models, Instruction-Following Tasks, Training-Inference Mismatches, Exposure Bias, Mode Averaging, Mode Collapse, Efficient KD Framework, Speedup in Model Performance


- [Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes](https://icml.cc/virtual/2024/poster/34159) (Poster)
  - **Authors:** [Hyunouk Ko](http://openreview.net/profile?id=~Hyunouk_Ko1), [Xiaoming Huo](http://openreview.net/profile?id=~Xiaoming_Huo1)
  - **Affiliations:** H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, United States, H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, United States
  - **TL;DR:** This paper establishes the universal consistency of wide and deep ReLU neural network classifiers and provides conditions under which these classifiers achieve minimax optimal rates of convergence. The findings offer theoretical guarantees for strong consistency in arbitrary distributions, addressing gaps in the existing literature on neural network classifiers.
  - **Keywords:** universal consistency, deep neural networks, wide neural networks, ReLU neural networks, minimax optimal rates of convergence, binary classification, classification risk, excess risk, benign overfitting, theoretical guarantees for strong consistency, computationally feasible classifiers, regression function, Bayes classifier, Barron space


- [Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design](https://icml.cc/virtual/2024/poster/34850) (Poster)
  - **Authors:** [Leo Klarner](http://openreview.net/profile?id=~Leo_Klarner2), [Tim G. J. Rudner](http://openreview.net/profile?id=~Tim_G._J._Rudner2), [Garrett Morris](http://openreview.net/profile?id=~Garrett_M_Morris1), [Charlotte Deane](http://openreview.net/profile?id=~Charlotte_Deane1), [Yee-Whye Teh](http://openreview.net/profile?id=~Yee_Whye_Teh2)
  - **Affiliations:** University of Oxford, UK, New York University, New York, USA, University of Oxford, UK, University of Oxford, UK, University of Oxford, UK
  - **TL;DR:** This study introduces context-guided diffusion (CGD) to enhance the out-of-distribution generalization of guided diffusion models in molecular and protein design. The method demonstrates significant performance improvements in generating novel, high-value compounds across various diffusion processes.
  - **Keywords:** molecular discovery, generative models, diffusion models, context-guided diffusion (CGD), denoising diffusion models, drug discovery, materials science, protein design, out-of-distribution generalization, combinatorial optimization, improved generalization under distribution shifts, high-value molecule generation


- [Provably Scalable Black-Box Variational Inference with Structured Variational Families](https://icml.cc/virtual/2024/poster/35184) (Poster)
  - **Authors:** [Joohwan Ko](http://openreview.net/profile?id=~Joohwan_Ko2), [Kyurae Kim](http://openreview.net/profile?id=~Kyurae_Kim1), [Woo Chang Kim](http://openreview.net/profile?id=~Woo_Chang_Kim1), [Jacob Gardner](http://openreview.net/profile?id=~Jacob_R._Gardner1)
  - **Affiliations:** KAIST, Daejeon, South Korea, Republic of, Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA, U.S.A., KAIST, Daejeon, South Korea, Republic of, Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA, U.S.A.
  - **TL;DR:** This paper investigates structured variational families as a middle ground between mean-field and full-rank variational families to improve the scalability of black-box variational inference. The authors prove that certain scale matrix structures can achieve better iteration complexity, demonstrating improved performance on large-scale hierarchical models.
  - **Keywords:** Black-box variational inference, hierarchical Bayesian models, Variational families, structured variational families, full-rank covariance approximations, mean-field approximations, Statistics, machine learning, signal processing, Poor scaling with dimensionality, excessive gradient variance, iteration complexity, Better iteration complexity, empirical verification on large-scale hierarchical models


- [Stochastic Conditional Diffusion Models for Robust Semantic Image Synthesis](https://icml.cc/virtual/2024/poster/32954) (Poster)
  - **Authors:** [Juyeon Ko](http://openreview.net/profile?id=~Juyeon_Ko1), [Inho Kong](http://openreview.net/profile?id=~Inho_Kong1), [Dogyun Park](http://openreview.net/profile?id=~Dogyun_Park2), [Hyunwoo Kim](http://openreview.net/profile?id=~Hyunwoo_J._Kim3)
  - **Affiliations:** Department of Computer Science, Korea University, Republic of Korea, Department of Computer Science, Korea University, Republic of Korea, Department of Computer Science, Korea University, Republic of Korea, Department of Computer Science, Korea University, Republic of Korea
  - **TL;DR:** This study introduces the Stochastic Conditional Diffusion Model (SCDM) to enhance the robustness of semantic image synthesis against noisy user inputs by stochastically perturbing semantic label maps. The proposed method effectively generates high-quality images that closely resemble clean images, addressing the challenges posed by real-world annotation errors.
  - **Keywords:** Semantic Image Synthesis, Robustness in Image Generation, Stochastic Conditional Diffusion Model, Label Diffusion, Conditional Generative Models, Photo Editing, Content Creation, Noisy User Inputs, Label Distribution Gap, Inconsistency in Annotations, High-Quality Sample Generation, Class-Wise Noise Schedule, Benchmark Datasets, Conditional GANs, Diffusion Models, SPADE, LDM


- [Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth](https://icml.cc/virtual/2024/poster/35158) (Poster)
  - **Authors:** [Kevin Kögler](http://openreview.net/profile?id=~Kevin_K%C3%B6gler1), [Aleksandr Shevchenko](http://openreview.net/profile?id=~Aleksandr_Shevchenko1), [Hamed Hassani](http://openreview.net/profile?id=~Hamed_Hassani2), [Marco Mondelli](http://openreview.net/profile?id=~Marco_Mondelli1)
  - **Affiliations:** ISTA, Klosterneuburg, Austria, ISTA, Klosterneuburg, Austria, Department of Electrical and Systems Engineering, University of Pennsylvania, USA, ISTA, Klosterneuburg, Austria
  - **TL;DR:** This study investigates the effectiveness of shallow autoencoders in compressing structured data, revealing that gradient descent may overlook data sparsity, leading to performance akin to compressing Gaussian data. The authors demonstrate that incorporating non-linearities and depth in the architecture significantly enhances compression performance for sparse data, validated through experiments on image datasets.
  - **Keywords:** Autoencoders, Data Compression, Machine Learning, Gradient Descent, Non-linear Activation, Denoising Function, Multi-layer Decoder, Image Datasets, Generative Modeling, Data Sparsity, Lossy Compression, Structure of Data Distribution, Improved Compression Performance, Phase Transition Phenomenon, CIFAR-10, MNIST


- [Estimating Barycenters of Distributions with Neural Optimal Transport](https://icml.cc/virtual/2024/poster/32654) (Poster)
  - **Authors:** [Alexander Kolesov](http://openreview.net/profile?id=~Alexander_Kolesov1), [Petr Mokrov](http://openreview.net/profile?id=~Petr_Mokrov1), [Igor Udovichenko](http://openreview.net/profile?id=~Igor_Udovichenko1), [Milena Gazdieva](http://openreview.net/profile?id=~Milena_Gazdieva1), [Gudmund Pammer](http://openreview.net/profile?id=~Gudmund_Pammer1), [Evgeny Burnaev](http://openreview.net/profile?id=~Evgeny_Burnaev1), [Alexander Korotin](http://openreview.net/profile?id=~Alexander_Korotin2)
  - **Affiliations:** Skolkovo Institute of Science and Technology, Moscow, Russia; Artificial Intelligence Research Institute, Moscow, Russia, Skolkovo Institute of Science and Technology, Moscow, Russia; Artificial Intelligence Research Institute, Moscow, Russia, Skolkovo Institute of Science and Technology, Moscow, Russia, Skolkovo Institute of Science and Technology, Moscow, Russia, Department of Mathematics, ETH Zürich, Zürich, Switzerland, Skolkovo Institute of Science and Technology, Moscow, Russia; Artificial Intelligence Research Institute, Moscow, Russia, Skolkovo Institute of Science and Technology, Moscow, Russia; Artificial Intelligence Research Institute, Moscow, Russia
  - **TL;DR:** This paper presents a novel scalable approach to solving the Wasserstein barycenter problem using a bi-level adversarial learning framework based on the Neural OT solver. The method is adaptable to various formulations and demonstrates effectiveness in high-dimensional data scenarios, establishing theoretical error bounds for the solutions.
  - **Keywords:** Wasserstein barycenter, Optimal Transport, Generative Modeling, Neural OT solver, bi-level adversarial learning, Bayesian inference, Geometric modeling, Style Transfer, Texture mixing, Reinforcement Learning, Federated Learning, Finding an average distribution, minimizing divergences, continuous learning setup, New scalable approach for solving the Wasserstein barycenter problem, theoretical error bounds


- [Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning](https://icml.cc/virtual/2024/poster/34150) (Poster)
  - **Authors:** [Donghu Kim](http://openreview.net/profile?id=~Donghu_Kim1), [Hojoon Lee](http://openreview.net/profile?id=~Hojoon_Lee1), [Kyungmin Lee](http://openreview.net/profile?id=~Kyungmin_Lee2), [Dongyoon Hwang](http://openreview.net/profile?id=~Dongyoon_Hwang1), [Jaegul Choo](http://openreview.net/profile?id=~Jaegul_Choo1)
  - **Affiliations:** KAIST, KAIST, KAIST, KAIST, KAIST
  - **TL;DR:** This study investigates the impact of various pre-training objectives on the generalization ability of vision-based reinforcement learning models. The findings reveal that task-agnostic pre-training enhances generalization across diverse environments, while task-specific pre-training improves performance in similar environments.
  - **Keywords:** Vision-Based Reinforcement Learning, Generalization, Pre-training, ResNet-50, Fine-tuning, Atari Games, Reinforcement Learning Environments, Generalization ability, Task-agnostic features, Task-specific knowledge, Atari Pre-training Benchmark (Atari-PB), Evaluation across diverse environments, 10 million transitions, Atari games


- [Attribute Based Interpretable Evaluation Metrics for Generative Models](https://icml.cc/virtual/2024/poster/34219) (Poster)
  - **Authors:** [Dongkyun Kim](http://openreview.net/profile?id=~Dongkyun_Kim2), [Mingi Kwon](http://openreview.net/profile?id=~Mingi_Kwon1), [Youngjung Uh](http://openreview.net/profile?id=~Youngjung_Uh2)
  - **Affiliations:** Department of Artificial Intelligence, Yonsei University, Seoul, Republic of Korea; AI Lab, CTO Division, LG Electronics, Seoul, Republic of Korea, Department of Artificial Intelligence, Yonsei University, Seoul, Republic of Korea, Department of Artificial Intelligence, Yonsei University, Seoul, Republic of Korea
  - **TL;DR:** This paper introduces new evaluation metrics for generative models that focus on the distribution of attribute strengths, addressing the limitations of existing metrics in capturing interpretability. Key findings reveal that certain generative models produce implausible attribute relationships and struggle with diversity, laying the groundwork for more explainable evaluations in the field.
  - **Keywords:** Generative models, Explainability, Single-attribute Divergence (SaD), Paired-attribute Divergence (PaD), Heterogeneous CLIPScore (HCS), Image generation, Evaluation metrics, Lack of interpretability in existing metrics, Attribute distribution discrepancies, New evaluation protocols for generative models, Insights into existing generative models' performance, GANs (Generative Adversarial Networks), Diffusion Models, VAEs (Variational Autoencoders)


- [Privacy-Preserving Embedding via Look-up Table Evaluation with Fully Homomorphic Encryption](https://icml.cc/virtual/2024/poster/33668) (Poster)
  - **Authors:** [Jae-yun Kim](http://openreview.net/profile?id=~Jae-yun_Kim1), [Saerom Park](http://openreview.net/profile?id=~Saerom_Park1), [Joohee Lee](http://openreview.net/profile?id=~Joohee_Lee2), [Jung Hee Cheon](http://openreview.net/profile?id=~Jung_Hee_Cheon2)
  - **Affiliations:** Department of Mathematical Sciences, Seoul National University, Seoul, South Korea, Department of Industrial Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea, Department of Convergence Security Engineering, Sungshin Women’s University, Seoul, South Korea, Department of Mathematical Sciences, Seoul National University, Seoul, South Korea
  - **TL;DR:** This study proposes an efficient algorithm for privacy-preserving embedding via look-up table evaluation with fully homomorphic encryption, addressing the challenges of implementing embedding layers for token inputs in natural language processing. The CodedHELUT algorithm demonstrates significant improvements in efficiency and memory usage while maintaining high precision in evaluations.
  - **Keywords:** Privacy-Preserving Machine Learning (PPML), Homomorphic Encryption (HE), Fully Homomorphic Encryption (FHE), CKKS scheme, Look-up Table (LUT) evaluation, Encrypted Indicator Function (EIF), CodedHELUT algorithm, Natural Language Processing (NLP), Efficient implementation of embedding layers, handling large-sized inputs, memory-intensive processes, CodedHELUT algorithm, high precision in embedding layers, improved efficiency and memory usage, GloVe, GPT-2, BERT


- [Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape](https://icml.cc/virtual/2024/poster/32694) (Oral)
  - **Authors:** [Juno Kim](http://openreview.net/profile?id=~Juno_Kim1), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1)
  - **Affiliations:** Department of Mathematical Informatics, University of Tokyo, Tokyo, Japan; Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan, Department of Mathematical Informatics, University of Tokyo, Tokyo, Japan; Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan
  - **TL;DR:** This study investigates the optimization dynamics of a Transformer model with a two-layer MLP followed by a linear attention layer, revealing how nonlinear representations enhance in-context learning. The authors demonstrate that the nonconvex loss landscape becomes manageable under mean-field dynamics, providing insights into saddle point avoidance and improvement rates.
  - **Keywords:** In-context learning, Transformer architecture, Nonlinear representations, Mean-field dynamics, Wasserstein gradient flow, Gradient descent, Natural language processing, Computer vision, Multi-modal learning, Nonconvex loss landscape, Saddle points, Optimization dynamics, Improvement rates, Saddle point analysis, Flexibility of ICL, MLP (Multi-Layer Perceptron), Barron space, Attention layer


- [Polynomial-based Self-Attention for Table Representation Learning](https://icml.cc/virtual/2024/poster/34076) (Poster)
  - **Authors:** [Jayoung Kim](http://openreview.net/profile?id=~Jayoung_Kim1), [Yehjin Shin](http://openreview.net/profile?id=~Yehjin_Shin1), [Jeongwhan Choi](http://openreview.net/profile?id=~Jeongwhan_Choi1), [Hyowon Wi](http://openreview.net/profile?id=~Hyowon_Wi1), [Noseong Park](http://openreview.net/profile?id=~Noseong_Park1)
  - **Affiliations:** Yonsei University, South Korea, Yonsei University, South Korea, Yonsei University, South Korea, Yonsei University, South Korea, KAIST, South Korea
  - **TL;DR:** This study addresses the oversmoothing issue in self-attention mechanisms used in Transformer models for tabular data by proposing a novel self-attention layer based on matrix polynomials. The new layer enhances representation performance and scalability, outperforming existing state-of-the-art methods in table representation learning.
  - **Keywords:** table representation learning, structured data, Transformer, self-attention, matrix polynomials, machine learning, data mining, oversmoothing issue in self-attention, novel self-attention layer, improved model scalability


- [AdsorbDiff: Adsorbate Placement via Conditional Denoising Diffusion](https://icml.cc/virtual/2024/poster/33741) (Poster)
  - **Authors:** [Adeesh Kolluru](http://openreview.net/profile?id=~Adeesh_Kolluru1), [John Kitchin](http://openreview.net/profile?id=~John_R._Kitchin1)
  - **Affiliations:** Department of Chemical Engineering, Carnegie Mellon University, Department of Chemical Engineering, Carnegie Mellon University
  - **TL;DR:** This study introduces a novel framework for adsorbate placement using denoising diffusion to predict optimal configurations on catalyst slabs, achieving up to 5x acceleration and 3.5x improvement in accuracy compared to previous methods. The approach integrates machine learning force fields and Density Functional Theory for enhanced efficiency in catalyst design.
  - **Keywords:** adsorbate placement, heterogeneous catalysis, optimization, denoising diffusion, machine learning force fields (MLFFs), Density Functional Theory (DFT), catalyst design, chemical reaction modeling, finding lowest energy configuration, global optimization, non-convex search, acceleration in optimization, improved accuracy in adsorbate configuration prediction, OC20-Dense dataset, adsorbate-slab configuration, AdsorbML, graph neural networks (GNN)


- [CLLMs: Consistency Large Language Models](https://icml.cc/virtual/2024/poster/34827) (Poster)
  - **Authors:** [Siqi Kou](http://openreview.net/profile?id=~Siqi_Kou1), [Lanxiang Hu](http://openreview.net/profile?id=~Lanxiang_Hu1), [Zhezhi He](http://openreview.net/profile?id=~Zhezhi_He1), [Zhijie Deng](http://openreview.net/profile?id=~Zhijie_Deng1), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang2)
  - **Affiliations:** Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, University of California, San Diego, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, University of California, San Diego
  - **TL;DR:** This study introduces a refined approach to Jacobi decoding for large language models, aiming to enhance inference speed while maintaining output quality. The proposed method demonstrates significant improvements in generation speed, achieving 2.4× to 3.4× faster performance compared to traditional autoregressive decoding.
  - **Keywords:** Large Language Models, Efficient Inference, Jacobi Decoding, Autoregressive Decoding, Inference Latency, Sequential Decoding, Fast Convergence, Improved Generation Speed


- [Discovering Features with Synergistic Interactions in Multiple Views](https://icml.cc/virtual/2024/poster/33402) (Poster)
  - **Authors:** [Chohee Kim](http://openreview.net/profile?id=~Chohee_Kim1), [M van der Schaar](http://openreview.net/profile?id=~Mihaela_van_der_Schaar2), [Changhee Lee](http://openreview.net/profile?id=~Changhee_Lee1)
  - **Affiliations:** Chung-Ang University, South Korea, University of Cambridge, UK; The Alan Turing Institute, UK, Chung-Ang University, South Korea
  - **TL;DR:** This study introduces a novel deep learning-based method for selecting synergistic and non-synergistic feature subsets in multi-view data, demonstrating its effectiveness through experiments on various datasets. The findings highlight the importance of understanding complex interactions across multiple views for improved analysis and clinical decision-making.
  - **Keywords:** Synergistic interactions, Multi-view data, Deep learning-based feature selection, Bernoulli relaxation technique, Multi-omics data analysis, Medical decision-making, Selecting synergistic and non-synergistic feature subsets, Complex interactions in data, Discovery of relevant feature subsets, Elucidation of complex dependencies, Synthetic datasets, Semi-synthetic datasets, Real-world datasets, Interaction information


- [The Computational Complexity of Finding Second-Order Stationary Points](https://icml.cc/virtual/2024/poster/32879) (Poster)
  - **Authors:** [Andreas Kontogiannis](http://openreview.net/profile?id=~Andreas_Kontogiannis1), [Vasilis Pollatos](http://openreview.net/profile?id=~Vasilis_Pollatos1), [Sotiris Kanellopoulos](http://openreview.net/profile?id=~Sotiris_Kanellopoulos1), [Panayotis Mertikopoulos](http://openreview.net/profile?id=~Panayotis_Mertikopoulos1), [Aris Pagourtzis](http://openreview.net/profile?id=~Aris_Pagourtzis1), [Ioannis Panageas](http://openreview.net/profile?id=~Ioannis_Panageas1)
  - **Affiliations:** National Technical University of Athens, School of Electrical and Computer Engineering, Athens, Greece; Archimedes/Athena RC, Greece, Archimedes/Athena RC, Greece, National Technical University of Athens, School of Electrical and Computer Engineering, Athens, Greece; Archimedes/Athena RC, Greece, Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France, National Technical University of Athens, School of Electrical and Computer Engineering, Athens, Greece; Archimedes/Athena RC, Greece, University of California, Irvine, USA
  - **TL;DR:** This paper investigates the computational complexity of finding second-order stationary points (SOSPs) in non-convex optimization problems, establishing that the problem is PLS-complete and thus as complex as finding first-order stationary points (FOSPs). The findings suggest that finding SOSPs in unconstrained domains is computationally easier than in constrained domains, which is NP-hard.
  - **Keywords:** Non-convex optimization, Stationary points, Approximate second-order optimality condition, First-order stationary points (FOSPs), Approximate second-order stationary points (SOSPs), Machine learning models, Continuous optimization problems, Finding local minima, Saddle points, Computational complexity of optimization, PLS-completeness of finding SOSPs, Complexity conjectures, Hessian, Negative curvature, NP-hard


- [On Convergence of Incremental Gradient for Non-convex Smooth Functions](https://icml.cc/virtual/2024/poster/33738) (Poster)
  - **Authors:** [Anastasiia Koloskova](http://openreview.net/profile?id=~Anastasia_Koloskova2), [Nikita Doikov](http://openreview.net/profile?id=~Nikita_Doikov1), [Sebastian Stich](http://openreview.net/profile?id=~Sebastian_U_Stich1), [Martin Jaggi](http://openreview.net/profile?id=~Martin_Jaggi1)
  - **Affiliations:** Machine Learning and Optimization Laboratory (MLO), EPFL, Lausanne, Switzerland, Machine Learning and Optimization Laboratory (MLO), EPFL, Lausanne, Switzerland, CISPA Helmholtz Center for Information Security, Saarbrücken, Germany, Machine Learning and Optimization Laboratory (MLO), EPFL, Lausanne, Switzerland
  - **TL;DR:** This paper investigates the convergence properties of incremental gradient and shuffle SGD algorithms for non-convex smooth functions, revealing improved convergence guarantees. The findings indicate a significant enhancement in the optimization term of the convergence guarantee, reducing the complexity from O(n/ε) to O(1/ε).
  - **Keywords:** Non-convex optimization, Stochastic Gradient Descent (SGD), Incremental gradient, Single shuffle SGD, Random Reshuffle, Machine learning, Neural network optimization, Convergence properties, Optimization guarantees, Enhanced convergence guarantees, Improved optimization term, Cache efficiency, Data ordering


- [convSeq: Fast and Scalable Method for Detecting Patterns in Spike Data](https://icml.cc/virtual/2024/poster/34340) (Poster)
  - **Authors:** [Roman Koshkin](http://openreview.net/profile?id=~Roman_Koshkin1), [Tomoki Fukai](http://openreview.net/profile?id=~Tomoki_Fukai1)
  - **Affiliations:** Neural Coding and Brain Computing Unit, Okinawa Institute of Science and Technology, Okinawa, Japan, Neural Coding and Brain Computing Unit, Okinawa Institute of Science and Technology, Okinawa, Japan
  - **TL;DR:** The study introduces convSeq, an unsupervised method for detecting spatiotemporal patterns in neural activity, which significantly outperforms existing methods in speed and scalability. The method provides uncertainty estimates for detected patterns and enhances our understanding of information processing in neural circuits.
  - **Keywords:** spontaneous neural activity, spatiotemporal patterns, memory, learning, spatial navigation, backpropagation, spatiotemporal filters, automatic differentiation, neural recordings, biological neural networks, efficient detection methods, scalability in analyzing large datasets, convSeq method, speed improvements over existing methods, uncertainty estimates for detected patterns, synthetic data, real neural recordings, hippocampal replay, constrained 2D filters, fixed-width truncated Gaussians


- [Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning](https://icml.cc/virtual/2024/poster/33549) (Poster)
  - **Authors:** [Xiangzhe Kong](http://openreview.net/profile?id=~Xiangzhe_Kong1), [Wenbing Huang](http://openreview.net/profile?id=~Wenbing_Huang1), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu19)
  - **Affiliations:** Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua University, Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods; Institute for AI Industry Research (AIR), Tsinghua University, Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua University
  - **TL;DR:** This study introduces the Generalist Equivariant Transformer (GET) for modeling 3D molecular interactions, proposing a unified representation for various molecule types. The method effectively captures interaction physics and demonstrates strong generalization across different molecular domains.
  - **Keywords:** 3D molecular interactions, drug discovery, molecular representation, Generalist Equivariant Transformer (GET), E(3) equivariant models, geometric graphs, Chemistry, biochemistry, biophysics, drug discovery, material design, Unified cross-domain molecular representation, limitations of domain-specific models, Effective encoding of molecular interactions, retention of fine-grained information, Graph Neural Networks (GNNs), equivariant GNNs, geometric graphs


- [A General Online Algorithm for Optimizing Complex Performance Metrics](https://icml.cc/virtual/2024/poster/33035) (Poster)
  - **Authors:** [Wojciech Kotlowski](http://openreview.net/profile?id=~Wojciech_Kotlowski1), [Marek Wydmuch](http://openreview.net/profile?id=~Marek_Wydmuch1), [Erik Schultheis](http://openreview.net/profile?id=~Erik_Schultheis1), [Rohit Babbar](http://openreview.net/profile?id=~Rohit_Babbar1), [Krzysztof Dembczynski](http://openreview.net/profile?id=~Krzysztof_Dembczynski1)
  - **Affiliations:** Poznan University of Technology, Poznan, Poland, Poznan University of Technology, Poznan, Poland, Aalto University, Helsinki, Finland; University of Bath, Bath, UK, University of Bath, Bath, UK; Yahoo Research, New York, United States, Poznan University of Technology, Poznan, Poland; Yahoo Research, New York, United States
  - **TL;DR:** This paper introduces a general online algorithm for optimizing complex performance metrics that are non-decomposable, applicable in various classification problems. The algorithm is shown to achieve O(ln n/n) regret for concave and smooth metrics, demonstrating its efficiency through empirical studies.
  - **Keywords:** online learning, performance metrics optimization, online algorithm, confusion matrix, binary classification, multi-class classification, multi-label classification, non-decomposable performance metrics, optimization challenges, no-regret algorithms, empirical studies, F-measure, area under the ROC curve (AUC), geometric mean, harmonic mean, Matthews coefficient


- [Geometry-Aware Instrumental Variable Regression](https://icml.cc/virtual/2024/poster/34930) (Poster)
  - **Authors:** [Heiner Kremer](http://openreview.net/profile?id=~Heiner_Kremer1), [Bernhard Schölkopf](http://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1)
  - **Affiliations:** Max Planck Institute for Intelligent Systems, Max Planck Institute for Intelligent Systems
  - **TL;DR:** This paper introduces the Sinkhorn Method of Moments, an optimal transport-based instrumental variable estimator that enhances robustness against data corruption and adversarial attacks. The proposed method outperforms traditional estimators in standard settings while addressing the limitations of existing approaches in the presence of corrupted data.
  - **Keywords:** Instrumental Variable Regression, Conditional Moment Restrictions, Data Robustness, Sinkhorn Method of Moments, Optimal Transport, Generalized Method of Moments, Data Corruption, Adversarial Attacks, Confounding Bias, Improved Robustness, Non-parametric Approximation, φ-divergence, Maximum Mean Discrepancy, Generalized Empirical Likelihood


- [Privately Learning Smooth Distributions on the Hypercube by Projections](https://icml.cc/virtual/2024/poster/34196) (Poster)
  - **Authors:** [Clément Lalanne](http://openreview.net/profile?id=~Cl%C3%A9ment_Lalanne1), [Sébastien Gadat](http://openreview.net/profile?id=~S%C3%A9bastien_Gadat1)
  - **Affiliations:** Toulouse School of Ecolomics, Université Toulouse 1 Capitole, Toulouse, France, Toulouse School of Ecolomics, Université Toulouse 1 Capitole, Toulouse, France
  - **TL;DR:** This article investigates the centrally-private estimation of Sobolev-smooth densities over the hypercube, extending previous one-dimensional results to high dimensions and non-integer smoothness levels. It introduces a data-driven adaptive estimation strategy that balances bias and variance while ensuring privacy, addressing the challenges posed by the curse of dimensionality.
  - **Keywords:** differential privacy, density estimation, Sobolev-smooth densities, private projection estimators, Lepskii method, high-dimensional data analysis, private data generation, curse of dimensionality, privacy concerns in data usage, adaptive estimation strategy, bias-variance trade-off, central zero-concentrated differential privacy, non-integer levels of smoothness


- [Sobolev Space Regularised Pre Density Models](https://icml.cc/virtual/2024/poster/34126) (Poster)
  - **Authors:** [Mark Kozdoba](http://openreview.net/profile?id=~Mark_Kozdoba1), [Binyamin Perets](http://openreview.net/profile?id=~Binyamin_Perets1), [Shie Mannor](http://openreview.net/profile?id=~Shie_Mannor2)
  - **Affiliations:** Technion Israel Institute of Technology, Haifa, Israel; None, Technion Israel Institute of Technology, Haifa, Israel; None, Technion Israel Institute of Technology, Haifa, Israel; NVIDIA Research
  - **TL;DR:** This paper presents a novel non-parametric density estimation method that regularizes a Sobolev norm, achieving statistical consistency and interpretability. The method is evaluated on the ADBench anomaly detection benchmark, ranking second among over 15 algorithms.
  - **Keywords:** non-parametric density estimation, Sobolev norm, Sobolev type Reproducing Kernel Hilbert Space (RKHS), natural gradients, Fisher divergence based score matching, anomaly detection, non-convex optimization, model interpretability, high-dimensional data, new density estimation method, statistical consistency, ADBench


- [Knowledge Graphs Can be Learned with Just Intersection Features](https://icml.cc/virtual/2024/poster/34756) (Poster)
  - **Authors:** [Duy Le](http://openreview.net/profile?id=~Duy_Le1), [Shaochen (Henry) Zhong](http://openreview.net/profile?id=~Shaochen_Zhong1), [Zirui Liu](http://openreview.net/profile?id=~Zirui_Liu1), [Shuai Xu](http://openreview.net/profile?id=~Shuai_Xu2), [Vipin Chaudhary](http://openreview.net/profile?id=~Vipin_Chaudhary2), [Kaixiong Zhou](http://openreview.net/profile?id=~Kaixiong_Zhou1), [Zhaozhuo Xu](http://openreview.net/profile?id=~Zhaozhuo_Xu1)
  - **Affiliations:** Department of Computer and Data Sciences, Case Western Reserve University, Department of Computer Science, Rice University, Department of Computer Science, Rice University, Department of Computer and Data Sciences, Case Western Reserve University, Department of Computer and Data Sciences, Case Western Reserve University, Department of Electrical and Computer Engineering, North Carolina State University, Department of Computer Science, Stevens Institute of Technology
  - **TL;DR:** This study introduces a novel randomized algorithm that utilizes intersection features from k-hop neighborhoods to enhance the validity assessment of triples in Knowledge Graphs. The proposed method outperforms existing KG embedding models and graph neural network baselines while achieving significant training time efficiency.
  - **Keywords:** Knowledge Graphs, Knowledge Representation, Link Prediction, Randomized Algorithm, Fully-Connected Network, Knowledge Completion, Incompleteness of Knowledge Graphs, Validity of Triples, Intersection Features, Performance Improvement over KG Embedding Models, k-hop Neighborhoods, Structural Features


- [Generalized Sobolev Transport for Probability Measures on a Graph](https://icml.cc/virtual/2024/poster/35211) (Poster)
  - **Authors:** [Tam Le](http://openreview.net/profile?id=~Tam_Le2), [Truyen Nguyen](http://openreview.net/profile?id=~Truyen_Nguyen1), [Kenji Fukumizu](http://openreview.net/profile?id=~Kenji_Fukumizu1)
  - **Affiliations:** Department of Advanced Data Science, The Institute of Statistical Mathematics (ISM), Tokyo, Japan; RIKEN AIP, Tokyo, Japan, The University of Akron, Ohio, US, Department of Advanced Data Science, The Institute of Statistical Mathematics (ISM), Tokyo, Japan
  - **TL;DR:** This study introduces the Generalized Sobolev Transport (GST) for optimal transport problems on graph metric spaces, which simplifies computation compared to existing methods like Orlicz-Wasserstein. The GST is shown to be significantly faster and beneficial for applications in document classification and topological data analysis.
  - **Keywords:** Optimal Transport, Graph Metric Space, Sobolev Transport (ST), Generalized Sobolev Transport (GST), Orlicz-Wasserstein (OW), Document Classification, Topological Data Analysis, Computational Complexity of Optimal Transport, Fast Computation of Transport Problems, Univariate Optimization Problem


- [Collective Certified Robustness against Graph Injection Attacks](https://icml.cc/virtual/2024/poster/34617) (Poster)
  - **Authors:** [Yuni Lai](http://openreview.net/profile?id=~Yuni_Lai1), [Bailin PAN](http://openreview.net/profile?id=~Bailin_PAN1), [kaihuang CHEN](http://openreview.net/profile?id=~kaihuang_CHEN1), [Yancheng Yuan](http://openreview.net/profile?id=~Yancheng_Yuan1), [Kai Zhou](http://openreview.net/profile?id=~Kai_Zhou2)
  - **Affiliations:** Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China, Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong, China, Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong, China, Department of Applied Mathematics, The Hong Kong Polytechnic University, Hong Kong, China, Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China
  - **TL;DR:** This paper presents a novel collective certification method for Graph Neural Networks (GNNs) to enhance their robustness against graph injection attacks, significantly improving certification performance from 0.0% to 81.2% with minimal computational overhead. The proposed method formulates the problem as a binary integer quadratic constrained linear programming and demonstrates its effectiveness through experiments on the Citeseer dataset.
  - **Keywords:** Certified robustness, Graph Neural Networks (GNNs), Graph injection attacks, Binary integer quadratic constrained linear programming (BQCLP), Linear programming (LP), Vulnerability of GNNs to adversarial attacks, Graph Injection Attack (GIA), cost-efficiency of attacks, Collective certification scheme, significant increase in certified ratio, Citeseer dataset, Graph Modification Attack (GMA), adversarial training, provable defense approaches


- [PcLast: Discovering Plannable Continuous Latent States](https://icml.cc/virtual/2024/poster/34761) (Poster)
  - **Authors:** [ANURAG KOUL](http://openreview.net/profile?id=~Anurag_Koul1), [Shivakanth Sujit](http://openreview.net/profile?id=~Shivakanth_Sujit1), [Shaoru Chen](http://openreview.net/profile?id=~Shaoru_Chen1), [Benjamin Evans](http://openreview.net/profile?id=~Ben_Evans1), [Lili Wu](http://openreview.net/profile?id=~Lili_Wu1), [Byron Xu](http://openreview.net/profile?id=~Byron_Xu1), [Rajan Chari](http://openreview.net/profile?id=~Rajan_Chari1), [Riashat Islam](http://openreview.net/profile?id=~Riashat_Islam1), [Raihan Seraj](http://openreview.net/profile?id=~Raihan_Seraj1), [Yonathan Efroni](http://openreview.net/profile?id=~Yonathan_Efroni2), [Lekan Molu](http://openreview.net/profile?id=~Lekan_P_Molu1), [Miroslav Dudik](http://openreview.net/profile?id=~Miroslav_Dud%C3%ADk1), [John Langford](http://openreview.net/profile?id=~John_Langford1), [Alex Lamb](http://openreview.net/profile?id=~Alex_Lamb1)
  - **Affiliations:** Microsoft Research, Mila - Quebec AI Institute; ETS Montreal, Microsoft Research, New York University, Microsoft Research, Microsoft Research, Microsoft Research, Mila - Quebec AI Institute; McGill University, Mila - Quebec AI Institute; McGill University, Meta, Microsoft Research, Microsoft Research, Microsoft Research, Microsoft Research
  - **TL;DR:** This paper presents a method for learning a representation that associates reachable states together to enhance goal-conditioned planning and policy learning in deep reinforcement learning. The proposed approach demonstrates significant improvements in sampling efficiency and enables efficient hierarchical planning without requiring additional samples.
  - **Keywords:** Goal-conditioned planning, Latent representations, Deep reinforcement learning, Variational autoencoders, Inverse dynamics, Multi-step inverse dynamics, Virtual environments, Software simulations, Autonomous driving, Healthcare, State reachability, Poor abstractions in latent representations, Improved sampling efficiency, Layered state abstractions, Hierarchical planning


- [Robust Inverse Graphics via Probabilistic Inference](https://icml.cc/virtual/2024/poster/33525) (Poster)
  - **Authors:** [Tuan Anh Le](http://openreview.net/profile?id=~Tuan_Anh_Le1), [Pavel Sountsov](http://openreview.net/profile?id=~Pavel_Sountsov2), [Matthew Hoffman](http://openreview.net/profile?id=~Matthew_Douglas_Hoffman1), [Ben Lee](http://openreview.net/profile?id=~Ben_Lee2), [Brian Patton](http://openreview.net/profile?id=~Brian_Patton1), [Rif Saurous](http://openreview.net/profile?id=~Rif_A._Saurous1)
  - **Affiliations:** Google, Google, Google, Google, Google, Google
  - **TL;DR:** The study introduces Robust Inverse Graphics (RIG), a Bayesian approach for inferring 3D scenes from single images that is robust to various corruptions. RIG outperforms existing methods by performing joint probabilistic inference over scene and corruption parameters, demonstrating its effectiveness even when trained solely on clean data.
  - **Keywords:** 3D scene inference, robustness to corruptions, Bayesian approach, probabilistic inference, neural radiance fields (NeRF), reconstruction-guidance with auxiliary latents (ReGAL), Autonomous driving, computer vision, Corruptions in 3D scene reconstruction, floaters in reconstructed scenes, Outperforms depth estimators, full probabilistic inference over scene and corruption


- [No Free Prune: Information-Theoretic Barriers to Pruning at Initialization](https://icml.cc/virtual/2024/poster/33928) (Poster)
  - **Authors:** [Tanishq Kumar](http://openreview.net/profile?id=~Tanishq_Kumar1), [Kevin Luo](http://openreview.net/profile?id=~Kevin_Luo1), [Mark Sellke](http://openreview.net/profile?id=~Mark_Sellke1)
  - **Affiliations:** Harvard University, Harvard University, Harvard University
  - **TL;DR:** The study investigates the challenges of identifying sparse networks (lottery tickets) at initialization without training dense models, proposing that effective parameter count and mutual information play crucial roles. It concludes that pruning at initialization may be infeasible, as training provides essential information that enhances model capacity.
  - **Keywords:** neural network pruning, sparse networks, lottery tickets, iterative magnitude pruning (IMP), effective parameter count (peff), pruning at initialization, model capacity, data-dependent masks, theoretical explanation for pruning challenges, impact of training on model capacity, mutual information, robustness


- [Understanding the Effects of Iterative Prompting on Truthfulness](https://icml.cc/virtual/2024/poster/34330) (Poster)
  - **Authors:** [Satyapriya Krishna](http://openreview.net/profile?id=~Satyapriya_Krishna2), [Chirag Agarwal](http://openreview.net/profile?id=~Chirag_Agarwal1), [Himabindu Lakkaraju](http://openreview.net/profile?id=~Himabindu_Lakkaraju1)
  - **Affiliations:** Harvard University, Harvard University, Harvard University
  - **TL;DR:** This study investigates the effects of iterative prompting on the truthfulness of Large Language Models (LLMs), revealing that naive prompting methods can lead to significant calibration errors. The authors propose new prompting variants that improve the accuracy and reliability of LLM outputs, contributing to the development of more trustworthy AI systems.
  - **Keywords:** Large Language Models, Truthfulness, Iterative Prompting, Finance, Healthcare, Autonomous Systems, Hallucination, Calibration Errors, Reliability of LLMs, Prompting Variants, Improved Truthfulness, AI Systems, Model Responses


- [Mean Estimation in the Add-Remove Model of Differential Privacy](https://icml.cc/virtual/2024/poster/33576) (Poster)
  - **Authors:** [Alex Kulesza](http://openreview.net/profile?id=~Alex_Kulesza2), [Ananda Suresh](http://openreview.net/profile?id=~Ananda_Theertha_Suresh1), [Yuyan Wang](http://openreview.net/profile?id=~Yuyan_Wang1)
  - **Affiliations:** Google Research, NYC, Google Research, NYC, Google Research, NYC
  - **TL;DR:** This study investigates one-dimensional mean estimation under the add-remove model of differential privacy, proposing a min-max optimal algorithm that achieves the best possible constant in the leading term of the mean squared error. The results indicate that the add-remove and swap models yield nearly identical errors, highlighting the effectiveness of the proposed method in practical applications.
  - **Keywords:** Differential Privacy, Mean Estimation, Add-Remove Model, Swap Model, Hourglass Mechanism, Statistical Queries, Estimation of mean under privacy constraints, Optimal error in mean estimation, Min-max optimal algorithm, Improvement in mean squared error, ε-Differential Privacy, Neighboring Datasets


- [Towards Understanding Inductive Bias in Transformers: A View From Infinity](https://icml.cc/virtual/2024/poster/34469) (Poster)
  - **Authors:** [Itay Lavie](http://openreview.net/profile?id=~Itay_Lavie1), [Guy Gur-Ari](http://openreview.net/profile?id=~Guy_Gur-Ari1), [Zohar Ringel](http://openreview.net/profile?id=~Zohar_Ringel1)
  - **Affiliations:** Racah Institute of Physics, Hebrew University of Jerusalem, Jerusalem 91904, Israel, Augment Computing, Racah Institute of Physics, Hebrew University of Jerusalem, Jerusalem 91904, Israel
  - **TL;DR:** This study investigates the inductive bias of Transformers in the context of infinitely over-parameterized Gaussian processes, revealing a tendency towards permutation symmetric functions. The authors provide analytical predictions for learnability and performance, demonstrating that datasets with permutation symmetry, like WikiText-2, enhance model learnability.
  - **Keywords:** Inductive bias, Transformers, Gaussian processes, Representation theory, Symmetric group, Learning curves, Natural language processing, Learnability, Permutation symmetry, Analytical predictions, Scaling laws for learnability, WikiText-2


- [Run-Time Task Composition with Safety Semantics](https://icml.cc/virtual/2024/poster/34599) (Poster)
  - **Authors:** [Kevin Leahy](http://openreview.net/profile?id=~Kevin_Leahy1), [Makai Mann](http://openreview.net/profile?id=~Makai_Mann1), [Zachary Serlin](http://openreview.net/profile?id=~Zachary_Serlin1)
  - **Affiliations:** Department of Robotics Engineering, Worcester Polytechnic Institute, Lexington, MA, USA, MIT Lincoln Laboratory, Lexington, MA, USA, MIT Lincoln Laboratory, Lexington, MA, USA
  - **TL;DR:** This paper presents a method for composing safety-aware policies in reinforcement learning, focusing on Boolean composition of learned tasks to ensure safety constraints are met. The authors demonstrate their approach through various algorithms and environments, highlighting the ability to compose policies at runtime without additional training.
  - **Keywords:** Task composition, Safety in reinforcement learning, Q-learning, Boolean composition, Value iteration, Deep Q-Network (DQN), Twin Delayed DDPG (TD3), Robotics, Autonomous systems, Safety constraints, Reachability problems, Avoidance of undesirable states, Safety-aware task composition methods, Composable policies, Continuous action spaces, Absorbing states, Stochastic shortest paths


- [Single-Model Attribution of Generative Models Through Final-Layer Inversion](https://icml.cc/virtual/2024/poster/34447) (Poster)
  - **Authors:** [Mike Laszkiewicz](http://openreview.net/profile?id=~Mike_Laszkiewicz1), [Jonas Ricker](http://openreview.net/profile?id=~Jonas_Ricker1), [Johannes Lederer](http://openreview.net/profile?id=~Johannes_Lederer1), [Asja Fischer](http://openreview.net/profile?id=~Asja_Fischer1)
  - **Affiliations:** Faculty of Computer Science, Ruhr University Bochum, Germany, Faculty of Computer Science, Ruhr University Bochum, Germany, Department of Mathematics, Computer Science, and Natural Sciences, University of Hamburg, Germany, Faculty of Computer Science, Ruhr University Bochum, Germany
  - **TL;DR:** This paper introduces FLIPAD, a novel approach for single-model attribution of generative models that utilizes final-layer inversion and anomaly detection to address the limitations of existing methods in an open-world setting. The findings demonstrate the effectiveness and computational efficiency of FLIPAD in attributing samples to their respective generative models, particularly in the context of intellectual property theft.
  - **Keywords:** generative modeling, single-model attribution, anomaly detection, final-layer inversion, convex lasso optimization, intellectual property theft detection, closed-world setting limitations, open-world attribution challenges, FLIPAD approach, computational efficiency


- [KISA: A Unified Keyframe Identifier and Skill Annotator for Long-Horizon Robotics Demonstrations](https://icml.cc/virtual/2024/poster/33091) (Poster)
  - **Authors:** [Longxin Kou](http://openreview.net/profile?id=~Longxin_Kou1), [Fei Ni](http://openreview.net/profile?id=~Fei_Ni1), [Yan Zheng](http://openreview.net/profile?id=~YAN_ZHENG1), [Jinyi Liu](http://openreview.net/profile?id=~Jinyi_Liu1), [Yifu Yuan](http://openreview.net/profile?id=~Yifu_Yuan1), [Zibin Dong](http://openreview.net/profile?id=~Zibin_Dong1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1)
  - **Affiliations:** College of Intelligence and Computing, Tianjin University, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China
  - **TL;DR:** The study introduces KISA, a framework for automatic keyframe identification and skill annotation in long-horizon robotics demonstrations, addressing challenges in learning from unlabeled data. Experimental results show that KISA significantly improves the accuracy and interpretability of keyframe identification while demonstrating robust generalization capabilities.
  - **Keywords:** Robotics manipulation, Long-horizon demonstrations, Keyframe identification, Pretrained visual-language representations, Temporal enhancement module, Coarse contrastive learning, Fine-grained monotonic encouragement, Robotics, Skill learning, Learning from long-horizon demonstrations, Keyframe guidance, Skill supervision, KISA framework, Improved accuracy and interpretability of keyframe identification


- [Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms](https://icml.cc/virtual/2024/poster/35121) (Poster)
  - **Authors:** [Filippo Lazzati](http://openreview.net/profile?id=~Filippo_Lazzati2), [Mirco Mutti](http://openreview.net/profile?id=~Mirco_Mutti1), [Alberto Maria Metelli](http://openreview.net/profile?id=~Alberto_Maria_Metelli2)
  - **Affiliations:** Politecnico di Milano, Milan, Italy, Technion, Haifa, Israel, Politecnico di Milano, Milan, Italy
  - **TL;DR:** This paper addresses the challenges of offline inverse reinforcement learning by introducing a novel notion of the feasible reward set and proposing two efficient algorithms, IRLO and PIRLO, to estimate it. The findings highlight the importance of understanding the limitations of offline datasets and the need for a new learning framework to tackle the inherent ambiguity in reward function recovery.
  - **Keywords:** Inverse Reinforcement Learning (IRL), Imitation Learning, Feasible Reward Set, Algorithms (IRLO, PIRLO), Offline Learning, Expert Behavior Analysis, Ill-posed problem, Ambiguity in reward functions, Data coverage issues, Novel learning framework, Inclusion monotonicity


- [Modeling Caption Diversity in Contrastive Vision-Language Pretraining](https://icml.cc/virtual/2024/poster/33348) (Poster)
  - **Authors:** [Samuel Lavoie](http://openreview.net/profile?id=~Samuel_Lavoie1), [Polina Kirichenko](http://openreview.net/profile?id=~Polina_Kirichenko1), [Mark Ibrahim](http://openreview.net/profile?id=~Mark_Ibrahim1), [Mahmoud Assran](http://openreview.net/profile?id=~Mido_Assran1), [Andrew Wilson](http://openreview.net/profile?id=~Andrew_Gordon_Wilson1), [Aaron Courville](http://openreview.net/profile?id=~Aaron_Courville3), [Nicolas Ballas](http://openreview.net/profile?id=~Nicolas_Ballas1)
  - **Affiliations:** FAIR at Meta; Mila, Université de Montréal, FAIR at Meta; New York University, FAIR at Meta, FAIR at Meta, New York University, Mila, Université de Montréal, FAIR at Meta
  - **TL;DR:** This study introduces Llip, a method for modeling caption diversity in vision-language pretraining, which enhances the representation of images by allowing for multiple valid captions. Llip outperforms existing models like CLIP in zero-shot classification and retrieval tasks, demonstrating improved accuracy and richer visual representations.
  - **Keywords:** Contrastive Language Pretraining, Vision-Language Models, Caption Diversity, Latent Language Image Pretraining (Llip), Visual Encoder, Contrastive Objective, Zero-shot Classification, Image Retrieval, Information Imbalance between Visual and Text Modalities, Representation Diversity, Improved Zero-shot Classification Accuracy, Enhanced Visual Representations, ImageNet, MS-COCO, CLIP (Contrastive Language-Image Pretraining), SigLIP


- [Chasing Convex Functions with Long-term Constraints](https://icml.cc/virtual/2024/poster/33395) (Poster)
  - **Authors:** [Adam Lechowicz](http://openreview.net/profile?id=~Adam_Lechowicz1), [Nicolas Christianson](http://openreview.net/profile?id=~Nicolas_Christianson1), [Bo Sun](http://openreview.net/profile?id=~Bo_Sun8), [Noman Bashir](http://openreview.net/profile?id=~Noman_Bashir1), [Mohammad Hajiesmaili](http://openreview.net/profile?id=~Mohammad_Hajiesmaili1), [Adam Wierman](http://openreview.net/profile?id=~Adam_Wierman1), [Prashant Shenoy](http://openreview.net/profile?id=~Prashant_Shenoy1)
  - **Affiliations:** Manning College of Information and Computer Sciences, University of Massachusetts Amherst, USA, Computing & Mathematical Sciences, California Institute of Technology, USA, Cheriton School of Computer Science, University of Waterloo, Ontario, Canada, Computer Science & Artificial Intelligence Laboratory, Massachusetts Institute of Technology, USA, Manning College of Information and Computer Sciences, University of Massachusetts Amherst, USA, Computing & Mathematical Sciences, California Institute of Technology, USA, Manning College of Information and Computer Sciences, University of Massachusetts Amherst, USA
  - **TL;DR:** This paper introduces a novel class of online metric problems with long-term demand constraints, focusing on minimizing costs in sustainable systems. The authors propose optimal algorithms that effectively address these challenges, demonstrating their performance through numerical experiments.
  - **Keywords:** online metric problems, long-term constraints, resource allocation, convex function chasing, competitive algorithms, learning-augmented algorithms, sustainable energy systems, carbon-aware control problems, electric vehicle charging, minimizing hitting and switching costs, satisfying long-term demand constraints, optimal algorithms, numerical experiments


- [Fundamental Benefit of Alternating Updates in Minimax Optimization](https://icml.cc/virtual/2024/poster/32925) (Spotlight Poster)
  - **Authors:** [Jaewook Lee](http://openreview.net/profile?id=~Jaewook_Lee6), [Hanseul Cho](http://openreview.net/profile?id=~Hanseul_Cho1), [Chulhee Yun](http://openreview.net/profile?id=~Chulhee_Yun1)
  - **Affiliations:** KAIST AI, South Korea, KAIST AI, South Korea, KAIST AI, South Korea
  - **TL;DR:** This study analyzes the convergence rates of the Gradient Descent-Ascent (GDA) algorithm for minimax optimization, demonstrating that the alternating update method (Alt-GDA) is provably faster than the simultaneous method (Sim-GDA). Additionally, the authors introduce a new framework, Alex-GDA, which achieves better iteration complexity and linear convergence for specific problems.
  - **Keywords:** minimax optimization, convergence analysis, Gradient Descent-Ascent (GDA), Simultaneous GDA (Sim-GDA), Alternating GDA (Alt-GDA), Alternating-Extrapolation GDA (Alex-GDA), Extra-gradient method, machine learning, generative adversarial networks (GANs), adversarial training, robust optimization, reinforcement learning, performance gap in convergence rates, iteration complexity, new iteration complexity upper bound for Alt-GDA, linear convergence for bilinear problems


- [SelMatch: Effectively Scaling Up Dataset Distillation via Selection-Based Initialization and Partial Updates by Trajectory Matching](https://icml.cc/virtual/2024/poster/33039) (Poster)
  - **Authors:** [Yongmin Lee](http://openreview.net/profile?id=~Yongmin_Lee1), [Hye Won Chung](http://openreview.net/profile?id=~Hye_Won_Chung2)
  - **Affiliations:** School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea
  - **TL;DR:** The study introduces SelMatch, a novel dataset distillation method that effectively scales with the number of images per class (IPC) by using selection-based initialization and partial updates. It outperforms existing methods in image classification tasks, addressing the challenges of incorporating complex features from harder samples into synthetic datasets.
  - **Keywords:** dataset distillation, data-efficient learning, trajectory matching, selection-based initialization, partial updates, image classification, performance loss in dataset distillation, coverage gap between easy and hard samples, SelMatch method, improved performance across IPC scales, CIFAR-10, CIFAR-100, TinyImageNet


- [Slow and Steady Wins the Race: Maintaining Plasticity with Hare and Tortoise Networks](https://icml.cc/virtual/2024/poster/33921) (Poster)
  - **Authors:** [Hojoon Lee](http://openreview.net/profile?id=~Hojoon_Lee1), [Hyeonseo Cho](http://openreview.net/profile?id=~Hyeonseo_Cho1), [Hyunseung Kim](http://openreview.net/profile?id=~Hyunseung_Kim1), [Donghu Kim](http://openreview.net/profile?id=~Donghu_Kim1), [Dugki Min](http://openreview.net/profile?id=~Dugki_Min1), [Jaegul Choo](http://openreview.net/profile?id=~Jaegul_Choo1), [Clare Lyle](http://openreview.net/profile?id=~Clare_Lyle1)
  - **Affiliations:** KAIST, Konkuk University, KAIST, KAIST, Konkuk University, KAIST, Deepmind
  - **TL;DR:** This study introduces the Hare & Tortoise network, which balances rapid adaptation and gradual knowledge integration to maintain plasticity in neural networks. The method effectively preserves generalization ability, enhancing performance in reinforcement learning tasks on the Atari-100k benchmark.
  - **Keywords:** plasticity, generalization, continual learning, reinforcement learning, Hare & Tortoise network, warm-starting experiments, loss of generalization ability, diminishing plasticity, gradient starvation, preservation of plasticity, improved generalization in reinforcement learning, Atari-100k benchmark


- [Training Greedy Policy for Proposal Batch Selection in Expensive Multi-Objective Combinatorial Optimization](https://icml.cc/virtual/2024/poster/33309) (Poster)
  - **Authors:** [Deokjae Lee](http://openreview.net/profile?id=~Deokjae_Lee1), [Hyun Oh Song](http://openreview.net/profile?id=~Hyun_Oh_Song1), [Kyunghyun Cho](http://openreview.net/profile?id=~Kyunghyun_Cho1)
  - **Affiliations:** Seoul National University; Neural Processing Research Center, Seoul National University; Neural Processing Research Center, New York University; Genentech
  - **TL;DR:** This study presents a novel greedy-style subset selection algorithm for optimizing batch acquisition in expensive multi-objective combinatorial optimization problems. The proposed method demonstrates improved efficiency, achieving baseline performance with 1.69× fewer queries in the context of red fluorescent protein design.
  - **Keywords:** multi-objective combinatorial optimization, active learning, greedy algorithm, batch acquisition function, biological sequence design, molecular graph optimization, chip design, subset selection problem, large search space, dependencies among candidates, novel greedy-style subset selection algorithm, efficiency in fewer queries


- [Pausing Policy Learning in Non-stationary Reinforcement Learning](https://icml.cc/virtual/2024/poster/32987) (Oral)
  - **Authors:** [Hyunin Lee](http://openreview.net/profile?id=~Hyunin_Lee1), [Ming Jin](http://openreview.net/profile?id=~Ming_Jin2), [Javad Lavaei](http://openreview.net/profile?id=~Javad_Lavaei1), [Somayeh Sojoudi](http://openreview.net/profile?id=~Somayeh_Sojoudi1)
  - **Affiliations:** University of California, Berkeley, Virginia Tech, University of California, Berkeley, University of California, Berkeley
  - **TL;DR:** This study investigates the impact of pausing policy updates in reinforcement learning to improve performance in non-stationary environments. The findings suggest that strategically managing decision updates can enhance overall rewards by addressing aleatoric uncertainty more effectively.
  - **Keywords:** reinforcement learning, real-time inference, non-stationary environments, online reinforcement learning framework, policy update strategies, recommendation systems, autonomous vehicle control, building control systems, temporal differences, aleatoric uncertainty, decision model updates, optimal ratio between policy update and hold duration, dynamic regret analysis


- [Stationary Latent Weight Inference for Unreliable Observations from Online Test-Time Adaptation](https://icml.cc/virtual/2024/poster/34450) (Poster)
  - **Authors:** [Jae-Hong Lee](http://openreview.net/profile?id=~Jae-Hong_Lee1), [Joon Hyuk Chang](http://openreview.net/profile?id=~Joon-Hyuk_Chang1)
  - **Affiliations:** Department of Electronic Engineering, Hanyang University, Seoul, Republic of Korea, Department of Electronic Engineering, Hanyang University, Seoul, Republic of Korea
  - **TL;DR:** This paper introduces a stationary latent weight inference (SLWI) framework to address challenges in online test-time adaptation (OTTA) by effectively managing distribution shifts and mitigating catastrophic forgetting. The proposed method enhances adaptability to ongoing changes in the target domain, demonstrating superior performance in various experimental scenarios.
  - **Keywords:** online test-time adaptation (OTTA), domain adaptation, distribution shifts, stationary latent weight inference (SLWI), Bayesian filtering, catastrophic forgetting, inadequate target domain information integration, model overfitting, improved methodologies for domain adaptation, robust solution to domain adaptation, deep neural networks (DNNs), entropy minimization


- [Supervised Matrix Factorization: Local Landscape Analysis and Applications](https://icml.cc/virtual/2024/poster/33764) (Poster)
  - **Authors:** [Joowon Lee](http://openreview.net/profile?id=~Joowon_Lee1), [Hanbaek Lyu](http://openreview.net/profile?id=~Hanbaek_Lyu1), [Weixin Yao](http://openreview.net/profile?id=~Weixin_Yao1)
  - **Affiliations:** Department of Mathematics, University of Wisconsin - Madison, WI, USA, Department of Statistics, University of Wisconsin - Madison, WI, USA, Department of Statistics, University of California, Riverside, CA, USA
  - **TL;DR:** This paper presents a comprehensive local landscape analysis of Supervised Matrix Factorization (SMF), addressing its non-convex optimization challenges and proposing a Block Coordinate Descent algorithm with guarantees for convergence and complexity. The findings enhance the understanding of SMF optimization and offer practical solutions for improved performance.
  - **Keywords:** Supervised Matrix Factorization, Low-dimensional feature extraction, Classification, Block Coordinate Descent (BCD) algorithm, Hessian analysis, Non-convex optimization, Factor-wise constrained optimization, Optimization landscape, Global convergence guarantees, Iteration complexity guarantees, Local estimation guarantee, Nonnegative Matrix Factorization (NMF), Statistical SMF model


- [Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective](https://icml.cc/virtual/2024/poster/33618) (Poster)
  - **Authors:** [Soo Yong Lee](http://openreview.net/profile?id=~Soo_Yong_Lee1), [Sunwoo Kim](http://openreview.net/profile?id=~Sunwoo_Kim4), [Fanchen Bu](http://openreview.net/profile?id=~Fanchen_Bu1), [Jaemin Yoo](http://openreview.net/profile?id=~Jaemin_Yoo1), [Jiliang Tang](http://openreview.net/profile?id=~Jiliang_Tang1), [Kijung Shin](http://openreview.net/profile?id=~Kijung_Shin2)
  - **Affiliations:** Kim Jaechul Graduate School of Artificial Intelligence, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea, Kim Jaechul Graduate School of Artificial Intelligence, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea, Kim Jaechul Graduate School of Artificial Intelligence, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea, School of Electrical Engineering, KAIST, Daejeon, Republic of Korea, Department of Computer Science and Engineering, Michigan State University, Michigan, US, Kim Jaechul Graduate School of Artificial Intelligence, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea
  - **TL;DR:** This study investigates the impact of shuffling feature vectors among nodes of the same class on the performance of graph neural networks (GNNs), revealing that such shuffling improves GNN performance by reducing the dependence between graph topology and features. The findings suggest that understanding and controlling this A-X dependence is crucial for enhancing GNN effectiveness in node classification tasks.
  - **Keywords:** Graph Neural Networks (GNNs), A-X Dependence, Homophily, Graph Convolution, Random Graph Model, Node Classification, Dependence between graph topology and features, Class distribution effects, Measurement of A-X dependence, Empirical analysis on real-world graphs, Cora-Full, Ogbn-ArXiv, Feature Shuffle, Class-Homophily


- [StrWAEs to Invariant Representations](https://icml.cc/virtual/2024/poster/33262) (Poster)
  - **Authors:** [Hyunjong Lee](http://openreview.net/profile?id=~Hyunjong_Lee1), [Yedarm Seong](http://openreview.net/profile?id=~Yedarm_Seong1), [Sungdong Lee](http://openreview.net/profile?id=~Sungdong_Lee1), [Joong-Ho (Johann) Won](http://openreview.net/profile?id=~Joong-Ho_Won1)
  - **Affiliations:** Department of Statistics, Seoul National University, Seoul, Korea, Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea, Department of Medicine, Yong Loo Lin School of Medicine, National University of Singapore, Singapore, Department of Statistics, Seoul National University, Seoul, Korea; Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea
  - **TL;DR:** This paper introduces StrWAEs, a framework that utilizes Wasserstein autoencoders to impose structural constraints for capturing invariance in latent variables, addressing challenges in generative modeling and representation learning. The findings demonstrate the flexibility of WAEs in handling conditional independence structures, leading to advancements in semi-supervised classification and invariant representation tasks.
  - **Keywords:** Autoencoders, Generative Modeling, Representation Learning, Wasserstein Autoencoders (WAEs), Variational Autoencoders (VAEs), Semi-Supervised Classification, Conditional Generation, Invariant Representation Tasks, Nuisance Information, Structural Constraints, Structural Uses of WAEs, Encoder Structure and Penalties, Conditional Independence, Latent Variables


- [Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains](https://icml.cc/virtual/2024/poster/34562) (Poster)
  - **Authors:** [Kyungeun Lee](http://openreview.net/profile?id=~Kyungeun_Lee1), [Ye Seul Sim](http://openreview.net/profile?id=~Ye_Seul_Sim1), [Hye-Seung Cho](http://openreview.net/profile?id=~Hyeseung_Cho1), [Moonjung Eo](http://openreview.net/profile?id=~Moonjung_Eo1), [Suhee Yoon](http://openreview.net/profile?id=~Suhee_Yoon1), [Sanghyu Yoon](http://openreview.net/profile?id=~Sanghyu_Yoon1), [Woohyung Lim](http://openreview.net/profile?id=~Woohyung_Lim1)
  - **Affiliations:** LG AI Research, Seoul, Republic of Korea, LG AI Research, Seoul, Republic of Korea, LG AI Research, Seoul, Republic of Korea, LG AI Research, Seoul, Republic of Korea, LG AI Research, Seoul, Republic of Korea, LG AI Research, Seoul, Republic of Korea, LG AI Research, Seoul, Republic of Korea
  - **TL;DR:** This study introduces a novel pretext task based on binning to enhance self-supervised learning in tabular domains, addressing the challenges of heterogeneous features and irregular functions. The proposed method consistently improves representation learning performance across various downstream tasks.
  - **Keywords:** Self-Supervised Learning, Tabular Data, Binning Method, Encoder Architecture, Financial Markets, Healthcare Diagnostics, E-commerce Personalization, Manufacturing Process Automation, Heterogeneous Features, Irregular Functions, Feature Discrepancies, Improved Representation Learning, Enhanced Performance on Downstream Tasks, Piecewise Constant Functions, Tree-Based Models (XGBoost, CatBoost)


- [DataFreeShield: Defending Adversarial Attacks without Training Data](https://icml.cc/virtual/2024/poster/32885) (Poster)
  - **Authors:** [Hyeyoon Lee](http://openreview.net/profile?id=~Hyeyoon_Lee1), [Kanghyun Choi](http://openreview.net/profile?id=~Kanghyun_Choi1), [Dain Kwon](http://openreview.net/profile?id=~Dain_Kwon1), [SunJong Park](http://openreview.net/profile?id=~SunJong_Park1), [Mayoore Jaiswal](http://openreview.net/profile?id=~Mayoore_Selvarasa_Jaiswal1), [Noseong Park](http://openreview.net/profile?id=~Noseong_Park1), [Jonghyun Choi](http://openreview.net/profile?id=~Jonghyun_Choi1), [Jinho Lee](http://openreview.net/profile?id=~Jinho_Lee2)
  - **Affiliations:** Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea, NVIDIA, Work done while at IBM, School of Computing, KAIST, Daejeon, South Korea, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea
  - **TL;DR:** The study introduces DataFreeShield, a novel method for achieving adversarial robustness without access to original training data. It demonstrates that the method effectively generates synthetic datasets and trains models to enhance robustness, addressing significant challenges in real-world applications where data privacy is a concern.
  - **Keywords:** Adversarial robustness, Data-free adversarial training, Adversarial training (AT), Synthetic sample diversification, Gradient refinement (GradRefine), Lack of access to original training data, Privacy-sensitive data issues, DataFreeShield, Robustness without real data, Adversarial examples, Pretrained models


- [Improving Gradient-Guided Nested Sampling for Posterior Inference](https://icml.cc/virtual/2024/poster/34356) (Poster)
  - **Authors:** [Pablo Lemos](http://openreview.net/profile?id=~Pablo_Lemos1), [Nikolay Malkin](http://openreview.net/profile?id=~Nikolay_Malkin1), [Will Handley](http://openreview.net/profile?id=~Will_Handley1), [Yoshua Bengio](http://openreview.net/profile?id=~Yoshua_Bengio1), [Yashar Hezaveh](http://openreview.net/profile?id=~Yashar_Hezaveh1), [Laurence Perreault-Levasseur](http://openreview.net/profile?id=~Laurence_Perreault-Levasseur1)
  - **Affiliations:** Mila – Québec Artificial Intelligence Institute; Ciela Institute; Université de Montréal; CCA – Flatiron Institute; Dreamfold, Mila – Québec Artificial Intelligence Institute; Université de Montréal, Cavendish Laboratory; Kavli Institute for Cosmology; Gonville and Caius College, University of Cambridge, Mila – Québec Artificial Intelligence Institute; Université de Montréal; Canadian Institute for Advanced Research (CIFAR), Mila – Québec Artificial Intelligence Institute; Ciela Institute; Université de Montréal; CCA – Flatiron Institute; Perimeter Institute; Trottier Space Institute, Mila – Québec Artificial Intelligence Institute; Ciela Institute; Université de Montréal; CCA – Flatiron Institute; Perimeter Institute; Trottier Space Institute
  - **TL;DR:** The study introduces a gradient-guided nested sampling algorithm (GGNS) that enhances Bayesian parameter estimation and model comparison by effectively combining various advanced techniques. The results demonstrate improved performance in high-dimensional and multimodal settings, leading to faster mode discovery and more accurate estimates of the partition function.
  - **Keywords:** Bayesian parameter estimation, model comparison, nested sampling, Gradient-guided nested sampling, Hamiltonian slice sampling, generative flow networks, Natural sciences, cosmology, astrophysics, particle physics, biology, High-dimensional data, multimodal settings, sampling from prior with likelihood constraints, GGNS algorithm, improved mode discovery, accurate partition function estimates, Differentiable programming, dynamic nested sampling, clustering, mode separation, parallelization


- [Drug Discovery with Dynamic Goal-aware Fragments](https://icml.cc/virtual/2024/poster/32688) (Poster)
  - **Authors:** [Seul Lee](http://openreview.net/profile?id=~Seul_Lee1), [Seanie Lee](http://openreview.net/profile?id=~Seanie_Lee1), [Kenji Kawaguchi](http://openreview.net/profile?id=~Kenji_Kawaguchi1), [Sung Ju Hwang](http://openreview.net/profile?id=~Sung_Ju_Hwang1)
  - **Affiliations:** KAIST, KAIST, National University of Singapore, KAIST; DeepAuto.ai
  - **TL;DR:** This study presents a novel molecular generative framework called GEAM for drug discovery, which effectively identifies and utilizes goal-aware fragments to enhance the discovery of drug candidates. The framework demonstrates significant improvements in optimization performance compared to existing methods by dynamically updating the fragment vocabulary based on target properties.
  - **Keywords:** Drug discovery, Fragment-based drug discovery (FBDD), Molecular generative models, Goal-aware fragment extraction, Fragment assembly, Fragment modification, Graph Information Bottleneck (GIB), Chemical space exploration, Drug candidate discovery, Limitations of existing fragment extraction methods, Target chemical properties consideration, Goal-aware fragment vocabulary, Improved optimization performance, Structure-activity relationship (SAR), Fragment-wise Graph Information Bottleneck (FGIB)


- [Robust Optimization in Protein Fitness Landscapes Using Reinforcement Learning in Latent Space](https://icml.cc/virtual/2024/poster/35169) (Spotlight Poster)
  - **Authors:** [Minji Lee](http://openreview.net/profile?id=~Minji_Lee1), [Luiz Felipe Vecchietti](http://openreview.net/profile?id=~Luiz_Felipe_Vecchietti1), [Hyunkyu Jung](http://openreview.net/profile?id=~Hyunkyu_Jung1), [Hyun Joo Ro](http://openreview.net/profile?id=~Hyun_Joo_Ro1), [MEEYOUNG CHA](http://openreview.net/profile?id=~Meeyoung_Cha2), [Ho Min Kim](http://openreview.net/profile?id=~Ho_Min_Kim1)
  - **Affiliations:** School of Computing, KAIST, Daejeon, South Korea, Center for Mathematical and Computational Sciences, Institute for Basic Science, Daejeon, South Korea, School of Computing, KAIST, Daejeon, South Korea; Center for Mathematical and Computational Sciences, Institute for Basic Science, Daejeon, South Korea, Center for Biomolecular and Cellular Structure, Institute for Basic Science, Daejeon, South Korea, School of Computing, KAIST, Daejeon, South Korea; Center for Mathematical and Computational Sciences, Institute for Basic Science, Daejeon, South Korea, Department of Biological Sciences, KAIST, Daejeon, South Korea; Center for Biomolecular and Cellular Structure, Institute for Basic Science, Daejeon, South Korea
  - **TL;DR:** This study presents LatProtRL, a novel optimization method for protein fitness landscapes that utilizes reinforcement learning in a latent space to efficiently navigate from low-fitness sequences. The approach demonstrates the ability to achieve high-fitness sequences, indicating significant potential for practical applications in protein engineering.
  - **Keywords:** Protein optimization, fitness landscapes, reinforcement learning, Reinforcement learning, encoder-decoder, Markov decision process, Protein engineering, computational biology, industrial applications, Low-fitness sequences, local optima, vast protein sequence space, LatProtRL optimization method, high-fitness sequence generation, Protein language model, latent space, directed evolution


- [3D Geometric Shape Assembly via Efficient Point Cloud Matching](https://icml.cc/virtual/2024/poster/34528) (Poster)
  - **Authors:** [Nahyuk Lee](http://openreview.net/profile?id=~Nahyuk_Lee1), [Juhong Min](http://openreview.net/profile?id=~Juhong_Min1), [Junha Lee](http://openreview.net/profile?id=~Junha_Lee2), [Seungwook Kim](http://openreview.net/profile?id=~Seungwook_Kim2), [Kanghee Lee](http://openreview.net/profile?id=~Kanghee_Lee2), [Jaesik Park](http://openreview.net/profile?id=~Jaesik_Park3), [Minsu Cho](http://openreview.net/profile?id=~Minsu_Cho1)
  - **Affiliations:** Department of Computer Science and Engineering, POSTECH, Pohang, Korea, Graduate School of Artificial Intelligence, POSTECH, Pohang, Korea, Department of Computer Science and Engineering, POSTECH, Pohang, Korea, Graduate School of Artificial Intelligence, POSTECH, Pohang, Korea, Department of Computer Science and Engineering, Seoul National University, Seoul, Korea; Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea, Department of Computer Science and Engineering, Seoul National University, Seoul, Korea; Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea, Department of Computer Science and Engineering, POSTECH, Pohang, Korea; Graduate School of Artificial Intelligence, POSTECH, Pohang, Korea
  - **TL;DR:** This study introduces a new framework, Proxy Match TransformeR (PMTR), for efficiently assembling 3D geometric shapes by establishing local correspondences between point clouds. The proposed method demonstrates superior performance and efficiency compared to existing state-of-the-art techniques on the Breaking Bad dataset.
  - **Keywords:** 3D geometric shape assembly, point cloud matching, Proxy Match Transform (PMT), high-order feature transform, Robotics, manufacturing, computer graphics, computer-aided design, Accurate assembly, local correspondence analysis, geometric compatibility, Proxy Match TransformeR (PMTR), improved performance and efficiency in shape assembly, Breaking Bad dataset


- [Defining Neural Network Architecture through Polytope Structures of Datasets](https://icml.cc/virtual/2024/poster/32988) (Spotlight Poster)
  - **Authors:** [Sangmin Lee](http://openreview.net/profile?id=~Sangmin_Lee3), [Abbas Mammadov](http://openreview.net/profile?id=~Abbas_Mammadov1), [Jong Chul YE](http://openreview.net/profile?id=~Jong_Chul_Ye1)
  - **Affiliations:** Department of Mathematical Science, KAIST, Daejeon, Korea, School of Computing, KAIST, Daejeon, Korea; None, Department of Mathematical Science, KAIST, Daejeon, Korea; Kim Jaechul Graduate School of AI, KAIST, Daejeon, Korea
  - **TL;DR:** This paper investigates the relationship between neural network architecture and the polytope structure of datasets, establishing upper and lower bounds for network widths necessary for effective classification. It also presents an algorithm to infer the geometric properties of datasets from trained neural networks, demonstrating efficient classification for popular datasets like MNIST and CIFAR10.
  - **Keywords:** neural networks, dataset classification, polytope structure, deep ReLU networks, upper and lower bounds, classification of complex datasets, determining effective depth and width for neural networks, multiple manifold problem, algorithm for inferring polytope structure from trained networks, efficient classification of datasets, MNIST, Fashion-MNIST, CIFAR10, universal approximation property (UAP), simplicial complexes, geometric complexity


- [Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation](https://icml.cc/virtual/2024/poster/33513) (Poster)
  - **Authors:** [JoonHo Lee](http://openreview.net/profile?id=~JoonHo_Lee1), [Jae Oh Woo](http://openreview.net/profile?id=~Jae_Oh_Woo1), [Juree Seok](http://openreview.net/profile?id=~Juree_Seok1), [Parisa Hassanzadeh](http://openreview.net/profile?id=~Parisa_Hassanzadeh1), [Wooseok Jang](http://openreview.net/profile?id=~Wooseok_Jang4), [JuYoun Son](http://openreview.net/profile?id=~JuYoun_Son1), [Sima Didari](http://openreview.net/profile?id=~Sima_Didari1), [Baruch Gutow](http://openreview.net/profile?id=~Baruch_Gutow1), [Heng Hao](http://openreview.net/profile?id=~Heng_Hao1), [Hankyu Moon](http://openreview.net/profile?id=~Hankyu_Moon1), [Wenjun Hu](http://openreview.net/profile?id=~Wenjun_Hu2), [Yeong-Dae Kwon](http://openreview.net/profile?id=~Yeong-Dae_Kwon1), [Taehee Lee](http://openreview.net/profile?id=~Taehee_Lee1), [Seungjai Min](http://openreview.net/profile?id=~Seungjai_Min1)
  - **Affiliations:** Samsung SDS Technology Research, Seoul, Korea, Samsung SDS America, San Jose, California, USA, Samsung SDS Technology Research, Seoul, Korea, Samsung SDS America, San Jose, California, USA, Samsung SDS Technology Research, Seoul, Korea, Samsung SDS Technology Research, Seoul, Korea, Samsung SDS America, San Jose, California, USA, Samsung SDS America, San Jose, California, USA, Samsung SDS America, San Jose, California, USA, Samsung SDS America, San Jose, California, USA, Samsung SDS America, San Jose, California, USA, Samsung SDS Technology Research, Seoul, Korea, Samsung SDS Technology Research, Seoul, Korea, Samsung SDS America, San Jose, California, USA
  - **TL;DR:** This study introduces a novel Uncertainty-aware Reward Model (URM) to enhance the instruction-following capabilities of language models by providing robust uncertainty estimation for response quality. The proposed method significantly improves training outcomes and surpasses existing benchmarks, addressing the challenges of ambiguous response quality assessment in complex human language interactions.
  - **Keywords:** Instruction following, Language models, Uncertainty-aware Reward Model (URM), Bayesian approximation, Reinforcement Learning from Human Feedback (RLHF), Language model training, Policy optimization, Ambiguity in response quality assessment, Complexity of human language, Improved instruction following capability, Enhanced data curation for training, Preference datasets, Vicuna, MT-bench, Uncertainty estimation


- [Data Poisoning Attacks against Conformal Prediction](https://icml.cc/virtual/2024/poster/33473) (Poster)
  - **Authors:** [Yangyi Li](http://openreview.net/profile?id=~Yangyi_Li1), [Aobo Chen](http://openreview.net/profile?id=~Aobo_Chen1), [Wei Qian](http://openreview.net/profile?id=~Wei_Qian5), [Chenxu Zhao](http://openreview.net/profile?id=~Chenxu_Zhao4), [Divya Lidder](http://openreview.net/profile?id=~Divya_Lidder1), [Mengdi Huai](http://openreview.net/profile?id=~Mengdi_Huai1)
  - **Affiliations:** Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States
  - **TL;DR:** This paper investigates data poisoning attacks against conformal prediction methods, proposing a new class of attacks that manipulate prediction uncertainty without misclassifying examples. The findings reveal that these attacks are more effective than traditional methods, highlighting a significant vulnerability in current conformal prediction techniques.
  - **Keywords:** Data poisoning attacks, Conformal prediction, Uncertainty quantification, Black-box data poisoning attacks, Optimization frameworks, Deep learning models, Safety-critical applications, Vulnerabilities of conformal prediction methods, Manipulation of prediction uncertainty, Effectiveness of proposed attacks, Comparison with traditional poisoning attacks, Conformal prediction (CP), Deep Neural Networks (DNNs)


- [Eluder-based Regret for Stochastic Contextual MDPs](https://icml.cc/virtual/2024/poster/35034) (Poster)
  - **Authors:** [Orin Levy](http://openreview.net/profile?id=~Orin_Levy1), [Asaf Cassel](http://openreview.net/profile?id=~Asaf_Cassel1), [Alon Cohen](http://openreview.net/profile?id=~Alon_Cohen1), [Yishay Mansour](http://openreview.net/profile?id=~Yishay_Mansour2)
  - **Affiliations:** Balavatnick school of Computer Science, Tel Aviv University, Tel Aviv, Israel, Balavatnick school of Computer Science, Tel Aviv University, Tel Aviv, Israel, School of Electrical Engineering, Tel Aviv University, Tel Aviv, Israel; Google Research, Tel Aviv, Israel, Balavatnick school of Computer Science, Tel Aviv University, Tel Aviv, Israel; Google Research, Tel Aviv, Israel
  - **TL;DR:** The paper introduces the E-UC3RL algorithm for efficient regret minimization in Stochastic Contextual Markov Decision Processes (CMDPs) under minimal assumptions. It achieves a rate-optimal regret guarantee and extends the concept of Eluder dimension to general bounded metrics, making it a significant contribution to the field of reinforcement learning.
  - **Keywords:** Stochastic Contextual Markov Decision Processes, Regret Minimization, E-UC3RL algorithm, Offline least squares regression, Log loss regression, Recommendation systems, Reinforcement Learning, Regret minimization, Context-dependent dynamics, Function approximation, Efficient regret minimization algorithm, Rate-optimal guarantees, Eluder dimension, Hellinger distance, Contextual MDPs


- [ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking](https://icml.cc/virtual/2024/poster/33381) (Oral)
  - **Authors:** [Wenshuo Li](http://openreview.net/profile?id=~Wenshuo_Li2), [Xinghao Chen](http://openreview.net/profile?id=~Xinghao_Chen1), [Han Shu](http://openreview.net/profile?id=~Han_Shu1), [Yehui Tang](http://openreview.net/profile?id=~Yehui_Tang1), [Yunhe Wang](http://openreview.net/profile?id=~Yunhe_Wang1)
  - **Affiliations:** Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab; University of Science and Technology of China, University of Science and Technology of China, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab
  - **TL;DR:** This paper presents the Extreme Checkpoint Compression (ExCP) framework, which effectively reduces the storage requirements for training checkpoints of large language models while maintaining nearly lossless performance. The proposed method achieves significant compression ratios, exemplified by a 70× reduction for the Pythia-410M model, addressing critical challenges in computational and storage capacities during model training.
  - **Keywords:** Large Language Models, Model Compression, Extreme Checkpoint Compression (ExCP), Weight-Momentum Joint Shrinking, Non-Uniform Quantization, Computational and storage challenges in training large language models, urgent need for checkpoint compression, Significant storage reduction, nearly lossless performance, 70× compression for Pythia-410M model, Pythia


- [Critical windows: non-asymptotic theory for feature emergence in diffusion models](https://icml.cc/virtual/2024/poster/33698) (Poster)
  - **Authors:** [Marvin Li](http://openreview.net/profile?id=~Marvin_Li1), [Sitan Chen](http://openreview.net/profile?id=~Sitan_Chen1)
  - **Affiliations:** Harvard College, Cambridge, MA, USA, John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA
  - **TL;DR:** This study investigates the phenomenon of critical windows in diffusion models for image generation, revealing that specific features of generated images emerge during narrow time intervals in the sampling process. The authors provide a formal framework to understand these windows and demonstrate their utility in diagnosing fairness and privacy issues in real-world applications.
  - **Keywords:** diffusion models, image generation, critical windows, hierarchical samplers, reverse process, generative modeling, image generation, interpretability, feature emergence, fairness and privacy violations, bounds on critical windows, formal framework for studying windows, log-concave densities, Gaussian mixtures


- [Winner-takes-all learners are geometry-aware conditional density estimators](https://icml.cc/virtual/2024/poster/34020) (Poster)
  - **Authors:** [Victor Letzelter](http://openreview.net/profile?id=~Victor_Letzelter1), [David Perera](http://openreview.net/profile?id=~David_Perera1), [C√©dric Rommel](http://openreview.net/profile?id=~C%C3%A9dric_Rommel1), [Mathieu Fontaine](http://openreview.net/profile?id=~Mathieu_Fontaine1), [Slim Essid](http://openreview.net/profile?id=~Slim_Essid1), [Gaël Richard](http://openreview.net/profile?id=~Ga%C3%ABl_Richard1), [Patrick Perez](http://openreview.net/profile?id=~Patrick_Perez1)
  - **Affiliations:** Valeo.ai, Paris, France; LTCI, Télécom Paris, Institut Polytechnique de Paris, France, LTCI, Télécom Paris, Institut Polytechnique de Paris, France, Meta AI, Paris, France, LTCI, Télécom Paris, Institut Polytechnique de Paris, France, LTCI, Télécom Paris, Institut Polytechnique de Paris, France, LTCI, Télécom Paris, Institut Polytechnique de Paris, France, Kyutai, Paris, France
  - **TL;DR:** This paper explores the use of Winner-takes-all training for conditional density estimation, leveraging its geometric properties to improve uncertainty quantification. The authors propose a novel kernel-based density estimator that enhances probabilistic predictions and demonstrate its effectiveness on various datasets, including audio data.
  - **Keywords:** Winner-takes-all training, conditional density estimation, uncertainty quantification, Centroidal Voronoi tessellations, kernel-based density estimator, Synthetic datasets, real-world datasets, audio data, Ambiguity in data, non-deterministic relationship between inputs and outputs, conditional distribution estimation, Probabilistic evaluation of WTA predictions, geometric quantization properties, probabilistic convergence


- [Convergence and Complexity Guarantee for Inexact First-order Riemannian Optimization Algorithms](https://icml.cc/virtual/2024/poster/34887) (Poster)
  - **Authors:** [Yuchen Li](http://openreview.net/profile?id=~Yuchen_Li11), [Laura Balzano](http://openreview.net/profile?id=~Laura_Balzano1), [Deanna Needell](http://openreview.net/profile?id=~Deanna_Needell2), [Hanbaek Lyu](http://openreview.net/profile?id=~Hanbaek_Lyu1)
  - **Affiliations:** Department of Mathematics, University of Wisconsin-Madison, WI 53706, USA, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI 48109, USA, Department of Mathematics, University of California, Los Angeles, CA 90025, USA, Department of Mathematics, University of Wisconsin-Madison, WI 53706, USA
  - **TL;DR:** This paper analyzes inexact Riemannian gradient descent (RGD) and establishes convergence and complexity guarantees for a general framework of tangential Block Majorization-Minimization (tBMM). The findings indicate that tBMM can effectively converge to an ϵ-stationary point and outperform existing methods in various optimization problems.
  - **Keywords:** Riemannian optimization, inexact optimization, Riemannian gradient descent (RGD), tangential Block Majorization-Minimization (tBMM), Nonnegative tensor decomposition, regularized nonnegative matrix factorization, low-rank matrix recovery, Convergence analysis, complexity guarantees, nonconvex optimization, Convergence to ϵ-stationary points, performance improvement over existing methods, Riemannian manifold, retraction, tangent space


- [DetKDS: Knowledge Distillation Search for Object Detectors](https://icml.cc/virtual/2024/poster/34018) (Poster)
  - **Authors:** [Lujun Li](http://openreview.net/profile?id=~Lujun_Li1), [Yufan Bao](http://openreview.net/profile?id=~Yufan_Bao1), [Peijie Dong](http://openreview.net/profile?id=~Peijie_Dong1), [Chuanguang Yang](http://openreview.net/profile?id=~Chuanguang_Yang1), [Anggeng Li](http://openreview.net/profile?id=~Anggeng_Li1), [Wenhan Luo](http://openreview.net/profile?id=~Wenhan_Luo1), [Qifeng Liu](http://openreview.net/profile?id=~Qifeng_Liu1), [Wei Xue](http://openreview.net/profile?id=~Wei_Xue5), [Yike Guo](http://openreview.net/profile?id=~Yike_Guo1)
  - **Affiliations:** The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology (Guang Zhou), Institute of Computing Technology, Chinese Academy of Sciences, The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology
  - **TL;DR:** This paper introduces DetKDS, a framework for automating the search for optimal knowledge distillation policies in object detection, addressing the challenges of manual design. The proposed method significantly improves detection performance across various detectors, achieving notable gains in accuracy.
  - **Keywords:** Knowledge Distillation, Object Detection, Search Algorithms, Distillation Policies, Evolutionary Algorithm, Detection Tasks, Instance Segmentation, Computational Complexity, Resource Requirements, Teacher-Student Gap, Optimal Distillers, Distillation Losses, Automation of Distillation Process, Teacher-Student Pairs, Global Features, Foreground-Background Features, Logits Response


- [Learning Causal Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition](https://icml.cc/virtual/2024/poster/34264) (Poster)
  - **Authors:** [Yuke Li](http://openreview.net/profile?id=~Yuke_Li1), [Guangyi Chen](http://openreview.net/profile?id=~Guangyi_Chen1), [Ben Abramowitz](http://openreview.net/profile?id=~Ben_Abramowitz1), [Stefano Anzellotti](http://openreview.net/profile?id=~Stefano_Anzellotti1), [Donglai Wei](http://openreview.net/profile?id=~Donglai_Wei1)
  - **Affiliations:** Boston College, Boston MA, USA, Carnegie Mellon University, Pittsburgh PA, USA; Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Tulane University, New Orleans LA, USA, Boston College, Boston MA, USA, Boston College, Boston MA, USA
  - **TL;DR:** This study introduces CDTD, a method for few-shot action recognition that effectively transfers causal domain-invariant temporal dynamics from pre-trained models to new data with limited samples. The approach demonstrates superior accuracy and adaptation efficiency compared to existing methods.
  - **Keywords:** Few-shot action recognition, knowledge transfer, Causal representation learning, temporal dynamics, Action recognition, Distribution shift, limited samples, transferable knowledge, non-transferable knowledge, Causal Domain-Invariant Temporal Dynamics (CDTD), adaptation efficiency


- [Full-Atom Peptide Design based on Multi-modal Flow Matching](https://icml.cc/virtual/2024/poster/34958) (Poster)
  - **Authors:** [Jiahan Li](http://openreview.net/profile?id=~Jiahan_Li2), [Chaoran Cheng](http://openreview.net/profile?id=~Chaoran_Cheng2), [Zuofan Wu](http://openreview.net/profile?id=~Zuofan_Wu1), [Ruihan Guo](http://openreview.net/profile?id=~Ruihan_Guo1), [Shitong Luo](http://openreview.net/profile?id=~Shitong_Luo1), [Zhizhou Ren](http://openreview.net/profile?id=~Zhizhou_Ren1), [Jian Peng](http://openreview.net/profile?id=~Jian_Peng1), [Jianzhu Ma](http://openreview.net/profile?id=~Jianzhu_Ma2)
  - **Affiliations:** Helixon Research; Institute for AI Industry Research, Tsinghua University, Department of Computer Science, University of Illinois Urbana-Champaign, Helixon Research, Helixon Research, Helixon Research, Helixon Research, Helixon Research, Helixon Research; Institute for AI Industry Research, Tsinghua University
  - **TL;DR:** This study introduces PepFlow, a novel multi-modal deep generative model for designing full-atom peptides targeting specific protein receptors, leveraging the flow-matching framework. The results demonstrate its superior performance in computational peptide design, addressing the challenges posed by the vast design space of peptides.
  - **Keywords:** peptide design, drug discovery, protein-peptide interactions, multi-modal deep generative model, flow-matching framework, diffusion probabilistic models, computational peptide design, therapeutic peptides, vast design space of peptides, limitations of traditional discovery methods, PepFlow model, fine-grained design of full-atom peptides, superior performance in benchmarks, SE(3) manifold, categorical distributions, high-dimensional tori


- [Two-sided Competing Matching Recommendation Markets With Quota and Complementary Preferences Constraints](https://icml.cc/virtual/2024/poster/34720) (Poster)
  - **Authors:** [Yuantong Li](http://openreview.net/profile?id=~Yuantong_Li1), [Guang Cheng](http://openreview.net/profile?id=~Guang_Cheng1), [Xiaowu Dai](http://openreview.net/profile?id=~Xiaowu_Dai1)
  - **Affiliations:** Department of Statistics and Data Science, UCLA; Department of Biostatistics, UCLA, Department of Statistics and Data Science, UCLA, Department of Statistics and Data Science, UCLA; Department of Biostatistics, UCLA
  - **TL;DR:** This paper presents a new recommendation algorithm, Multi-agent Multi-type Thompson Sampling (MMTS), to address the challenges of two-sided online matching markets with complementary preferences and quota constraints. The proposed method achieves stability in matching outcomes while effectively learning agents' preferences from data, demonstrating a total Bayesian regret that scales with the total quota and time horizon.
  - **Keywords:** Two-sided matching markets, complementary preferences, recommendation systems, Multi-agent Multi-type Thompson Sampling (MMTS), bandit learning framework, Thompson Sampling, Job market, online matching markets, Instability in matching process, unknown preferences, quota constraints, Stability in matching outcomes, Bayesian regret analysis, Complementary preferences, quota constraints


- [Positive and Unlabeled Learning with Controlled Probability Boundary Fence](https://icml.cc/virtual/2024/poster/32702) (Poster)
  - **Authors:** [Changchun Li](http://openreview.net/profile?id=~Changchun_Li1), [Yuanchao Dai](http://openreview.net/profile?id=~Yuanchao_Dai1), [Lei Feng](http://openreview.net/profile?id=~Lei_Feng1), [Ximing Li](http://openreview.net/profile?id=~Ximing_Li1), [Bing Wang](http://openreview.net/profile?id=~Bing_Wang12), [Jihong Ouyang](http://openreview.net/profile?id=~Jihong_Ouyang2)
  - **Affiliations:** College of Computer Science and Technology, Jilin University, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China, College of Computer Science and Technology, Jilin University, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China, Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore, College of Computer Science and Technology, Jilin University, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China, College of Computer Science and Technology, Jilin University, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China, College of Computer Science and Technology, Jilin University, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China
  - **TL;DR:** This paper presents a two-stage method for Positive and Unlabeled (PU) learning, called PUL-CPBF, which effectively controls the probability boundary through asymmetric penalties and empirical evaluations. The method demonstrates competitive performance compared to existing PU learning approaches.
  - **Keywords:** Positive and Unlabeled Learning, Binary Classification, Asymmetric Disambiguation-Free Expected Risk, Weak Binary Classifiers, Stochastic Label Generation, Data Sparsity, Boundary Deviation Phenomenon, Positive and Unlabeled Learning with Controlled Probability Boundary Fence (PUL-CPBF), Empirical Evaluation


- [Completing Visual Objects via Bridging Generation and Segmentation](https://icml.cc/virtual/2024/poster/33128) (Poster)
  - **Authors:** [Xiang Li](http://openreview.net/profile?id=~Xiang_Li35), [Yinpeng Chen](http://openreview.net/profile?id=~Yinpeng_Chen1), [Chung-Ching Lin](http://openreview.net/profile?id=~Chung-Ching_Lin2), [Hao Chen](http://openreview.net/profile?id=~Hao_Chen15), [Kai Hu](http://openreview.net/profile?id=~Kai_Hu2), [Rita Singh](http://openreview.net/profile?id=~Rita_Singh1), [Bhiksha Raj](http://openreview.net/profile?id=~Bhiksha_Raj1), [Lijuan Wang](http://openreview.net/profile?id=~Lijuan_Wang1), [Zicheng Liu](http://openreview.net/profile?id=~Zicheng_Liu1)
  - **Affiliations:** Carnegie Mellon University, Microsoft, Microsoft, Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University; MBZUAI, Carnegie Mellon University; MBZUAI, Microsoft, Microsoft
  - **TL;DR:** This paper introduces MaskComp, a novel method for object completion that iteratively refines partial object masks through generation and segmentation stages. The approach demonstrates superior performance compared to existing methods like ControlNet and Stable Diffusion in reconstructing occluded objects.
  - **Keywords:** object completion, image editing, generation and segmentation, mask denoising, image generation, creative image editing, restoration of partially occluded objects, alignment of generated content with partial objects, MaskComp method, iterative completion process


- [Purifying Quantization-conditioned Backdoors via Layer-wise Activation Correction with Distribution Approximation](https://icml.cc/virtual/2024/poster/34693) (Poster)
  - **Authors:** [Boheng Li](http://openreview.net/profile?id=~Boheng_Li1), [Yishuo Cai](http://openreview.net/profile?id=~Yishuo_Cai1), [Jisong Cai](http://openreview.net/profile?id=~Jisong_Cai1), [Yiming Li](http://openreview.net/profile?id=~Yiming_Li1), [Han Qiu](http://openreview.net/profile?id=~Han_Qiu3), [Run Wang](http://openreview.net/profile?id=~Run_Wang1), [Tianwei Zhang](http://openreview.net/profile?id=~Tianwei_Zhang1)
  - **Affiliations:** School of Cyber Science and Engineering, Wuhan University, Central South University, School of Cyber Science and Engineering, Wuhan University, Nanyang Technological University, Tsinghua University, School of Cyber Science and Engineering, Wuhan University, Nanyang Technological University
  - **TL;DR:** This study investigates the vulnerabilities of quantized deep neural networks to quantization-conditioned backdoors (QCBs) and proposes a novel defense mechanism that aligns layer-wise activations of quantized models with their full-precision counterparts to mitigate these threats. The effectiveness of the proposed method is validated through extensive experiments.
  - **Keywords:** Model quantization, Backdoor attacks, Deep neural networks, Quantization-conditioned backdoors (QCBs), Layer-wise activation correction, Distribution approximation, Security-critical scenarios, Facial recognition, Vulnerability to backdoor attacks, Dormant backdoors in full-precision models, Purification of quantized models, Activation alignment, Defense mechanisms against QCBs


- [Evolving Subnetwork Training for Large Language Models](https://icml.cc/virtual/2024/poster/34620) (Poster)
  - **Authors:** [hanqi li](http://openreview.net/profile?id=~Hanqi_Li3), [Lu Chen](http://openreview.net/profile?id=~Lu_Chen3), [Da Ma](http://openreview.net/profile?id=~Da_Ma2), [Zijian Wu](http://openreview.net/profile?id=~Zijian_Wu5), [Su Zhu](http://openreview.net/profile?id=~Su_Zhu1), [Kai Yu](http://openreview.net/profile?id=~Kai_Yu3)
  - **Affiliations:** 1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China; None, 1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China; Suzhou Laboratory, Suzhou, China, 1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China, 1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China, AISpeech Co., Ltd., Suzhou, China, 1X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China; Suzhou Laboratory, Suzhou, China
  - **TL;DR:** This paper introduces Evolving Subnetwork Training (EST) as a novel approach to reduce training costs for large language models while maintaining performance. The method demonstrates significant savings in computational resources and enhances generalization in downstream tasks.
  - **Keywords:** Large Language Models, Efficient Training, Evolving Subnetwork Training (EST), Multi-Head Attention (MHA), Multi-Layer Perceptron (MLP), Natural Language Processing (NLP), High training costs, Model over-parameterization, Training cost savings, Performance improvements in downstream tasks, GPT2, TinyLlama, Transformer


- [FightLadder: A Benchmark for Competitive Multi-Agent Reinforcement Learning](https://icml.cc/virtual/2024/poster/34534) (Poster)
  - **Authors:** [Wenzhe Li](http://openreview.net/profile?id=~Wenzhe_Li2), [Zihan Ding](http://openreview.net/profile?id=~Zihan_Ding1), [Seth Karten](http://openreview.net/profile?id=~Seth_Karten1), [Chi Jin](http://openreview.net/profile?id=~Chi_Jin1)
  - **Affiliations:** Princeton University, Princeton University, Princeton University, Princeton University
  - **TL;DR:** The study introduces FightLadder, a benchmark platform for competitive multi-agent reinforcement learning, designed to facilitate research in challenging gaming dynamics. It demonstrates the platform's effectiveness by training an agent that defeats built-in characters and highlights the challenges of creating non-exploitable agents in competitive settings.
  - **Keywords:** Multi-Agent Reinforcement Learning (MARL), Competitive Gaming, Reinforcement Learning (RL), State-of-the-Art MARL Algorithms, Real-Time Fighting Games, Game Theory, Difficulty of training non-exploitable agents, Need for competitive benchmarks, FightLadder platform, Evaluation metrics for agent performance and exploitability


- [Graph Structure Extrapolation for Out-of-Distribution Generalization](https://icml.cc/virtual/2024/poster/33811) (Poster)
  - **Authors:** [Xiner Li](http://openreview.net/profile?id=~Xiner_Li1), [Shurui Gui](http://openreview.net/profile?id=~Shurui_Gui1), [Youzhi Luo](http://openreview.net/profile?id=~Youzhi_Luo1), [Shuiwang Ji](http://openreview.net/profile?id=~Shuiwang_Ji1)
  - **Affiliations:** Department of Computer Science and Engineering, Texas A&M University, Texas, USA, Department of Computer Science and Engineering, Texas A&M University, Texas, USA, Department of Computer Science and Engineering, Texas A&M University, Texas, USA, Department of Computer Science and Engineering, Texas A&M University, Texas, USA
  - **TL;DR:** This study addresses the challenge of out-of-distribution generalization in graph classification by proposing a novel data augmentation method based on non-Euclidean-space linear extrapolation. The method effectively generates out-of-distribution graph data tailored for specific shifts, demonstrating significant improvements in model performance across various tasks.
  - **Keywords:** Out-of-distribution generalization, graph OOD problems, Non-Euclidean-space linear extrapolation, data augmentation, Graph classification, Distribution shift, model performance degradation, Environment-aware data augmentation, graph-space extrapolation, Causal mechanisms, topological irregularity


- [Q-Probe: A Lightweight Approach to Reward Maximization for Language Models](https://icml.cc/virtual/2024/poster/33407) (Poster)
  - **Authors:** [Kenneth Li](http://openreview.net/profile?id=~Kenneth_Li1), [Samy Jelassi](http://openreview.net/profile?id=~Samy_Jelassi1), [Hugh Zhang](http://openreview.net/profile?id=~Hugh_Zhang1), [Sham Kakade](http://openreview.net/profile?id=~Sham_M._Kakade1), [Martin Wattenberg](http://openreview.net/profile?id=~Martin_Wattenberg1), [David Brandfonbrener](http://openreview.net/profile?id=~David_Brandfonbrener1)
  - **Affiliations:** John A. Paulson School Of Engineering And Applied Sciences, Harvard University; Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Center of Mathematical Sciences and Applications, Harvard University, John A. Paulson School Of Engineering And Applied Sciences, Harvard University; Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, John A. Paulson School Of Engineering And Applied Sciences, Harvard University; Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, John A. Paulson School Of Engineering And Applied Sciences, Harvard University, Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University
  - **TL;DR:** This paper introduces Q-probing, a lightweight method for adapting pre-trained language models to maximize task-specific rewards without modifying the model's weights. The approach demonstrates significant improvements in performance on coding benchmarks, outperforming traditional finetuning methods, especially in data-limited scenarios.
  - **Keywords:** reward maximization, language model adaptation, Q-probing, KL-constrained maximization, importance-weighted policy gradients, code generation, mathematical reasoning, dialogue systems, improved accuracy on coding benchmarks, novel direct policy learning objectives, MBPP, Code-LLaMA-7B, LORA, large language models (LLMs), reinforcement learning from human feedback (RLHF)


- [Accelerating Convergence of Score-Based Diffusion Models, Provably](https://icml.cc/virtual/2024/poster/34352) (Poster)
  - **Authors:** [Gen Li](http://openreview.net/profile?id=~Gen_Li2), [Yu Huang](http://openreview.net/profile?id=~Yu_Huang3), [Timofey Efimov](http://openreview.net/profile?id=~Timofey_Efimov1), [Yuting Wei](http://openreview.net/profile?id=~Yuting_Wei1), [Yuejie Chi](http://openreview.net/profile?id=~Yuejie_Chi1), [Yuxin Chen](http://openreview.net/profile?id=~Yuxin_Chen5)
  - **Affiliations:** Department of Statistics, The Chinese University of Hong Kong, Hong Kong, Department of Statistics and Data Science, Wharton School, University of Pennsylvania, Philadelphia, PA 19104, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA, Department of Statistics and Data Science, Wharton School, University of Pennsylvania, Philadelphia, PA 19104, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA, Department of Statistics and Data Science, Wharton School, University of Pennsylvania, Philadelphia, PA 19104, USA
  - **TL;DR:** This paper presents novel training-free algorithms to accelerate score-based diffusion models, specifically improving the convergence rates of deterministic and stochastic samplers. The proposed methods significantly enhance sampling speed while maintaining performance across various generative modeling applications.
  - **Keywords:** score-based diffusion models, generative modeling, DDIM (Denoising Diffusion Implicit Models), DDPM (Denoising Diffusion Probabilistic Models), higher-order approximation, ODE solvers, computer vision, natural language processing, medical imaging, bioinformatics, low sampling speed, extensive function evaluations, accelerated deterministic sampler, accelerated stochastic sampler, convergence rates, GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders), stochastic differential equations (SDEs)


- [Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions](https://icml.cc/virtual/2024/poster/32806) (Poster)
  - **Authors:** [Weihan Li](http://openreview.net/profile?id=~Weihan_Li1), [Chengrui Li](http://openreview.net/profile?id=~Chengrui_Li1), [Yule Wang](http://openreview.net/profile?id=~Yule_Wang1), [Anqi Wu](http://openreview.net/profile?id=~Anqi_Wu3)
  - **Affiliations:** School of Computational Science & Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science & Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science & Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science & Engineering, Georgia Institute of Technology, Atlanta, USA
  - **TL;DR:** This study introduces the Multi-Region Markovian Gaussian Process (MRM-GP), which combines Gaussian Process and Linear Dynamical System methodologies to effectively model and reveal communication directions across multiple brain regions. The model achieves efficient linear inference and separates oscillatory communications into distinct frequency bands, addressing the complexities of high-dimensional neural recordings.
  - **Keywords:** brain region communication, neuroscience, Gaussian Process (GP), Linear Dynamical System (LDS), Multi-Region Markovian Gaussian Process (MRM-GP), neural recordings, inter-areal communication, high-dimensional neural recordings, concurrent communication, interpretable low-dimensional representation, frequency band separation


- [A Generative Approach for Treatment Effect Estimation under Collider Bias: From an Out-of-Distribution Perspective](https://icml.cc/virtual/2024/poster/33253) (Poster)
  - **Authors:** [Baohong Li](http://openreview.net/profile?id=~Baohong_Li1), [Haoxuan Li](http://openreview.net/profile?id=~Haoxuan_Li6), [Anpeng Wu](http://openreview.net/profile?id=~Anpeng_Wu1), [Minqin Zhu](http://openreview.net/profile?id=~Minqin_Zhu1), [shiyuan Peng](http://openreview.net/profile?id=~shiyuan_Peng1), [Qingyu Cao](http://openreview.net/profile?id=~Qingyu_Cao1), [Kun Kuang](http://openreview.net/profile?id=~Kun_Kuang1)
  - **Affiliations:** College of Computer Science and Technology, Zhejiang University, Hangzhou, China, Center for Data Science, Peking University, Beijing, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, Alibaba Group, Hangzhou, China, Alibaba Group, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China
  - **TL;DR:** This study addresses the challenge of collider bias in treatment effect estimation from observational data by proposing a novel method called Coupled Counterfactual Generative Adversarial Model (C2GAM). The method effectively generates missing samples and improves the accuracy of treatment effect estimators, demonstrating significant performance enhancements through extensive experiments.
  - **Keywords:** Treatment effect estimation, Collider bias, Observational data, Coupled Counterfactual Generative Adversarial Model (C2GAM), Collider bias, Non-random sample selection, Treatment effect estimation challenges, Significant performance improvements in treatment effect estimators, Synthetic data, Real-world data, Out-of-distribution (OOD) perspective


- [Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for Zero-Shot Semantic Segmentation](https://icml.cc/virtual/2024/poster/33865) (Poster)
  - **Authors:** [Yunheng Li](http://openreview.net/profile?id=~Yunheng_Li1), [Zhong-Yu Li](http://openreview.net/profile?id=~Zhong-Yu_Li1), [Quan-Sheng Zeng](http://openreview.net/profile?id=~Quan-Sheng_Zeng1), [Qibin Hou](http://openreview.net/profile?id=~Qibin_Hou1), [Ming-Ming Cheng](http://openreview.net/profile?id=~Ming-Ming_Cheng3)
  - **Affiliations:** VCIP, School of Computer Science, Nankai University, VCIP, School of Computer Science, Nankai University, VCIP, School of Computer Science, Nankai University, VCIP, School of Computer Science, Nankai University; Nankai International Advanced Research Institute (Shenzhen Futian), VCIP, School of Computer Science, Nankai University; Nankai International Advanced Research Institute (Shenzhen Futian)
  - **TL;DR:** The study introduces Cascade-CLIP, a novel framework that enhances zero-shot semantic segmentation by aligning multi-level visual features with text embeddings through independent decoders. Experimental results demonstrate its superior performance on various segmentation benchmarks compared to existing methods.
  - **Keywords:** Zero-shot semantic segmentation, Vision-language models, CLIP, Cascaded decoders, Semantic segmentation, Computer vision, Alignment of visual features with text embeddings, Weak zero-shot ability for novel classes, Cascade-CLIP framework, Improved zero-shot performance, COCO-Stuff, Pascal-VOC, Pascal-Context


- [Enhancing Class-Imbalanced Learning with Pre-Trained Guidance through Class-Conditional Knowledge Distillation](https://icml.cc/virtual/2024/poster/34178) (Poster)
  - **Authors:** [Lan Li](http://openreview.net/profile?id=~Lan_Li2), [Xin-Chun Li](http://openreview.net/profile?id=~Xin-Chun_Li1), [Han-Jia Ye](http://openreview.net/profile?id=~Han-Jia_Ye1), [De-Chuan Zhan](http://openreview.net/profile?id=~De-Chuan_Zhan1)
  - **Affiliations:** School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China
  - **TL;DR:** This study proposes Class-Conditional Knowledge Distillation (CCKD) to enhance class-imbalanced learning by leveraging large-scale pre-trained models to improve generalization for minority classes. Experimental results show a significant average accuracy improvement of 7.4% on various imbalanced datasets.
  - **Keywords:** Class-imbalanced learning, Knowledge distillation, Class-Conditional Knowledge Distillation (CCKD), Augmented CCKD (ACCKD), Class imbalance, Generalization of minority classes, Classifier bias, Average accuracy improvement of 7.4%


- [RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning](https://icml.cc/virtual/2024/poster/33056) (Poster)
  - **Authors:** [Boning Li](http://openreview.net/profile?id=~Boning_Li3), [Zhixuan Fang](http://openreview.net/profile?id=~Zhixuan_Fang1), [Longbo Huang](http://openreview.net/profile?id=~Longbo_Huang2)
  - **Affiliations:** Tsinghua University, IIIS, Beijing, China, Tsinghua University, IIIS, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China, Tsinghua University, IIIS, Beijing, China
  - **TL;DR:** The study introduces RL-CFR, a novel reinforcement learning approach for dynamic action abstraction in Imperfect Information Extensive-Form Games, addressing challenges of large action spaces and computational complexity. RL-CFR outperforms existing methods, achieving higher expected payoffs without increasing solving time.
  - **Keywords:** Imperfect Information Extensive-Form Games, Action Abstraction, Reinforcement Learning, Counterfactual Regret Minimization (CFR), Markov Decision Process (MDP), Poker, Multi-player Turn-taking Games, Large action spaces, Computational complexity, Fixed action abstractions, Sub-optimal performance, Dynamic action abstraction, Higher expected payoff, Efficient CFR solving, Heads-up No-limit Texas Hold’em


- [Debiased Distribution Compression](https://icml.cc/virtual/2024/poster/34316) (Poster)
  - **Authors:** [Lingxiao Li](http://openreview.net/profile?id=~Lingxiao_Li1), [Raaz Dwivedi](http://openreview.net/profile?id=~Raaz_Dwivedi1), [Lester Mackey](http://openreview.net/profile?id=~Lester_Mackey1)
  - **Affiliations:** MIT CSAIL, Cornell Tech, Microsoft Research New England
  - **TL;DR:** This paper introduces new compression methods that effectively summarize a target distribution even when using biased input sequences, achieving better approximation errors than traditional i.i.d. sampling. Key methods include Stein Kernel Thinning and Low-rank SKT, which provide efficient solutions for overcoming biases in Markov chain Monte Carlo sampling.
  - **Keywords:** Distribution compression, biased input sequences, Stein Kernel Thinning (SKT), Low-rank SKT, Stein Recombination, Stein Cholesky, maximum mean discrepancy (MMD), Markov chain Monte Carlo (MCMC) inference, computational modeling, Bias in sampling, burn-in, approximate MCMC, Efficient coreset constructions, improved approximation error


- [Neural Collapse in Multi-label Learning with Pick-all-label Loss](https://icml.cc/virtual/2024/poster/32680) (Poster)
  - **Authors:** [Pengyu Li](http://openreview.net/profile?id=~Pengyu_Li6), [Xiao Li](http://openreview.net/profile?id=~Xiao_Li8), [Yutong Wang](http://openreview.net/profile?id=~Yutong_Wang1), [Qing Qu](http://openreview.net/profile?id=~Qing_Qu2)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA
  - **TL;DR:** This study investigates the phenomenon of neural collapse in multi-label classification tasks, demonstrating that the last-layer features and classifiers exhibit a generalized form of neural collapse, termed multi-label neural collapse (M-lab NC). The findings suggest that this structure can enhance training efficiency and improve test performance in multi-label learning scenarios.
  - **Keywords:** multi-label classification, neural collapse, pick-all-label loss, equi-angular tight frame (ETF), variability of features, feature mean collapse, multi-label neural collapse (M-lab NC), tag-wise average property


- [Value-Evolutionary-Based Reinforcement Learning](https://icml.cc/virtual/2024/poster/33803) (Poster)
  - **Authors:** [Pengyi Li](http://openreview.net/profile?id=~Pengyi_Li1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Hongyao Tang](http://openreview.net/profile?id=~Hongyao_Tang1), [Yan Zheng](http://openreview.net/profile?id=~YAN_ZHENG1), [Fazl Barez](http://openreview.net/profile?id=~Fazl_Barez1)
  - **Affiliations:** College of Intelligence and Computing, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China, Edinburgh Centre for Robotics; University of Oxford; Centre for the Study of Existential Risk, University of Cambridge
  - **TL;DR:** This paper introduces Value-Evolutionary-Based Reinforcement Learning (VEB-RL), which integrates Evolutionary Algorithms with value-based Reinforcement Learning to enhance sample efficiency and performance. Experiments demonstrate that VEB-RL significantly improves existing algorithms like DQN, Rainbow, and SPR.
  - **Keywords:** Value-based Reinforcement Learning, Evolutionary Algorithms, Temporal Difference error, DQN, Rainbow, SPR, Game AI, Robot Control, Automatic Driving, Poor exploration capability, convergence issues, reliance on reward signal accuracy, Value-Evolutionary-Based Reinforcement Learning (VEB-RL), improved sample efficiency, MinAtar, Atari


- [A Differentiable Partially Observable Generalized Linear Model with Forward-Backward Message Passing](https://icml.cc/virtual/2024/poster/35114) (Poster)
  - **Authors:** [Chengrui Li](http://openreview.net/profile?id=~Chengrui_Li1), [Weihan Li](http://openreview.net/profile?id=~Weihan_Li1), [Yule Wang](http://openreview.net/profile?id=~Yule_Wang1), [Anqi Wu](http://openreview.net/profile?id=~Anqi_Wu3)
  - **Affiliations:** School of Computational Science & Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science & Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science & Engineering, Georgia Institute of Technology, Atlanta, USA, School of Computational Science & Engineering, Georgia Institute of Technology, Atlanta, USA
  - **TL;DR:** This study introduces a differentiable partially observable generalized linear model (POGLM) that addresses challenges in learning neural connectivity from visible neurons by enabling a more effective gradient estimation method and a novel sampling scheme. The proposed model demonstrates improved performance and interpretability in experiments on synthetic and real-world datasets.
  - **Keywords:** neural connectivity, latent variable models, neuroscience, partially observable generalized linear model (POGLM), variational inference (VI), pathwise gradient estimator, forward-backward message passing, neuroscience, difficulties in learning latent variable models, sampled Poisson hidden spike count, inefficiency of existing variational models, new differentiable POGLM, improved gradient estimation, interpretable parameters, synthetic datasets, real-world datasets, generalized linear model (GLM), evidence lower bound (ELBO)


- [Preventing Model Collapse in Gaussian Process Latent Variable Models](https://icml.cc/virtual/2024/poster/35013) (Poster)
  - **Authors:** [Ying Li](http://openreview.net/profile?id=~Ying_Li21), [Zhidi Lin](http://openreview.net/profile?id=~Zhidi_Lin1), [Feng Yin](http://openreview.net/profile?id=~Feng_Yin1), [Michael Minyi Zhang](http://openreview.net/profile?id=~Michael_Minyi_Zhang1)
  - **Affiliations:** Department of Statistics & Actuarial Science, The University of Hong Kong, Hong Kong, China, School of Science & Engineering, The Chinese University of Hong Kong, Shenzhen, China, School of Science & Engineering, The Chinese University of Hong Kong, Shenzhen, China, Department of Statistics & Actuarial Science, The University of Hong Kong, Hong Kong, China
  - **TL;DR:** This paper addresses model collapse in Gaussian Process Latent Variable Models (GPLVMs) by examining the impact of projection variance and enhancing kernel flexibility through the integration of spectral mixture and random Fourier feature kernels. The proposed method, advised RFLVM, demonstrates superior performance in generating informative latent representations and handling missing data across various datasets.
  - **Keywords:** Gaussian Process Latent Variable Models, Dimensionality Reduction, Unsupervised Learning, Spectral Mixture Kernel, Random Fourier Feature Kernel, Variational Inference, Image Recognition, Human Pose Estimation, Intrusion Detection, Image-Text Retrieval, Model Collapse, Inadequate Kernel Flexibility, Projection Noise, Advised RFLVM, Informative Latent Representations, Missing Data Imputation, Gaussian Process, Latent Variable Model, Variational Autoencoders


- [VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context](https://icml.cc/virtual/2024/poster/33414) (Poster)
  - **Authors:** [yunxin li](http://openreview.net/profile?id=~yunxin_li1), [Baotian Hu](http://openreview.net/profile?id=~Baotian_Hu1), [Haoyuan Shi](http://openreview.net/profile?id=~Haoyuan_Shi2), [Wei Wang](http://openreview.net/profile?id=~Wei_Wang57), [Longyue Wang](http://openreview.net/profile?id=~Longyue_Wang3), [Min Zhang](http://openreview.net/profile?id=~Min_Zhang9)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Cyber Science and Technology, Shenzhen campus of Sun Yat-sen University, Shenzhen, China, None, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China
  - **TL;DR:** This study introduces VisionGraph, a benchmark for evaluating Large Multimodal Models' capabilities in solving complex graph theory problems. The findings reveal that while LMMs struggle with graphical structure perception, the proposed DPR chain significantly enhances their multi-step reasoning abilities, with GPT-4V achieving state-of-the-art performance.
  - **Keywords:** Large Multimodal Models, Graph Theory, Visual Reasoning, Description-Program-Reasoning (DPR) chain, Multimodal graph theory problems, Visual mathematics, Inferior perception accuracy for graphical structures, Multi-step reasoning challenges, SOTA performance of GPT-4V (DPR) agent, Benchmark named VisionGraph


- [Vague Prototype-Oriented Diffusion Model for Multi-Class Anomaly Detection](https://icml.cc/virtual/2024/poster/34520) (Poster)
  - **Authors:** [yuxin li](http://openreview.net/profile?id=~Yuxin_Li3), [Yaoxuan Feng](http://openreview.net/profile?id=~Yaoxuan_Feng1), [Bo Chen](http://openreview.net/profile?id=~Bo_Chen1), [Wenchao Chen](http://openreview.net/profile?id=~Wenchao_Chen1), [Yubiao Wang](http://openreview.net/profile?id=~Yubiao_Wang1), [Xinyue Hu](http://openreview.net/profile?id=~Xinyue_Hu1), [baolin sun](http://openreview.net/profile?id=~baolin_sun1), [QuChunhui](http://openreview.net/profile?id=~QuChunhui1), [Mingyuan Zhou](http://openreview.net/profile?id=~Mingyuan_Zhou1)
  - **Affiliations:** National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, 710071, China, National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, 710071, China, National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, 710071, China, National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, 710071, China, National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, 710071, China, National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, 710071, China, Xidian University, Xi’an, 710071, China, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China; National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, 710071, China, McCombs School of Business, The University of Texas at Austin, Austin, TX 78712
  - **TL;DR:** This study presents a Vague Prototype-Oriented Diffusion Model (VPDM) for multi-class unsupervised anomaly detection, addressing the "identical shortcut" problem by utilizing vague prototypes and a novel conditional diffusion approach. The proposed method demonstrates state-of-the-art performance across various datasets, including MVTec, VisA, and MPDD.
  - **Keywords:** Multi-class unsupervised anomaly detection, Anomaly detection, Vague Prototype-Oriented Diffusion Model (VPDM), Conditional diffusion model, Vague Prototype-Oriented Optimal Transport (VPOT), Medical image analysis, Video inspection, Defect detection, Identical shortcut problem, Infiltration of abnormal information, Memory-intensive modeling, State-of-the-art results in anomaly detection, Unified optimization objective, MVTec, VisA, MPDD


- [Measuring Stochastic Data Complexity with Boltzmann Influence Functions](https://icml.cc/virtual/2024/poster/34382) (Poster)
  - **Authors:** [Nathan Ng](http://openreview.net/profile?id=~Nathan_Hoyen_Ng1), [Roger Grosse](http://openreview.net/profile?id=~Roger_Baker_Grosse1), [Marzyeh Ghassemi](http://openreview.net/profile?id=~Marzyeh_Ghassemi2)
  - **Affiliations:** Massachusetts Institute of Technology; University of Toronto; Vector Institute, University of Toronto; Vector Institute, Massachusetts Institute of Technology
  - **TL;DR:** This study introduces IF-COMP, an efficient approximation of the predictive normalized maximum likelihood (pNML) distribution, aimed at improving uncertainty estimation in machine learning models. The method demonstrates strong performance in uncertainty calibration, mislabel detection, and out-of-distribution detection tasks, outperforming existing baseline methods.
  - **Keywords:** uncertainty estimation, model reliability, calibration, predictive normalized maximum likelihood (pNML), Boltzmann influence function, minimum description length (MDL), healthcare, medical imaging, self-driving cars, distribution shifts, model overfitting, uncertainty in predictions, IF-COMP (approximation of pNML), well-calibrated predictions, complexity measurement


- [A Neural-Guided Dynamic Symbolic Network for Exploring Mathematical Expressions from Data](https://icml.cc/virtual/2024/poster/34421) (Poster)
  - **Authors:** [Wenqiang Li](http://openreview.net/profile?id=~Wenqiang_Li2), [Weijun Li](http://openreview.net/profile?id=~Weijun_Li1), [Lina Yu](http://openreview.net/profile?id=~Lina_Yu1), [Min Wu](http://openreview.net/profile?id=~Min_Wu5), [Linjun Sun](http://openreview.net/profile?id=~Linjun_Sun1), [Jingyi Liu](http://openreview.net/profile?id=~Jingyi_Liu2), [Yanjie Li](http://openreview.net/profile?id=~Yanjie_Li4), [Shu Wei](http://openreview.net/profile?id=~Shu_Wei1), [Deng Yusong](http://openreview.net/profile?id=~Deng_Yusong1), [Meilan Hao](http://openreview.net/profile?id=~Meilan_Hao1)
  - **Affiliations:** AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China; Center of Materials Science and Optoelectronics Engineering, University of Chinese Academy of Sciences, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China, AnnLab, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering & School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Beijing Key Laboratory of Semiconductor Neural Network Intelligent Sensing and Computing Technology, Beijing, China
  - **TL;DR:** This study introduces DYSYMNET, a neural-guided Dynamic Symbolic Network designed to improve symbolic regression by optimizing symbolic networks through reinforcement learning. The proposed method demonstrates superior performance on various benchmarks compared to existing models, addressing challenges in high-dimensional data and large search spaces.
  - **Keywords:** Symbolic regression, mathematical expressions, deep learning, Neural-guided Dynamic Symbolic Network, reinforcement learning, Data analysis, mathematical modeling, High-dimensional problems, large search space, learning constants, DYSYMNET, optimization of symbolic networks, SRBench, public standard benchmarks


- [Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once](https://icml.cc/virtual/2024/poster/34140) (Poster)
  - **Authors:** [Zhangheng Li](http://openreview.net/profile?id=~Zhangheng_LI2), [Shiwei Liu](http://openreview.net/profile?id=~Shiwei_Liu2), [Tianlong Chen](http://openreview.net/profile?id=~Tianlong_Chen1), [Ajay Jaiswal](http://openreview.net/profile?id=~AJAY_KUMAR_JAISWAL1), [Zhenyu Zhang](http://openreview.net/profile?id=~Zhenyu_Zhang4), [Dilin Wang](http://openreview.net/profile?id=~Dilin_Wang1), [Raghuraman Krishnamoorthi](http://openreview.net/profile?id=~Raghuraman_Krishnamoorthi1), [Shiyu Chang](http://openreview.net/profile?id=~Shiyu_Chang2), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1)
  - **Affiliations:** University of Texas at Austin, Eindhoven University of Technology; University of Oxford, University of North Carolina at Chapel Hill, University of Texas at Austin, University of Texas at Austin, Meta, Meta, University of California, Santa Barbara, University of Texas at Austin
  - **TL;DR:** This study introduces Sparse Cocktail, a novel framework for co-training multiple sparse subnetworks with diverse sparsity patterns and ratios, enhancing flexibility and efficiency in resource-constrained environments. Experimental results demonstrate its effectiveness in various applications, including image classification and object detection.
  - **Keywords:** Sparse Neural Networks, Sparse Co-training, Image Classification, Object Detection, Instance Segmentation, Computational Costs, Memory Footprints, Resource Variability, Sparse Cocktail Framework, Unified Mask Generation, Dense Pivot Co-training, Sparsity Patterns, Sparsity Ratios


- [Concentration Inequalities for General Functions of Heavy-Tailed Random Variables](https://icml.cc/virtual/2024/poster/34389) (Spotlight Poster)
  - **Authors:** [Shaojie Li](http://openreview.net/profile?id=~Shaojie_Li2), [Yong Liu](http://openreview.net/profile?id=~Yong_Liu7)
  - **Affiliations:** Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China
  - **TL;DR:** This paper presents unbounded analogues of the bounded difference inequality for functions of independent random variables with heavy-tailed distributions, providing a framework applicable to all heavy-tailed distributions with finite variance. The results extend existing concentration inequalities to address challenges in statistical learning theory related to heavy-tailed distributions.
  - **Keywords:** Concentration inequalities, heavy-tailed distributions, statistical learning theory, Bounded difference inequality, unbounded analogues, Machine learning, high-dimensional statistics, Generalization bounds, unbounded situations, conditional ranges, Extension of existing results to heavy-tailed distributions


- [Image Clustering with External Guidance](https://icml.cc/virtual/2024/poster/34375) (Oral)
  - **Authors:** [Yunfan Li](http://openreview.net/profile?id=~Yunfan_Li1), [Peng Hu](http://openreview.net/profile?id=~Peng_Hu2), [Dezhong Peng](http://openreview.net/profile?id=~Dezhong_Peng1), [Jiancheng Lv](http://openreview.net/profile?id=~Jiancheng_Lv2), [Jianping Fan](http://openreview.net/profile?id=~Jianping_Fan4), [Xi Peng](http://openreview.net/profile?id=~Xi_Peng3)
  - **Affiliations:** School of Computer Science, Sichuan University, Chengdu, China, School of Computer Science, Sichuan University, Chengdu, China, School of Computer Science, Sichuan University, Chengdu, China, School of Computer Science, Sichuan University, Chengdu, China, AI Lab at Lenovo Research, Beijing, China, School of Computer Science, Sichuan University, Chengdu, China; AI Lab at Lenovo Research, Beijing, China
  - **TL;DR:** This study introduces a novel externally guided clustering method called Text-Aided Clustering (TAC) that utilizes external knowledge from WordNet to enhance image clustering performance. The proposed method achieves state-of-the-art results on multiple image clustering benchmarks, demonstrating the effectiveness of leveraging textual semantics in clustering tasks.
  - **Keywords:** image clustering, external guidance, Text-Aided Clustering (TAC), WordNet, image clustering, data compactness, high-dimensional data, state-of-the-art performance, cross-modal neighborhood information, ImageNet-1K


- [Configurable Mirror Descent: Towards a Unification of Decision Making](https://icml.cc/virtual/2024/poster/33954) (Poster)
  - **Authors:** [Pengdeng Li](http://openreview.net/profile?id=~Pengdeng_Li1), [Shuxin Li](http://openreview.net/profile?id=~Shuxin_Li1), [Chang Yang](http://openreview.net/profile?id=~Chang_Yang3), [Xinrun Wang](http://openreview.net/profile?id=~Xinrun_Wang1), [Shuyue Hu](http://openreview.net/profile?id=~Shuyue_Hu1), [Xiao Huang](http://openreview.net/profile?id=~Xiao_Huang1), [Hau Chan](http://openreview.net/profile?id=~Hau_Chan1), [Bo An](http://openreview.net/profile?id=~Bo_An2)
  - **Affiliations:** Nanyang Technological University, Nanyang Technological University, The Hong Kong Polytechnic University, Singapore Management University; work done while at NTU, Shanghai Artificial Intelligence Laboratory, The Hong Kong Polytechnic University, University of Nebraska-Lincoln, Nanyang Technological University; Skywork AI
  - **TL;DR:** This paper proposes a unified algorithm, Configurable Mirror Descent (CMD), to address various decision-making problems across single-agent and multi-agent categories. The authors demonstrate that CMD outperforms existing methods while providing a comprehensive benchmark, GAMEBENCH, for evaluating decision-making strategies.
  - **Keywords:** Decision-making, Multi-agent systems, Generalized mirror descent (GMD), Configurable mirror descent (CMD), Game theory, Reinforcement learning, Generalization across decision-making categories, Different agent relationships, GAMEBENCH, Empirical performance comparison, GAMEBENCH, Bregman divergences, Policy optimization


- [Learning Shadow Variable Representation for Treatment Effect Estimation under Collider Bias](https://icml.cc/virtual/2024/poster/32661) (Poster)
  - **Authors:** [Baohong Li](http://openreview.net/profile?id=~Baohong_Li1), [Haoxuan Li](http://openreview.net/profile?id=~Haoxuan_Li6), [Ruoxuan Xiong](http://openreview.net/profile?id=~Ruoxuan_Xiong1), [Anpeng Wu](http://openreview.net/profile?id=~Anpeng_Wu1), [Fei Wu](http://openreview.net/profile?id=~Fei_Wu1), [Kun Kuang](http://openreview.net/profile?id=~Kun_Kuang1)
  - **Affiliations:** College of Computer Science and Technology, Zhejiang University, Hangzhou, China, Center for Data Science, Peking University, Beijing, China, Department of Quantitative Theory & Methods, Emory University, Atlanta, USA, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China
  - **TL;DR:** This paper addresses the challenge of estimating treatment effects under collider bias by proposing a novel method for automatically learning shadow-variable representations from observational data. The results demonstrate that the proposed methods outperform existing treatment effect estimation techniques in the presence of collider bias.
  - **Keywords:** Treatment effect estimation, Collider bias, Causal inference, Shadow variable representation learning, Hypothesis testing, Observational data analysis, Collider bias, Sample selection bias, Novel treatment effect estimator, Improved estimation methods


- [Promises and Pitfalls of Generative Masked Language Modeling: Theoretical Framework and Practical Guidelines](https://icml.cc/virtual/2024/poster/34721) (Poster)
  - **Authors:** [Yuchen Li](http://openreview.net/profile?id=~Yuchen_Li5), [Alexandre Kirchmeyer](http://openreview.net/profile?id=~Alexandre_Kirchmeyer1), [Aashay Mehta](http://openreview.net/profile?id=~Aashay_Mehta1), [Yilong Qin](http://openreview.net/profile?id=~Yilong_Qin1), [Boris Dadachev](http://openreview.net/profile?id=~Boris_Dadachev1), [Kishore Papineni](http://openreview.net/profile?id=~Kishore_A_Papineni1), [Sanjiv Kumar](http://openreview.net/profile?id=~Sanjiv_Kumar1), [Andrej Risteski](http://openreview.net/profile?id=~Andrej_Risteski2)
  - **Affiliations:** Carnegie Mellon University; Google Research, Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, Google Research, Google Research, Google Research, Carnegie Mellon University
  - **TL;DR:** This paper explores Generative Masked Language Models (GMLMs) as a non-autoregressive approach to text generation, addressing the limitations of autoregressive models. The authors present a mathematical framework and empirical results demonstrating a 2-3x speedup in machine translation with minimal quality sacrifice compared to traditional methods.
  - **Keywords:** Generative Masked Language Models, Non-autoregressive text generation, Markov Chain, Gibbs sampler, Parallel decoding, Transformer, Machine translation, Sequential generation limitations, High latency in autoregressive models, Speed-quality trade-off, Iteratively-refined parallel decoding, Sample complexity analysis, T5 model


- [Privacy Preserving Adaptive Experiment Design](https://icml.cc/virtual/2024/poster/35150) (Oral)
  - **Authors:** [Jiachun Li](http://openreview.net/profile?id=~Jiachun_Li1), [Kaining Shi](http://openreview.net/profile?id=~Kaining_Shi1), [David Simchi-Levi](http://openreview.net/profile?id=~David_Simchi-Levi2)
  - **Affiliations:** Laboratory for Information and Decision Systems, MIT, Cambridge, U.S., Department of Statistics, The University of Chicago, Chicago, U.S., Laboratory for Information and Decision Systems, MIT, Cambridge, U.S.
  - **TL;DR:** This paper investigates the tradeoff between social welfare loss and statistical power in estimating conditional average treatment effects (CATE) within a contextual bandit framework, proposing differentially private algorithms that maintain estimation accuracy. The findings highlight the importance of adaptive treatment allocation in clinical trials, particularly for diverse patient profiles, while ensuring robust privacy measures.
  - **Keywords:** Adaptive experiment, Conditional average treatment effect (CATE), Contextual bandit framework, Differentially private algorithms, Pareto optimality, Multi-objective optimization, Clinical trials, Pharmaceutical interventions, Tradeoff between social welfare and statistical power, Regret minimization, Heterogeneous treatment effects, Matched upper and lower bounds for optimization, Asymptotic normality of the estimator, Privacy protection measures


- [Learning Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking](https://icml.cc/virtual/2024/poster/33491) (Poster)
  - **Authors:** [Yongxin Li](http://openreview.net/profile?id=~Yongxin_Li1), [Mengyuan Liu](http://openreview.net/profile?id=~Mengyuan_Liu3), [You Wu](http://openreview.net/profile?id=~You_Wu8), [Xucheng Wang](http://openreview.net/profile?id=~Xucheng_Wang1), [Xiangyang Yang](http://openreview.net/profile?id=~Xiangyang_Yang1), [Shuiwang Li](http://openreview.net/profile?id=~Shuiwang_Li1)
  - **Affiliations:** College of Computer Science and Engineering, Guilin University of Technology, Guilin, China, College of Computer Science and Engineering, Guilin University of Technology, Guilin, China, College of Computer Science and Engineering, Guilin University of Technology, Guilin, China, College of Computer Science and Engineering, Guilin University of Technology, Guilin, China, College of Computer Science and Engineering, Guilin University of Technology, Guilin, China, College of Computer Science and Engineering, Guilin University of Technology, Guilin, China
  - **TL;DR:** This study presents AVTrack, an adaptive computation framework that enhances real-time UAV tracking by selectively activating transformer blocks and learning view-invariant representations. Extensive experiments demonstrate its effectiveness and efficiency, achieving state-of-the-art performance in visual tracking.
  - **Keywords:** UAV tracking, visual tracking, transformer-based models, Vision Transformer (ViT), Activation Module (AM), Unmanned Aerial Vehicles (UAVs), real-time tracking, Extreme view angle changes, motion blur, severe occlusion, computational efficiency, AVTrack framework, view-invariant representations, mutual information maximization, VisDrone2018


- [DiffFPR: Diffusion Prior for Oversampled Fourier Phase Retrieval](https://icml.cc/virtual/2024/poster/34889) (Poster)
  - **Authors:** [Ji Li](http://openreview.net/profile?id=~Ji_Li3), [Chao Wang](http://openreview.net/profile?id=~Chao_Wang35)
  - **Affiliations:** Academy for Multidisciplinary Studies, Capital Normal University, Beijing, China, University of Kansas Medical Center, Kansas City, US
  - **TL;DR:** This study presents DiffFPR, an algorithm that integrates a diffusion model with traditional iterative methods to effectively address the challenges of Fourier phase retrieval, particularly for multi-channel color images. The approach mitigates issues related to solution ambiguity and misalignment, leading to improved image reconstruction outcomes.
  - **Keywords:** Fourier phase retrieval, image reconstruction, Relaxed Averaged Alternating Reflections (RAAR), diffusion model, Coherent diffraction imaging (CDI), optics, astronomy, Trivial solution ambiguity, relative uniqueness of solutions, orientation ambiguity in multi-channel images, DiffFPR algorithm, effective solution to oversampled Fourier phase retrieval


- [Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental Learning](https://icml.cc/virtual/2024/poster/33511) (Poster)
  - **Authors:** [Depeng Li](http://openreview.net/profile?id=~Depeng_Li1), [Tianqi Wang](http://openreview.net/profile?id=~Tianqi_Wang4), [Junwei Chen](http://openreview.net/profile?id=~Junwei_Chen1), [Wei Dai](http://openreview.net/profile?id=~Wei_Dai14), [Zhigang Zeng](http://openreview.net/profile?id=~Zhigang_Zeng1)
  - **Affiliations:** School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China, School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, China
  - **TL;DR:** This paper introduces AutoActivator, a novel connectionist model designed for class-incremental learning that adapts neural unit dynamics to effectively learn new classes while minimizing forgetting of old ones. The model demonstrates strong performance in rehearsal-free and minimal-expansion settings, addressing the challenges of catastrophic forgetting in non-stationary data environments.
  - **Keywords:** Class-Incremental Learning (CIL), Neural Networks, Neural Unit Dynamics, Network Expansion, Catastrophic Forgetting, Non-Stationary Data Streams, AutoActivator Model, Convergence Property Analysis


- [OODRobustBench: a Benchmark and Large-Scale Analysis of Adversarial Robustness under Distribution Shift](https://icml.cc/virtual/2024/poster/33270) (Poster)
  - **Authors:** [Lin Li](http://openreview.net/profile?id=~Lin_Li12), [Yifei Wang](http://openreview.net/profile?id=~Yifei_Wang1), [Chawin Sitawarin](http://openreview.net/profile?id=~Chawin_Sitawarin1), [Michael Spratling](http://openreview.net/profile?id=~Michael_W._Spratling1)
  - **Affiliations:** Department of Informatics, King’s College London, UK, MIT CSAIL, USA, UC Berkeley, USA, University of Luxembourg, Luxembourg
  - **TL;DR:** This study introduces OODRobustBench, a benchmark for evaluating adversarial robustness under distribution shifts, revealing significant OOD generalization issues in current models. The findings indicate a strong correlation between in-distribution and out-of-distribution robustness, suggesting that novel methods are necessary to enhance OOD robustness.
  - **Keywords:** Adversarial Robustness, Out-of-Distribution Testing, OODRobustBench, Adversarial Evaluations, Machine Learning, Adversarial Machine Learning, OOD Generalization Issue, ID Robustness Correlation, Benchmark for OOD Adversarial Robustness, Novel Methods for OOD Robustness, Dataset Shift, Threat Shift


- [Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach](https://icml.cc/virtual/2024/poster/34960) (Poster)
  - **Authors:** [Zhihao Li](http://openreview.net/profile?id=~Zhihao_Li14), [Yufei Wang](http://openreview.net/profile?id=~Yufei_Wang5), [Alex Kot](http://openreview.net/profile?id=~Alex_Kot1), [Bihan Wen](http://openreview.net/profile?id=~Bihan_Wen2)
  - **Affiliations:** Department of EEE, Nanyang Technology University, Singapore, Department of EEE, Nanyang Technology University, Singapore, Department of EEE, Nanyang Technology University, Singapore, Department of EEE, Nanyang Technology University, Singapore
  - **TL;DR:** This study presents a novel self-supervised approach for compressing raw images by selectively targeting the noise-free components, significantly enhancing coding efficiency and reconstruction quality. Experimental results show improvements in compression performance, achieving a better rate-distortion balance and substantial bit savings compared to existing methods.
  - **Keywords:** raw image compression, self-supervised learning, image denoising, low-light image enhancement, noise in raw images, compression challenges, improved rate-distortion balance, bit savings, full-day dataset of raw images


- [VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling](https://icml.cc/virtual/2024/poster/34725) (Poster)
  - **Authors:** [Siyuan Li](http://openreview.net/profile?id=~Siyuan_Li6), [Zedong Wang](http://openreview.net/profile?id=~Zedong_Wang1), [Zicheng Liu](http://openreview.net/profile?id=~Zicheng_Liu2), [Di Wu](http://openreview.net/profile?id=~Di_Wu10), [Cheng Tan](http://openreview.net/profile?id=~Cheng_Tan1), [Jiangbin Zheng](http://openreview.net/profile?id=~Jiangbin_Zheng3), [Yufei Huang](http://openreview.net/profile?id=~Yufei_Huang4), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, 310024, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, 310024, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, 310024, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, 310024, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, 310024, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, 310024, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, 310024, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, 310024, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, 310058, China
  - **TL;DR:** This study introduces VQDNA, a novel framework for genomic sequence modeling that enhances genome tokenization through vector quantization and hierarchical residual quantization. The results demonstrate VQDNA's superior performance and efficiency across various genomic tasks, revealing significant insights into SARS-CoV-2 mutations.
  - **Keywords:** genome language models, genomic sequence modeling, vector quantization, hierarchical residual quantization (HRQ), genomics, biological research, limitations of hand-crafted tokenization, encoding discriminative patterns, VQDNA framework, adaptive tokenization, pattern-aware embeddings, 32 genome datasets, SARS-CoV-2 mutations


- [LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models](https://icml.cc/virtual/2024/poster/33160) (Poster)
  - **Authors:** [guangyan li](http://openreview.net/profile?id=~Guangyan_Li1), [Yongqiang Tang](http://openreview.net/profile?id=~Yongqiang_Tang1), [Wensheng Zhang](http://openreview.net/profile?id=~Wensheng_Zhang5)
  - **Affiliations:** State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences; Guangzhou University
  - **TL;DR:** This study introduces LoRAP, a novel structured compression method for large language models that combines low-rank matrix approximation and structured pruning, effectively reducing parameter scale while maintaining performance. The proposed method demonstrates superior results in zero-shot tasks compared to existing compression techniques.
  - **Keywords:** Large Language Models, Model Compression, Low-Rank Matrix Approximation, Structured Pruning, Singular Value Decomposition (SVD), Natural Language Processing (NLP), High memory and computational resource requirements, Parameter scale reduction, LoRAP method, Input activation weighted SVD, Gradient-free structured channel pruning, Multi-Head Self-Attention (MHA), Feed-Forward Network (FFN), Zero-shot perplexity, Zero-shot task classification


- [Emergent Representations of Program Semantics in Language Models Trained on Programs](https://icml.cc/virtual/2024/poster/34849) (Poster)
  - **Authors:** [Charles Jin](http://openreview.net/profile?id=~Charles_Jin1), [Martin Rinard](http://openreview.net/profile?id=~Martin_Rinard1)
  - **Affiliations:** CSAIL, MIT, Cambridge, MA, USA, CSAIL, MIT, Cambridge, MA, USA
  - **TL;DR:** This study investigates whether language models trained solely for next-token prediction can learn to represent the formal semantics of programming languages. The findings indicate that the models acquire an emergent ability to interpret programs, as evidenced by the successful extraction of intermediate states during training.
  - **Keywords:** Language Models, Program Semantics, Program Synthesis, Transformer Model, Probing Classifier, 2D Grid World Environments, Code Generation, Formal Semantics Representation, Next-Token Prediction, Emergent Ability to Interpret Programs, Phase Transition in Probing Accuracy


- [Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?](https://icml.cc/virtual/2024/poster/33516) (Poster)
  - **Authors:** [Huy Nguyen](http://openreview.net/profile?id=~Huy_Nguyen5), [Pedram Akbarian](http://openreview.net/profile?id=~Pedram_Akbarian1), [Nhat Ho](http://openreview.net/profile?id=~Nhat_Ho1)
  - **Affiliations:** Department of Statistics and Data Sciences; Department of Electrical and Computer Engineering, The University of Texas at Austin, Department of Electrical and Computer Engineering, The University of Texas at Austin, Department of Statistics and Data Sciences, The University of Texas at Austin
  - **TL;DR:** This study investigates the effects of a dense-to-sparse gating mechanism in Gaussian Mixture of Experts models, revealing that traditional parameter estimation rates are slow due to temperature interactions. The authors propose a novel activation gate that significantly improves these rates to polynomial levels, supported by empirical validation.
  - **Keywords:** Mixture of Experts (MoE), Dense-to-Sparse Gating, Softmax, Maximum Likelihood Estimation, Partial Differential Equations, Large Language Models, Computer Vision, Multi-task Learning, Speech Recognition, Expert Selection Instability, Parameter Estimation Rates, Novel Activation Dense-to-Sparse Gate, Improved Parameter Estimation Rates


- [PID: Prompt-Independent Data Protection Against Latent Diffusion Models](https://icml.cc/virtual/2024/poster/35154) (Poster)
  - **Authors:** [Ang Li](http://openreview.net/profile?id=~Ang_Li20), [Yichuan Mo](http://openreview.net/profile?id=~Yichuan_Mo1), [Mingjie Li](http://openreview.net/profile?id=~Mingjie_Li1), [Yisen Wang](http://openreview.net/profile?id=~Yisen_Wang1)
  - **Affiliations:** School of EECS, Peking University, China, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, China, CISPA Helmholtz Center for Information Security, Germany, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, China; Institute for Artificial Intelligence, Peking University, China
  - **TL;DR:** This study addresses the privacy concerns associated with few-shot fine-tuning of Latent Diffusion Models (LDMs) and proposes a new method called Prompt-Independent Defense (PID) to enhance data protection against unauthorized image exploitation. The findings indicate that PID effectively safeguards privacy while requiring less computational power compared to existing methods.
  - **Keywords:** Data protection, Privacy concerns, Latent Diffusion Models, Few-shot fine-tuning, Prompt-Independent Defense (PID), Image synthesis, Generative models, Civil privacy, Misuse of personal images, Defense against generative models, New defense method (PID), Insights into visual encoder manipulation, Latent Diffusion Models (LDMs), Adversarial attacks


- [DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching](https://icml.cc/virtual/2024/poster/33032) (Poster)
  - **Authors:** [Guanghe Li](http://openreview.net/profile?id=~Guanghe_Li1), [Yixiang Shan](http://openreview.net/profile?id=~Yixiang_Shan1), [Zhengbang Zhu](http://openreview.net/profile?id=~Zhengbang_Zhu1), [Ting Long](http://openreview.net/profile?id=~Ting_Long1), [Weinan Zhang](http://openreview.net/profile?id=~Weinan_Zhang1)
  - **Affiliations:** Jilin University, Jilin University, Shanghai Jiao Tong University, Jilin University, Shanghai Jiao Tong University
  - **TL;DR:** This study introduces Diffusion-based Trajectory Stitching (DiffStitch) to enhance offline reinforcement learning by generating optimal trajectories that connect low-reward and high-reward paths. Empirical results show significant performance improvements across various RL methodologies, particularly in one-step and imitation learning methods.
  - **Keywords:** Offline Reinforcement Learning, Data Augmentation, Diffusion-based Trajectory Stitching, IQL, TD3+BC, DT, Autonomous Driving, Health Care, Commercial Recommendation, Limited Optimal Trajectories, Transition to High-Reward Regions, Enhanced Performance of Offline RL Algorithms, D4RL


- [Combining Experimental and Historical Data for Policy Evaluation](https://icml.cc/virtual/2024/poster/33135) (Poster)
  - **Authors:** [Ting Li](http://openreview.net/profile?id=~Ting_Li7), [Chengchun Shi](http://openreview.net/profile?id=~Chengchun_Shi1), [Qianglin Wen](http://openreview.net/profile?id=~Qianglin_Wen1), [Yang Sui](http://openreview.net/profile?id=~Yang_Sui2), [Yongli Qin](http://openreview.net/profile?id=~Yongli_Qin1), [Chunbo Lai](http://openreview.net/profile?id=~Chunbo_Lai1), [Hongtu Zhu](http://openreview.net/profile?id=~Hongtu_Zhu3)
  - **Affiliations:** School of Statistics and Management, Shanghai University of Finance and Economics, Department of Statistics, London School of Economics and Political Science, Yunnan Key Laboratory of Statistical Modeling and Data Analysis, Yunnan University, School of Statistics and Management, Shanghai University of Finance and Economics, Didi Chuxing, Didi Chuxing, Department of Biostatistics, The University of North Carolina at Chapel Hill
  - **TL;DR:** This paper presents novel methods for evaluating policy effectiveness by integrating experimental and historical data, addressing challenges like limited sample sizes and distributional shifts. The proposed estimators demonstrate superior performance in ridesharing applications, with established theoretical properties for robustness and efficiency.
  - **Keywords:** Policy evaluation, Data integration, Mean square error (MSE) optimization, Pessimistic principle, Ridesharing, Healthcare, Limited sample size, Distributional shifts, Temporal non-stationarity, Robust estimators, Oracle properties, Efficiency properties, Experimental datasets, Historical datasets, A/B testing, Randomized controlled trial (RCT), Causal learning


- [A Contextual Combinatorial Bandit Approach to Negotiation](https://icml.cc/virtual/2024/poster/34817) (Poster)
  - **Authors:** [Yexin Li](http://openreview.net/profile?id=~Yexin_Li1), [Zhancun Mu](http://openreview.net/profile?id=~Zhancun_Mu1), [Siyuan Qi](http://openreview.net/profile?id=~Siyuan_Qi1)
  - **Affiliations:** State Key Laboratory of General Artificial Intelligence, BIGAI, Beijing, China, Peking University, State Key Laboratory of General Artificial Intelligence, BIGAI, Beijing, China; Peking University
  - **TL;DR:** This paper presents a novel approach to negotiation using contextual combinatorial multi-armed bandits to address the exploration-exploitation dilemma and large action spaces. The proposed method, NegUCB, effectively handles partial observations and complex reward functions, demonstrating superior performance in negotiation tasks.
  - **Keywords:** negotiation strategies, exploration-exploitation dilemma, contextual combinatorial multi-armed bandits, NegUCB, trading, resource allocation, multi-issue negotiation, exploration-exploitation dilemma, large action spaces, partial observations, complicated acceptance functions, sub-linear regret upper bound, novel negotiation method


- [The Good, The Bad, and Why: Unveiling Emotions in Generative AI](https://icml.cc/virtual/2024/poster/32738) (Poster)
  - **Authors:** [CHENG LI](http://openreview.net/profile?id=~CHENG_LI26), [Jindong Wang](http://openreview.net/profile?id=~Jindong_Wang1), [Yixuan Zhang](http://openreview.net/profile?id=~Yixuan_Zhang7), [Kaijie Zhu](http://openreview.net/profile?id=~Kaijie_Zhu1), [Xinyi Wang](http://openreview.net/profile?email=wxymmhww86%40163.com), [Wenxin Hou](http://openreview.net/profile?id=~Wenxin_Hou1), [Jianxun Lian](http://openreview.net/profile?id=~Jianxun_Lian1), [Fang Luo](http://openreview.net/profile?email=luof%40bnu.edu.cn), [Qiang Yang](http://openreview.net/profile?id=~Qiang_Yang1), [Xing Xie](http://openreview.net/profile?id=~Xing_Xie3)
  - **Affiliations:** Microsoft Research, Beijing, China; CAS, Institute of Software, Beijing, China, Microsoft Research, Beijing, China, Department of Computer Science, William & Mary, Williamsburg, Virginia, America, Microsoft Research, Beijing, China, School of Psychology, Beijing Normal University, Beijing, China, Microsoft Research, Beijing, China, Microsoft Research, Beijing, China, School of Psychology, Beijing Normal University, Beijing, China, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China, Microsoft Research, Beijing, China
  - **TL;DR:** This paper investigates the emotional comprehension of generative AI models by introducing three approaches—EmotionPrompt, EmotionAttack, and EmotionDecode—grounded in psychological theories. The findings suggest that emotional stimuli can enhance or impair AI performance, revealing that AI models can understand emotions similarly to human brain mechanisms, which is crucial for ethical AI integration.
  - **Keywords:** Emotions in AI, Generative AI, EmotionPrompt, EmotionAttack, EmotionDecode, Human-AI collaboration, AI ethics, Understanding emotional comprehension in AI, Lack of empathy in AI systems, Enhanced performance of AI models, Emotional comprehension similar to human dopamine mechanisms, Large language models, Multi-modal models, Psychological theories, Emotional stimuli


- [Improved Bounds for Pure Private Agnostic Learning: Item-Level and User-Level Privacy](https://icml.cc/virtual/2024/poster/33904) (Poster)
  - **Authors:** [Bo Li](http://openreview.net/profile?id=~Bo_Li33), [Wei Wang](http://openreview.net/profile?id=~Wei_Wang50), [Peng Ye](http://openreview.net/profile?id=~Peng_Ye5)
  - **Affiliations:** Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China
  - **TL;DR:** This study investigates pure private learning within the agnostic model, focusing on item-level and user-level privacy, and derives improved upper bounds for sample complexity. The findings highlight the necessity of privacy-preserving algorithms in machine learning, particularly in scenarios involving sensitive data.
  - **Keywords:** Private Learning, Agnostic Learning, Differential Privacy, Sample Complexity, Privacy Protection, Improved Upper Bounds, Learning Thresholds, PAC Learning, Realizable Learning, User-Level Privacy, Item-Level Privacy


- [$\bf{\Phi}_\textrm{Flow}$: Differentiable Simulations for PyTorch, TensorFlow and Jax](https://icml.cc/virtual/2024/poster/35005) (Poster)
  - **Authors:** [Philipp Holl](http://openreview.net/profile?id=~Philipp_Holl1), [Nils Thuerey](http://openreview.net/profile?id=~Nils_Thuerey1)
  - **Affiliations:** School of Computation, Information and Technology, Technical University of Munich, Germany, School of Computation, Information and Technology, Technical University of Munich, Germany
  - **TL;DR:** The paper presents ΦFlow, a Python toolkit designed to facilitate differentiable simulations in machine learning by integrating seamlessly with popular libraries like PyTorch, TensorFlow, and Jax. It offers essential features that enhance simulation code development, making it easier to apply machine learning techniques to scientific and engineering problems.
  - **Keywords:** differentiable simulations, machine learning, physics simulations, automatic differentiation, reverse-mode differentiation, adjoint method, scientific computing, engineering simulations, fluid simulations, integration of ML with physics simulations, differentiability in simulations, ΦFlow toolkit, features for simulation code simplification, PyTorch, TensorFlow, Jax, NumPy, SciPy, PDE (Partial Differential Equations), ODE (Ordinary Differential Equations)


- [Statistical Properties of Robust Satisficing](https://icml.cc/virtual/2024/poster/33338) (Poster)
  - **Authors:** [zhiyi li](http://openreview.net/profile?id=~zhiyi_li3), [Yunbei Xu](http://openreview.net/profile?id=~Yunbei_Xu1), [Ruohan Zhan](http://openreview.net/profile?id=~Ruohan_Zhan1)
  - **Affiliations:** School of Mathematical Sciences, Peking University, Beijing, China, Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA, Department of Industrial Engineering and Decision Analytics, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong SAR; HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute, Futian, Shenzhen, China
  - **TL;DR:** This paper analyzes the statistical properties of the Robust Satisficing (RS) model, demonstrating its advantages over Distributionally Robust Optimization (DRO) in providing statistical guarantees and generalization error bounds. The RS model shows improved performance in small-sample regimes and under distribution shifts, with lower sensitivity to hyperparameter tuning.
  - **Keywords:** Robust optimization, Robust Satisficing (RS) model, Distributionally Robust Optimization (DRO), statistical guarantees, minimax optimization, Machine learning, energy systems, supply chains, Distribution shifts, limited sample sizes, performance under uncertainty, Two-sided confidence intervals, finite-sample generalization error bounds


- [FlashST: A Simple and Universal Prompt-Tuning Framework for Traffic Prediction](https://icml.cc/virtual/2024/poster/32765) (Poster)
  - **Authors:** [Zhonghang Li](http://openreview.net/profile?id=~Zhonghang_Li1), [Lianghao Xia](http://openreview.net/profile?id=~Lianghao_Xia1), [Yong Xu](http://openreview.net/profile?id=~Yong_Xu2), [Chao Huang](http://openreview.net/profile?id=~Chao_Huang7)
  - **Affiliations:** South China University of Technology; University of Hong Kong, University of Hong Kong, South China University of Technology; PAZHOU LAB, University of Hong Kong
  - **TL;DR:** This paper presents FlashST, a spatio-temporal prompt-tuning framework designed to improve traffic prediction by adapting pre-trained models to diverse datasets and addressing distribution shifts. Empirical evaluations show that FlashST enhances generalization across various urban traffic prediction tasks.
  - **Keywords:** Traffic prediction, Urban computing, Spatio-temporal prompt-tuning, In-context learning, Smart city applications, Traffic management, Distribution shift, Generalization challenges, FlashST framework, Knowledge transfer mechanisms, Diverse urban datasets, Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs)


- [GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model](https://icml.cc/virtual/2024/poster/33861) (Poster)
  - **Authors:** [Ling Li](http://openreview.net/profile?id=~Ling_Li12), [Yu Ye](http://openreview.net/profile?id=~Yu_Ye2), [Bingchuan Jiang](http://openreview.net/profile?id=~Bingchuan_Jiang1), [Wei Zeng](http://openreview.net/profile?id=~Wei_Zeng7)
  - **Affiliations:** The Hong Kong University of Science and Technology (Guangzhou), Tongji University, Strategic Support Force Information Engineering University; The Hong Kong University of Science and Technology, The Hong Kong University of Science and Technology
  - **TL;DR:** This study presents GeoReasoner, a novel approach for geo-localization using a large vision-language model enhanced with human inference knowledge. The method significantly improves locatability and reasoning capabilities, outperforming existing models in geo-localization tasks.
  - **Keywords:** geo-localization, reasoning capability, large vision-language model (LVLM), CLIP-based network, street-view images, urban planning, navigation, data scarcity, low-quality images, lack of reasoning inference, GeoReasoner, new dataset of locatable street views, fine-tuning methods


- [FedBAT: Communication-Efficient Federated Learning via Learnable Binarization](https://icml.cc/virtual/2024/poster/32728) (Poster)
  - **Authors:** [Shiwei Li](http://openreview.net/profile?id=~Shiwei_Li3), [Wenchao Xu](http://openreview.net/profile?id=~Wenchao_Xu1), [Haozhao Wang](http://openreview.net/profile?id=~Haozhao_Wang1), [Xing Tang](http://openreview.net/profile?id=~Xing_Tang2), [Yining Qi](http://openreview.net/profile?id=~Yining_Qi1), [Shijie Xu](http://openreview.net/profile?id=~Shijie_Xu1), [weihongluo](http://openreview.net/profile?id=~weihongluo1), [Yuhua Li](http://openreview.net/profile?id=~Yuhua_Li2), [xiuqiang He](http://openreview.net/profile?id=~xiuqiang_He3), [Ruixuan Li](http://openreview.net/profile?id=~Ruixuan_Li1)
  - **Affiliations:** Huazhong University of Science and Technology, Wuhan, China, The Hong Kong Polytechnic University, Hong Kong, China, Huazhong University of Science and Technology, Wuhan, China, FiT, Tencent, Shenzhen, China, Huazhong University of Science and Technology, Wuhan, China, FiT, Tencent, Shenzhen, China, FiT, Tencent, Shenzhen, China, Huazhong University of Science and Technology, Wuhan, China, FiT, Tencent, Shenzhen, China, Huazhong University of Science and Technology, Wuhan, China
  - **TL;DR:** The study introduces Federated Binarization-Aware Training (FedBAT), a novel framework that learns binary model updates during local training to reduce approximation errors and improve model accuracy in federated learning. Experimental results demonstrate that FedBAT accelerates convergence and outperforms baseline methods, including FedAvg, by up to 9%.
  - **Keywords:** Federated Learning, Privacy Preservation, Binarization, SignSGD, Federated Binarization-Aware Training (FedBAT), Distributed Machine Learning, Communication Overhead, Approximation Errors, Model Accuracy, Convergence Acceleration, Accuracy Improvement


- [Towards efficient deep spiking neural networks construction with spiking activity based pruning](https://icml.cc/virtual/2024/poster/33505) (Poster)
  - **Authors:** [Yaxin Li](http://openreview.net/profile?id=~Yaxin_Li4), [Qi Xu](http://openreview.net/profile?id=~Qi_Xu1), [Jiangrong Shen](http://openreview.net/profile?id=~Jiangrong_Shen1), [Hongming Xu](http://openreview.net/profile?id=~Hongming_Xu3), [Long Chen](http://openreview.net/profile?id=~Long_Chen18), [Gang Pan](http://openreview.net/profile?id=~Gang_Pan1)
  - **Affiliations:** School of Computer Science and Technology, Dalian University of Technology, Dalian, China; Guangdong Laboratory of Artificial Intelligence and Digital Economy, Shenzhen, China, School of Computer Science and Technology, Dalian University of Technology, Dalian, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, School of Computer Science and Technology, Dalian University of Technology, Dalian, China, Centre for Systems Engineering and Innovation, Imperial College London, London, UK, College of Computer Science and Technology, Zhejiang University, Hangzhou, China
  - **TL;DR:** This study proposes a Spiking Channel Activity-based (SCA) network pruning framework for Spiking Neural Networks (SNNs) that dynamically adjusts the network structure to enhance performance and reduce computational load. The approach leverages synaptic plasticity mechanisms to improve adaptation to target tasks while maintaining energy efficiency.
  - **Keywords:** Spiking Neural Networks (SNNs), model compression, energy efficiency, Spiking Channel Activity-based (SCA) network pruning, structured pruning, synaptic plasticity, Low-power computing, neuromorphic computing, Redundant structural units in SNNs, adaptation to target tasks, Dynamic adjustment of network structure, reduction of computational load, acceleration of inference process


- [Two-Stage Shadow Inclusion Estimation: An IV Approach for Causal Inference under Latent Confounding and Collider Bias](https://icml.cc/virtual/2024/poster/33773) (Poster)
  - **Authors:** [Baohong Li](http://openreview.net/profile?id=~Baohong_Li1), [Anpeng Wu](http://openreview.net/profile?id=~Anpeng_Wu1), [Ruoxuan Xiong](http://openreview.net/profile?id=~Ruoxuan_Xiong1), [Kun Kuang](http://openreview.net/profile?id=~Kun_Kuang1)
  - **Affiliations:** College of Computer Science and Technology, Zhejiang University, Hangzhou, China, College of Computer Science and Technology, Zhejiang University, Hangzhou, China, Department of Quantitative Theory & Methods, Emory University, Atlanta, USA, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Department of Quantitative Theory & Methods, Emory University, Atlanta, USA
  - **TL;DR:** This paper presents a novel Two-Stage Shadow Inclusion (2SSI) approach to address both latent confounding bias and collider bias in causal inference from observational data. The proposed method demonstrates significant performance improvements over existing techniques when both biases are present.
  - **Keywords:** Causal inference, observational studies, Instrumental Variable (IV) approach, Two-Stage Shadow Inclusion (2SSI), Latent confounding bias, collider bias, Performance improvement in causal inference methods, Benchmark synthetic datasets, real-world dataset, Shadow variables, unmeasured covariates


- [Diving into Underwater: Segment Anything Model Guided Underwater Salient Instance Segmentation and A Large-scale Dataset](https://icml.cc/virtual/2024/poster/32896) (Poster)
  - **Authors:** [Shijie Lian](http://openreview.net/profile?id=~Shijie_Lian1), [Ziyi Zhang](http://openreview.net/profile?id=~Ziyi_Zhang6), [Hua Li](http://openreview.net/profile?id=~Hua_Li8), [Wenjie Li](http://openreview.net/profile?id=~Wenjie_Li10), [Laurence Yang](http://openreview.net/profile?id=~Laurence_Tianruo_Yang1), [Sam Kwong](http://openreview.net/profile?id=~Sam_Kwong1), [Runmin Cong](http://openreview.net/profile?id=~Runmin_Cong1)
  - **Affiliations:** Hainan University, The Hong Kong University of Science and Technology (Guangzhou), Hainan University; Key Laboratory of New Generation Artificial Intelligence Technology & Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, Hainan University, Hainan University; Huazhong University of Science and Technology; St. Francis Xavier University, Lingnan University, School of Data Science, Hong Kong, Shandong University, School of Control Science and Engineering; Key Laboratory of Machine Intelligence and System Control, Ministry of Education, Jinan, China
  - **TL;DR:** This study introduces the USIS10K dataset for underwater salient instance segmentation and proposes the USIS-SAM architecture, which leverages the Segment Anything Model to improve segmentation accuracy in complex underwater environments. Experimental results demonstrate that USIS-SAM outperforms existing state-of-the-art methods on the USIS10K dataset.
  - **Keywords:** Underwater vision, Salient instance segmentation, Segment Anything Model (SAM), Underwater Adaptive Visual Transformer (UA-ViT), Salient Feature Prompter Generator (SFPG), Underwater exploration, Computer vision, Low segmentation accuracy, Lack of large-scale datasets with pixel-level annotations, USIS-SAM method, USIS10K dataset, USIS10K


- [Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks](https://icml.cc/virtual/2024/poster/34870) (Poster)
  - **Authors:** [Haoyu Li](http://openreview.net/profile?id=~Haoyu_Li4), [Shichang Zhang](http://openreview.net/profile?id=~Shichang_Zhang2), [Longwen Tang](http://openreview.net/profile?id=~Longwen_Tang1), [Mathieu Bauchy](http://openreview.net/profile?id=~Mathieu_Bauchy1), [Yizhou Sun](http://openreview.net/profile?id=~Yizhou_Sun1)
  - **Affiliations:** University of California, Los Angeles, CA, USA, University of California, Los Angeles, CA, USA, University of California, Los Angeles, CA, USA, University of California, Los Angeles, CA, USA, University of California, Los Angeles, CA, USA
  - **TL;DR:** This study presents a novel Symmetrized Graph Neural Network (SymGNN) model to predict energy barriers in metallic glasses, significantly reducing computation time from 41 days to under one second while improving accuracy compared to traditional methods. The findings enhance the understanding of the structure-property relationships in metallic glasses, which is crucial for advancing materials science research.
  - **Keywords:** Metallic Glasses, Structure-Property Relationship, Energy Barriers, Graph Neural Networks (GNNs), Symmetrized GNN (SymGNN), Molecular Dynamics (MD), Materials Science, Aerospace, Biomedical Devices, Understanding structure-property relationships, Energy barrier simulation challenges, New dataset for EB prediction, Improved prediction accuracy, Reduced inference time, Energy Barrier (EB), Medium-Range Order (MRO), E(3)-invariance


- [From Fourier to Neural ODEs: Flow Matching for Modeling Complex Systems](https://icml.cc/virtual/2024/poster/34598) (Poster)
  - **Authors:** [Xin Li](http://openreview.net/profile?id=~Xin_Li59), [Jingdong Zhang](http://openreview.net/profile?id=~Jingdong_Zhang1), [Qunxi Zhu](http://openreview.net/profile?id=~Qunxi_Zhu1), [Chengli Zhao](http://openreview.net/profile?id=~Chengli_Zhao1), [Xue Zhang](http://openreview.net/profile?id=~Xue_Zhang6), [Xiaojun Duan](http://openreview.net/profile?id=~Xiaojun_Duan1), [Wei Lin](http://openreview.net/profile?id=~Wei_Lin1)
  - **Affiliations:** College of Science, National University of Defense Technology, Changsha, Hunan 410073, China, School of Mathematical Sciences, LMNS, and SCMS, Fudan University, China; Research Institute of Intelligent Complex Systems, Fudan University, China, Research Institute of Intelligent Complex Systems, Fudan University, China, College of Science, National University of Defense Technology, Changsha, Hunan 410073, China; State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Institutes of Brain Science, Fudan University, China, College of Science, National University of Defense Technology, Changsha, Hunan 410073, China, College of Science, National University of Defense Technology, Changsha, Hunan 410073, China, School of Mathematical Sciences, LMNS, and SCMS, Fudan University, China; Research Institute of Intelligent Complex Systems, Fudan University, China; State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Institutes of Brain Science, Fudan University, China; Shanghai Artificial Intelligence Laboratory, China
  - **TL;DR:** This study introduces a simulation-free framework called Fourier NODEs (FNODEs) for modeling complex systems, which addresses challenges in standard neural ordinary differential equations (NODEs) by utilizing Fourier analysis for gradient estimation. The proposed method demonstrates superior performance in training time, dynamics prediction, and robustness compared to existing approaches.
  - **Keywords:** Complex systems modeling, Neural ordinary differential equations (NODEs), Fourier analysis, Flow matching, Dynamics prediction, Control systems, High computational costs, Susceptibility to local optima, Noisy observational data, Simulation-free framework (Fourier NODEs), Improved training time and robustness, Positive feedback loop in modeling


- [KernelWarehouse: Rethinking the Design of Dynamic Convolution](https://icml.cc/virtual/2024/poster/35192) (Poster)
  - **Authors:** [Chao Li](http://openreview.net/profile?id=~Chao_Li16), [Anbang Yao](http://openreview.net/profile?id=~Anbang_Yao1)
  - **Affiliations:** Intel Labs China, Intel Labs China
  - **TL;DR:** This paper introduces KernelWarehouse, a novel approach to dynamic convolution that enhances parameter efficiency while maintaining or improving model accuracy. The method demonstrates significant performance gains on various ConvNet architectures and is applicable to Vision Transformers, achieving notable improvements on standard datasets like ImageNet and MS-COCO.
  - **Keywords:** dynamic convolution, parameter efficiency, KernelWarehouse, CondConv, DY-Conv, computer vision, image recognition, parameter inefficiency, model size increase, improved model accuracy, model size reduction, ImageNet, MS-COCO, convolutional neural networks (ConvNets)


- [ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models](https://icml.cc/virtual/2024/poster/33988) (Poster)
  - **Authors:** [Ziniu Li](http://openreview.net/profile?id=~Ziniu_Li1), [Tian Xu](http://openreview.net/profile?id=~Tian_Xu2), [Yushun Zhang](http://openreview.net/profile?id=~Yushun_Zhang1), [Zhihang Lin](http://openreview.net/profile?id=~Zhihang_Lin2), [Yang Yu](http://openreview.net/profile?id=~Yang_Yu5), [Ruoyu Sun](http://openreview.net/profile?id=~Ruoyu_Sun1), [Zhi-Quan Luo](http://openreview.net/profile?id=~Zhi-Quan_Luo1)
  - **Affiliations:** School of Data Science, The Chinese University of Hong Kong, Shenzhen; Shenzhen Research Institute of Big Data, National Key Laboratory for Novel Software Technology, Nanjing University; Polixir.ai, School of Data Science, The Chinese University of Hong Kong, Shenzhen; Shenzhen Research Institute of Big Data, School of Data Science, The Chinese University of Hong Kong, Shenzhen, National Key Laboratory for Novel Software Technology, Nanjing University; Pazhou Laboratory (Huangpu), School of Data Science, The Chinese University of Hong Kong, Shenzhen; Shenzhen Research Institute of Big Data, School of Data Science, The Chinese University of Hong Kong, Shenzhen; Shenzhen Research Institute of Big Data
  - **TL;DR:** The study introduces ReMax, a simplified and efficient reinforcement learning method for aligning large language models, which addresses the computational challenges of traditional methods like PPO. ReMax demonstrates significant improvements in memory usage and training efficiency, achieving state-of-the-art results on benchmark evaluations.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Alignment of Large Language Models (LLMs), Proximal Policy Optimization (PPO), REINFORCE algorithm, variance reduction technique, Large Language Models (LLMs), Heavy computation burden, hyper-parameter tuning, ReMax method, efficiency improvements, GPU memory usage reduction


- [Receptive Fields As Experts in Convolutional Neural Architectures](https://icml.cc/virtual/2024/poster/34473) (Poster)
  - **Authors:** [Dongze Lian](http://openreview.net/profile?id=~Dongze_Lian1), [Weihao Yu](http://openreview.net/profile?id=~Weihao_Yu2), [Xinchao Wang](http://openreview.net/profile?id=~Xinchao_Wang1)
  - **Affiliations:** Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Department of Electrical and Computer Engineering, National University of Singapore, Singapore, Department of Electrical and Computer Engineering, National University of Singapore, Singapore
  - **TL;DR:** This paper introduces a Mixture of Receptive Fields (MoRF) approach that combines multiple receptive fields of varying sizes to enhance convolutional neural network architectures. The proposed method demonstrates improved performance in image classification, object detection, and segmentation tasks while maintaining efficient inference speeds.
  - **Keywords:** Mixture of Receptive Fields, Convolutional Neural Networks, MoRF (Mixture of Receptive Fields), hard routing, soft routing, re-parameterization, Image classification, object detection, segmentation, Selection of appropriate receptive field size, optimization difficulty, redundant computation costs, Improved network capacity, effective integration into architectures like ResNet and ConvNeXt, Receptive fields, VGGNet, Vision Transformer (ViT), Swin Transformer


- [Realistic Unsupervised CLIP Fine-tuning with Universal Entropy Optimization](https://icml.cc/virtual/2024/poster/33795) (Spotlight Poster)
  - **Authors:** [Jian Liang](http://openreview.net/profile?id=~Jian_Liang1), [Sheng](http://openreview.net/profile?id=~Lijun_Sheng1), [Zhengbo Wang](http://openreview.net/profile?id=~Zhengbo_Wang1), [Ran He](http://openreview.net/profile?id=~Ran_He1), [Tieniu Tan](http://openreview.net/profile?id=~Tieniu_Tan1)
  - **Affiliations:** NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences; University of Science and Technology of China, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences; University of Science and Technology of China, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; Nanjing University
  - **TL;DR:** This paper presents a novel unsupervised fine-tuning approach for CLIP called Universal Entropy Optimization (UEO), which effectively enhances out-of-distribution detection while recognizing known classes. The method demonstrates significant improvements across various domains compared to baseline approaches.
  - **Keywords:** Unsupervised fine-tuning, Vision-language models, Universal Entropy Optimization (UEO), Contrastive Language-Image Pretraining (CLIP), Image recognition, Zero-shot classification, Out-of-distribution detection, Class name reliance in unsupervised learning, Enhanced recognition of known classes, Improved out-of-distribution detection


- [Beyond Point Prediction: Score Matching-based Pseudolikelihood Estimation of Neural Marked Spatio-Temporal Point Process](https://icml.cc/virtual/2024/poster/34661) (Poster)
  - **Authors:** [Zichong Li](http://openreview.net/profile?id=~Zichong_Li2), [Qunzhi Xu](http://openreview.net/profile?id=~Qunzhi_Xu1), [Zhenghao Xu](http://openreview.net/profile?id=~Zhenghao_Xu1), [Yajun Mei](http://openreview.net/profile?id=~Yajun_Mei1), [Tuo Zhao](http://openreview.net/profile?id=~Tuo_Zhao2), [Hongyuan Zha](http://openreview.net/profile?id=~Hongyuan_Zha1)
  - **Affiliations:** Georgia Institute of Technology, Atlanta, USA, Georgia Institute of Technology, Atlanta, USA, Georgia Institute of Technology, Atlanta, USA, Georgia Institute of Technology, Atlanta, USA, Georgia Institute of Technology, Atlanta, USA; The Chinese University of Hong Kong, Hongkong, China, The Chinese University of Hong Kong, Hongkong, China
  - **TL;DR:** This study introduces SMASH, a novel framework for learning marked spatio-temporal point processes that addresses the challenges of likelihood estimation and uncertainty quantification. The framework demonstrates superior performance in predicting event times and locations through score matching and sampling techniques.
  - **Keywords:** Spatio-temporal point processes, event prediction, Score matching, pseudolikelihood estimation, score-based sampling, Ecology, physiology, epidemiology, Intractable likelihood computation, uncertainty quantification, approximation errors, SMASH framework, confidence interval/region prediction, Neural STPPs, intensity functions, Monte Carlo integration


- [Seesaw: Compensating for Nonlinear Reduction with Linear Computations for Private Inference](https://icml.cc/virtual/2024/poster/33290) (Poster)
  - **Authors:** [Fabing Li](http://openreview.net/profile?id=~Fabing_Li1), [Yuanhao Zhai](http://openreview.net/profile?id=~Yuanhao_Zhai1), [Shuangyu Cai](http://openreview.net/profile?id=~Shuangyu_Cai1), [Mingyu Gao](http://openreview.net/profile?id=~Mingyu_Gao1)
  - **Affiliations:** Xi’an Jiaotong University, Xi’an, China; Institute for Interdisciplinary Information Core Technology, Xi’an, China, State University of New York at Buffalo, New York, USA, Tsinghua University, Beijing, China, Tsinghua University, Beijing, China; Shanghai Artificial Intelligence Lab, Shanghai, China; Shanghai Qi Zhi Institute, Shanghai, China
  - **TL;DR:** The study introduces Seesaw, a novel neural architecture search method designed for privacy-preserving machine learning, which reduces the computational cost associated with nonlinear operations. It achieves significant improvements in online latency and accuracy compared to existing methods while maintaining a focus on data privacy.
  - **Keywords:** privacy-preserving machine learning (PPML), neural architecture search, linear computations, nonlinear result reuse, pruning strategies, image classification, online inference, computational cost of nonlinear computations, data privacy concerns, lower online latency, improved accuracy, efficient model architecture, ImageNet


- [Single-Trajectory Distributionally Robust Reinforcement Learning](https://icml.cc/virtual/2024/poster/35077) (Poster)
  - **Authors:** [Zhipeng Liang](http://openreview.net/profile?id=~Zhipeng_Liang1), [Xiaoteng Ma](http://openreview.net/profile?id=~Xiaoteng_Ma1), [Jose Blanchet](http://openreview.net/profile?id=~Jose_Blanchet1), [Jun Yang](http://openreview.net/profile?id=~Jun_Yang6), [Jiheng Zhang](http://openreview.net/profile?id=~Jiheng_Zhang1), [Zhengyuan Zhou](http://openreview.net/profile?id=~Zhengyuan_Zhou2)
  - **Affiliations:** Department of Industrial Engineering and Decision Analytics, Hong Kong University of Science and Technology, Department of Automation, Tsinghua University, Department of Management Science and Engineering, Stanford University, Department of Automation, Tsinghua University, Department of Industrial Engineering and Decision Analytics, Hong Kong University of Science and Technology; Department of Mathematics, Hong Kong University of Science and Technology, Stern School of Business, New York University; Arena Technologies
  - **TL;DR:** This paper presents a novel model-free algorithm for Distributionally Robust Reinforcement Learning (DRRL) that learns optimal policies from a single sample trajectory, addressing the challenges posed by discrepancies between training and test environments. The proposed method demonstrates superior robustness and sample efficiency compared to existing non-robust and robust RL algorithms.
  - **Keywords:** Distributionally Robust Reinforcement Learning, Model-Free Learning, Distributionally Robust Q-learning, Multi-Timescale Framework, Reinforcement Learning in Complex Environments, Financial Markets, Robotic Control, Discrepancy between training and test environments, Sensitivity of optimal policy to model, Asymptotic convergence guarantees, Improved robustness and sample complexity, Markov Decision Process (MDP), Ambiguity Set


- [Graph External Attention Enhanced Transformer](https://icml.cc/virtual/2024/poster/35178) (Poster)
  - **Authors:** [Jianqing Liang](http://openreview.net/profile?id=~Jianqing_Liang1), [Min Chen](http://openreview.net/profile?id=~chenmin1), [Jiye Liang](http://openreview.net/profile?id=~Jiye_Liang1)
  - **Affiliations:** Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan 030006, Shanxi, China, Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan 030006, Shanxi, China, Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan 030006, Shanxi, China
  - **TL;DR:** This study introduces the Graph External Attention (GEA) mechanism and the Graph External Attention Enhanced Transformer (GEAET) to improve graph representation learning by leveraging inter-graph correlations. The proposed methods demonstrate state-of-the-art performance on benchmark datasets, addressing limitations of existing Graph Neural Networks.
  - **Keywords:** Graph representation learning, inter-graph correlations, Graph External Attention (GEA), Graph External Attention Enhanced Transformer (GEAET), self-attention, Social network analysis, drug discovery, protein design, medical diagnosis, Limitations of Graph Neural Networks (GNNs), over-smoothing, limited expressiveness, poor long-range dependencies, State-of-the-art empirical performance, novel attention mechanism, Graph Neural Networks (GNNs), message-passing strategies, Graph Transformers (GTs)


- [EvoRainbow: Combining Improvements in Evolutionary Reinforcement Learning for Policy Search](https://icml.cc/virtual/2024/poster/34895) (Poster)
  - **Authors:** [Pengyi Li](http://openreview.net/profile?id=~Pengyi_Li1), [Yan Zheng](http://openreview.net/profile?id=~YAN_ZHENG1), [Hongyao Tang](http://openreview.net/profile?id=~Hongyao_Tang1), [Xian Fu](http://openreview.net/profile?id=~Xian_Fu1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1)
  - **Affiliations:** College of Intelligence and Computing, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China
  - **TL;DR:** This study explores the integration of Evolutionary Algorithms and Reinforcement Learning for efficient policy optimization, presenting EvoRainbow and EvoRainbow-Exp as effective solutions that outperform existing methods across various tasks. The findings highlight the complementary strengths of both approaches in addressing challenges in policy search.
  - **Keywords:** Policy search, Evolutionary Algorithms, Reinforcement Learning, Evolutionary Algorithms (EAs), Reinforcement Learning (RL), Deep Deterministic Policy Gradient (DDPG), Genetic Algorithm (GA), Robot control, Game AI, Recommender systems, Sample inefficiency, Exploration and convergence challenges, EvoRainbow, EvoRainbow-Exp, Mechanism evaluation


- [On the Error-Propagation of Inexact Hotelling's Deflation for Principal Component Analysis](https://icml.cc/virtual/2024/poster/34842) (Poster)
  - **Authors:** [Fangshuo Liao](http://openreview.net/profile?id=~Fangshuo_Liao1), [J. Lyle Kim](http://openreview.net/profile?id=~Junhyung_Lyle_Kim1), [Cruz Barnum](http://openreview.net/profile?id=~Cruz_Barnum1), [Anastasios Kyrillidis](http://openreview.net/profile?id=~Anastasios_Kyrillidis2)
  - **Affiliations:** Department of Computer Science, Rice University, Houston, U.S.A., Department of Computer Science, Rice University, Houston, U.S.A., Department of Computer Science, UIUC, Department of Computer Science, Rice University, Houston, U.S.A.
  - **TL;DR:** This paper mathematically characterizes the error propagation in the inexact Hotelling’s deﬂation method for Principal Component Analysis (PCA), highlighting how numerical errors affect the estimation of subsequent principal components. The study provides tighter error bounds when using power iteration compared to a more general approach.
  - **Keywords:** Principal Component Analysis (PCA), Error Propagation, Deﬂation Method, Hotelling’s Deﬂation, Power Iteration, Dimensionality Reduction, Classification, Clustering, Numerical Errors, Imprecise Estimation, High-Dimensional Data, Error Characterization, Tighter Error Bounds, MNIST Dataset, Eigenvector, Empirical Covariance Matrix, Orthogonal Matrix


- [Learning the Uncertainty Sets of Linear Control Systems via Set Membership: A Non-asymptotic Analysis](https://icml.cc/virtual/2024/poster/33143) (Poster)
  - **Authors:** [Yingying Li](http://openreview.net/profile?id=~Yingying_Li3), [Jing Yu](http://openreview.net/profile?id=~Jing_Yu1), [Lauren Conger](http://openreview.net/profile?id=~Lauren_Conger1), [Taylan Kargin](http://openreview.net/profile?id=~Taylan_Kargin1), [Adam Wierman](http://openreview.net/profile?id=~Adam_Wierman1)
  - **Affiliations:** University of Illinois Urbana-Champaign, California Institute of Technology, California Institute of Technology, California Institute of Technology, California Institute of Technology
  - **TL;DR:** This paper investigates uncertainty set estimation for unknown linear systems using set membership estimation (SME), providing the first convergence rate bounds for SME. The findings highlight the importance of accurately estimating uncertainty sets to ensure robust control in safety-critical applications.
  - **Keywords:** uncertainty set estimation, robust control, linear systems, set membership estimation (SME), least squares estimation (LSE), safety-critical applications, power systems, unmanned aerial vehicles (UAV), building control, estimating unknown linear dynamical systems, quantifying uncertainties, satisfying safety constraints, convergence rate bounds for SME, numerical results demonstrating SME's practical promise


- [Revisiting the Role of Language Priors in Vision-Language Models](https://icml.cc/virtual/2024/poster/34400) (Poster)
  - **Authors:** [Zhiqiu Lin](http://openreview.net/profile?id=~Zhiqiu_Lin1), [Xinyue Chen](http://openreview.net/profile?id=~Xinyue_Chen5), [Deepak Pathak](http://openreview.net/profile?id=~Deepak_Pathak1), [Pengchuan Zhang](http://openreview.net/profile?id=~Pengchuan_Zhang1), [Deva Ramanan](http://openreview.net/profile?id=~Deva_Ramanan1)
  - **Affiliations:** CMU, CMU, CMU, Meta, CMU
  - **TL;DR:** This study investigates the performance of generative vision-language models (VLMs) in zero-shot image-text retrieval tasks, revealing that while they can achieve high accuracy, they also face challenges due to linguistic biases in benchmarks. The authors propose a debiasing method that enhances the Visual Generative Pre-Training Score, establishing it as a strong baseline for vision-language understanding.
  - **Keywords:** Vision-language models, zero-shot learning, visual understanding, Generative VLMs, next-word generation, probabilistic scoring, Image-text retrieval, visual-question answering, Linguistic bias, unnatural language distributions, challenges in visual understanding, Visual Generative Pre-Training Score (Visual-GPTScore), debiasing techniques, Nine popular vision-language benchmarks, CLIP


- [Graph Geometry-Preserving Autoencoders](https://icml.cc/virtual/2024/poster/33680) (Poster)
  - **Authors:** [Jungbin Lim](http://openreview.net/profile?id=~Jungbin_Lim1), [Jihwan Kim](http://openreview.net/profile?id=~Jihwan_Kim2), [Yonghyeon Lee](http://openreview.net/profile?id=~Yonghyeon_Lee2), [Cheongjae Jang](http://openreview.net/profile?id=~Cheongjae_Jang1), [Frank Chongwoo Park](http://openreview.net/profile?id=~Frank_C._Park1)
  - **Affiliations:** Department of Mechanical Engineering, Seoul National University, Seoul, Republic of Korea, Department of Mechanical Engineering, Seoul National University, Seoul, Republic of Korea, Center for AI and Natural Sciences, Korea Institute for Advanced Study, Seoul, Republic of Korea, The AI Institute, Hanyang University, Seoul, Republic of Korea, Department of Mechanical Engineering, Seoul National University, Seoul, Republic of Korea
  - **TL;DR:** This paper introduces the Graph Geometry-Preserving Autoencoder (GGAE), a novel framework that utilizes a similarity graph to better capture the geometry of high-dimensional data manifolds. The proposed method outperforms existing autoencoders in preserving graph geometry and effectively learning dynamics in the latent space.
  - **Keywords:** autoencoders, geometry preservation, manifold learning, Riemannian geometric distortion measure, graph Laplacian, high-dimensional data analysis, latent space representation, geometry distortion in latent representations, non-Euclidean ambient space metrics, Graph Geometry-Preserving Autoencoder (GGAE), improved latent structure learning, similarity graph, latent representations


- [IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual Speech Separation](https://icml.cc/virtual/2024/poster/34541) (Poster)
  - **Authors:** [Kai Li](http://openreview.net/profile?id=~Kai_Li15), [Runxuan Yang](http://openreview.net/profile?id=~Runxuan_Yang1), [Fuchun Sun](http://openreview.net/profile?id=~Fuchun_Sun1), [Xiaolin Hu](http://openreview.net/profile?id=~Xiaolin_Hu1)
  - **Affiliations:** Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China, Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China, Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China, Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China; Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing 100084, China; Chinese Institute for Brain Research (CIBR), Beijing 100010, China
  - **TL;DR:** The study introduces IIANet, a novel model that utilizes intra- and inter-modality attention mechanisms for effective audio-visual speech separation. Experimental results show that IIANet outperforms existing methods while being computationally efficient, demonstrating the potential of attention mechanisms in multimodal fusion.
  - **Keywords:** audio-visual speech separation, multimodal fusion, attention mechanism, Intra- and Inter-Attention Network (IIANet), speech separation, auditory experience enhancement, noise interference in speech separation, limitations of audio-only speech separation, improved separation quality, efficient multimodal fusion, LRS2, LRS3, VoxCeleb2, cocktail party effect


- [Algorithmic Stability Unleashed: Generalization Bounds with Unbounded Losses](https://icml.cc/virtual/2024/poster/34899) (Poster)
  - **Authors:** [Shaojie Li](http://openreview.net/profile?id=~Shaojie_Li2), [Bowei Zhu](http://openreview.net/profile?id=~Bowei_Zhu1), [Yong Liu](http://openreview.net/profile?id=~Yong_Liu7)
  - **Affiliations:** Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China
  - **TL;DR:** This paper explores the generalization ability of learning algorithms by relaxing the assumption of bounded loss functions, introducing new generalization bounds for algorithmic stability with unbounded losses. The authors present refined stability analysis that can achieve √n-times faster results, contributing to the understanding of generalization in various machine learning contexts.
  - **Keywords:** algorithmic stability, generalization bounds, unbounded loss functions, subweibull diameter, concentration inequalities, regularized regression, signal processing, neural networks, sample bias correction, domain adaptation, boosting, importance-weighting, generalization ability of learning algorithms, bounded vs unbounded losses, new generalization bounds, refined stability analysis


- [Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse Training Data](https://icml.cc/virtual/2024/poster/33102) (Poster)
  - **Authors:** [Kang Lin](http://openreview.net/profile?id=~Kang_Lin1), [Reinhard Heckel](http://openreview.net/profile?id=~Reinhard_Heckel1)
  - **Affiliations:** Department of Computer Engineering, Technical University of Munich, Munich, Germany, Department of Computer Engineering, Technical University of Munich, Munich, Germany
  - **TL;DR:** This study investigates the impact of diverse training data on the performance and robustness of deep learning models for accelerated MRI. It finds that training on a combination of various data distributions enhances robustness without compromising in-distribution performance, suggesting a more effective approach than maintaining separate models for individual distributions.
  - **Keywords:** deep learning, image reconstruction, robustness, accelerated magnetic resonance imaging (MRI), distribution shifts, performance degradation, improved robustness, effective model training, diverse training data


- [Structured Inverse-Free Natural Gradient Descent: Memory-Efficient & Numerically-Stable KFAC](https://icml.cc/virtual/2024/poster/33790) (Poster)
  - **Authors:** [Wu Lin](http://openreview.net/profile?id=~Wu_Lin2), [Felix Dangel](http://openreview.net/profile?id=~Felix_Dangel1), [Runa Eschenhagen](http://openreview.net/profile?id=~Runa_Eschenhagen1), [Kirill Neklyudov](http://openreview.net/profile?id=~Kirill_Neklyudov1), [Agustinus Kristiadi](http://openreview.net/profile?id=~Agustinus_Kristiadi1), [Richard E Turner](http://openreview.net/profile?id=~Richard_E_Turner1), [Alireza Makhzani](http://openreview.net/profile?id=~Alireza_Makhzani1)
  - **Affiliations:** Vector Institute, Vector Institute, University of Cambridge, Vector Institute, Vector Institute, University of Cambridge, Vector Institute; University of Toronto
  - **TL;DR:** This paper introduces structured inverse-free natural gradient descent (SINGD) as a memory-efficient and numerically stable alternative to KFAC for training neural networks, demonstrating its effectiveness over AdamW in low-precision settings. The authors also establish a connection between SINGD and existing natural gradient methods, addressing challenges in second-order optimization.
  - **Keywords:** deep learning, second-order methods, neural network training, KFAC (Kronecker-factored approximate curvature), natural gradient descent (NGD), structured inverse-free natural gradient descent (SINGD), inverse-free KFAC (IKFAC), memory inefficiency, numerical instability in low precision, high memory consumption, iteration cost, memory-efficient training methods, numerically robust optimization algorithms


- [Non-confusing Generation of Customized Concepts in Diffusion Models](https://icml.cc/virtual/2024/poster/33802) (Poster)
  - **Authors:** [Wang Lin](http://openreview.net/profile?id=~Wang_Lin2), [Jingyuan CHEN](http://openreview.net/profile?id=~Jingyuan_Chen3), [Jiaxin Shi](http://openreview.net/profile?id=~Jiaxin_Shi3), [Yichen Zhu](http://openreview.net/profile?id=~Yichen_Zhu2), [Chen Liang](http://openreview.net/profile?id=~Chen_Liang18), [Junzhong Miao](http://openreview.net/profile?id=~Junzhong_Miao1), [Tao Jin](http://openreview.net/profile?id=~Tao_Jin2), [Zhou Zhao](http://openreview.net/profile?id=~Zhou_Zhao3), [Fei Wu](http://openreview.net/profile?id=~Fei_Wu2), [Shuicheng YAN](http://openreview.net/profile?id=~Shuicheng_YAN3), [Hanwang Zhang](http://openreview.net/profile?id=~Hanwang_Zhang3)
  - **Affiliations:** Zhejiang University, Zhejiang University; Huawei Cloud Computing, Huawei Cloud Computing, Zhejiang University, Tsinghua University, Harbin Institute of Technology, Zhejiang University, Zhejiang University, Zhejiang University, Skywork AI; Singapore, Skywork AI; Nanyang Technological University
  - **TL;DR:** This study addresses the challenge of inter-concept visual confusion in generating customized concepts using text-guided diffusion models. The authors propose a novel method called CLIF to enhance the clarity of textual embeddings, demonstrating its effectiveness in improving the generation of multi-customized concepts.
  - **Keywords:** Customized concept generation, Text-guided diffusion models, Contrastive image-language pre-training (CLIP), Text-to-image decoding, Inter-concept visual confusion, Scarcity of user-provided visual examples, CLIF (contrastive image-language fine-tuning), TGDM (Text-Guided Diffusion Models), U-net decoder


- [Equivariance via Minimal Frame Averaging for More Symmetries and Efficiency](https://icml.cc/virtual/2024/poster/33408) (Spotlight Poster)
  - **Authors:** [Yuchao Lin](http://openreview.net/profile?id=~Yuchao_Lin1), [Jacob Helwig](http://openreview.net/profile?id=~Jacob_Helwig1), [Shurui Gui](http://openreview.net/profile?id=~Shurui_Gui1), [Shuiwang Ji](http://openreview.net/profile?id=~Shuiwang_Ji1)
  - **Affiliations:** Department of Computer Science and Engineering, Texas A&M University, Texas, USA, Department of Computer Science and Engineering, Texas A&M University, Texas, USA, Department of Computer Science and Engineering, Texas A&M University, Texas, USA, Department of Computer Science and Engineering, Texas A&M University, Texas, USA
  - **TL;DR:** This paper introduces Minimal Frame Averaging (MFA), a framework for achieving exact equivariance in machine learning systems efficiently. The method extends frame averaging to more symmetry groups and demonstrates effectiveness across various tasks, including n-body simulation and top tagging.
  - **Keywords:** equivariance, frame averaging, machine learning, Minimal Frame Averaging (MFA), n-body simulation, top tagging in collider physics, relaxed energy prediction, computational efficiency, exact equivariance, provably minimal frames, encoding symmetries, Lorentz group, unitary group, proper Lorentz group, special unitary group, general linear group, special linear group


- [Graph-enhanced Large Language Models in Asynchronous Plan Reasoning](https://icml.cc/virtual/2024/poster/33498) (Poster)
  - **Authors:** [Fangru Lin](http://openreview.net/profile?id=~Fangru_Lin1), [Emanuele La Malfa](http://openreview.net/profile?id=~Emanuele_La_Malfa2), [Valentin Hofmann](http://openreview.net/profile?id=~Valentin_Hofmann1), [Elle Michelle Yang](http://openreview.net/profile?id=~Elle_Michelle_Yang1), [Anthony Cohn](http://openreview.net/profile?id=~Anthony_G._Cohn2), [Janet Pierrehumbert](http://openreview.net/profile?id=~Janet_B._Pierrehumbert1)
  - **Affiliations:** University of Oxford, Alan Turing Institute; University of Leeds, Allen Institute for AI; LMU Munich, University of Oxford, Alan Turing Institute; University of Leeds, University of Oxford
  - **TL;DR:** This study investigates the ability of large language models (LLMs) to perform asynchronous planning, revealing that while LLMs can generate planning steps, they struggle to optimize plans without additional guidance. The proposed Plan Like a Graph (PLaG) technique improves performance but highlights limitations in handling complex tasks.
  - **Keywords:** Asynchronous planning, Large language models, Plan Like a Graph (PLaG), Graphs with natural language prompts, Autonomous agents, Task-solving processes, Sequential and parallel planning, Time optimization, Task complexity, State-of-the-art results in planning, Performance degradation with complexity, Large Language Models (LLMs), GPT-4, LLaMA-2


- [Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium](https://icml.cc/virtual/2024/poster/34136) (Poster)
  - **Authors:** [Luofeng Liao](http://openreview.net/profile?id=~Luofeng_Liao1), [Christian Kroer](http://openreview.net/profile?id=~Christian_Kroer1)
  - **Affiliations:** IEOR, Columbia University, IEOR, Columbia University
  - **TL;DR:** This paper introduces statistically valid bootstrap inference procedures for the linear Fisher market and first-price pacing equilibrium models, addressing the lack of existing theory for their application. The findings demonstrate the effectiveness of these procedures under mild degeneracy conditions, verified through experiments with synthetic and semi-real data.
  - **Keywords:** Fisher market equilibrium, first-price pacing equilibrium, statistical inference, Bootstrap procedures, constrained M-estimators, epi-convergence theory, Advertising auctions, resource allocation, budget management, Valid application of bootstrap procedures in equilibrium settings, Statistically valid bootstrap inference procedures for LFM and FPPE, Synthetic data, semi-real data


- [Momentum Particle Maximum Likelihood](https://icml.cc/virtual/2024/poster/33109) (Poster)
  - **Authors:** [Jen Ning Lim](http://openreview.net/profile?id=~Jen_Ning_Lim1), [Juan Kuntz](http://openreview.net/profile?id=~Juan_Kuntz1), [Samuel Power](http://openreview.net/profile?id=~Samuel_Power1), [Adam M. Johansen](http://openreview.net/profile?id=~Adam_Michael_Johansen1)
  - **Affiliations:** University of Warwick, Polygeist, University of Bristol, University of Warwick
  - **TL;DR:** This study presents a novel dynamical systems-inspired approach to minimize the free energy functional for maximum likelihood estimation in latent variable models, combining elements of accelerated gradient methods and Langevin diffusion. The proposed algorithm demonstrates superior performance compared to existing particle methods in numerical experiments.
  - **Keywords:** Maximum likelihood estimation, latent variable models, free energy minimization, Nesterov’s Accelerated Gradient method, underdamped Langevin diffusion, particle methods, particle gradient descent, Optimization of marginal likelihood, challenges in closed-form expressions for integrals, Practical algorithm for MLE, improved performance over existing particle methods, Kullback–Leibler divergence, gradient flow


- [On Hypothesis Transfer Learning of Functional Linear Models](https://icml.cc/virtual/2024/poster/33182) (Poster)
  - **Authors:** [Haotian Lin](http://openreview.net/profile?id=~Haotian_Lin1), [Matthew Reimherr](http://openreview.net/profile?id=~Matthew_Reimherr1)
  - **Affiliations:** Department of Statistics, The Pennsylvania State University, University Park, PA, USA, Department of Statistics, The Pennsylvania State University, University Park, PA, USA
  - **TL;DR:** This study develops transfer learning algorithms for functional linear regression within the Reproducing Kernel Hilbert Space framework, addressing the challenges posed by high-dimensional functional data. The proposed methods demonstrate statistical convergence guarantees and are validated through synthetic and real-world data applications.
  - **Keywords:** Transfer Learning, Functional Linear Regression, Reproducing Kernel Hilbert Space (RKHS), Truncation-based FLR methods, Hypothesis offset transfer learning, Functional Data Analysis (FDA), Data sparsity, High-dimensional data, Lack of training samples, New transfer learning algorithms for FLR, Statistical convergence rate guarantees, Scalar-on-Function regression, Infinite-dimensional data


- [Layer-Aware Analysis of Catastrophic Overfitting: Revealing the Pseudo-Robust Shortcut Dependency](https://icml.cc/virtual/2024/poster/33187) (Poster)
  - **Authors:** [Runqi Lin](http://openreview.net/profile?id=~Runqi_Lin1), [Chaojian Yu](http://openreview.net/profile?id=~Chaojian_Yu1), [Bo Han](http://openreview.net/profile?id=~Bo_Han1), [Hang Su](http://openreview.net/profile?id=~Hang_Su3), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1)
  - **Affiliations:** Sydney AI Centre, School of Computer Science, The University of Sydney, Sydney, Australia, Sydney AI Centre, School of Computer Science, The University of Sydney, Sydney, Australia, Department of Computer Science, Hong Kong Baptist University, Hong Kong, China, Department of Computer Science and Technology, Institute for AI, BNRist Center, Tsinghua University, Beijing, China, Sydney AI Centre, School of Computer Science, The University of Sydney, Sydney, Australia; Department of Computer Science and Technology, Institute for AI, BNRist Center, Tsinghua University, Beijing, China
  - **TL;DR:** This study investigates catastrophic overfitting in single-step adversarial training, revealing that earlier layers of deep neural networks are more susceptible to distortion due to pseudo-robust shortcuts. The proposed method, Layer-Aware Adversarial Weight Perturbation (LAP), effectively mitigates this issue and enhances robustness against adversarial attacks.
  - **Keywords:** Catastrophic Overfitting, Adversarial Training, Layer-Aware Adversarial Weight Perturbation (LAP), Deep Neural Networks (DNNs), Catastrophic Overfitting (CO), Decision Boundary Distortion, Improved robustness in DNNs, Mitigation of CO, Pseudo-Robust Shortcuts


- [An Effective Dynamic Gradient Calibration Method for Continual Learning](https://icml.cc/virtual/2024/poster/33012) (Poster)
  - **Authors:** [Weichen Lin](http://openreview.net/profile?id=~Weichen_Lin1), [Jiaxiang Chen](http://openreview.net/profile?id=~Jiaxiang_Chen2), [Ruomin Huang](http://openreview.net/profile?id=~Ruomin_Huang1), [Hu Ding](http://openreview.net/profile?id=~Hu_Ding1)
  - **Affiliations:** School of Data Science, University of Science and Technology of China, Anhui, China, School of Data Science, University of Science and Technology of China, Anhui, China, Duke University, School of Computer Science and Technology, University of Science and Technology of China, Anhui, China
  - **TL;DR:** This paper presents a novel algorithm for dynamic gradient calibration in continual learning to mitigate catastrophic forgetting, particularly when historical data is limited. The proposed method shows potential for improved performance in training models with continuously incoming data and tasks.
  - **Keywords:** Continual Learning, Catastrophic Forgetting, Gradient Calibration, Stochastic Variance Reduction, SVRG, SAGA, Machine Learning, Deep Neural Networks, Catastrophic Forgetting, Limited Memory, Non-i.i.d. Samples, Effective Algorithms, Improved Performance, Benchmark Datasets, Experience Replay, Reservoir Sampling


- [Autonomous Sparse Mean-CVaR Portfolio Optimization](https://icml.cc/virtual/2024/poster/35204) (Poster)
  - **Authors:** [Yizun Lin](http://openreview.net/profile?id=~Yizun_Lin1), [Yangyu Zhang](http://openreview.net/profile?id=~Yangyu_Zhang1), [Zhao-Rong Lai](http://openreview.net/profile?id=~Zhao-Rong_Lai1), [Cheng Li](http://openreview.net/profile?id=~Cheng_Li24)
  - **Affiliations:** Department of Mathematics, College of Information Science and Technology, Jinan University, Guangzhou, China, Jinan University-University of Birmingham Joint Institute, Jinan University, Guangzhou, China, Department of Mathematics, College of Information Science and Technology, Jinan University, Guangzhou, China, Department of Mathematics, College of Information Science and Technology, Jinan University, Guangzhou, China
  - **TL;DR:** This study introduces an autonomous sparse mean-CVaR portfolio model that approximates the (cid:96)0-constrained mean-CVaR model with arbitrary accuracy, addressing the computational challenges associated with NP-hard problems. The proposed framework enhances computational efficiency and provides a robust asset selection strategy.
  - **Keywords:** Portfolio Optimization, Risk Management, Mean-CVaR Model, Proximal Alternating Linearized Minimization Algorithm, Tailed Approximation, Financial Portfolio Management, NP-hard Problems, Computational Complexity, Asset Selection, Autonomous Sparse Mean-CVaR Model, Approximation of (cid:96)0-constrained Mean-CVaR Model, Value-at-Risk (VaR), Conditional Value-at-Risk (CVaR), (cid:96)0 Constraint


- [A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts](https://icml.cc/virtual/2024/poster/35105) (Poster)
  - **Authors:** [Huy Nguyen](http://openreview.net/profile?id=~Huy_Nguyen5), [Pedram Akbarian](http://openreview.net/profile?id=~Pedram_Akbarian1), [TrungTin Nguyen](http://openreview.net/profile?id=~TrungTin_Nguyen1), [Nhat Ho](http://openreview.net/profile?id=~Nhat_Ho1)
  - **Affiliations:** Department of Statistics and Data Sciences; The University of Texas at Austin, Department of Electrical and Computer Engineering, The University of Texas at Austin, School of Mathematics and Physics; The University of Queensland, Department of Statistics and Data Sciences; The University of Texas at Austin
  - **TL;DR:** This study establishes the convergence rates of density and parameter estimation in the softmax gating multinomial logistic mixture of experts model, addressing the slow rates caused by interactions between gating and expert functions. The authors propose modified softmax gating functions to significantly improve parameter estimation rates.
  - **Keywords:** Mixture of Experts (MoE), Softmax Gating, Classification, Regression, Maximum Likelihood Estimation (MLE), Gaussian MoE, Modified Softmax Gating Functions, Large Language Models, Computer Vision, Speech Recognition, Reinforcement Learning, Multi-task Learning, Convergence Rates, Parameter Estimation, Interaction between Gating and Expert Functions, Improved Parameter Estimation Rates, Novel Gating Functions


- [Parsimonious Learning-Augmented Approximations for Dense Instances of $\mathcal{NP}$-hard Problems](https://icml.cc/virtual/2024/poster/34780) (Poster)
  - **Authors:** [Evripidis Bampis](http://openreview.net/profile?id=~Evripidis_Bampis1), [Bruno Escoffier](http://openreview.net/profile?id=~Bruno_Escoffier1), [Michalis Xefteris](http://openreview.net/profile?id=~Michalis_Xefteris1)
  - **Affiliations:** Sorbonne Université, CNRS, LIP6, F-75005 Paris, France, Sorbonne Université, CNRS, LIP6, F-75005 Paris, France; Institut Universitaire de France, Paris, France, Sorbonne Université, CNRS, LIP6, F-75005 Paris, France
  - **TL;DR:** This paper extends and accelerates existing approximation algorithms for dense instances of NP-hard problems by utilizing a logarithmic number of one-bit predictions. The proposed learning-augmented framework ensures performance consistency and robustness against prediction errors while addressing computational limitations.
  - **Keywords:** Learning-augmented algorithms, NP-hard problems, Approximation algorithms, one-bit predictions, Optimization problems, MAX-CUT, MAX-k-SAT, Worst-case computational limitations, prediction error, Fast algorithms, approximation consistency, smoothness, robustness, Learning-augmented framework, online algorithms


- [A Single-Loop Robust Policy Gradient Method for Robust Markov Decision Processes](https://icml.cc/virtual/2024/poster/33907) (Poster)
  - **Authors:** [Zhenwei Lin](http://openreview.net/profile?id=~Zhenwei_Lin3), [Chenyu Xue](http://openreview.net/profile?id=~Chenyu_Xue1), [Qi Deng](http://openreview.net/profile?id=~Qi_Deng1), [Yinyu Ye](http://openreview.net/profile?id=~Yinyu_Ye1)
  - **Affiliations:** Shanghai University of Finance and Economics, Shanghai University of Finance and Economics, Antai College of Economics and Management, Shanghai Jiao Tong University, Stanford University
  - **TL;DR:** This paper introduces a novel single-loop robust policy gradient method for solving robust Markov Decision Processes, ensuring global optimality and demonstrating improved convergence compared to traditional nested-loop methods. The proposed method effectively addresses challenges related to estimation errors and dynamic environments in decision-making scenarios.
  - **Keywords:** Robust Markov Decision Processes, Policy Gradient Methods, Single-Loop Robust Policy Gradient (SRPG), Minimax Formulation, Finance, Autonomous Driving, Revenue Management, Estimation errors in transition matrix, Dynamic environments, Limited data, Global optimality guarantee, Faster and more robust convergence behavior


- [HGAP: Boosting Permutation Invariant and Permutation Equivariant in Multi-Agent Reinforcement Learning via Graph Attention Network](https://icml.cc/virtual/2024/poster/34327) (Poster)
  - **Authors:** [Bor Jiun Lin](http://openreview.net/profile?id=~Bor-Jiun_Lin1), [Chun-Yi Lee](http://openreview.net/profile?id=~Chun-Yi_Lee1)
  - **Affiliations:** ELSA Lab, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ELSA Lab, Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan
  - **TL;DR:** This study introduces the Hyper Graphical Attention Policy (HGAP) Network to enhance training efficiency in Multi-Agent Reinforcement Learning by addressing the challenges posed by Permutation Invariant and Permutation Equivariant properties. The effectiveness and adaptability of HGAP are validated through various benchmarks and ablation studies.
  - **Keywords:** Multi-Agent Reinforcement Learning (MARL), Graph Representation, Hyper Graphical Attention Policy (HGAP), Graph Attention Mechanism, Training difficulties in MARL, Permutation Invariant (PI) and Permutation Equivariant (PE) properties, Effectiveness and efficiency of HGAP, Adaptability, Transferability, Centralized Training with Decentralized Execution (CTDE), Partially Observable Markov Decision Process (POMDP)


- [Graph Neural Stochastic Diffusion for Estimating Uncertainty in Node Classification](https://icml.cc/virtual/2024/poster/32717) (Poster)
  - **Authors:** [Xixun Lin](http://openreview.net/profile?id=~Xixun_Lin3), [Wenxiao Zhang](http://openreview.net/profile?id=~Wenxiao_Zhang2), [Fengzhao Shi](http://openreview.net/profile?id=~Fengzhao_Shi1), [Chuan Zhou](http://openreview.net/profile?id=~Chuan_Zhou3), [Lixin Zou](http://openreview.net/profile?id=~Lixin_Zou1), [Xiangyu Zhao](http://openreview.net/profile?id=~Xiangyu_Zhao1), [Dawei Yin](http://openreview.net/profile?id=~Dawei_Yin1), [Shirui Pan](http://openreview.net/profile?id=~Shirui_Pan1), [Yanan Cao](http://openreview.net/profile?id=~Yanan_Cao1)
  - **Affiliations:** Institute of Information Engineering, Chinese Academy of Sciences, Beijing Jiaotong University, Institute of Information Engineering, Chinese Academy of Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences, Wuhan University, City University of Hong Kong, Baidu Inc., Griffith University, Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences
  - **TL;DR:** This paper introduces Graph Neural Stochastic Diffusion (GNSD), a novel framework for estimating predictive uncertainty in graph neural networks by modeling the stochastic evolution of node representations. The proposed method demonstrates superior performance in uncertainty estimation compared to existing approaches across multiple detection tasks.
  - **Keywords:** Graph Neural Networks (GNNs), Uncertainty Estimation, Graph Neural Stochastic Diffusion (GNSD), Stochastic Partial Differential Equation, Q-Wiener Process, Molecular Prediction, Recommender Systems, Traffic Forecasting, Uncertainty in GNN Predictions, Aleatoric Uncertainty, Epistemic Uncertainty, Drift Network, Stochastic Forcing Network


- [SparseTSF: Modeling Long-term Time Series Forecasting with *1k* Parameters](https://icml.cc/virtual/2024/poster/34991) (Oral)
  - **Authors:** [Shengsheng Lin](http://openreview.net/profile?id=~Shengsheng_Lin1), [Weiwei Lin](http://openreview.net/profile?id=~Weiwei_Lin1), [Wentai Wu](http://openreview.net/profile?id=~Wentai_Wu1), [Haojun Chen](http://openreview.net/profile?id=~Haojun_Chen3), [Junjie Yang](http://openreview.net/profile?id=~Junjie_Yang7)
  - **Affiliations:** School of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China, School of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China; Peng Cheng Laboratory, Shenzhen 518066, China, College of Information Science and Technology, Jinan University, Guangzhou 510632, China, School of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China, School of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China
  - **TL;DR:** This paper presents SparseTSF, a lightweight model for Long-term Time Series Forecasting that utilizes the Cross-Period Sparse Forecasting technique to effectively decouple periodicity and trend, achieving competitive performance with less than 1k parameters. The model is designed for scenarios with limited computational resources and demonstrates strong generalization capabilities.
  - **Keywords:** Long-term Time Series Forecasting (LTSF), lightweight modeling, Cross-Period Sparse Forecasting technique, Traffic flow, product sales, energy consumption, Modeling complex temporal dependencies, computational resource limitations, SparseTSF model, competitive performance with fewer than 1k parameters


- [Scaling Tractable Probabilistic Circuits: A Systems Perspective](https://icml.cc/virtual/2024/poster/34732) (Poster)
  - **Authors:** [Anji Liu](http://openreview.net/profile?id=~Anji_Liu1), [Kareem Ahmed](http://openreview.net/profile?id=~Kareem_Ahmed2), [Guy Van den Broeck](http://openreview.net/profile?id=~Guy_Van_den_Broeck1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, USA, Department of Computer Science, University of California, Los Angeles, USA, Department of Computer Science, University of California, Los Angeles, USA
  - **TL;DR:** This paper introduces PyJuice, a highly efficient GPU implementation for Probabilistic Circuits that significantly improves training speed and memory usage, enabling the scaling of large models for complex real-world tasks. The findings demonstrate that PyJuice can enhance state-of-the-art PCs across various image and language datasets, establishing new benchmarks for future research.
  - **Keywords:** Probabilistic Circuits, tractable deep generative models, efficient probabilistic inference, PyJuice, compilation process, block-based parallelization, image datasets, language datasets, explainability, causality, time and memory inefficiency, scaling up PC learning and inference, faster training, reduced GPU memory consumption, new baselines for PCs, ImageNet32, WikiText, CommonGen


- [Graph Distillation with Eigenbasis Matching](https://icml.cc/virtual/2024/poster/34622) (Poster)
  - **Authors:** [Yang Liu](http://openreview.net/profile?id=~Yang_Liu105), [Deyu Bo](http://openreview.net/profile?id=~Deyu_Bo1), [Chuan Shi](http://openreview.net/profile?id=~Chuan_Shi1)
  - **Affiliations:** Department of Computer Science, Beijing University of Posts and Telecommunication, Beijing, China, Department of Computer Science, Beijing University of Posts and Telecommunication, Beijing, China, Department of Computer Science, Beijing University of Posts and Telecommunication, Beijing, China
  - **TL;DR:** This study introduces Graph Distillation with Eigenbasis Matching (GDEM) to efficiently distill synthetic graphs that maintain comparable performance to real graphs while addressing spectrum bias and computational costs associated with various GNN architectures. Extensive experiments show that GDEM outperforms existing graph distillation methods, demonstrating significant efficiency and generalization capabilities.
  - **Keywords:** Graph Neural Networks (GNNs), Graph Distillation (GD), Eigenbasis Matching, Spectral Approximations, Graph Data Processing, Machine Learning, Spectrum Bias, Computational Costs in GNNs, Graph Distillation with Eigenbasis Matching (GDEM), Cross-architecture Generalization


- [Beyond Regular Grids: Fourier-Based Neural Operators on Arbitrary Domains](https://icml.cc/virtual/2024/poster/33684) (Poster)
  - **Authors:** [Levi Lingsch](http://openreview.net/profile?id=~Levi_E._Lingsch1), [Mike Yan Michelis](http://openreview.net/profile?id=~Mike_Yan_Michelis1), [Emmanuel de Bézenac](http://openreview.net/profile?id=~Emmanuel_de_Bezenac2), [Sirani M. Perera](http://openreview.net/profile?id=~Sirani_M._Perera1), [Robert Katzschmann](http://openreview.net/profile?id=~Robert_K._Katzschmann1), [Siddhartha Mishra](http://openreview.net/profile?id=~Siddhartha_Mishra1)
  - **Affiliations:** Seminar for Applied Mathematics, ETH Zurich, Switzerland; ETH AI Center, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland, ETH AI Center, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland, Seminar for Applied Mathematics, ETH Zurich, Switzerland, Department of Mathematics, Embry-Riddle Aeronautical University, Daytona Beach, FL, USA, ETH AI Center, ETH Zurich, Switzerland; Soft Robotics Lab, ETH Zurich, Switzerland, Seminar for Applied Mathematics, ETH Zurich, Switzerland; ETH AI Center, ETH Zurich, Switzerland
  - **TL;DR:** This study proposes a method to extend neural operators to arbitrary domains by efficiently evaluating spectral transformations, addressing the limitations of traditional FFT-based methods on non-equispaced point distributions. The method demonstrates significant gains in training speed while maintaining or improving the accuracy of Fourier neural operators.
  - **Keywords:** Neural Operators, Partial Differential Equations (PDEs), Fast Fourier Transform (FFT), Fourier Neural Operator (FNO), High computational cost of traditional numerical methods, non-equispaced point distributions, Efficient direct evaluation of spectral transformations, improved training speed, accuracy retention, Spectral computations, operator learning


- [Graph Adversarial Diffusion Convolution](https://icml.cc/virtual/2024/poster/34432) (Poster)
  - **Authors:** [Songtao Liu](http://openreview.net/profile?id=~Songtao_Liu2), [Jinghui Chen](http://openreview.net/profile?id=~Jinghui_Chen1), [Tianfan Fu](http://openreview.net/profile?id=~Tianfan_Fu1), [Lu Lin](http://openreview.net/profile?id=~Lu_Lin2), [Marinka Zitnik](http://openreview.net/profile?id=~Marinka_Zitnik1), [Dinghao Wu](http://openreview.net/profile?id=~Dinghao_Wu1)
  - **Affiliations:** The Pennsylvania State University, The Pennsylvania State University, Rensselaer Polytechnic Institute, The Pennsylvania State University, Harvard University, The Pennsylvania State University
  - **TL;DR:** This paper presents a new architecture called Graph Adversarial Diffusion Convolution (GADC) that enhances the robustness of Graph Diffusion Convolution (GDC) against adversarial attacks and noise in node features. The proposed method effectively addresses challenges in heterophilic graphs and demonstrates improved performance through extensive experiments.
  - **Keywords:** Graph Signal Denoising, Graph Neural Networks, Adversarial Robustness, Min-max optimization, Graph Diffusion Convolution, Adversarial Graph Signal Denoising, Adversarial attacks on graph structure, Noise in node features, Heterophilic graphs, Graph Adversarial Diffusion Convolution (GADC), Enhanced robustness against adversarial attacks, Laplacian distance, Graph structure perturbations


- [ESNet: Evolution and Succession Network for High-Resolution Salient Object Detection](https://icml.cc/virtual/2024/poster/34015) (Poster)
  - **Authors:** [Hongyu Liu](http://openreview.net/profile?id=~Hongyu_Liu4), [Runmin Cong](http://openreview.net/profile?id=~Runmin_Cong1), [Hua Li](http://openreview.net/profile?id=~Hua_Li8), [Qianqian Xu](http://openreview.net/profile?id=~Qianqian_Xu2), [Qingming Huang](http://openreview.net/profile?id=~Qingming_Huang1), [Wei Zhang](http://openreview.net/profile?id=~Wei_Zhang7)
  - **Affiliations:** Institute of Information Science, Beijing Jiaotong University & Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing, China, School of Control Science and Engineering, Shandong University & Key Laboratory of Machine Intelligence and System Control, Ministry of Education, Jinan, China, School of Computer Science and Technology, Hainan University, Hainan, China, Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, CAS, Beijing, China, School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China, School of Control Science and Engineering, Shandong University & Key Laboratory of Machine Intelligence and System Control, Ministry of Education, Jinan, China
  - **TL;DR:** This paper presents a two-stage model for High-Resolution Salient Object Detection (HRSOD) that effectively preserves details while minimizing computational costs. The proposed model outperforms state-of-the-art methods in real-time speed and introduces a new evaluation metric for assessing detail detection in high-resolution images.
  - **Keywords:** High-Resolution Salient Object Detection (HRSOD), Salient Object Detection (SOD), Low-resolution Location Model (LrLM), High-resolution Refinement Model (HrRM), Boundary-Detail-aware Mean Absolute Error (MAEBD), Image processing, Computer vision, Detail preservation, High computational costs, Overlooking detailed areas during model training, Two-stage HRSOD model, Real-time performance (49 FPS), Evolution and succession mechanisms


- [Unifying Image Processing as Visual Prompting Question Answering](https://icml.cc/virtual/2024/poster/34237) (Poster)
  - **Authors:** [Yihao Liu](http://openreview.net/profile?id=~Yihao_Liu1), [Xiangyu Chen](http://openreview.net/profile?id=~Xiangyu_Chen5), [Xianzheng Ma](http://openreview.net/profile?id=~Xianzheng_Ma1), [Xintao Wang](http://openreview.net/profile?id=~Xintao_Wang1), [Jiantao Zhou](http://openreview.net/profile?id=~Jiantao_Zhou1), [Yu Qiao](http://openreview.net/profile?id=~Yu_Qiao1), [Chao Dong](http://openreview.net/profile?id=~Chao_Dong4)
  - **Affiliations:** Shanghai Artificial Intelligence Laboratory; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shanghai Artificial Intelligence Laboratory; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences; University of Macau, Shanghai Artificial Intelligence Laboratory, Kuaishou Technology, University of Macau, Shanghai Artificial Intelligence Laboratory; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences; Shanghai Artificial Intelligence Laboratory
  - **TL;DR:** The study proposes PromptGIP, a universal model for image processing that integrates various tasks such as restoration and enhancement into a single framework using a visual prompting question answering approach. This model reduces the need for task-specific finetuning and demonstrates versatility across multiple image processing tasks.
  - **Keywords:** image processing, computer vision, unified model, large language models (LLMs), pretraining, in-context learning, visual prompting question answering, image restoration, image enhancement, image feature extraction, reliance on task-specific models, low-level vision tasks, PromptGIP framework, versatile and adaptive approach to image processing


- [More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms](https://icml.cc/virtual/2024/poster/33024) (Poster)
  - **Authors:** [Hossein Zakerinia](http://openreview.net/profile?id=~Hossein_Zakerinia1), [Amin Behjati](http://openreview.net/profile?id=~Amin_Behjati1), [Christoph Lampert](http://openreview.net/profile?id=~Christoph_H._Lampert6)
  - **Affiliations:** Institute of Science and Technology Austria (ISTA), Sharif University of Technology, Institute of Science and Technology Austria (ISTA)
  - **TL;DR:** This paper introduces a flexible framework for meta-learning using PAC-Bayesian theory, allowing for direct learning of learning algorithms for future tasks. The authors demonstrate that their approach improves prediction quality in practical meta-learning scenarios.
  - **Keywords:** meta-learning, PAC-Bayesian theory, learning algorithms, generalization bounds, data sparsity, model overfitting, new framework for meta-learning, improved prediction quality


- [GeoAB: Towards Realistic Antibody Design and Reliable Affinity Maturation](https://icml.cc/virtual/2024/poster/34902) (Poster)
  - **Authors:** [Haitao Lin](http://openreview.net/profile?id=~Haitao_Lin2), [Lirong Wu](http://openreview.net/profile?id=~Lirong_Wu1), [Yufei Huang](http://openreview.net/profile?id=~Yufei_Huang4), [Yunfan Liu](http://openreview.net/profile?id=~Yunfan_Liu2), [Odin Zhang](http://openreview.net/profile?id=~Odin_Zhang1), [Yuanqing Zhou](http://openreview.net/profile?id=~YuanqZhou1), [Rui Sun](http://openreview.net/profile?id=~Rui_Sun13), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** Zhejiang University; AI Lab, Research Center for Industries of the Future, Westlake University, AI Lab, Research Center for Industries of the Future, Westlake University, AI Lab, Research Center for Industries of the Future, Westlake University, AI Lab, Research Center for Industries of the Future, Westlake University, Zhejiang University; AI Lab, Research Center for Industries of the Future, Westlake University, AI Lab, Research Center for Industries of the Future, Westlake University, AI Lab, Research Center for Industries of the Future, Westlake University, AI Lab, Research Center for Industries of the Future, Westlake University
  - **TL;DR:** The study presents GEOAB, a framework for realistic antibody design and reliable affinity maturation, addressing issues of structural authenticity and rationality in mutation effects. It demonstrates state-of-the-art performance in co-designing CDRs and predicting mutation effects, enhancing the design of antibodies with desired binding properties.
  - **Keywords:** Antibody design, Affinity maturation, Deep learning, Generative geometry initializer, Position refiner, Mutation effect prediction, Protein design, Drug discovery, Authenticity of generated structures, Rationality of affinity maturation, GEOAB framework, State-of-the-art performance in CDR co-design and mutation effect predictions, Complementarity Determining Regions (CDRs), Iterative Target Augmentation (ITA), Binding free energy (∆∆G)


- [Zeroth-Order Methods for Constrained Nonconvex Nonsmooth Stochastic Optimization](https://icml.cc/virtual/2024/poster/34103) (Oral)
  - **Authors:** [Zhuanghua Liu](http://openreview.net/profile?id=~Zhuanghua_Liu2), [Cheng Chen](http://openreview.net/profile?id=~Cheng_Chen9), [Luo Luo](http://openreview.net/profile?id=~Luo_Luo1), [Bryan Kian Hsiang Low](http://openreview.net/profile?id=~Bryan_Kian_Hsiang_Low1)
  - **Affiliations:** Department of Computer Science, National University of Singapore, Singapore; CNRS@CREATE LTD, 1 Create Way, #08-01 CREATE Tower, Singapore 138602, Shanghai Key Laboratory of Trustworthy Computing, East China Normal University, Shanghai, China, School of Data Science, Fudan University, Shanghai, China; Shanghai Key Laboratory for Contemporary Applied Mathematics, Department of Computer Science, National University of Singapore, Singapore
  - **TL;DR:** This paper presents a non-asymptotic convergence analysis for solving constrained nonconvex nonsmooth optimization problems using novel zeroth-order algorithms. The authors introduce generalized gradient mapping and δ-Frank–Wolfe gap concepts, demonstrating their effectiveness through numerical experiments.
  - **Keywords:** nonconvex optimization, nonsmooth optimization, constrained optimization, zeroth-order algorithms, generalized gradient mapping, δ-Frank–Wolfe gap, machine learning, adversarial attacks, support vector machines, Generative Adversarial Networks (GANs), non-asymptotic convergence, approximate stationarity, convergence criteria, non-asymptotic convergence guarantees, stochastic optimization algorithms, Clarke subdifferential, Goldstein δ-subdifferential, (γ, δ, ϵ)-generalized Goldstein stationary point, (δ, ϵ)-Goldstein Frank–Wolfe stationary point


- [Position: A Call to Action for a Human-Centered AutoML Paradigm](https://icml.cc/virtual/2024/poster/32753) (Poster)
  - **Authors:** [Marius Lindauer](http://openreview.net/profile?id=~Marius_Lindauer1), [Florian Karl](http://openreview.net/profile?id=~Florian_Karl1), [Anne Klier](http://openreview.net/profile?id=~Anne_Klier1), [Julia Moosbauer](http://openreview.net/profile?id=~Julia_Moosbauer1), [Alexander Tornede](http://openreview.net/profile?id=~Alexander_Tornede1), [Andreas Mueller](http://openreview.net/profile?id=~Andreas_C_Mueller1), [Frank Hutter](http://openreview.net/profile?id=~Frank_Hutter1), [Matthias Feurer](http://openreview.net/profile?id=~Matthias_Feurer2), [Bernd Bischl](http://openreview.net/profile?id=~Bernd_Bischl1)
  - **Affiliations:** Institute of Artificial Intelligence (LUH|AI), Leibniz University Hannover, Germany; L3S Research Center, Hannover, Germany, Fraunhofer Institute for Integrated Circuits IIS, Fraunhofer IIS, Nuremberg, Germany; Ludwig-Maximilians-Universität München, Munich, Germany; Munich Center for Machine Learning, Munich, Germany, Fraunhofer Institute for Integrated Circuits IIS, Fraunhofer IIS, Nuremberg, Germany, Ludwig-Maximilians-Universität München, Munich, Germany; Munich Center for Machine Learning, Munich, Germany, Institute of Artificial Intelligence (LUH|AI), Leibniz University Hannover, Germany, Microsoft, Redmond, USA, Albert-Ludwigs-Universität Freiburg, Freiburg, Germany, Ludwig-Maximilians-Universität München, Munich, Germany; Munich Center for Machine Learning, Munich, Germany, Ludwig-Maximilians-Universität München, Munich, Germany; Munich Center for Machine Learning, Munich, Germany
  - **TL;DR:** This position paper advocates for a human-centered approach in Automated Machine Learning (AutoML) research, emphasizing the importance of user interaction and diverse roles in the design of ML systems. The authors argue that addressing these aspects is crucial for unlocking the full potential of AutoML and achieving its original goals of accessibility and efficiency.
  - **Keywords:** Automated Machine Learning (AutoML), Human-Centered Design, Hyperparameter Optimization (HPO), Neural Architecture Search (NAS), Machine Learning Workflows, Data Science, Model Selection, Configuration Challenges, Democratization of ML, Collaborative Design of ML Systems, Integration of Human Expertise with AutoML


- [Adaptive Text Watermark for Large Language Models](https://icml.cc/virtual/2024/poster/34875) (Poster)
  - **Authors:** [Yepeng Liu](http://openreview.net/profile?id=~Yepeng_Liu1), [Yuheng Bu](http://openreview.net/profile?id=~Yuheng_Bu1)
  - **Affiliations:** University of Florida, University of Florida
  - **TL;DR:** This paper presents an adaptive text watermarking strategy for Large Language Models to enhance the robustness and security of AI-generated text detection. The proposed method maintains text quality while achieving comparable performance to existing watermarking techniques.
  - **Keywords:** Large Language Models, AI-generated text, watermarking, adaptive text watermarking, token distributions, semantic mapping model, AI safety, text detection, misuse of AI-generated text, robustness of watermarking, forgery, improved watermarking strategy, comparable robustness performance


- [PPFLOW: Target-Aware Peptide Design with Torsional Flow Matching](https://icml.cc/virtual/2024/poster/34898) (Poster)
  - **Authors:** [Haitao Lin](http://openreview.net/profile?id=~Haitao_Lin2), [Odin Zhang](http://openreview.net/profile?id=~Odin_Zhang1), [Huifeng Zhao](http://openreview.net/profile?id=~Huifeng_Zhao1), [Dejun Jiang](http://openreview.net/profile?id=~Dejun_Jiang2), [Lirong Wu](http://openreview.net/profile?id=~Lirong_Wu1), [Zicheng Liu](http://openreview.net/profile?id=~Zicheng_Liu2), [Yufei Huang](http://openreview.net/profile?id=~Yufei_Huang4), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** Zhejiang University; AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, Zhejiang University, Zhejiang University, Zhejiang University, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China
  - **TL;DR:** The study introduces PPFLOW, a target-aware peptide design method utilizing conditional flow matching to enhance peptide drug discovery. It establishes the PPBench2024 dataset and demonstrates that PPFLOW achieves state-of-the-art performance in peptide drug generation and optimization.
  - **Keywords:** Therapeutic peptides, AI-assisted peptide drug discovery, structure-based drug design, Conditional flow matching, deep learning methods, Peptide drug generation, optimization, docking, side-chain packing, Challenges in target-aware peptide drug design, generating valid peptides, optimizing natural peptides, PPFLOW method, PPBench2024 dataset, PPBench2024


- [Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras](https://icml.cc/virtual/2024/poster/35141) (Poster)
  - **Authors:** [Tzu-Yuan Lin](http://openreview.net/profile?id=~Tzu-Yuan_Lin1), [Minghan Zhu](http://openreview.net/profile?id=~Minghan_Zhu1), [Maani Ghaffari](http://openreview.net/profile?id=~Maani_Ghaffari1)
  - **Affiliations:** University of Michigan, Ann Arbor, MI, USA, University of Michigan, Ann Arbor, MI, USA, University of Michigan, Ann Arbor, MI, USA
  - **TL;DR:** This paper introduces Lie Neurons, an adjoint-equivariant neural network framework designed for finite-dimensional semisimple Lie algebras, enhancing modeling capabilities through novel layers. The proposed network demonstrates strong performance across various tasks, including system dynamics learning and shape classification.
  - **Keywords:** equivariant neural networks, Lie algebras, adjoint-equivariant networks, Lie bracket layers, geometric channel mixing layers, control theory, robotics, computer vision, graphics, data representation in Lie algebras, modeling continuous symmetries, generalization of Vector Neurons, competitive performance in various domains, SO(3)-equivariant, semisimple Lie algebra, Killing form


- [The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks](https://icml.cc/virtual/2024/poster/35089) (Poster)
  - **Authors:** [Ziquan Liu](http://openreview.net/profile?id=~Ziquan_Liu1), [Yufei Cui](http://openreview.net/profile?id=~Yufei_CUI2), [Yan Yan](http://openreview.net/profile?id=~Yan_Yan3), [Yi Xu](http://openreview.net/profile?id=~Yi_Xu8), [Xiangyang Ji](http://openreview.net/profile?id=~Xiangyang_Ji1), [Xue Liu](http://openreview.net/profile?id=~Xue_Liu1), [Antoni Chan](http://openreview.net/profile?id=~Antoni_B._Chan1)
  - **Affiliations:** Queen Mary University of London, McGill University; Mila, Washington State University, Dalian University of Technology, Tsinghua University, McGill University, City University of Hong Kong
  - **TL;DR:** This study investigates the uncertainty of deep learning models through conformal prediction in the context of adversarial attacks, highlighting the necessity of adversarial training for effective uncertainty quantification. The proposed Uncertainty-Reducing AT (AT-UR) method improves prediction set size, demonstrating its effectiveness across multiple datasets.
  - **Keywords:** Adversarial robustness, Uncertainty quantification, Conformal prediction, Adversarial training (AT), Beta-weighting loss, Entropy minimization regularizer, Medical imaging, Autonomous driving, Predictive uncertainty, Informative prediction sets, Adversarial attacks, Uncertainty-Reducing AT (AT-UR), Improved prediction set size (PSS), CIFAR dataset, l∞-norm bounded attack, PGD-based attacks, Non-i.i.d. settings


- [Bidirectional Reciprocative Information Communication for Few-Shot Semantic Segmentation](https://icml.cc/virtual/2024/poster/32827) (Poster)
  - **Authors:** [Yuanwei Liu](http://openreview.net/profile?id=~Yuanwei_Liu1), [Junwei Han](http://openreview.net/profile?id=~Junwei_Han1), [Xiwen Yao](http://openreview.net/profile?id=~Xiwen_Yao1), [Salman Khan](http://openreview.net/profile?id=~Salman_Khan4), [Hisham Cholakkal](http://openreview.net/profile?id=~Hisham_Cholakkal2), [Rao Anwer](http://openreview.net/profile?id=~Rao_Muhammad_Anwer2), [Nian Liu](http://openreview.net/profile?id=~Nian_Liu1), [Fahad Khan](http://openreview.net/profile?id=~Fahad_Shahbaz_Khan1)
  - **Affiliations:** Northwestern Polytechnical University; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Northwestern Polytechnical University; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Northwestern Polytechnical University, Mohamed bin Zayed University of Artificial Intelligence; Australian National University, Mohamed bin Zayed University of Artificial Intelligence, Mohamed bin Zayed University of Artificial Intelligence, Mohamed bin Zayed University of Artificial Intelligence, Mohamed bin Zayed University of Artificial Intelligence; CVL, Linkoping University
  - **TL;DR:** This study introduces a novel approach for few-shot semantic segmentation that incorporates bidirectional information flow between query and support, addressing intra-class diversity and enhancing segmentation accuracy. The proposed method, validated on PASCAL-5i and COCO-20i datasets, demonstrates significant improvements in performance through the integration of feedback and rectification mechanisms.
  - **Keywords:** Few-shot semantic segmentation, computer vision, Query Feedback Branch (QFB), Query Amplifier Branch (QAB), Query Rectification Module (QRM), Image segmentation, Intra-class diversity, data sparsity, Bidirectional information flow, query-adaptive support information, PASCAL-5i, COCO-20i


- [DIDI: Diffusion-Guided Diversity for Offline Behavioral Generation](https://icml.cc/virtual/2024/poster/34859) (Poster)
  - **Authors:** [Jinxin Liu](http://openreview.net/profile?id=~Jinxin_Liu1), [Xinghong Guo](http://openreview.net/profile?id=~Xinghong_Guo1), [Zifeng Zhuang](http://openreview.net/profile?id=~Zifeng_Zhuang1), [Donglin Wang](http://openreview.net/profile?id=~Donglin_Wang1)
  - **Affiliations:** School of Engineering, Westlake University, Hangzhou, China; Zhejiang University, Hangzhou, China, School of Engineering, Westlake University, Hangzhou, China, School of Engineering, Westlake University, Hangzhou, China, School of Engineering, Westlake University, Hangzhou, China; Institute of Advanced Technology, Westlake Institute for Advanced Study, Hangzhou, China
  - **TL;DR:** The study introduces DIDI, a novel approach for offline behavioral generation that leverages diffusion probabilistic models to learn diverse skills from label-free data. Experimental results demonstrate its effectiveness in discovering diverse and optimal behaviors across various decision-making domains.
  - **Keywords:** offline behavioral generation, skill learning, diversity in behaviors, diffusion probabilistic models, contextual policy, reinforcement learning, decision-making domains, robotics, learning from label-free offline data, multimodality in datasets, suboptimal data challenges, DIDI approach, skill stitching, skill interpolation, reward-guided behavior generation, D4RL tasks


- [Decoding-time Realignment of Language Models](https://icml.cc/virtual/2024/poster/33140) (Spotlight Poster)
  - **Authors:** [Tianlin Liu](http://openreview.net/profile?id=~Tianlin_Liu2), [Shangmin Guo](http://openreview.net/profile?id=~Shangmin_Guo1), [Leonardo Martins Bianco](http://openreview.net/profile?id=~Leonardo_Bianco1), [Daniele Calandriello](http://openreview.net/profile?id=~Daniele_Calandriello1), [Quentin Berthet](http://openreview.net/profile?id=~Quentin_Berthet2), [Felipe Llinares-Lopez](http://openreview.net/profile?id=~Felipe_Llinares-L%C3%B3pez1), [Jessica Hoffmann](http://openreview.net/profile?id=~Jessica_Hoffmann1), [Lucas Dixon](http://openreview.net/profile?id=~Lucas_Dixon1), [Michal Valko](http://openreview.net/profile?id=~Michal_Valko1), [Mathieu Blondel](http://openreview.net/profile?id=~Mathieu_Blondel1)
  - **Affiliations:** University of Basel, University of Edinburgh, Université Paris-Saclay, Google DeepMind, Google DeepMind, Google DeepMind, Google Research, Google Research, Google DeepMind, Google DeepMind
  - **TL;DR:** This study introduces decoding-time realignment (DeRa), a method for adjusting the alignment of language models with human preferences without the need for retraining. It addresses the challenge of selecting optimal regularization strengths, enhancing the efficiency of hyperparameter tuning while maintaining model capabilities.
  - **Keywords:** Language model alignment, Human preferences, Reinforcement learning from human feedback (RLHF), Proximity regularization, Kullback-Leibler (KL) divergence, Errors and biases in language models, Reward hacking, Regularization strength optimization, Decoding-time realignment (DeRa), Hyperparameter tuning efficiency


- [Preference Optimization for Molecule Synthesis with Conditional Residual Energy-based Models](https://icml.cc/virtual/2024/poster/33088) (Oral)
  - **Authors:** [Songtao Liu](http://openreview.net/profile?id=~Songtao_Liu2), [Hanjun Dai](http://openreview.net/profile?id=~Hanjun_Dai1), [Yue Zhao](http://openreview.net/profile?id=~Yue_Zhao13), [Peng Liu](http://openreview.net/profile?id=~Peng_Liu3)
  - **Affiliations:** The Pennsylvania State University, Google DeepMind, University of Southern California, The Pennsylvania State University
  - **TL;DR:** This study presents a framework using conditional residual energy-based models to optimize molecule synthesis routes in drug discovery, addressing limitations of existing strategies by focusing on criteria such as material costs and yields. The proposed method consistently improves the quality of synthetic routes and outperforms previous state-of-the-art accuracy by 2.5%.
  - **Keywords:** Molecule synthesis, Drug discovery, Conditional residual energy-based models (EBMs), One-step retrosynthesis models, Chemical manufacturing, Synthetic route generation, Limitations in synthetic route generation, Greedy selection of molecules, Evaluation of synthetic routes based on criteria, Enhanced quality of synthetic routes, Improved top-1 accuracy, USPTO-full (reaction database), Retrosynthetic planning, Synthetic routes


- [ELTA: An Enhancer against Long-Tail for Aesthetics-oriented Models](https://icml.cc/virtual/2024/poster/33539) (Poster)
  - **Authors:** [Limin Liu](http://openreview.net/profile?id=~Limin_Liu1), [Shuai He](http://openreview.net/profile?id=~Shuai_He2), [Anlong Ming](http://openreview.net/profile?id=~Anlong_Ming1), [Rui Xie](http://openreview.net/profile?id=~Rui_Xie5), [Huadong Ma](http://openreview.net/profile?id=~Huadong_Ma1)
  - **Affiliations:** School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China, School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China, School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China, School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China, School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications, Beijing, China
  - **TL;DR:** This study introduces ELTA, a method designed to address the long-tailed distribution problem in Image Aesthetics Assessment (IAA) by enhancing minority feature representation and aligning features with labels. The proposed method demonstrates state-of-the-art performance across multiple datasets, highlighting its effectiveness in mitigating biases in aesthetic evaluation models.
  - **Keywords:** Image Aesthetics Assessment (IAA), Long-tailed distributions, Mixup technique, Similarity consistency approach, Computational photography, Art design, Recommender systems, Data imbalance, Distribution mismatch, Model bias, Enhancer against Long-Tail for Aesthetics-oriented models (ELTA), State-of-the-art performance, AVA, AADB, TAD66K, PARA


- [PAPM: A Physics-aware Proxy Model for Process Systems](https://icml.cc/virtual/2024/poster/34037) (Poster)
  - **Authors:** [Pengwei Liu](http://openreview.net/profile?id=~Pengwei_Liu1), [Zhongkai Hao](http://openreview.net/profile?id=~Zhongkai_Hao1), [Xingyu Ren](http://openreview.net/profile?id=~Xingyu_Ren2), [Hangjie Yuan](http://openreview.net/profile?id=~Hangjie_Yuan1), [Jiayang Ren](http://openreview.net/profile?id=~Jiayang_Ren1), [Dong Ni](http://openreview.net/profile?id=~Dong_Ni3)
  - **Affiliations:** Zhejiang University, Hangzhou, Zhejiang, China, Tsinghua University, Beijing, China, Zhejiang University, Hangzhou, Zhejiang, China, Zhejiang University, Hangzhou, Zhejiang, China, University of British Columbia, Vancouver, BC, Canada, Zhejiang University, Hangzhou, Zhejiang, China
  - **TL;DR:** The study introduces a physics-aware proxy model (PAPM) that integrates partial physics knowledge to improve generalization and reduce training costs in process systems modeling. PAPM demonstrates a 6.7% performance improvement over existing methods while requiring significantly fewer computational resources.
  - **Keywords:** physics-aware modeling, proxy modeling, process systems, physics-informed deep learning (PIDL), physics-aware models, PINNs, PI-DeepONet, PINO, computational modeling, simulation, engineering domains, high training costs, limited generalization capabilities, dependence on extensive labeled datasets, physics-aware proxy model (PAPM), performance improvement, fewer FLOPs, reduced parameters


- [Unlock the Cognitive Generalization of Deep Reinforcement Learning via Granular Ball Representation](https://icml.cc/virtual/2024/poster/34653) (Poster)
  - **Authors:** [Jiashun Liu](http://openreview.net/profile?id=~Jiashun_Liu1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Yi Ma](http://openreview.net/profile?id=~Yi_Ma5), [Shuyin Xia](http://openreview.net/profile?id=~Shuyin_Xia1)
  - **Affiliations:** COLLAGE OF INTELLIGENCE AND COMPUTING, Tianjin University, China, COLLAGE OF INTELLIGENCE AND COMPUTING, Tianjin University, China, COLLAGE OF INTELLIGENCE AND COMPUTING, Tianjin University, China, CQUPT, China
  - **TL;DR:** This paper proposes a framework to enhance cognitive generalization in deep reinforcement learning, enabling effective policy transfer from simple to complex scenarios through reward-agnostic fine-tuning. The proposed Granular Ball Reinforcement Learning (GBRL) algorithm demonstrates successful policy generalization across various challenging tasks with the same underlying logic.
  - **Keywords:** Cognitive generalization, Deep reinforcement learning (DRL), Granular Ball Reinforcement Learning (GBRL), Variational Autoencoder (VAE), Low generalization ability, Need for reward functions in complex scenarios, Effective policy generalization, Reward-agnostic fine-tuning, Systematic generalization


- [Online Speculative Decoding](https://icml.cc/virtual/2024/poster/34724) (Poster)
  - **Authors:** [Xiaoxuan Liu](http://openreview.net/profile?id=~Xiaoxuan_Liu2), [Lanxiang Hu](http://openreview.net/profile?id=~Lanxiang_Hu1), [Peter Bailis](http://openreview.net/profile?id=~Peter_Bailis2), [Alvin Cheung](http://openreview.net/profile?id=~Alvin_Cheung2), [Zhijie Deng](http://openreview.net/profile?id=~Zhijie_Deng1), [Ion Stoica](http://openreview.net/profile?id=~Ion_Stoica1), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang2)
  - **Affiliations:** UC Berkeley, UCSD, Google Inc., UC Berkeley, SJTU, UC Berkeley, UCSD
  - **TL;DR:** This paper introduces online speculative decoding (OSD) to enhance the efficiency of large language models by continuously updating a smaller draft model based on user query data. The method significantly improves token acceptance rates and reduces latency, demonstrating its effectiveness in optimizing LLM serving for online applications.
  - **Keywords:** Speculative decoding, Large language models (LLMs), Knowledge distillation, Online services, Chatbots, Virtual assistants, High latency in LLM serving, Low predictive accuracy of draft models, Online speculative decoding (OSD), Token acceptance rate improvement, Latency reduction, Draft model, Target model, Token probability distribution


- [On the Feasibility of Single-Pass Full-Capacity Learning in Linear Threshold Neurons with Binary Input Vectors](https://icml.cc/virtual/2024/poster/33223) (Poster)
  - **Authors:** [Ruipeng Liu](http://openreview.net/profile?id=~Ruipeng_Liu1), [Borui He](http://openreview.net/profile?id=~Borui_He1), [Naveed Tahir](http://openreview.net/profile?id=~Naveed_Tahir1), [Garrett Katz](http://openreview.net/profile?id=~Garrett_Ethan_Katz1)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, New York, USA, Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, New York, USA, Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, New York, USA, Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, New York, USA
  - **TL;DR:** This study investigates the feasibility of single-pass learning rules that achieve full capacity in linear threshold neurons with binary input vectors. The authors derive a necessary condition for such learning rules and demonstrate that it is mathematically impossible for them to exist when the input dimension is 8 or greater.
  - **Keywords:** single-pass learning, full-capacity learning, span rules, linear threshold neurons, low storage capacity, computational complexity, impossibility result for single-pass full-capacity learning rules, linear associative networks, Hopfield networks, perceptron learning


- [Multi-Source Conformal Inference Under Distribution Shift](https://icml.cc/virtual/2024/poster/32978) (Poster)
  - **Authors:** [Yi Liu](http://openreview.net/profile?id=~Yi_Liu49), [Alexander Levis](http://openreview.net/profile?id=~Alexander_W._Levis1), [Sharon-Lise Normand](http://openreview.net/profile?email=larryhan%40fas.harvard.edu), [Larry Han](http://openreview.net/profile?id=~Larry_Han1)
  - **Affiliations:** North Carolina State University, Department of Statistics, Raleigh, NC, USA, Carnegie Mellon University, Department of Statistics, Pittsburgh, PA, USA, Harvard Medical School, Department of Health Care Policy, Boston, MA, USA, Northeastern University, Department of Health Sciences, Boston, MA, USA
  - **TL;DR:** This study addresses the challenge of obtaining distribution-free prediction intervals in multi-source environments affected by distribution shifts and privacy concerns. The authors propose a data-adaptive strategy to enhance efficiency and reduce bias, demonstrating its application in predicting hospital length of stay for pediatric patients undergoing high-risk cardiac surgery.
  - **Keywords:** Multi-source data integration, Conformal inference, Distribution shift, Machine learning prediction algorithms, Efficient influence functions, Healthcare, Pediatric cardiac surgery, Distribution shifts, Privacy concerns, Uncertainty quantification, Distribution-free prediction intervals, Data-adaptive strategies, Conformal scores, Covariate shift, Label shift


- [Convergence of Online Learning Algorithm for a Mixture of Multiple Linear Regressions](https://icml.cc/virtual/2024/poster/33947) (Poster)
  - **Authors:** [Yujing Liu](http://openreview.net/profile?id=~Yujing_Liu2), [Zhixin Liu](http://openreview.net/profile?id=~Zhixin_Liu2), [Lei Guo](http://openreview.net/profile?id=~Lei_Guo11)
  - **Affiliations:** The Key Laboratory of Systems and Control, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China; School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China, The Key Laboratory of Systems and Control, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China; School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China, The Key Laboratory of Systems and Control, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China; School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This paper presents a novel online learning algorithm for parameter estimation in mixed linear regression models, addressing the limitations of previous offline methods. The proposed algorithm demonstrates almost sure convergence without the i.i.d. assumption and provides a convergence rate analysis, enhancing the performance of online data clustering.
  - **Keywords:** Mixed Linear Regression, Online Learning, Data Clustering, Expectation Maximization (EM), Ljung’s ODE method, Lyapunov stability theorem, Stochastic Lyapunov function method, Statistical Learning, System Identification, Computer Science, Parameter Estimation, Nonlinear Relationships, Unknown Linear Regression Models, Almost Sure Convergence Results, Convergence Rate Analysis


- [Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-Loop and Hessian-Free Solution Strategy](https://icml.cc/virtual/2024/poster/32946) (Poster)
  - **Authors:** [Risheng Liu](http://openreview.net/profile?id=~Risheng_Liu1), [Zhu Liu](http://openreview.net/profile?id=~Zhu_Liu3), [Wei Yao](http://openreview.net/profile?id=~Wei_Yao3), [Shangzhi Zeng](http://openreview.net/profile?id=~Shangzhi_Zeng1), [Jin Zhang](http://openreview.net/profile?id=~Jin_Zhang8)
  - **Affiliations:** School of Software Technology, Dalian University of Technology, Dalian, China; Pazhou Laboratory (Huangpu), Guangzhou, China, School of Software Technology, Dalian University of Technology, Dalian, China, National Center for Applied Mathematics Shenzhen, Southern University of Science and Technology, Shenzhen, China; Department of Mathematics, Southern University of Science and Technology, Shenzhen, China, Department of Mathematics and Statistics, University of Victoria, Victoria, British Columbia, Canada; National Center for Applied Mathematics Shenzhen, Southern University of Science and Technology, Shenzhen, China, Department of Mathematics, Southern University of Science and Technology, Shenzhen, China; National Center for Applied Mathematics Shenzhen, Southern University of Science and Technology, Shenzhen, China
  - **TL;DR:** This study presents a novel single-loop gradient-based algorithm for large-scale nonconvex Bi-Level Optimization (BLO) problems, addressing computational efficiency and theoretical guarantees. The proposed method demonstrates superior performance in various synthetic problems and real-world applications, particularly in hyper-parameter learning and neural architecture search.
  - **Keywords:** Bi-Level Optimization (BLO), Nonconvex Optimization, Single-loop gradient-based algorithm, Moreau envelope-based reformulation, Hyper-parameter optimization, Neural architecture search, Computational efficiency, Theoretical guarantees, Nonconvexity in optimization, Non-asymptotic convergence analysis, Enhanced practicality and efficiency for large-scale tasks


- [Position: Foundation Agents as the Paradigm Shift for Decision Making](https://icml.cc/virtual/2024/poster/33278) (Poster)
  - **Authors:** [Xiaoqian Liu](http://openreview.net/profile?id=~Xiaoqian_Liu1), [Xingzhou Lou](http://openreview.net/profile?id=~Xingzhou_Lou1), [Jianbin Jiao](http://openreview.net/profile?id=~Jianbin_Jiao1), [Junge Zhang](http://openreview.net/profile?id=~Junge_Zhang1)
  - **Affiliations:** School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China, School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China, School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This paper advocates for the development of foundation agents as a transformative approach to decision making, highlighting their potential to overcome challenges faced by traditional methods through rapid adaptation and enhanced reasoning capabilities. The authors outline a roadmap for implementing these agents, emphasizing their applicability across various domains and the need for alignment with large language models.
  - **Keywords:** Foundation agents, decision making, paradigm shift, Reinforcement learning (RL), imitation learning (IL), self-supervised pretraining, Traffic control, energy management, drug discovery, robotics, healthcare, Low sample efficiency, poor generalization, environment stochasticity, uncertainty, Roadmap for foundation agents, knowledge and value alignment with LLMs, Large language models (LLMs), multi-modality perception, few-shot generalization


- [Floating Anchor Diffusion Model for Multi-motif Scaffolding](https://icml.cc/virtual/2024/poster/34654) (Poster)
  - **Authors:** [Ke Liu](http://openreview.net/profile?id=~Ke_Liu3), [Weian Mao](http://openreview.net/profile?id=~Weian_Mao2), [Shuaike Shen](http://openreview.net/profile?id=~Shuaike_Shen1), [Xiaoran Jiao](http://openreview.net/profile?id=~Xiaoran_Jiao1), [Zheng Sun](http://openreview.net/profile?id=~Zheng_Sun7), [Hao Chen](http://openreview.net/profile?id=~Hao_Chen17), [Chunhua Shen](http://openreview.net/profile?id=~Chunhua_Shen2)
  - **Affiliations:** Zhejiang University, China, The University of Adelaide, Australia, Zhejiang University, China, Zhejiang University, China, Swansea University, UK, Zhejiang University, China; Ant Group, Zhejiang University, China
  - **TL;DR:** The study introduces the Floating Anchor Diffusion (FADiff) model for motif scaffolding, allowing for the flexible positioning of multiple motifs in protein design without prior knowledge of their relative positions. The model demonstrates high success rates and the ability to create novel scaffold structures, which is significant for applications in vaccines and enzymes.
  - **Keywords:** motif scaffolding, protein design, multi-function proteins, Floating Anchor Diffusion (FADiff), diffusion models, vaccines, enzymes, drug design, scaffolding multiple motifs, fixed motif positions, lack of prior knowledge on motif positions, novel scaffold designs, high success rates in motif scaffolding


- [Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion](https://icml.cc/virtual/2024/poster/33389) (Poster)
  - **Authors:** [Xuantong Liu](http://openreview.net/profile?id=~Xuantong_LIU1), [Tianyang Hu](http://openreview.net/profile?id=~Tianyang_Hu1), [Wenjia Wang](http://openreview.net/profile?id=~Wenjia_Wang2), [Kenji Kawaguchi](http://openreview.net/profile?id=~Kenji_Kawaguchi1), [Yuan Yao](http://openreview.net/profile?id=~Yuan_Yao1)
  - **Affiliations:** The Hong Kong University of Science and Technology, Huawei Noah’s Ark Lab, Hong Kong University of Science and Technology; Hong Kong University of Science and Technology (Guangzhou), National University of Singapore, The Hong Kong University of Science and Technology
  - **TL;DR:** This study presents a novel approach to enhance controllability in text-to-image generation by inverting Vision-Language Models, proposing a training-free method that optimizes images directly with VLM supervision. The results demonstrate improved text-image alignment and high-quality image generation, achieving near state-of-the-art performance.
  - **Keywords:** text-to-image generation, controllability, conditional generation, Diffusion Probabilistic Models (DPMs), Vision-Language Models (VLMs), Score Distillation Sampling, Artificial Intelligence Generated Content (AIGC), alignment challenge, compatibility between generated image and input text, training-free approach, improved text-image alignment, high-quality image generation, T2I-Compbench, BLIP-2


- [From Generalization Analysis to Optimization Designs for State Space Models](https://icml.cc/virtual/2024/poster/33855) (Poster)
  - **Authors:** [Fusheng Liu](http://openreview.net/profile?id=~Fusheng_Liu1), [Qianxiao Li](http://openreview.net/profile?id=~Qianxiao_Li1)
  - **Affiliations:** Department of Mathematics, National University of Singapore; Institute of Data Science, National University of Singapore, Department of Mathematics, National University of Singapore
  - **TL;DR:** This paper investigates the generalization of State Space Models (SSMs) in sequence modeling and proposes enhancements to training algorithms based on a newly established generalization bound. The findings include a scaling rule for model initialization and a new regularization method that improve the robustness and generalization performance of SSMs.
  - **Keywords:** State Space Models, Generalization in Sequence Modeling, Generalization Bound, Model Initialization, Regularization Method, Time Series Analysis, Sequence Modeling, Generalization Performance, Temporal Dependencies, Improved Training Algorithms, Robustness of Output Value Scales, SSM (State Space Model), RNN (Recurrent Neural Network), HiPPO Framework


- [Stereo Risk: A Continuous Modeling Approach to Stereo Matching](https://icml.cc/virtual/2024/poster/34231) (Oral)
  - **Authors:** [Ce Liu](http://openreview.net/profile?id=~Ce_Liu3), [Suryansh Kumar](http://openreview.net/profile?id=~Suryansh_Kumar1), [Shuhang Gu](http://openreview.net/profile?id=~Shuhang_Gu3), [Radu Timofte](http://openreview.net/profile?id=~Radu_Timofte1), [Yao Yao](http://openreview.net/profile?id=~Yao_Yao1), [Luc Van Gool](http://openreview.net/profile?id=~Luc_Van_Gool1)
  - **Affiliations:** Nanjing University, China; ETH Zürich, Switzerland, VCCM, Texas A&M University, USA, UESTC, China, University of Würzburg, Germany, Nanjing University, China, ETH Zürich, Switzerland; KU Leuven, Belgium; INSAIT, Bulgaria
  - **TL;DR:** The study introduces "Stereo Risk," a novel deep-learning method for stereo matching that formulates scene disparity as a solution to a continuous risk minimization problem, improving performance over traditional discretization methods. The method demonstrates superior results across various benchmark datasets, particularly in capturing high-frequency details.
  - **Keywords:** stereo matching, deep learning, L1 minimization, continuous risk minimization, computer vision, autonomous navigation, robotic vision, per-pixel disparity estimation, discretization of scene disparity values, enhanced stereo-matching performance, fully differentiable network, KITTI 2012, KITTI 2015, ETH3D, SceneFlow, Middlebury 2014, disparity map, multi-modal probability distributions


- [Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents](https://icml.cc/virtual/2024/poster/34247) (Poster)
  - **Authors:** [Zhihan Liu](http://openreview.net/profile?id=~Zhihan_Liu1), [Hao Hu](http://openreview.net/profile?id=~Hao_Hu3), [Shenao Zhang](http://openreview.net/profile?id=~Shenao_Zhang1), [Hongyi Guo](http://openreview.net/profile?id=~Hongyi_Guo1), [Shuqi Ke](http://openreview.net/profile?id=~Shuqi_Ke1), [Boyi Liu](http://openreview.net/profile?id=~Boyi_Liu1), [Zhaoran Wang](http://openreview.net/profile?id=~Zhaoran_Wang1)
  - **Affiliations:** Northwestern University, USA, Institute for Interdisciplinary Information Sciences, Tsinghua University, China, Northwestern University, USA, Northwestern University, USA, The Chinese University of Hong Kong, China, Northwestern University, USA, Northwestern University, USA
  - **TL;DR:** This paper proposes a framework called "reason for future, act for now" (RAFA) to enhance the interaction of autonomous LLM agents with the real world by optimizing actions through iterative reasoning and feedback. The framework provides provable regret guarantees and demonstrates superior empirical performance in minimizing interactions with the environment.
  - **Keywords:** Large Language Models, Autonomous Agents, Reasoning and Acting, Bayesian Adaptive Markov Decision Processes, Actor-Critic Update, Interactive Decision-Making Tasks, Reasoning-Acting Gap, Sample Efficiency, Costly and Risky Interactions, Framework with Provable Regret Guarantees, Optimal Trajectory Generation


- [Tuning-free Estimation and Inference of Cumulative Distribution Function under Local Differential Privacy](https://icml.cc/virtual/2024/poster/35166) (Poster)
  - **Authors:** [Yi Liu](http://openreview.net/profile?id=~Yi_Liu13), [Qirui Hu](http://openreview.net/profile?id=~Qirui_Hu1), [Linglong Kong](http://openreview.net/profile?id=~Linglong_Kong2)
  - **Affiliations:** Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Canada, Center for Statistical Science, Department of Industrial Engineering, Tsinghua University, Beijing, China, Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Canada; Center for Statistical Science, Department of Industrial Engineering, Tsinghua University, Beijing, China
  - **TL;DR:** This study presents a novel algorithm for estimating CDF values under Local Differential Privacy by leveraging a connection to the current status problem, achieving robust error bounds and efficient computation without the need for hyperparameters. The findings highlight the potential for improved privacy-preserving statistical techniques in data analysis.
  - **Keywords:** Local Differential Privacy, Cumulative Distribution Function (CDF) estimation, Constrained isotonic estimation, binary queries, Privacy-preserving statistical techniques, Privacy concerns, data exploitation, re-identification risks, Uniform and L2 error bounds, asymptotic normal distribution of estimator, Current status problem, mathematical proofs, numerical testing


- [Class-Imbalanced Graph Learning without Class Rebalancing](https://icml.cc/virtual/2024/poster/33042) (Poster)
  - **Authors:** [Zhining Liu](http://openreview.net/profile?id=~Zhining_Liu1), [Ruizhong Qiu](http://openreview.net/profile?id=~Ruizhong_Qiu1), [Zhichen Zeng](http://openreview.net/profile?id=~Zhichen_Zeng1), [Hyunsik Yoo](http://openreview.net/profile?id=~Hyunsik_Yoo1), [David Zhou](http://openreview.net/profile?id=~David_Zhou1), [Zhe Xu](http://openreview.net/profile?id=~Zhe_Xu5), [Yada Zhu](http://openreview.net/profile?id=~Yada_Zhu1), [Kommy Weldemariam](http://openreview.net/profile?id=~Kommy_Weldemariam1), [Jingrui He](http://openreview.net/profile?id=~Jingrui_He1), [Hanghang Tong](http://openreview.net/profile?id=~Hanghang_Tong3)
  - **Affiliations:** University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, IBM Research, Amazon Science, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign
  - **TL;DR:** This study addresses the challenges of class imbalance in graph learning by introducing a topological augmentation framework (BAT) that mitigates predictive bias without class rebalancing. The proposed method significantly improves performance and reduces bias in imbalanced graph learning tasks, demonstrating up to 46.27% performance gain and 72.74% bias reduction.
  - **Keywords:** Class imbalance, Graph learning, Node classification, Graph Neural Networks (GNNs), Topological augmentation, Real-world graph applications, Imbalanced graph learning tasks, Class imbalance, Predictive bias, Performance degradation on minority classes, Topological augmentation framework (BAT), Performance gain, Bias reduction, Ambivalent message-passing (AMP), Distant message-passing (DMP), Class-rebalancing (CR), Topological imbalance


- [Generative Marginalization Models](https://icml.cc/virtual/2024/poster/33806) (Poster)
  - **Authors:** [Sulin Liu](http://openreview.net/profile?id=~Sulin_Liu1), [Peter Ramadge](http://openreview.net/profile?id=~Peter_Ramadge1), [Ryan P. Adams](http://openreview.net/profile?id=~Ryan_P_Adams1)
  - **Affiliations:** Princeton University, Princeton University, Princeton University
  - **TL;DR:** This paper introduces Marginalization Models (MAMs), a new family of generative models that efficiently approximate arbitrary marginal probabilities in high-dimensional discrete data. MAMs significantly improve scalability and speed in generative modeling tasks, enabling effective applications across various domains such as image generation and protein design.
  - **Keywords:** generative models, high-dimensional discrete data, marginalization models (MAMs), energy-based training, maximum likelihood estimation (MLE), image generation, audio synthesis, natural language modeling, scientific discovery, outlier detection, masked language modeling, image inpainting, protein/molecule design, arbitrary marginal inference, scalability bottleneck, efficient probabilistic inference, scalable methods for learning marginals, marginalization self-consistency, neural autoregressive models (ARMs), unnormalized log-probability function


- [Causal Discovery via Conditional Independence Testing with Proxy Variables](https://icml.cc/virtual/2024/poster/33230) (Poster)
  - **Authors:** [Mingzhou Liu](http://openreview.net/profile?id=~Mingzhou_Liu1), [Xinwei Sun](http://openreview.net/profile?id=~Xinwei_Sun1), [YU QIAO](http://openreview.net/profile?id=~Yu_QIAO3), [Yizhou Wang](http://openreview.net/profile?id=~Yizhou_Wang1)
  - **Affiliations:** School of Computer Science, Peking University; Center on Frontiers of Computing Studies (CFCS), Peking University, Sch. of Data Science, Fudan University, Dep. of Automation, Shanghai Jiao Tong University, Center on Frontiers of Computing Studies (CFCS), Peking University; Inst. for Artificial Intelligence, Peking University; Nat’l Eng. Research Center of Visual Technology, Peking University; Nat’l Key Lab of General Artificial Intelligence, Peking University
  - **TL;DR:** This paper presents a novel hypothesis-testing procedure for identifying causal relationships between continuous variables without parametric assumptions, addressing biases introduced by unobserved variables. The proposed method utilizes discretization to establish identifiable linear equations under the causal null hypothesis, validated through synthetic and real-world data.
  - **Keywords:** Causal discovery, Conditional independence testing, Hypothesis-testing procedure, Discretization, Economics, Healthcare, Policy formulation, Unobserved variables, Latent confounders, Bias in causal inference, Novel testing procedure for causal relationships, Identifiable coefficient vector, Proxy variables, Causal null hypothesis


- [Symmetric Matrix Completion with ReLU Sampling](https://icml.cc/virtual/2024/poster/33895) (Poster)
  - **Authors:** [Huikang Liu](http://openreview.net/profile?id=~Huikang_Liu2), [Peng Wang](http://openreview.net/profile?id=~Peng_Wang23), [Longxiu Huang](http://openreview.net/profile?id=~Longxiu_Huang1), [Qing Qu](http://openreview.net/profile?id=~Qing_Qu2), [Laura Balzano](http://openreview.net/profile?id=~Laura_Balzano1)
  - **Affiliations:** Antai College of Economics and Management, Shanghai Jiao Tong University, Department of Electrical Engineering and Computer Science, University of Michigan, Department of Computational Mathematics, Science and Engineering & Department of Mathematics, Michigan State University, Department of Electrical Engineering and Computer Science, University of Michigan, Department of Electrical Engineering and Computer Science, University of Michigan
  - **TL;DR:** This study investigates symmetric positive semi-definite low-rank matrix completion with entry-dependent sampling, particularly focusing on ReLU sampling. The authors demonstrate that while gradient descent may converge to suboptimal points, they provide a tailored initialization method that consistently leads to global minima.
  - **Keywords:** low-rank matrix completion, symmetric positive semi-definite matrices, gradient descent, ReLU sampling, threshold-based sampling, recommendation systems, bioinformatics, computer vision, covariance matrix completion, entry-dependent sampling, nonconvex optimization, convergence to stationary points, geodesically strong convexity, initialization for gradient descent


- [Perfect Alignment May be Poisonous to Graph Contrastive Learning](https://icml.cc/virtual/2024/poster/32744) (Poster)
  - **Authors:** [Jingyu Liu](http://openreview.net/profile?id=~Jingyu_Liu4), [Huayi Tang](http://openreview.net/profile?id=~Huayi_Tang1), [Yong Liu](http://openreview.net/profile?id=~Yong_Liu7)
  - **Affiliations:** Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; None, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; None, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China
  - **TL;DR:** This study investigates the impact of data augmentation on Graph Contrastive Learning (GCL) and finds that while perfect alignment can help contrastive loss, it may hinder generalization and downstream performance. The authors propose that specifically designed augmentations are necessary to achieve optimal results in GCL.
  - **Keywords:** Graph Contrastive Learning, Node Representation Learning, Data Augmentation, Contrastive Learning, Graph Neural Networks, Representation Learning, Generalization, Downstream Performance, Class Separation, Theoretical Analysis, Augmentation Design, Positive Pairs, Negative Samples, Graph Spectrum Theory


- [Energy-Guided Diffusion Sampling for Offline-to-Online Reinforcement Learning](https://icml.cc/virtual/2024/poster/33372) (Poster)
  - **Authors:** [Xu-Hui Liu](http://openreview.net/profile?id=~Xu-Hui_Liu1), [Tian-Shuo Liu](http://openreview.net/profile?id=~Tian-Shuo_Liu1), [Shengyi Jiang](http://openreview.net/profile?id=~Shengyi_Jiang2), [Ruifeng Chen](http://openreview.net/profile?id=~Ruifeng_Chen1), [Zhilong Zhang](http://openreview.net/profile?id=~Zhilong_Zhang2), [Xinwei Chen](http://openreview.net/profile?id=~Xinwei_Chen3), [Yang Yu](http://openreview.net/profile?id=~Yang_Yu5)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, Department of Computer Science, The University of Hong Kong, Hong Kong, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, Polixir Technologies, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies
  - **TL;DR:** This study introduces Energy-guided DIffusion Sampling (EDIS) to enhance data generation in offline-to-online reinforcement learning, addressing challenges like data distribution shift and inefficiency in online fine-tuning. The implementation of EDIS with existing methods shows a 20% average improvement in performance across various environments.
  - **Keywords:** Offline-to-online reinforcement learning, efficient learning, safe learning, Energy-guided DIffusion Sampling (EDIS), diffusion model, energy functions, MuJoCo, AntMaze, Adroit environments, Data distribution shift, inefficiency in online fine-tuning, incomplete utilization of offline dataset, Reduced suboptimality, notable performance improvement, Reinforcement learning (RL), online RL, offline RL


- [Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement](https://icml.cc/virtual/2024/poster/33716) (Poster)
  - **Authors:** [che liu](http://openreview.net/profile?id=~Che_Liu3), [Zhongwei Wan](http://openreview.net/profile?id=~Zhongwei_Wan1), [Cheng Ouyang](http://openreview.net/profile?id=~Cheng_Ouyang2), [Anand Shah](http://openreview.net/profile?id=~Anand_Shah1), [Wenjia Bai](http://openreview.net/profile?id=~Wenjia_Bai1), [Rossella Arcucci](http://openreview.net/profile?id=~Rossella_Arcucci1)
  - **Affiliations:** Data Science Institute, Imperial College London, UK; Department of Earth Science and Engineering, Imperial College London, UK, Ohio State University, Columbus, US, Department of Engineering Science, University of Oxford, Oxford, UK; Institute of Clinical Sciences, Imperial College London, UK, Department of Infectious Disease Epidemiology, Imperial College London, UK; Royal Brompton and Harefield Hospitals, UK, Data Science Institute, Imperial College London, UK; Department of Computing, Imperial College London, UK; Department of Brain Sciences, Imperial College London, UK, Data Science Institute, Imperial College London, UK; Department of Earth Science and Engineering, Imperial College London, UK
  - **TL;DR:** This study introduces the Multimodal ECG Representation Learning (MERL) framework, which enables zero-shot ECG classification using text prompts and enhances performance through Clinical Knowledge Enhanced Prompt Engineering (CKEPE). The results demonstrate that MERL outperforms existing ECG self-supervised learning methods, achieving an average AUC score of 75.2% without the need for training data.
  - **Keywords:** Zero-shot classification, Multimodal learning, ECG analysis, Multimodal ECG Representation Learning (MERL), Clinical Knowledge Enhanced Prompt Engineering (CKEPE), Large Language Models (LLMs), Cardiac arrhythmic disease detection, Clinical practice, Lack of annotated samples, Limitations of existing eSSL methods, Improved zero-shot classification performance, Enhanced prompt generation, Six public ECG datasets, ECG Self-supervised Learning (eSSL), Contrastive eSSL (C-eSSL), Generative eSSL (G-eSSL), Hallucination


- [Amortized Equation Discovery in Hybrid Dynamical Systems](https://icml.cc/virtual/2024/poster/32807) (Poster)
  - **Authors:** [Yongtuo Liu](http://openreview.net/profile?id=~Yongtuo_Liu1), [Sara Magliacane](http://openreview.net/profile?id=~Sara_Magliacane1), [Miltiadis (Miltos) Kofinas](http://openreview.net/profile?id=~Miltiadis_Kofinas2), [Efstratios Gavves](http://openreview.net/profile?id=~Stratis_Gavves1)
  - **Affiliations:** University of Amsterdam, University of Amsterdam, University of Amsterdam, University of Amsterdam
  - **TL;DR:** This paper presents Amortized Equation Discovery (AMORE), an end-to-end learning framework for jointly categorizing modes and discovering equations in hybrid dynamical systems. The proposed method outperforms previous approaches in equation discovery, segmentation, and forecasting across various systems.
  - **Keywords:** hybrid dynamical systems, equation discovery, Amortized Equation Discovery (AMORE), SINDy, science, engineering, epidemiology, robotics, cyber-physical systems, unknown number of modes, dynamic switching, interpretability of predictive models, end-to-end learning framework, improved equation discovery, segmentation, forecasting


- [Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences](https://icml.cc/virtual/2024/poster/33974) (Poster)
  - **Authors:** [Zicheng Liu](http://openreview.net/profile?id=~Zicheng_Liu2), [Siyuan Li](http://openreview.net/profile?id=~Siyuan_Li6), [Li Wang](http://openreview.net/profile?id=~Li_Wang34), [Zedong Wang](http://openreview.net/profile?id=~Zedong_Wang1), [Yunfan Liu](http://openreview.net/profile?id=~Yunfan_Liu2), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China
  - **TL;DR:** This study proposes CHELA, a method that combines short-long convolutions with hardware-efficient linear attention to effectively process long sequences while addressing computational complexity and model overfitting. Comprehensive experiments demonstrate its effectiveness in improving performance on language modeling tasks.
  - **Keywords:** linear attention, long sequences, hybrid models, short-long convolutions, state space models (SSMs), natural language processing, language modeling, computational complexity, model overfitting, signal-to-noise ratio, CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), hardware-efficient implementation, Long Range Arena benchmark, Transformer models, attention mechanism


- [Federated Representation Learning in the Under-Parameterized Regime](https://icml.cc/virtual/2024/poster/34302) (Poster)
  - **Authors:** [Renpu Liu](http://openreview.net/profile?id=~Renpu_Liu1), [Cong Shen](http://openreview.net/profile?id=~Cong_Shen1), [Jing Yang](http://openreview.net/profile?id=~Jing_Yang3)
  - **Affiliations:** Department of Electrical Engineering, The Pennsylvania State University, University Park, PA, USA, Department of Electrical and Computer Engineering, University of Virginia, Charlottesville, Virginia, USA, Department of Electrical Engineering, The Pennsylvania State University, University Park, PA, USA
  - **TL;DR:** This paper introduces FLUTE, a novel federated representation learning algorithm designed for the under-parameterized regime, addressing challenges such as computation limitations and privacy concerns. Experimental results show that FLUTE outperforms existing state-of-the-art solutions in both synthetic and real-world tasks.
  - **Keywords:** Federated representation learning, personalized federated learning, FLUTE algorithm, low-rank matrix approximation, Under-parameterized regime, computation limitation, communication overhead, privacy concern, Sample complexity, convergence rate, performance guarantees


- [Language-Driven Cross-Modal Classifier for Zero-Shot Multi-Label Image Recognition](https://icml.cc/virtual/2024/poster/32917) (Poster)
  - **Authors:** [Yicheng Liu](http://openreview.net/profile?id=~Yicheng_Liu6), [Jie Wen](http://openreview.net/profile?id=~Jie_Wen1), [Chengliang Liu](http://openreview.net/profile?id=~Chengliang_Liu1), [xiaozhao fang](http://openreview.net/profile?id=~Xiaozhao_Fang1), [Zuoyong Li](http://openreview.net/profile?id=~Zuoyong_Li1), [Yong Xu](http://openreview.net/profile?id=~Yong_Xu9), [Zheng Zhang](http://openreview.net/profile?id=~Zheng_Zhang18)
  - **Affiliations:** Harbin Institute of Technology, Shenzhen, China, Harbin Institute of Technology, Shenzhen, China, Harbin Institute of Technology, Shenzhen, China, Guangdong University of Technology, Guangzhou, China, Minjiang University, Fuzhou, China, Harbin Institute of Technology, Shenzhen, China, Harbin Institute of Technology, Shenzhen, China
  - **TL;DR:** This paper presents a novel language-driven framework for zero-shot multi-label image recognition that eliminates the need for annotated images during training. The proposed method leverages CLIP's multi-modal embedding space and demonstrates superior performance compared to existing zero-shot methods and competitive results against few-shot approaches.
  - **Keywords:** zero-shot learning, multi-label image recognition, CLIP, cross-modal classifier, cross-modal mapping, image recognition, lack of annotated image data, modality gap, new language-driven framework, improved performance in zero-shot multi-label recognition


- [Partial Multi-View Multi-Label Classification via Semantic Invariance Learning and Prototype Modeling](https://icml.cc/virtual/2024/poster/34972) (Poster)
  - **Authors:** [Chengliang Liu](http://openreview.net/profile?id=~Chengliang_Liu1), [Gehui Xu](http://openreview.net/profile?id=~Gehui_Xu2), [Jie Wen](http://openreview.net/profile?id=~Jie_Wen1), [Yabo Liu](http://openreview.net/profile?id=~Yabo_Liu1), [Chao Huang](http://openreview.net/profile?id=~Chao_Huang16), [Yong Xu](http://openreview.net/profile?id=~Yong_Xu9)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China
  - **TL;DR:** This study addresses the challenges of partial multi-view multi-label learning by developing a model that compresses cross-view representations to enhance semantic tag prediction. The proposed method demonstrates superior performance on various public datasets while effectively managing incomplete data scenarios.
  - **Keywords:** Partial multi-view learning, Multi-label classification, Information bottleneck theory, Cross-view shared representation, Deep multi-view learning networks, Data analysis, Pattern recognition, Partial views and labels unavailable, Incomplete multi-view learning, Improved prediction of semantic tags, Enhanced compatibility with partial and complete data, Multiple public datasets, Semantic invariance, Label correlations


- [Building Socially-Equitable Public Models](https://icml.cc/virtual/2024/poster/34104) (Poster)
  - **Authors:** [Yejia Liu](http://openreview.net/profile?id=~Yejia_Liu1), [Jianyi Yang](http://openreview.net/profile?id=~Jianyi_Yang1), [Pengfei Li](http://openreview.net/profile?id=~Pengfei_Li2), [Tongxin Li](http://openreview.net/profile?id=~Tongxin_Li1), [Shaolei Ren](http://openreview.net/profile?id=~Shaolei_Ren1)
  - **Affiliations:** University of California, Riverside, United States, University of California, Riverside, United States, University of California, Riverside, United States, The Chinese University of Hong Kong, Shenzhen, China, University of California, Riverside, United States
  - **TL;DR:** The study proposes a novel Equitable Objective to enhance the fairness and performance equity of public models used by diverse downstream agents, addressing the limitations of traditional prediction accuracy-focused approaches. Empirical case studies demonstrate the effectiveness of this method in optimizing service quality across various applications.
  - **Keywords:** Social equity in AI, Public models, Fairness in predictions, Policy gradient algorithm, Equitable Objective, Climate modeling, Traffic prediction, Healthcare resource allocation, Performance disparities, Decision-focused learning, Prediction accuracy vs. service quality, Equitable performance distribution, Improved service for diverse agents


- [Causality Based Front-door Defense Against Backdoor Attack on Language Models](https://icml.cc/virtual/2024/poster/33536) (Poster)
  - **Authors:** [Yiran Liu](http://openreview.net/profile?id=~Yiran_Liu1), [Xiaoang Xu](http://openreview.net/profile?id=~Xiaoang_Xu1), [Zhiyi Hou](http://openreview.net/profile?id=~Zhiyi_Hou1), [Yang Yu](http://openreview.net/profile?id=~Yang_Yu13)
  - **Affiliations:** Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China, School of Computer Science and Technology, Harbin University of Science and Technology, Harbin, China, Faculty of Computing, Harbin Institute of Technology, Harbin, China, School of Economics and Management, China University of Petroleum, Beijing, China
  - **TL;DR:** This study presents a novel defense framework called Front-door Adjustment for Backdoor Elimination (FABE) to protect language models from diverse backdoor attacks by leveraging causal inference. The proposed method significantly reduces the attack success rate from 93.63% to 15.12%, demonstrating its effectiveness compared to existing approaches.
  - **Keywords:** Backdoor attacks, Language models, Causal inference, Front-door Adjustment for Backdoor Elimination (FABE), causal reasoning, Language model security, AI safety, Backdoor attacks, model poisoning, trigger diversity, Reduction of attack success rate, state-of-the-art defense results


- [Pairwise Alignment Improves Graph Domain Adaptation](https://icml.cc/virtual/2024/poster/32853) (Spotlight Poster)
  - **Authors:** [Shikun Liu](http://openreview.net/profile?id=~Shikun_Liu3), [Deyu Zou](http://openreview.net/profile?id=~Deyu_Zou1), [Han Zhao](http://openreview.net/profile?id=~Han_Zhao1), [Pan Li](http://openreview.net/profile?id=~Pan_Li2)
  - **Affiliations:** Department of Electrical and Computer Engineering, Georgia Institute of Technology, Georgia, USA, School of Data Science, University of Science and Technology of China, Hefei, China, Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, USA, Department of Electrical and Computer Engineering, Georgia Institute of Technology, Georgia, USA
  - **TL;DR:** This study introduces Pairwise Alignment (Pair-Align), a novel method for Graph Domain Adaptation (GDA) that addresses challenges related to distribution shifts in graph data. The method shows superior performance in real-world applications, including social network node classification and particle collision experiments.
  - **Keywords:** Graph Domain Adaptation (GDA), label inference, Pairwise Alignment (Pair-Align), Graph Neural Networks (GNNs), Social networks, particle colliding experiments, Generalization challenges, distribution shifts, conditional structure shift (CSS), label shift (LS), Novel method for handling graph structure shift, Largest dataset for GDA studies


- [On the Last-Iterate Convergence of Shuffling Gradient Methods](https://icml.cc/virtual/2024/poster/33815) (Oral)
  - **Authors:** [Zijian Liu](http://openreview.net/profile?id=~Zijian_Liu1), [Zhengyuan Zhou](http://openreview.net/profile?id=~Zhengyuan_Zhou2)
  - **Affiliations:** Stern School of Business, New York University, Stern School of Business, New York University; Arena Technologies
  - **TL;DR:** This paper investigates the last-iterate convergence rates of shuffling gradient methods in convex optimization, demonstrating that these rates can be established even without strong convexity. The findings provide theoretical guarantees that align with empirical observations in various optimization settings.
  - **Keywords:** shuffling gradient methods, convex optimization, machine learning, Random Reshuffle (RR), Shuffle Once (SO), Incremental Gradient (IG), stochastic gradient descent (SGD), convergence rates, constrained optimization, theoretical guarantees, last-iterate convergence rates, optimization algorithms, strong convexity, empirical risk minimization (ERM)


- [Neural Operators with Localized Integral and Differential Kernels](https://icml.cc/virtual/2024/poster/32773) (Poster)
  - **Authors:** [Miguel Liu-Schiaffini](http://openreview.net/profile?id=~Miguel_Liu-Schiaffini1), [Julius Berner](http://openreview.net/profile?id=~Julius_Berner1), [Boris Bonev](http://openreview.net/profile?id=~Boris_Bonev1), [Thorsten Kurth](http://openreview.net/profile?id=~Thorsten_Kurth1), [Kamyar Azizzadenesheli](http://openreview.net/profile?id=~Kamyar_Azizzadenesheli1), [Anima Anandkumar](http://openreview.net/profile?id=~Anima_Anandkumar1)
  - **Affiliations:** Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena CA 91125, Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena CA 91125, NVIDIA, Santa Clara, CA 95051, NVIDIA, Santa Clara, CA 95051, NVIDIA, Santa Clara, CA 95051, Department of Computing and Mathematical Sciences, California Institute of Technology, Pasadena CA 91125
  - **TL;DR:** This study presents a novel approach to operator learning that enhances the Fourier neural operator by incorporating differential and integral operators with locally supported kernels, significantly improving performance in predicting solutions to PDEs. The proposed methods reduce the relative L2-error by 34-72% in various scientific applications, including turbulent fluid dynamics.
  - **Keywords:** Neural operators, operator learning, function spaces, Fourier neural operator (FNO), convolutional neural networks (CNN), differential operators, integral operators, Computational science, engineering, weather forecasting, seismology, reservoir engineering, numerical solutions of PDEs, Over-smoothing, capturing local details, training and inference at a single resolution, Improved performance of FNOs, reduced relative L2-error, Stencil methods, discrete-continuous convolutions


- [Restoring balance: principled under/oversampling of data for optimal classification](https://icml.cc/virtual/2024/poster/32963) (Poster)
  - **Authors:** [Emanuele Loffredo](http://openreview.net/profile?id=~Emanuele_Loffredo1), [Mauro Pastore](http://openreview.net/profile?email=mauro.pastore%40phys.ens.fr), [Simona Cocco](http://openreview.net/profile?id=~Simona_Cocco1), [Remi Monasson](http://openreview.net/profile?id=~Remi_Monasson1)
  - **Affiliations:** Laboratoire de physique de l’École normale supérieure, CNRS-UMR8023, PSL University, Sorbonne University, Université Paris-Cité, Laboratoire de physique de l’École normale supérieure, CNRS-UMR8023, PSL University, Sorbonne University, Université Paris-Cité, Laboratoire de physique de l’École normale supérieure, CNRS-UMR8023, PSL University, Sorbonne University, Université Paris-Cité, Laboratoire de physique de l’École normale supérieure, CNRS-UMR8023, PSL University, Sorbonne University, Université Paris-Cité
  - **TL;DR:** This study addresses the challenge of class imbalance in machine learning by deriving analytical expressions for the performance of linear classifiers and proposing optimal under/oversampling strategies. The findings indicate that mixed sampling strategies can significantly enhance classification performance on imbalanced datasets.
  - **Keywords:** Class imbalance, machine learning, generalization, Linear classifiers, Support Vector Machines, under/oversampling strategies, Automated medical diagnostics, molecular biology, text classification, Class imbalance, under-represented examples, generalization challenges, Analytical expressions of generalization curves, performance improvement strategies, Statistical mechanics of learning, replica method


- [A Tensor Decomposition Perspective on Second-order RNNs](https://icml.cc/virtual/2024/poster/34560) (Spotlight Poster)
  - **Authors:** [Maude Lizaire](http://openreview.net/profile?id=~Maude_Lizaire1), [Michael Rizvi-Martel](http://openreview.net/profile?id=~Michael_Rizvi-Martel1), [Marawan Gamal](http://openreview.net/profile?id=~Marawan_Gamal1), [Guillaume Rabusseau](http://openreview.net/profile?id=~Guillaume_Rabusseau1)
  - **Affiliations:** Mila & DIRO, Université de Montréal, Montreal, Canada, Mila & DIRO, Université de Montréal, Montreal, Canada, Mila & DIRO, Université de Montréal, Montreal, Canada, Mila & DIRO, Université de Montréal, Montreal, Canada; CIFAR AI Chair
  - **TL;DR:** This study explores the expressivity and computational efficiency of Second-order Recurrent Neural Networks (2RNNs) by introducing a model called CPRNN, which utilizes CP decomposition to reduce parameter count. The findings indicate that CPRNNs can outperform traditional RNNs and MIRNNs when appropriately configured, particularly in language modelling tasks.
  - **Keywords:** Second-order Recurrent Neural Networks, sequence modelling, CP decomposition, Multiplicative Integration RNN (MIRNN), CPRNN, language modelling, large parameter tensor, intractable computations, vanishing/exploding gradient problem, improved performance with fixed parameter budget, expressivity relationships among RNN architectures, Penn Treebank dataset, RNNs, 2RNNs, BIRNN, MIRNN


- [High-Probability Bound for Non-Smooth Non-Convex Stochastic Optimization with Heavy Tails](https://icml.cc/virtual/2024/poster/33240) (Poster)
  - **Authors:** [Langqi Liu](http://openreview.net/profile?id=~Langqi_Liu1), [Yibo Wang](http://openreview.net/profile?id=~Yibo_Wang2), [Lijun Zhang](http://openreview.net/profile?id=~Lijun_Zhang1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Artificial Intelligence, Nanjing University, Nanjing, China
  - **TL;DR:** This study proposes a novel algorithm within the online-to-non-convex framework to find (δ, ϵ)-stationary points in non-smooth non-convex optimization problems with heavy-tailed stochastic gradients, achieving high probability results with improved gradient complexity. The method incorporates gradient clipping and validation techniques to enhance the efficiency of identifying stationary points.
  - **Keywords:** Non-smooth non-convex optimization, Stochastic optimization, Online-to-non-convex framework, Gradient clipping, Machine learning, Deep neural networks, Heavy-tailed distributions, Bounded variance assumption, (δ, ϵ)-stationary points, High-probability bounds


- [Geometry-Calibrated DRO: Combating Over-Pessimism with Free Energy Implications](https://icml.cc/virtual/2024/poster/34193) (Poster)
  - **Authors:** [Jiashuo Liu](http://openreview.net/profile?id=~Jiashuo_Liu1), [Jiayun Wu](http://openreview.net/profile?id=~Jiayun_Wu1), [Tianyu Wang](http://openreview.net/profile?id=~Tianyu_Wang6), [Hao Zou](http://openreview.net/profile?id=~Hao_Zou1), [Bo Li](http://openreview.net/profile?id=~Bo_Li29), [Peng Cui](http://openreview.net/profile?id=~Peng_Cui1)
  - **Affiliations:** Department of Computer Science and Technology, Tsinghua University, Department of Computer Science and Technology, Tsinghua University, Department of Industrial Engineering and Operations Research, Columbia University, Zhongguancun Lab, School of Economics and Management, Tsinghua University, Department of Computer Science and Technology, Tsinghua University
  - **TL;DR:** This study introduces Geometry-Calibrated DRO (GC-DRO) to address over-pessimism in distributionally robust optimization by incorporating data geometry into calibration terms. The proposed method demonstrates improved generalization and mitigates the effects of noisy samples through a novel minimax optimization approach.
  - **Keywords:** Distributionally Robust Optimization (DRO), over-pessimism in machine learning, Geometry-Calibrated DRO (GC-DRO), minimax optimization algorithm, total variation, entropy regularization, Regression, machine learning under distribution shifts, Over-pessimism, low-confidence predictions, poor parameter estimations, noise in samples, New calibration terms for DRO, improved generalization performance, Helmholtz free energy, Wasserstein space


- [Correlation-Induced Label Prior for Semi-Supervised Multi-Label Learning](https://icml.cc/virtual/2024/poster/34412) (Poster)
  - **Authors:** [Biao Liu](http://openreview.net/profile?id=~Biao_Liu1), [Ning Xu](http://openreview.net/profile?id=~Ning_Xu5), [Xiangyu Fang](http://openreview.net/profile?id=~Xiangyu_Fang1), [Xin Geng](http://openreview.net/profile?id=~Xin_Geng1)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China
  - **TL;DR:** This study introduces a novel semi-supervised multi-label learning method called PCLP, which utilizes a correlation-induced label prior to enhance pseudo-labeling and improve model performance despite limited labeled data. The method demonstrates accuracy in generated pseudo-labels and shows superior performance through comprehensive experiments on benchmark datasets.
  - **Keywords:** Semi-supervised multi-label learning (SSMLL), Multi-label learning (MLL), Correlation-induced label prior, Structural causal model (SCM), Variational label enhancement framework, Image annotation, Text classification, Facial expression recognition, Limited labeled data availability, Difficulty in estimating reliable label correlation, Novel SSMLL method (PCLP), Enhanced pseudo-labeling, Improved model performance, Benchmark datasets


- [KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache](https://icml.cc/virtual/2024/poster/34318) (Poster)
  - **Authors:** [Zirui Liu](http://openreview.net/profile?id=~Zirui_Liu1), [Jiayi Yuan](http://openreview.net/profile?id=~Jiayi_Yuan1), [Hongye Jin](http://openreview.net/profile?id=~Hongye_Jin1), [Shaochen (Henry) Zhong](http://openreview.net/profile?id=~Shaochen_Zhong1), [Zhaozhuo Xu](http://openreview.net/profile?id=~Zhaozhuo_Xu2), [Vladimir Braverman](http://openreview.net/profile?id=~Vladimir_Braverman1), [Beidi Chen](http://openreview.net/profile?id=~Beidi_Chen1), [Xia Hu](http://openreview.net/profile?id=~Xia_Hu4)
  - **Affiliations:** Department of Computer Science, Rice University, Department of Computer Science, Rice University, Department of Computer Science, Texas A&M University, Department of Computer Science, Rice University, Department of Computer Science, Stevens Institute of Technology, Department of Computer Science, Rice University, Department of Electrical and Computer Engineering, Carnegie Mellon University, Department of Computer Science, Rice University
  - **TL;DR:** The study presents KIVI, a tuning-free 2bit quantization algorithm for optimizing key-value caches in large language models, achieving a 2.6× reduction in peak memory usage while maintaining model quality. This advancement allows for larger batch sizes and significantly improves throughput during inference.
  - **Keywords:** Large Language Models (LLMs), KV Cache Optimization, Quantization, Tuning-Free Algorithms, Memory Bottleneck, Inference Speed Limitations, KV Cache Size Reduction, KIVI Algorithm, Memory Usage Reduction, Increased Throughput, Llama (Llama-2), Falcon, Mistral, Key-Value Cache, Attention Mechanism


- [DNA-SE: Towards Deep Neural-Nets Assisted Semiparametric Estimation](https://icml.cc/virtual/2024/poster/34174) (Poster)
  - **Authors:** [Qinshuo Liu](http://openreview.net/profile?id=~Qinshuo_Liu2), [Zixin Wang](http://openreview.net/profile?id=~Zixin_Wang2), [Xi'an Li](http://openreview.net/profile?id=~Xi%27an_Li1), [Xinyao Ji](http://openreview.net/profile?id=~Xinyao_Ji1), [Lei Zhang](http://openreview.net/profile?id=~Lei_Zhang42), [Lin Liu](http://openreview.net/profile?id=~Lin_Liu7), [Zhonghua Liu](http://openreview.net/profile?id=~Zhonghua_Liu2)
  - **Affiliations:** Department of Statistics and Actuarial Science, The University of Hong Kong, Hong Kong SAR, China, School of Life Sciences, Shanghai Jiao Tong University, Shanghai, China, Institute of Natural Sciences, MOE-LSC, School of Mathematical Sciences, CMA-Shanghai, Shanghai Jiao Tong University, Shanghai, China, None, Institute of Natural Sciences, MOE-LSC, School of Mathematical Sciences, CMA-Shanghai, Shanghai Jiao Tong University, Shanghai, China, Institute of Natural Sciences, MOE-LSC, School of Mathematical Sciences, CMA-Shanghai, Shanghai Jiao Tong University, Shanghai, China; SJTU-Yale Joint Center for Biostatistics and Data Science, Shanghai Jiao Tong University, Shanghai, China, Department of Biostatistics, Columbia University, New York, NY, USA
  - **TL;DR:** This study introduces a novel framework for semiparametric estimation using Deep Neural-Nets (DNN) as a numerical solver, addressing challenges in traditional methods. The proposed Deep Neural-Nets Assisted Semiparametric Estimation (DNA-SE) demonstrates significant numerical and statistical advantages in various applications, including missing data and causal inference.
  - **Keywords:** Semiparametric statistics, Deep Neural-Nets (DNN), Bi-level optimization, Numerical solver, Missing data, Causal inference, Transfer learning, Model misspecification, High-dimensional problems, Deep Neural-Nets Assisted Semiparametric Estimation (DNA-SE), Statistical advantages over traditional methods, Fredholm integral equations, Nuisance parameters


- [Timer: Generative Pre-trained Transformers Are Large Time Series Models](https://icml.cc/virtual/2024/poster/33634) (Poster)
  - **Authors:** [Yong Liu](http://openreview.net/profile?id=~Yong_Liu15), [Haoran Zhang](http://openreview.net/profile?id=~Haoran_Zhang9), [Chenyu Li](http://openreview.net/profile?id=~Chenyu_Li2), [Xiangdong Huang](http://openreview.net/profile?id=~Xiangdong_Huang1), [Jianmin Wang](http://openreview.net/profile?id=~Jianmin_Wang1), [Mingsheng Long](http://openreview.net/profile?id=~Mingsheng_Long5)
  - **Affiliations:** School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University
  - **TL;DR:** This study focuses on developing large time series models (LTSM) through generative pre-training to address performance issues in data-scarce scenarios. The proposed Time Series Transformer (Timer) demonstrates promising capabilities across various tasks such as forecasting, imputation, and anomaly detection.
  - **Keywords:** time series analysis, large time series models (LTSM), deep learning, generative pre-training (GPT), Time Series Transformer (Timer), forecasting, imputation, anomaly detection, data scarcity, performance bottlenecks, unified generative task for time series, large-scale datasets


- [Reparameterized Importance Sampling for Robust Variational Bayesian Neural Networks](https://icml.cc/virtual/2024/poster/33546) (Poster)
  - **Authors:** [Yunfei Long](http://openreview.net/profile?id=~Yunfei_Long5), [Zilin Tian](http://openreview.net/profile?id=~Zilin_Tian1), [Liguo Zhang](http://openreview.net/profile?id=~Liguo_Zhang1), [Huosheng Xu](http://openreview.net/profile?id=~Huosheng_Xu2)
  - **Affiliations:** College Of Computer Science And Technology, Harbin Engineering University, Harbin, Heilongjiang, China, College Of Computer Science And Technology, Harbin Engineering University, Harbin, Heilongjiang, China, College Of Computer Science And Technology, Harbin Engineering University, Harbin, Heilongjiang, China; Modeling and Emulation in E-Government National Engineering Laboratory, Harbin Engineering University, Harbin, Heilongjiang, China, College Of Computer Science And Technology, Harbin Engineering University, Harbin, Heilongjiang, China
  - **TL;DR:** This paper introduces a novel sampling method called Reparameterized Importance Sampling (RIS) to address the limitations of mean-field variational inference in Bayesian Neural Networks, particularly the Monte Carlo sampling problem. The proposed method demonstrates improved convergence, predictive performance, and effective uncertainty estimation for out-of-distribution data.
  - **Keywords:** Bayesian Neural Networks, Variational Inference, Mean-field variational inference (MFVI), Reparameterized Importance Sampling (RIS), Monte Carlo sampling problem, high variance in gradient estimates, slow convergence, Improved convergence, enhanced predictive performance, uncertainty estimation for out-of-distribution data, Kullback-Leibler (KL) divergence, variational distribution


- [Attention Meets Post-hoc Interpretability: A Mathematical Perspective](https://icml.cc/virtual/2024/poster/32735) (Poster)
  - **Authors:** [Gianluigi Lopardo](http://openreview.net/profile?id=~Gianluigi_Lopardo1), [Frederic Precioso](http://openreview.net/profile?id=~Frederic_Precioso2), [Damien Garreau](http://openreview.net/profile?id=~Damien_Garreau1)
  - **Affiliations:** Université Côte d’Azur, Inria, CNRS; LJAD; I3S, Université Côte d’Azur, Inria, CNRS; LJAD; I3S, Julius-Maximilians-Universität Würzburg
  - **TL;DR:** This paper mathematically analyzes attention-based architectures, particularly focusing on the differences between post-hoc and attention-based explanations. It concludes that while attention weights provide insights, post-hoc methods can yield more useful interpretations of model predictions.
  - **Keywords:** Attention Mechanism, Explainability, Post-hoc Interpretability, Transformers, Attention-based Architectures, Natural Language Processing (NLP), Relationship between attention weights and model output, Clarity of explanations, Mathematical analysis of attention-based models, Insights on model behavior, BERT, GPT-3, Long-range dependencies


- [Non-Vacuous Generalization Bounds for Large Language Models](https://icml.cc/virtual/2024/poster/34929) (Poster)
  - **Authors:** [Sanae Lotfi](http://openreview.net/profile?id=~Sanae_Lotfi1), [Marc Finzi](http://openreview.net/profile?id=~Marc_Anton_Finzi1), [Yilun Kuang](http://openreview.net/profile?id=~Yilun_Kuang1), [Tim G. J. Rudner](http://openreview.net/profile?id=~Tim_G._J._Rudner2), [Micah Goldblum](http://openreview.net/profile?id=~Micah_Goldblum1), [Andrew Wilson](http://openreview.net/profile?id=~Andrew_Gordon_Wilson1)
  - **Affiliations:** New York University, Carnegie Mellon University, New York University, New York University, New York University, New York University
  - **TL;DR:** This study presents the first non-vacuous generalization bounds for large language models, demonstrating their ability to generalize beyond training data. The authors introduce a novel compression technique, SubLoRA, which significantly enhances the efficiency of bound computation on massive datasets.
  - **Keywords:** Large Language Models, Generalization, Non-vacuous generalization bounds, Compression bounds, Prediction smoothing, SubLoRA, Generalization beyond training data, Memorization of training data, Non-vacuous generalization bounds, Efficient computation of bounds, OpenWebText, Autoregressive token prediction, Negative log-likelihood (NLL), Low-rank adaptation (LoRA), Subspace training


- [HumanTOMATO: Text-aligned Whole-body Motion Generation](https://icml.cc/virtual/2024/poster/33167) (Poster)
  - **Authors:** [Shunlin Lu](http://openreview.net/profile?id=~Shunlin_Lu1), [Ling-Hao Chen](http://openreview.net/profile?id=~Ling-Hao_Chen1), [Ailing Zeng](http://openreview.net/profile?id=~Ailing_Zeng1), [Jing Lin](http://openreview.net/profile?id=~Jing_Lin3), [Ruimao Zhang](http://openreview.net/profile?id=~Ruimao_Zhang1), [Lei Zhang](http://openreview.net/profile?id=~Lei_Zhang23), [Heung-Yeung Shum](http://openreview.net/profile?id=~Heung-Yeung_Shum1)
  - **Affiliations:** Tsinghua University; International Digital Economy Academy (IDEA); School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-SZ), Tsinghua University; International Digital Economy Academy (IDEA); School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-SZ), International Digital Economy Academy (IDEA), Tsinghua University; International Digital Economy Academy (IDEA), School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-SZ), International Digital Economy Academy (IDEA), Tsinghua University; International Digital Economy Academy (IDEA)
  - **TL;DR:** This study presents HumanTOMATO, a novel framework for generating text-aligned whole-body motions that incorporates fine-grained control of facial expressions and hand gestures. The proposed method significantly enhances the quality and alignment of generated motions with textual descriptions, addressing key limitations in existing models.
  - **Keywords:** text-driven motion generation, whole-body motion generation, Holistic Hierarchical VQ-VAE (H2VQ), text-motion-alignment model, animation, robotics, games, films, lack of fine-grained control in motion generation, poor alignment between text and motion, improved quality of generated motions, better alignment with text


- [Optimal Differentially Private Model Training with Public Data](https://icml.cc/virtual/2024/poster/34208) (Poster)
  - **Authors:** [Andrew Lowy](http://openreview.net/profile?id=~Andrew_Lowy1), [Zeman Li](http://openreview.net/profile?id=~Zeman_Li1), [Tianjian Huang](http://openreview.net/profile?id=~Tianjian_Huang2), [Meisam Razaviyayn](http://openreview.net/profile?id=~Meisam_Razaviyayn1)
  - **Affiliations:** University of Wisconsin-Madison, Wisconsin Institute of Discovery, Madison, WI, USA, Department of Industrial & Systems Engineering, University of Southern California, Los Angeles, CA, USA, Department of Industrial & Systems Engineering, University of Southern California, Los Angeles, CA, USA, Department of Industrial & Systems Engineering, University of Southern California, Los Angeles, CA, USA
  - **TL;DR:** This study investigates the optimal error rates of differentially private (DP) models trained with access to public data, addressing how public data can enhance DP model training. The authors develop novel algorithms that outperform existing methods, demonstrating significant improvements in accuracy while maintaining privacy.
  - **Keywords:** Differential Privacy, Machine Learning, Public Data, Mean Estimation, Empirical Risk Minimization, Stochastic Convex Optimization, Privacy Leakage, Accuracy Gap in DP Models, Optimal Error Rates, Novel Algorithms for DP Training


- [EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in Linear Time](https://icml.cc/virtual/2024/poster/34471) (Poster)
  - **Authors:** [Shengyao Lu](http://openreview.net/profile?id=~Shengyao_Lu1), [Bang Liu](http://openreview.net/profile?id=~Bang_Liu1), [Keith Mills](http://openreview.net/profile?id=~Keith_G_Mills1), [Jiao He](http://openreview.net/profile?id=~Jiao_He1), [Di Niu](http://openreview.net/profile?id=~Di_Niu1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Alberta, DIRO, Université de Montréal & Mila, Canada, Department of Electrical and Computer Engineering, University of Alberta, Kirin AI Algorithm & Solution, Huawei, Department of Electrical and Computer Engineering, University of Alberta
  - **TL;DR:** This paper introduces EiG-Search, a training-free approach for generating edge-induced subgraphs to explain Graph Neural Networks (GNNs) efficiently in linear time. The method addresses the challenges of existing explainers by balancing intuitiveness and efficiency, demonstrating superior performance across multiple datasets.
  - **Keywords:** Explainability, Graph Neural Networks (GNNs), Edge-induced subgraphs, linear-time search algorithm, gradient-based importance, GNN explanation, AI safety, Efficiency challenges in GNN explainability, balance between intuitiveness and efficiency, EiG-Search approach, superior performance and efficiency, Seven datasets (specific names not provided), Subgraph-level explanations, node-level explanations, edge-level explanations


- [Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search](https://icml.cc/virtual/2024/poster/33015) (Poster)
  - **Authors:** [Kejing Lu](http://openreview.net/profile?id=~Kejing_Lu1), [Chuan Xiao](http://openreview.net/profile?id=~Chuan_Xiao2), [Yoshiharu Ishikawa](http://openreview.net/profile?id=~Yoshiharu_Ishikawa1)
  - **Affiliations:** Graduate School of Informatics, Nagoya University, Japan; Graduate School of Information Science and Technology, Osaka University, Japan, Graduate School of Informatics, Nagoya University, Japan; Graduate School of Information Science and Technology, Osaka University, Japan, Graduate School of Informatics, Nagoya University, Japan
  - **TL;DR:** This paper presents a novel method for enhancing routing in graph-based approximate nearest neighbor search (ANNS) by introducing probabilistic guarantees for neighbor exploration. The proposed approach, PEOs, significantly improves efficiency, increasing throughput on common graph indexes by 1.6 to 2.5 times compared to existing techniques.
  - **Keywords:** Approximate Nearest Neighbor Search (ANNS), Graph-Based Methods, Probabilistic Routing, Locality-Sensitive Techniques, High-Dimensional Data, Vector Search Tools, Efficiency in Nearest Neighbor Search, Heuristic Methods in ANNS, PEOs (Probabilistic Efficient Neighbors), Increased Throughput in Graph Indexes, HNSW (Hierarchical Navigable Small World), NSSG (Navigable Small World Graph)


- [Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models](https://icml.cc/virtual/2024/poster/34282) (Poster)
  - **Authors:** [Zhihe Lu](http://openreview.net/profile?id=~Zhihe_Lu1), [Jiawang Bai](http://openreview.net/profile?id=~Jiawang_Bai2), [Xin Li](http://openreview.net/profile?id=~Xin_Li28), [Zeyu Xiao](http://openreview.net/profile?id=~Zeyu_Xiao1), [Xinchao Wang](http://openreview.net/profile?id=~Xinchao_Wang1)
  - **Affiliations:** National University of Singapore, Tsinghua University; University of Science and Technology of China, National University of Singapore; University of Science and Technology of China, National University of Singapore; University of Science and Technology of China, National University of Singapore
  - **TL;DR:** This study explores the use of ensemble strategies to enhance the generalization capabilities of vision-language models (VLMs), particularly focusing on leveraging weaker models alongside stronger ones. The proposed methods demonstrate significant improvements in performance across various datasets compared to existing approaches.
  - **Keywords:** Vision-Language Models, Open-World Generalization, Ensemble Strategies, Zero-Shot Ensemble, Training-Free Ensemble, Tuning Ensemble, Generalization Problem, Performance Limitations of Single Models, Enhanced Generalization through Ensemble of VLMs, CLIP, ALIGN, Prompt Learning


- [FiT: Flexible Vision Transformer for Diffusion Model](https://icml.cc/virtual/2024/poster/33297) (Spotlight Poster)
  - **Authors:** [Zeyu Lu](http://openreview.net/profile?id=~Zeyu_Lu1), [ZiDong Wang](http://openreview.net/profile?id=~ZiDong_Wang2), [Di Huang](http://openreview.net/profile?id=~Di_Huang6), [CHENGYUE WU](http://openreview.net/profile?id=~Chengyue_Wu1), [Xihui Liu](http://openreview.net/profile?id=~Xihui_Liu1), [Wanli Ouyang](http://openreview.net/profile?id=~Wanli_Ouyang1), [LEI BAI](http://openreview.net/profile?id=~LEI_BAI1)
  - **Affiliations:** Shanghai Artificial Intelligence Laboratory; Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory; Tsinghua University, Shanghai Artificial Intelligence Laboratory; Sydney University, The University of Hong Kong, The University of Hong Kong, Shanghai Artificial Intelligence Laboratory, Shanghai Artificial Intelligence Laboratory
  - **TL;DR:** The study introduces the Flexible Vision Transformer (FiT), a novel architecture designed to generate images at unrestricted resolutions and aspect ratios, addressing challenges faced by existing diffusion models. FiT demonstrates exceptional performance in resolution extrapolation, promoting flexibility and generalization in image generation tasks.
  - **Keywords:** image generation, resolution generalization, Flexible Vision Transformer (FiT), diffusion models, challenges in processing image resolutions, biases from image cropping, flexible training strategy, resolution extrapolation generation


- [SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models](https://icml.cc/virtual/2024/poster/34806) (Poster)
  - **Authors:** [Xudong LU](http://openreview.net/profile?id=~Xudong_Lu1), [Aojun Zhou](http://openreview.net/profile?id=~Aojun_Zhou2), [Yuhui Xu](http://openreview.net/profile?id=~Yuhui_Xu2), [Renrui Zhang](http://openreview.net/profile?id=~Renrui_Zhang1), [Peng Gao](http://openreview.net/profile?id=~Peng_Gao3), [Hongsheng Li](http://openreview.net/profile?id=~Hongsheng_Li3)
  - **Affiliations:** Multimedia Laboratory (MMLab), The Chinese University of Hong Kong, Multimedia Laboratory (MMLab), The Chinese University of Hong Kong, Salesforce AI Research, Shanghai Artificial Intelligence Laboratory; CPII under InnoHK, Shanghai Artificial Intelligence Laboratory, Multimedia Laboratory (MMLab), The Chinese University of Hong Kong
  - **TL;DR:** This paper introduces SPP, a method for sparsity-preserved parameter-efficient fine-tuning of large language models, addressing the challenges of model performance retention during pruning. The results demonstrate significant performance improvements for models with high sparsity ratios, making SPP a promising solution for efficient fine-tuning of sparse LLMs.
  - **Keywords:** Large Language Models, Fine-Tuning, Parameter Efficiency, Sparsity-Preserved Parameter-efficient fine-tuning, Post-training pruning, Model performance retention, Parameter reduction, Deployment challenges, SPP method, Enhanced performance of sparse models, LLaMA, LLaMA-2, Unstructured sparsity, N:M sparsity


- [WebLINX: Real-World Website Navigation with Multi-Turn Dialogue](https://icml.cc/virtual/2024/poster/33174) (Spotlight Poster)
  - **Authors:** [Xing Han Lù](http://openreview.net/profile?id=~Xing_Han_Lu2), [Zdeněk Kasner](http://openreview.net/profile?id=~Zden%C4%9Bk_Kasner1), [Siva Reddy](http://openreview.net/profile?id=~Siva_Reddy1)
  - **Affiliations:** Mila Quebec AI Institute; McGill University, Mila Quebec AI Institute; McGill University; Institute of Formal and Applied Linguistics, Charles University, Mila Quebec AI Institute; McGill University; Facebook CIFAR AI Chair
  - **TL;DR:** This study introduces WEBLINX, a benchmark for conversational web navigation, where a digital agent interacts with web browsers to perform tasks through multi-turn dialogue. The findings reveal that smaller finetuned models outperform larger zero-shot LLMs, but all models face challenges in generalizing to new websites, highlighting the need for improved multimodal models.
  - **Keywords:** conversational web navigation, digital agent, multi-turn dialogue, retrieval-inspired model, ranking relevant elements, web browsing, task automation, inability of Large Language Models (LLMs) to process entire web pages in real-time, generalization to unseen websites, large-scale benchmark, assessment of models replicating human behavior, WEBLINX benchmark, 100K interactions, 2300 expert demonstrations, Large Language Models (LLMs), multimodal models


- [CauDiTS: Causal Disentangled Domain Adaptation of Multivariate Time Series](https://icml.cc/virtual/2024/poster/33195) (Poster)
  - **Authors:** [Junxin Lu](http://openreview.net/profile?id=~junxin_lu1), [Shiliang Sun](http://openreview.net/profile?id=~Shiliang_Sun1)
  - **Affiliations:** School of Computer Science and Technology, East China Normal University, Shanghai, China, School of Computer Science and Technology, East China Normal University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** The study presents CauDiTS, a framework for unsupervised domain adaptation of multivariate time series that disentangles causal rationales from domain-specific correlations. It demonstrates improved performance across various domains by addressing the challenges of domain shift and enhancing the extraction of domain-invariant representations.
  - **Keywords:** Unsupervised domain adaptation, multivariate time series, Causal disentanglement, adaptive rationale disentangler, Finance, healthcare, weather, energy, Domain shift, differences in data distribution, Causal prototype consistency, domain-intervention causality invariance, Benchmark datasets, Causal rationales, domain-invariant representations


- [Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://icml.cc/virtual/2024/poster/34686) (Best Paper)
  - **Authors:** [Aaron Lou](http://openreview.net/profile?id=~Aaron_Lou1), [Chenlin Meng](http://openreview.net/profile?id=~Chenlin_Meng1), [Stefano Ermon](http://openreview.net/profile?id=~Stefano_Ermon1)
  - **Affiliations:** Stanford University; Pika Labs, Stanford University; Pika Labs, Stanford University
  - **TL;DR:** This study introduces Score Entropy Discrete Diffusion models (SEDD) to improve generative modeling for discrete data, particularly in natural language processing. SEDD significantly outperforms existing language diffusion models and is competitive with autoregressive models, offering better quality and efficiency in text generation.
  - **Keywords:** generative modeling, discrete data, language modeling, diffusion models, score matching, score entropy, natural language processing, limitations of autoregressive models, slow sampling, control issues, Score Entropy Discrete Diffusion models (SEDD), improved generative perplexity


- [How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization](https://icml.cc/virtual/2024/poster/33804) (Poster)
  - **Authors:** [Andrew Lowy](http://openreview.net/profile?id=~Andrew_Lowy1), [Jonathan Ullman](http://openreview.net/profile?id=~Jonathan_Ullman1), [Stephen Wright](http://openreview.net/profile?id=~Stephen_Wright1)
  - **Affiliations:** University of Wisconsin-Madison, Wisconsin Institute of Discovery, Madison, WI, USA, Northeastern University, Khoury College of Computer Sciences, Boston, MA, USA, University of Wisconsin-Madison, Wisconsin Institute of Discovery, Madison, WI, USA
  - **TL;DR:** This study presents a framework for designing differentially private algorithms to find approximate stationary points of non-convex loss functions, achieving improved rates for various classes of functions, including those satisfying the Kurdyka-Łojasiewicz condition. The findings enhance the understanding of optimal rates in differentially private optimization, particularly in machine learning contexts.
  - **Keywords:** Differential privacy, Non-convex optimization, Private approximate risk minimizer, Stationary point finding algorithms, Machine learning, Neural networks, Non-convex loss functions, Sample complexity, Stationary points, Improved rates for finding stationary points, Optimal algorithms for KL condition, Kurdyka-Łojasiewicz (KL) condition, Quasar-convex functions


- [Offline-Boosted Actor-Critic: Adaptively Blending Optimal Historical Behaviors in Deep Off-Policy RL](https://icml.cc/virtual/2024/poster/34872) (Poster)
  - **Authors:** [Yu Luo](http://openreview.net/profile?id=~Yu_Luo5), [Tianying Ji](http://openreview.net/profile?id=~Tianying_Ji2), [Fuchun Sun](http://openreview.net/profile?id=~Fuchun_Sun1), [Jianwei Zhang](http://openreview.net/profile?id=~Jianwei_Zhang2), [Huazhe Xu](http://openreview.net/profile?id=~Huazhe_Xu1), [Xianyuan Zhan](http://openreview.net/profile?id=~Xianyuan_Zhan1)
  - **Affiliations:** Department of Computer Science and Technology, Tsinghua University, Department of Computer Science and Technology, Tsinghua University, Department of Computer Science and Technology, Tsinghua University, Department of Informatics, University of Hamburg, Institute for Interdisciplinary Information Sciences, Tsinghua University; Shanghai Qi Zhi Institute; Shanghai Artificial Intelligence Laboratory; Institute for AI Industry Research, Tsinghua University, Institute for AI Industry Research, Tsinghua University
  - **TL;DR:** This study introduces Offline-Boosted Actor-Critic (OBAC), a novel framework that enhances online reinforcement learning by leveraging optimal offline policies from a shared replay buffer. The experiments show that OBAC significantly improves sample efficiency and performance compared to existing model-free and model-based RL methods across various tasks.
  - **Keywords:** Off-policy reinforcement learning, Policy learning, Offline-Boosted Actor-Critic (OBAC), Model-free RL, Complex real-world tasks, Gaming AI, Chip design, Automatic driving, Sample efficiency, Policy performance, Exploiting replay buffer, Improved online policy learning, Adaptive constraint mechanism


- [OLLIE: Imitation Learning from Offline Pretraining to Online Finetuning](https://icml.cc/virtual/2024/poster/33508) (Poster)
  - **Authors:** [Sheng Yue](http://openreview.net/profile?id=~Sheng_Yue1), [Xingyuan Hua](http://openreview.net/profile?id=~Xingyuan_Hua1), [Ju Ren](http://openreview.net/profile?id=~Ju_Ren1), [Sen Lin](http://openreview.net/profile?id=~Sen_Lin1), [Junshan Zhang](http://openreview.net/profile?id=~Junshan_Zhang1), [Yaoxue Zhang](http://openreview.net/profile?id=~Yaoxue_Zhang3)
  - **Affiliations:** Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China, Department of Computer Science, University of Houston, Texas, US, Department of Electrical and Computer Engineering, University of California, Davis, US, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China
  - **TL;DR:** This paper presents OLLIE, a novel method for offline-to-online imitation learning that effectively pretrains a policy from static data and fine-tunes it with minimal environmental interaction. The method addresses challenges related to discriminator misalignment and significantly outperforms existing approaches across various tasks.
  - **Keywords:** Imitation Learning, Offline-to-Online Learning, GAIL (Generative Adversarial Imitation Learning), BC (Behavior Cloning), Robot Control, Autonomous Driving, Natural Language Processing, Discriminator Misalignment, Covariate Shift, Reward Ambiguity, OLLIE (Offline-to-Online Imitation Learning Method), Near-Expert Policy Initialization, Aligned Discriminator Initialization


- [OMPO: A Unified Framework for RL under Policy and Dynamics Shifts](https://icml.cc/virtual/2024/poster/34060) (Oral)
  - **Authors:** [Yu Luo](http://openreview.net/profile?id=~Yu_Luo5), [Tianying Ji](http://openreview.net/profile?id=~Tianying_Ji2), [Fuchun Sun](http://openreview.net/profile?id=~Fuchun_Sun1), [Jianwei Zhang](http://openreview.net/profile?id=~Jianwei_Zhang2), [Huazhe Xu](http://openreview.net/profile?id=~Huazhe_Xu1), [Xianyuan Zhan](http://openreview.net/profile?id=~Xianyuan_Zhan1)
  - **Affiliations:** Department of Computer Science and Technology, Tsinghua University, Department of Computer Science and Technology, Tsinghua University, Department of Computer Science and Technology, Tsinghua University, Department of Informatics, University of Hamburg, Institute for Interdisciplinary Information Sciences, Tsinghua University; Shanghai Qi Zhi Institute; Shanghai Artificial Intelligence Laboratory, Institute for AI Industry Research, Tsinghua University; Shanghai Artificial Intelligence Laboratory
  - **TL;DR:** This paper presents OMPO, a unified framework for reinforcement learning that addresses challenges posed by policy and dynamics shifts through transition occupancy matching. The method demonstrates superior performance in various environments, particularly in robotics applications when combined with domain randomization.
  - **Keywords:** Reinforcement Learning, Policy Learning, Transition Occupancy Matching, Min-Max Optimization, Actor-Critic Structure, Robotics, Domain Randomization, Policy and Dynamics Shifts, Distribution Discrepancies, Erroneous Policy Evaluation, Occupancy-Matching Policy Optimization (OMPO), Improved Policy Performance, OpenAI Gym, Meta-World, Panda Robots


- [Potential Based Diffusion Motion Planning](https://icml.cc/virtual/2024/poster/34074) (Poster)
  - **Authors:** [Yunhao Luo](http://openreview.net/profile?id=~Yunhao_Luo1), [Chen Sun](http://openreview.net/profile?id=~Chen_Sun1), [Josh Tenenbaum](http://openreview.net/profile?id=~Joshua_B._Tenenbaum1), [Yilun Du](http://openreview.net/profile?id=~Yilun_Du1)
  - **Affiliations:** Brown University, Brown University, MIT, MIT
  - **TL;DR:** This paper presents a novel approach to potential-based motion planning in robotics using diffusion models to learn optimizable potential landscapes, significantly improving performance and avoiding local minima issues. The method demonstrates composability, allowing for the integration of various motion constraints in planning trajectories.
  - **Keywords:** motion planning, robotics, potential-based motion planning, neural networks, diffusion models, manipulation, navigation, high dimensional spaces, local minima issues, implicit obstacle representations, learning potential landscapes, optimization across potential energy landscapes, composability of motion constraints


- [OxyGenerator: Reconstructing Global Ocean Deoxygenation Over a Century with Deep Learning](https://icml.cc/virtual/2024/poster/35210) (Poster)
  - **Authors:** [Bin Lu](http://openreview.net/profile?id=~Bin_Lu2), [Ze Zhao](http://openreview.net/profile?id=~Ze_Zhao1), [Luyu Han](http://openreview.net/profile?id=~Luyu_Han1), [Xiaoying Gan](http://openreview.net/profile?id=~Xiaoying_Gan1), [Yuntao Zhou](http://openreview.net/profile?id=~Yuntao_Zhou1), [Lei Zhou](http://openreview.net/profile?id=~Lei_Zhou3), [Luoyi Fu](http://openreview.net/profile?id=~Luoyi_Fu1), [Xinbing Wang](http://openreview.net/profile?id=~Xinbing_Wang1), [Chenghu Zhou](http://openreview.net/profile?id=~Chenghu_Zhou3), [Jing Zhang](http://openreview.net/profile?id=~Jing_Zhang40)
  - **Affiliations:** Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China, Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Oceanography, Shanghai Jiao Tong University, Shanghai, China, Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Oceanography, Shanghai Jiao Tong University, Shanghai, China, School of Oceanography, Shanghai Jiao Tong University, Shanghai, China, School of Oceanography, Shanghai Jiao Tong University, Shanghai, China, Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China, Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China, School of Oceanography, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** The study introduces OXYGENERATOR, a deep learning model designed to reconstruct global ocean deoxygenation from 1920 to 2023, addressing the challenges posed by sparse historical data. The model significantly outperforms existing numerical simulations, highlighting its potential for understanding the impacts of global warming and human activities on marine ecosystems.
  - **Keywords:** global ocean deoxygenation, marine ecosystem protection, deep learning, zoning-varying graph message-passing, oceanography, environmental science, data sparsity, historical observation challenges, OXYGENERATOR model, improved reconstruction accuracy, World Ocean Database (WOD), dissolved oxygen (DO), oxygen minimum zones, CMIP6


- [Unveiling the Cycloid Trajectory of EM Iterations in Mixed Linear Regression](https://icml.cc/virtual/2024/poster/33762) (Poster)
  - **Authors:** [Zhankun Luo](http://openreview.net/profile?id=~Zhankun_Luo1), [Abolfazl Hashemi](http://openreview.net/profile?id=~Abolfazl_Hashemi1)
  - **Affiliations:** School of Electrical and Computer Engineering, Purdue University, IN, USA, School of Electrical and Computer Engineering, Purdue University, IN, USA
  - **TL;DR:** This study investigates the convergence rates and iteration trajectories of the Expectation-Maximization (EM) algorithm for two-component Mixed Linear Regression (2MLR), revealing that iterations follow a cycloid trajectory and establishing a theoretical estimate for super-linear convergence. The findings enhance understanding of EM behavior and improve statistical error bounds in finite-sample scenarios.
  - **Keywords:** Mixed Linear Regression, Expectation-Maximization, EM algorithm, Bessel functions, High-dimensional data, non-convexity, spurious local maxima, Super-linear convergence, statistical error bounds, Two-component mixed linear regression (2MLR), Maximum Likelihood Estimation (MLE)


- [Cross-Domain Policy Adaptation by Capturing Representation Mismatch](https://icml.cc/virtual/2024/poster/35044) (Poster)
  - **Authors:** [Jiafei Lyu](http://openreview.net/profile?id=~Jiafei_Lyu1), [Chenjia Bai](http://openreview.net/profile?id=~Chenjia_Bai2), [Jing-Wen Yang](http://openreview.net/profile?id=~Jing-Wen_Yang3), [Zongqing Lu](http://openreview.net/profile?id=~Zongqing_Lu2), [Xiu Li](http://openreview.net/profile?id=~Xiu_Li1)
  - **Affiliations:** Tsinghua Shenzhen International Graduate School, Tsinghua University, Shanghai Artificial Intelligence Laboratory, Tencent IEG, School of Computer Science, Peking University; Beijing Academy of Artificial Intelligence, Tsinghua Shenzhen International Graduate School, Tsinghua University
  - **TL;DR:** This paper presents a novel approach to cross-domain policy adaptation in reinforcement learning by addressing dynamics mismatch through representation learning. The method effectively utilizes representation deviations as a reward penalty, demonstrating strong performance in various tasks despite limited interactions in the target domain.
  - **Keywords:** Cross-domain policy adaptation, dynamics mismatch, reinforcement learning, Representation learning, reward penalty, Robotics, autonomous driving, Dynamics discrepancies, limited interactions with target domain, New method for policy adaptation using representation deviations


- [RoboMP$^2$: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models](https://icml.cc/virtual/2024/poster/33506) (Poster)
  - **Authors:** [Qi Lv](http://openreview.net/profile?id=~Qi_Lv1), [Hao Li](http://openreview.net/profile?id=~Hao_Li59), [Xiang Deng](http://openreview.net/profile?id=~Xiang_Deng1), [Rui Shao](http://openreview.net/profile?id=~Rui_Shao1), [Michael Wang](http://openreview.net/profile?id=~Michael_Y_Wang1), [Liqiang Nie](http://openreview.net/profile?id=~Liqiang_Nie2)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen); School of Engineering, Great Bay University; School of Computing and Information Technology, Great Bay University, School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), School of Engineering, Great Bay University, School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen)
  - **TL;DR:** The study introduces the RoboMP2 framework for robotic manipulation, which enhances decision-making through a Goal-Conditioned Multimodal Preceptor and a Retrieval-Augmented Multimodal Planner. Extensive experiments show a 10% improvement over existing methods on both benchmark and real-world tasks.
  - **Keywords:** Robotic manipulation, Multimodal perception-planning, Multimodal Large Language Models (MLLMs), Goal-Conditioned Multimodal Preceptor (GCMP), Retrieval-Augmented Multimodal Planner (RAMP), Robotics, Embodied agents, Limited generalization capabilities, Overlooking multimodal environment information, Novel RoboMP2 framework, Improved planning and decision-making, VIMA benchmark


- [Efficient and Effective Time-Series Forecasting with Spiking Neural Networks](https://icml.cc/virtual/2024/poster/33994) (Poster)
  - **Authors:** [Changze Lv](http://openreview.net/profile?id=~Changze_Lv1), [Yansen Wang](http://openreview.net/profile?id=~Yansen_Wang2), [Dongqi Han](http://openreview.net/profile?id=~Dongqi_Han1), [Xiaoqing Zheng](http://openreview.net/profile?id=~Xiaoqing_Zheng2), [Xuanjing Huang](http://openreview.net/profile?id=~Xuanjing_Huang1), [Dongsheng Li](http://openreview.net/profile?id=~Dongsheng_Li2)
  - **Affiliations:** School of Computer Science, Fudan University, Shanghai, China; Microsoft Research Asia, Shanghai, China, Microsoft Research Asia, Shanghai, China, Microsoft Research Asia, Shanghai, China, School of Computer Science, Fudan University, Shanghai, China; Microsoft Research Asia, Shanghai, China, School of Computer Science, Fudan University, Shanghai, China, Microsoft Research Asia, Shanghai, China
  - **TL;DR:** This study proposes a framework for using spiking neural networks (SNNs) in time-series forecasting, addressing challenges in temporal alignment and encoding. The results show that SNN-based approaches can achieve comparable or superior performance to traditional methods while significantly reducing energy consumption.
  - **Keywords:** Spiking Neural Networks (SNNs), Time-Series Forecasting, Temporal Data Analysis, Energy Efficiency, Effective Temporal Alignment, Complex Encoding Processes, Model Selection Challenges, SNN-based Approaches, Energy Consumption Reduction, CIFAR-10-DVS, DVS-128-Gesture, Biological Plausibility, Event-Driven Paradigm


- [Sampling is as easy as keeping the consistency: convergence guarantee for Consistency Models](https://icml.cc/virtual/2024/poster/33740) (Poster)
  - **Authors:** [Junlong Lyu](http://openreview.net/profile?id=~Junlong_Lyu1), [Zhitang Chen](http://openreview.net/profile?id=~Zhitang_Chen1), [Shoubo Feng](http://openreview.net/profile?id=~Shoubo_Feng1)
  - **Affiliations:** Theory Lab, Huawei, Noah’s Ark Lab, Huawei, Noah’s Ark Lab, Huawei
  - **TL;DR:** This paper presents the first convergence guarantee for Consistency Models (CMs), demonstrating their ability to efficiently generate high-quality samples in one step, comparable to state-of-the-art diffusion models. The findings indicate that CMs can significantly reduce computational costs while maintaining sampling quality, paving the way for their practical application in various generative tasks.
  - **Keywords:** Consistency Models, one-step generative models, convergence guarantee, score-matching, consistency functions, Multi-step Consistency Sampling, image generation, audio synthesis, video generation, high computational cost of diffusion models, sampling quality, convergence guarantees, efficient sample generation, Diffusion Models, Generative Models, SGMs (Score-based Generative Models)


- [Cluster-Aware Similarity Diffusion for Instance Retrieval](https://icml.cc/virtual/2024/poster/32994) (Poster)
  - **Authors:** [Jifei Luo](http://openreview.net/profile?id=~Jifei_Luo1), [Hantao Yao](http://openreview.net/profile?id=~Hantao_Yao2), [Changsheng Xu](http://openreview.net/profile?id=~Changsheng_Xu1)
  - **Affiliations:** University of Science and Technology of China, Hefei, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences
  - **TL;DR:** This study introduces a novel Cluster-Aware Similarity (CAS) diffusion method for instance retrieval, which mitigates misinformation propagation from outliers by conducting similarity diffusion within local clusters. The proposed method enhances the robustness of diffusion-based re-ranking, as validated through evaluations in instance retrieval and object re-identification.
  - **Keywords:** Instance retrieval, Diffusion-based re-ranking, Cluster-Aware Similarity (CAS), Bidirectional Similarity Diffusion, Neighbor-guided Similarity Smoothing, Object re-identification, Misinformation propagation, Influence of outliers, Data manifold structure, Enhanced similarity propagation, Robustness in re-ranking


- [CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables](https://icml.cc/virtual/2024/poster/35137) (Poster)
  - **Authors:** [Jiecheng Lu](http://openreview.net/profile?id=~Jiecheng_Lu1), [Xu Han](http://openreview.net/profile?id=~Xu_Han14), [Sun](http://openreview.net/profile?id=~Yan_Sun5), [Shihao Yang](http://openreview.net/profile?id=~Shihao_Yang1)
  - **Affiliations:** Georgia Institute of Technology, Amazon Web Services, Georgia Institute of Technology, Georgia Institute of Technology
  - **TL;DR:** This paper introduces the Construct Auxiliary Time Series (CATS) method to enhance multivariate time series forecasting by generating auxiliary time series that represent inter-series relationships. The approach significantly improves forecasting performance while maintaining a simple model architecture, achieving state-of-the-art results with reduced complexity.
  - **Keywords:** Multivariate Time Series Forecasting, Deep Learning, Construct Auxiliary Time Series (CATS), 2D temporal-contextual attention mechanism, MLP (Multi-Layer Perceptron), Finance, Health, Traffic, Overfitting, Weak inter-series relationships, Temporal dynamics, Efficient and transferable MTSF solution, State-of-the-art performance, Auxiliary Time Series (ATS), Original Time Series (OTS), Continuity, Sparsity, Variability


- [Position: Exploring the Robustness of Pipeline-Parallelism-Based Decentralized Training](https://icml.cc/virtual/2024/poster/33873) (Poster)
  - **Authors:** [Lin Lu](http://openreview.net/profile?id=~Lin_Lu3), [Chenxi Dai](http://openreview.net/profile?id=~Chenxi_Dai1), [Wangcheng Tao](http://openreview.net/profile?id=~Wangcheng_Tao1), [Binhang Yuan](http://openreview.net/profile?id=~Binhang_Yuan1), [Yanan Sun](http://openreview.net/profile?id=~Yanan_Sun4), [Pan Zhou](http://openreview.net/profile?id=~Pan_Zhou5)
  - **Affiliations:** Huazhong University of Science and Technology, Huazhong University of Science and Technology, Hong Kong University of Science and Technology, Hong Kong University of Science and Technology, Sichuan University, Huazhong University of Science and Technology
  - **TL;DR:** This paper explores the robustness of decentralized training frameworks, particularly focusing on potential threats and proposing a robust training framework to mitigate security risks, including a nascent poisoning attack. The findings emphasize the urgency of addressing these security concerns to facilitate the democratization of large language models.
  - **Keywords:** Decentralized training, Robustness, Pipeline parallelism, Large language models (LLMs), Deep neural networks (DNNs), Potential threats in decentralized training, Security threats, Poisoning attacks, Robust training framework, Detection strategy


- [Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation](https://icml.cc/virtual/2024/poster/35122) (Poster)
  - **Authors:** [Xinyu Ma](http://openreview.net/profile?id=~Xinyu_Ma3), [Xu Chu](http://openreview.net/profile?id=~Xu_Chu1), [Zhibang Yang](http://openreview.net/profile?id=~Zhibang_Yang1), [Yang Lin](http://openreview.net/profile?id=~Yang_Lin2), [Xin Gao](http://openreview.net/profile?id=~Xin_Gao4), [Junfeng Zhao](http://openreview.net/profile?id=~Junfeng_Zhao1)
  - **Affiliations:** School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China, School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China; Center on Frontiers of Computing Studies, Peking University, Beijing, China, Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China, School of Computer Science, Peking University, Beijing, China, School of Computer Science, Peking University, Beijing, China, School of Computer Science, Peking University, Beijing, China
  - **TL;DR:** This paper introduces quasi-Givens Orthogonal Fine-Tuning (qGOFT) to enhance parameter efficiency in fine-tuning pretrained models by reducing parameter complexity and improving adaptation capabilities for downstream tasks. The proposed method leverages Givens rotations and soft orthogonality regularization to maintain angular distances and semantic information during adaptation.
  - **Keywords:** Parameter-efficient fine-tuning, pretrained models, Orthogonal Fine-tuning (OFT), Givens rotations, quasi-Givens Orthogonal Fine-Tuning (qGOFT), Downstream tasks in NLP and vision, Low parameter efficiency in fine-tuning, adaptation capability limitations, Reduction of parameter complexity from O(d²) to O(d), introduction of flexible norm and relative angular adjustments


- [Better Locally Private Sparse Estimation Given Multiple Samples Per User](https://icml.cc/virtual/2024/poster/32739) (Poster)
  - **Authors:** [Yuheng Ma](http://openreview.net/profile?id=~Yuheng_Ma1), [Ke Jia](http://openreview.net/profile?id=~Ke_Jia1), [Hanfang Yang](http://openreview.net/profile?id=~Hanfang_Yang2)
  - **Affiliations:** School of Statistics, Renmin University of China, School of Statistics, Renmin University of China, Center for Applied Statistics, Renmin University of China
  - **TL;DR:** This study investigates user-level locally differentially private sparse linear regression, demonstrating that utilizing multiple samples per user can significantly improve estimation accuracy. The proposed framework achieves a better error rate compared to traditional methods, particularly in high-dimensional data contexts.
  - **Keywords:** Locally Differential Privacy, Sparse Linear Regression, User-Level LDP, Variable Selection, Estimation Protocols, High-Dimensional Data Analysis, Data Sparsity, Privacy Challenges in Machine Learning, Improved Error Bounds, Theoretical Guarantees for Sparse Estimation, Synthetic and Real Datasets, Minimax Rate, Error Upper Bound


- [Rethinking Decision Transformer via Hierarchical Reinforcement Learning](https://icml.cc/virtual/2024/poster/33846) (Poster)
  - **Authors:** [Yi Ma](http://openreview.net/profile?id=~Yi_Ma5), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Hebin Liang](http://openreview.net/profile?id=~Hebin_Liang2), [Chenjun Xiao](http://openreview.net/profile?id=~Chenjun_Xiao1)
  - **Affiliations:** College of Intelligence and Computing, Tianjin University, College of Intelligence and Computing, Tianjin University; Huawei, Noah’s Ark Lab, College of Intelligence and Computing, Tianjin University, The Chinese University of Hongkong, Shenzhen
  - **TL;DR:** This study introduces a hierarchical reinforcement learning framework to enhance the Decision Transformer by enabling the integration of suboptimal trajectories. The proposed algorithms significantly outperform the Decision Transformer on various control and navigation tasks, addressing its limitations in robustness and trajectory stitching.
  - **Keywords:** Decision Transformer, Hierarchical Reinforcement Learning, Transformer architecture, autoregressive generative model, Control benchmarks, navigation benchmarks, Reliance on recalling trajectories, stitching ability, robustness to data distribution, New offline RL algorithms, optimization of high-level and low-level policies, Reinforcement Learning (RL), return-to-go


- [Contamination-Resilient Anomaly Detection via Adversarial Learning on Partially-Observed Normal and Anomalous Data](https://icml.cc/virtual/2024/poster/34922) (Poster)
  - **Authors:** [Wenxi Lv](http://openreview.net/profile?id=~Wenxi_Lv1), [Qinliang Su](http://openreview.net/profile?id=~Qinliang_Su3), [Hai Wan](http://openreview.net/profile?id=~Hai_Wan3), [Hongteng Xu](http://openreview.net/profile?id=~Hongteng_Xu1), [Wenchao Xu](http://openreview.net/profile?id=~Wenchao_Xu1)
  - **Affiliations:** School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China, Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR
  - **TL;DR:** This study addresses the challenge of accurately detecting anomalies in contaminated datasets by proposing a method that learns the normal-data distribution through adversarial learning and additional small datasets. The results demonstrate that the proposed approach outperforms existing methods in anomaly detection under such conditions.
  - **Keywords:** Anomaly Detection, Contaminated Datasets, Adversarial Learning, One-Class Classifier, Reconstruction Methods, Self-Supervised Methods, Disease Diagnosis, Industrial Defect Detection, Cyber-Crimes Defense, Financial Fraud Detection, Data Contamination, Overfitting, Diversity of Anomalies, Correct Normal-Data Distribution Learning, Flipping Mechanism


- [Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum](https://icml.cc/virtual/2024/poster/34626) (Poster)
  - **Authors:** [Tin Sum Cheng](http://openreview.net/profile?id=~Tin_Sum_Cheng1), [Aurelien Lucchi](http://openreview.net/profile?id=~Aurelien_Lucchi1), [Anastasis Kratsios](http://openreview.net/profile?id=~Anastasis_Kratsios1), [David Belius](http://openreview.net/profile?id=~David_Belius1)
  - **Affiliations:** Department of Mathematics and Computer Science, University of Basel, Switzerland, Department of Mathematics and Computer Science, University of Basel, Switzerland, Department of Mathematics and Statistics, McMaster University and Vector Institute, Canada, Faculty of Mathematics and Computer Science, UniDistance Suisse, Switzerland
  - **TL;DR:** This study investigates overfitting in kernel ridge regression (KRR) by deriving new bounds for test error in the over-parameterized regime, demonstrating that polynomial spectral decay leads to tempered overfitting while exponential decay results in catastrophic overfitting. The findings highlight the importance of feature independence in ensuring robust generalization in KRR.
  - **Keywords:** kernel ridge regression, overfitting, machine learning, kernel matrices, non-asymptotic test error bounds, minimum norm interpolant, overfitting phenomena, high-dimensional data, fixed input dimension, tempered overfitting, catastrophic overfitting, new bounds for test error, sub-Gaussian design assumption, polynomial spectral decay, exponential spectral decay


- [Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models](https://icml.cc/virtual/2024/poster/34392) (Poster)
  - **Authors:** [Qitan Lv](http://openreview.net/profile?id=~Qitan_Lv1), [Jie Wang](http://openreview.net/profile?id=~Jie_Wang1), [Hanzhu Chen](http://openreview.net/profile?id=~Hanzhu_Chen1), [Bin Li](http://openreview.net/profile?id=~Bin_Li8), [Yongdong Zhang](http://openreview.net/profile?id=~Yongdong_Zhang2), [Feng Wu](http://openreview.net/profile?id=~Feng_Wu1)
  - **Affiliations:** CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China
  - **TL;DR:** This study introduces COFT, a novel method designed to reduce knowledge hallucination in large language models by focusing on key texts at different granularity levels. The method demonstrates over 30% improvement in F1 score on knowledge hallucination benchmarks and shows versatility across various long-form tasks.
  - **Keywords:** Knowledge Hallucination, Large Language Models, Retrieval-Augmented Language Model (RALM), COFT (COarse-to-Fine Highlighting), Reading Comprehension, Question Answering, Knowledge Hallucination, Long-form Contexts, COFT method, Performance Improvement in F1 Score


- [High-dimensional Linear Bandits with Knapsacks](https://icml.cc/virtual/2024/poster/35007) (Poster)
  - **Authors:** [Wanteng Ma](http://openreview.net/profile?id=~Wanteng_Ma1), [Dong Xia](http://openreview.net/profile?id=~Dong_Xia1), [Jiashuo Jiang](http://openreview.net/profile?id=~Jiashuo_Jiang1)
  - **Affiliations:** Department of Mathematics, Hong Kong University of Science and Technology, Hong Kong SAR, Department of Mathematics, Hong Kong University of Science and Technology, Hong Kong SAR, Department of Industrial Engineering and Decision Analytics, Hong Kong University of Science and Technology, Hong Kong SAR
  - **TL;DR:** This study addresses the high-dimensional contextual bandit with knapsacks problem by developing an online hard thresholding algorithm combined with a primal-dual framework, achieving sublinear regret that logarithmically depends on feature dimension. The proposed methods also provide insights into online sparse estimation and improve upon existing approaches in both data-poor and data-rich regimes.
  - **Keywords:** high-dimensional contextual bandits, knapsack problem, hard thresholding algorithm, primal-dual framework, online learning, ad allocation, dynamic pricing, online recommendation, online advertising, high-dimensional data, regret minimization, optimal sparse estimation, sublinear regret, unified sparse bandit algorithm framework, LASSO, sparse recovery


- [HarmonyDream: Task Harmonization Inside World Models](https://icml.cc/virtual/2024/poster/32730) (Poster)
  - **Authors:** [Haoyu Ma](http://openreview.net/profile?id=~Haoyu_Ma3), [Jialong Wu](http://openreview.net/profile?id=~Jialong_Wu1), [Ningya Feng](http://openreview.net/profile?id=~Ningya_Feng1), [Chenjun Xiao](http://openreview.net/profile?id=~Chenjun_Xiao1), [Dong Li](http://openreview.net/profile?id=~Dong_Li10), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Jianmin Wang](http://openreview.net/profile?id=~Jianmin_Wang1), [Mingsheng Long](http://openreview.net/profile?id=~Mingsheng_Long5)
  - **Affiliations:** School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab; College of Intelligence and Computing, Tianjin University, Huawei Noah’s Ark Lab; College of Intelligence and Computing, Tianjin University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University
  - **TL;DR:** This study introduces HarmonyDream, an approach for task harmonization in model-based reinforcement learning (MBRL) that balances observation and reward modeling to enhance sample efficiency. The method demonstrates significant performance improvements on visual robotic tasks and sets a new state-of-the-art on the Atari 100K benchmark.
  - **Keywords:** Model-based reinforcement learning (MBRL), world models, task harmonization, observation modeling, reward modeling, multi-task learning, visual robotic tasks, Atari 100K benchmark, sample efficiency, environment complexity, model capacity, HarmonyDream approach, dynamic equilibrium in task learning, Atari 100K


- [Outlier-aware Slicing for Post-Training Quantization in Vision Transformer](https://icml.cc/virtual/2024/poster/33935) (Poster)
  - **Authors:** [Yuexiao Ma](http://openreview.net/profile?id=~Yuexiao_Ma1), [Huixia Li](http://openreview.net/profile?id=~Huixia_Li2), [Xiawu Zheng](http://openreview.net/profile?id=~Xiawu_Zheng1), [Feng Ling](http://openreview.net/profile?id=~Feng_Ling1), [Xuefeng Xiao](http://openreview.net/profile?id=~Xuefeng_Xiao1), [Rui Wang](http://openreview.net/profile?id=~Rui_Wang32), [Shilei Wen](http://openreview.net/profile?id=~Shilei_Wen1), [Fei Chao](http://openreview.net/profile?id=~Fei_Chao1), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China; School of Informatics, Xiamen University, 361005, P.R. China, ByteDance Inc., Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China; Institute of Artificial Intelligence, Xiamen University, ByteDance Inc., ByteDance Inc., ByteDance Inc., ByteDance Inc., Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China; Institute of Artificial Intelligence, Xiamen University
  - **TL;DR:** This study introduces a novel approach to Post-Training Quantization (PTQ) that addresses the outlier problem in transformer architectures by varying reconstruction granularity. The proposed method achieves state-of-the-art performance, significantly improving accuracy in quantized models like the Swin Transformer and ViT-Small on the ImageNet classification task.
  - **Keywords:** Post-Training Quantization, Transformer Architectures, Reconstruction Granularity, Quantization Algorithms, Vision Tasks, Image Classification, Impact of Outliers on Quantization Accuracy, State-of-the-Art Performance in PTQ, Optimal Reconstruction Granularity, ImageNet, Vision Transformer (ViT), Swin Transformer


- [X-Oscar: A Progressive Framework for High-quality Text-guided 3D Animatable Avatar Generation](https://icml.cc/virtual/2024/poster/34764) (Poster)
  - **Authors:** [Yiwei Ma](http://openreview.net/profile?id=~Yiwei_Ma1), [Zhekai Lin](http://openreview.net/profile?id=~Zhekai_Lin1), [Jiayi Ji](http://openreview.net/profile?id=~Jiayi_Ji1), [Yijun Fan](http://openreview.net/profile?id=~Yijun_Fan2), [Xiaoshuai Sun](http://openreview.net/profile?id=~Xiaoshuai_Sun3), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China
  - **TL;DR:** The study presents X-Oscar, a progressive framework for generating high-quality animatable avatars from text prompts, addressing challenges like oversaturation and low-quality output through innovative techniques. Extensive evaluations demonstrate its superiority over existing text-to-3D and text-to-avatar methods.
  - **Keywords:** 3D avatar generation, text-guided generation, Adaptive Variational Parameter (AVP), Avatar-aware Score Distillation Sampling (ASDS), Cartoon production, virtual try-on, immersive telepresence, video game design, Oversaturation, low-quality output, High-quality animatable avatars, step-by-step generation paradigm


- [On the Hardness of Probabilistic Neurosymbolic Learning](https://icml.cc/virtual/2024/poster/32766) (Poster)
  - **Authors:** [Jaron Maene](http://openreview.net/profile?id=~Jaron_Maene1), [Vincent Derkinderen](http://openreview.net/profile?id=~Vincent_Derkinderen1), [Luc De Raedt](http://openreview.net/profile?id=~Luc_De_Raedt1)
  - **Affiliations:** KU Leuven, Department of Computer Science, Leuven, Belgium; Örebro University, Centre for Applied Autonomous Sensor Systems, Örebro, Sweden, KU Leuven, Department of Computer Science, Leuven, Belgium, KU Leuven, Department of Computer Science, Leuven, Belgium; Örebro University, Centre for Applied Autonomous Sensor Systems, Örebro, Sweden
  - **TL;DR:** This study investigates the challenges of differentiating probabilistic reasoning in neurosymbolic models and introduces WeightME, an unbiased gradient estimator that provides probabilistic guarantees during training. The findings highlight the limitations of existing biased approximations in optimizing even solvable benchmarks, suggesting a need for more principled methods in complex reasoning tasks.
  - **Keywords:** probabilistic neurosymbolic models, neural networks, probabilistic reasoning, gradient descent, gradient approximation, WeightME, weighted model sampling, intractability of probabilistic inference, scalability issues, optimization challenges, unbiased gradient estimator, probabilistic guarantees, logarithmic SAT solver calls, #P-hard, SAT oracle, weighted model counting (WMC)


- [Neighboring Perturbations of Knowledge Editing on Large Language Models](https://icml.cc/virtual/2024/poster/34353) (Poster)
  - **Authors:** [Jun-Yu Ma](http://openreview.net/profile?id=~Jun-Yu_Ma1), [Zhen-Hua Ling](http://openreview.net/profile?id=~Zhen-Hua_Ling1), [Ningyu Zhang](http://openreview.net/profile?id=~Ningyu_Zhang1), [Jia-Chen Gu](http://openreview.net/profile?id=~Jia-Chen_Gu1)
  - **Affiliations:** NERC-SLIP, University of Science and Technology of China, NERC-SLIP, University of Science and Technology of China, Zhejiang University, University of California, Los Angeles
  - **TL;DR:** This study investigates the impact of appending new knowledge to large language models on their existing knowledge, focusing on issues like catastrophic forgetting and unintentional inclusion of incorrect answers. A new metric and benchmark are introduced to evaluate these perturbations, along with a framework to mitigate them, demonstrating effectiveness across multiple editing methods and models.
  - **Keywords:** Knowledge Editing, Large Language Models (LLMs), Perturbation Evaluation, Appending via Preservation and Prevention (APP), Natural Language Processing (NLP), Catastrophic Forgetting, Hallucination, Additivity Metric, PEAK Benchmark


- [SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment](https://icml.cc/virtual/2024/poster/33300) (Poster)
  - **Authors:** [Ziping Ma](http://openreview.net/profile?id=~Ziping_Ma1), [Furong Xu](http://openreview.net/profile?id=~Furong_Xu1), [Jian liu](http://openreview.net/profile?id=~Jian_liu8), [Ming Yang](http://openreview.net/profile?id=~Ming_Yang2), [Qingpei Guo](http://openreview.net/profile?id=~Qingpei_Guo1)
  - **Affiliations:** Ant Group, Ant Group, Ant Group, Ant Group, Ant Group
  - **TL;DR:** This paper introduces Symmetrizing Contrastive Captioners (SyCoCa), a framework that enhances multimodal alignment by incorporating bidirectional interactions between images and texts at both global and local representation levels. The proposed method demonstrates improved performance across various vision-language tasks through the use of a Text-Guided Masked Image Modeling head and an attentive masking strategy.
  - **Keywords:** Multimodal alignment, Vision-language models, Contrastive Language-Image Pretraining (CLIP), Image Captioning (IC), Text-Guided Masked Image Modeling (TG-MIM), Image-text retrieval, Image captioning, Visual question answering, Image classification, Lack of local text-to-image reconstruction, Fine-grained image understanding, Improved multimodal alignment, Bidirectional interactions, Contrastive Captioners (CoCa), Image-Text Contrastive (ITC), Attentive masking strategy


- [A Provable Decision Rule for Out-of-Distribution Detection](https://icml.cc/virtual/2024/poster/34009) (Poster)
  - **Authors:** [Xinsong Ma](http://openreview.net/profile?id=~Xinsong_Ma1), [Xin Zou](http://openreview.net/profile?id=~Xin_Zou3), [Weiwei Liu](http://openreview.net/profile?id=~Weiwei_Liu1)
  - **Affiliations:** School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China, School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China, School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China
  - **TL;DR:** This paper proposes a novel decision rule for out-of-distribution detection using a generalized Benjamini Hochberg procedure, which provides rigorous theoretical guarantees and demonstrates superior empirical performance compared to traditional methods. The findings indicate that the false positive rate of the proposed method converges to zero in probability, enhancing reliability in safety-critical applications.
  - **Keywords:** Out-of-distribution detection, hypothesis testing, Generalized Benjamini Hochberg (g-BH) procedure, score function, Medical diagnosis, finance, self-driving, False discovery rate (FDR), false positive rate (FPR), Theoretical guarantees for decision rules, empirical performance evaluation


- [Faithfulness Measurable Masked Language Models](https://icml.cc/virtual/2024/poster/32851) (Spotlight Poster)
  - **Authors:** [Andreas Madsen](http://openreview.net/profile?id=~Andreas_Madsen1), [Siva Reddy](http://openreview.net/profile?id=~Siva_Reddy1), [Sarath Chandar](http://openreview.net/profile?id=~Sarath_Chandar1)
  - **Affiliations:** Mila, Montreal, Canada; Computer Engineering and Software Engineering Department, Polytechnique Montreal, Montreal, Canada, Mila, Montreal, Canada; Computer Science and Linguistics, McGill University, Montreal, Canada; Facebook CIFAR AI Chair, Mila, Montreal, Canada; Canada CIFAR AI Chair
  - **TL;DR:** This study proposes a novel fine-tuning method for NLP models that enables in-distribution masking of tokens to measure the faithfulness of importance measures. The approach addresses challenges related to out-of-distribution issues and allows for optimizing explanations towards maximal faithfulness, making the model indirectly explainable.
  - **Keywords:** Explainability, Faithfulness in NLP, Importance measures, Masking, Fine-tuning, Natural Language Processing (NLP), Out-of-distribution issues, False explanations, Measuring faithfulness, Inherently faithfulness measurable model, Statistical in-distribution tests, 16 different datasets


- [LASER: Linear Compression in Wireless Distributed Optimization](https://icml.cc/virtual/2024/poster/32920) (Poster)
  - **Authors:** [Ashok Vardhan Makkuva](http://openreview.net/profile?id=~Ashok_Vardhan_Makkuva1), [Marco Bondaschi](http://openreview.net/profile?id=~Marco_Bondaschi1), [Thijs Vogels](http://openreview.net/profile?id=~Thijs_Vogels1), [Martin Jaggi](http://openreview.net/profile?id=~Martin_Jaggi1), [Hyeji Kim](http://openreview.net/profile?id=~Hyeji_Kim1), [Michael Gastpar](http://openreview.net/profile?id=~Michael_Gastpar1)
  - **Affiliations:** School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland, School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland, School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland, School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland, Department of Electrical and Computer Engineering, UT Austin, Austin, TX, USA, School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland
  - **TL;DR:** The paper introduces LASER, a gradient compression scheme designed for efficient distributed optimization over noisy wireless channels, addressing the communication bottleneck in data-parallel SGD. LASER demonstrates significant performance improvements on practical benchmarks, particularly in computer vision and language modeling tasks.
  - **Keywords:** distributed optimization, data-parallel SGD, communication-efficient algorithms, gradient compression, quantization, sparsification, low-rank compression, over-the-air aggregation, large scale machine learning, computer vision, language modeling, communication bottleneck, noisy communication channels, scalability issues, LASER (LineAr CompreSsion in WirEless DistRibuted Optimization), performance improvement in perplexity


- [Correcting Diffusion-Based Perceptual Image Compression with Privileged End-to-End Decoder](https://icml.cc/virtual/2024/poster/35146) (Poster)
  - **Authors:** [Yiyang Ma](http://openreview.net/profile?id=~Yiyang_Ma1), [Wenhan Yang](http://openreview.net/profile?id=~Wenhan_Yang1), [Jiaying Liu](http://openreview.net/profile?id=~Jiaying_Liu1)
  - **Affiliations:** Wangxuan Institute of Computer Technology, Peking University, Beijing, China, Pengcheng Laboratory, Shenzhen, China, Wangxuan Institute of Computer Technology, Peking University, Beijing, China
  - **TL;DR:** This paper presents a novel diffusion-based image compression method that integrates a privileged end-to-end decoder to enhance perceptual quality while managing distortion. Experiments show that this approach outperforms previous methods in both distortion and perceptual quality.
  - **Keywords:** image compression, diffusion models, end-to-end decoder, convolutional decoder, video streaming, augmented reality, distortion, perceptual quality, better approximation of score function, improved perceptual quality


- [Tilting the Odds at the Lottery: the Interplay of Overparameterisation and Curricula in Neural Networks](https://icml.cc/virtual/2024/poster/34809) (Poster)
  - **Authors:** [Stefano Mannelli](http://openreview.net/profile?id=~Stefano_Sarao_Mannelli1), [Yaraslau Ivashynka](http://openreview.net/profile?id=~Yaraslau_Ivashynka1), [Andrew Saxe](http://openreview.net/profile?id=~Andrew_M_Saxe1), [Luca Saglietti](http://openreview.net/profile?id=~Luca_Saglietti1)
  - **Affiliations:** Gatsby Computational Neuroscience Unit & Sainsbury Wellcome Centre, University College London, London, UK, Department of Computing Sciences, Bocconi University, Milano, Italy, Gatsby Computational Neuroscience Unit & Sainsbury Wellcome Centre, University College London, London, UK, Department of Computing Sciences, Bocconi University, Milano, Italy
  - **TL;DR:** This study investigates the interplay between overparameterisation and curriculum learning in neural networks, revealing that while overparameterisation simplifies learning, it can diminish the benefits of curriculum strategies. The findings provide a theoretical explanation for the limited effectiveness of curriculum learning in deep learning applications.
  - **Keywords:** Overparameterisation, Curriculum Learning, Neural Networks, Lottery Ticket Hypothesis, Online Learning, Deep Learning, Machine Learning, Ineffectiveness of Curriculum Learning, High Degree of Overparameterisation, Impact on Manifold Discovery, Labeling Rule Identification, XOR-like Gaussian Mixture Model


- [Measures of diversity and space-filling designs for categorical data](https://icml.cc/virtual/2024/poster/33760) (Poster)
  - **Authors:** [Cedric Malherbe](http://openreview.net/profile?id=~Cedric_Malherbe1), [Emilio Domínguez-Sánchez](http://openreview.net/profile?id=~Emilio_Dom%C3%ADnguez-S%C3%A1nchez1), [Merwan Barlier](http://openreview.net/profile?id=~Merwan_Barlier1), [Igor Colin](http://openreview.net/profile?id=~Igor_Colin1), [Haitham Bou Ammar](http://openreview.net/profile?id=~Haitham_Bou_Ammar1), [Tom Diethe](http://openreview.net/profile?id=~Tom_Diethe1)
  - **Affiliations:** DS&AI, BioPharmaceuticals R&D, AstraZeneca, UK, None, None, LTCI, Télécom Paris, University College London; Huawei Noah’s Ark Lab, None
  - **TL;DR:** This paper addresses the challenge of selecting diverse subsets from categorical data using novel combinatorial optimization methods. It presents theoretical insights and practical approaches to improve diversity metrics in data analysis and machine learning applications.
  - **Keywords:** diversity in categorical data, combinatorial optimization, linear programming, submodular optimization, data analysis, machine learning, challenges in selecting diverse subsets from categorical features, lack of natural ordering in categorical data, novel methods for subset selection, theoretical insights into diversity metrics, average covering radius, packing radius, covering radius


- [Submodular framework for structured-sparse optimal transport](https://icml.cc/virtual/2024/poster/33628) (Poster)
  - **Authors:** [Piyushi Manupriya](http://openreview.net/profile?id=~Piyushi_Manupriya1), [Pratik Kumar Jawanpuria](http://openreview.net/profile?id=~Pratik_Jawanpuria1), [Karthik Gurumoorthy](http://openreview.net/profile?id=~Karthik_S._Gurumoorthy2), [Sakethanath Jagarlapudi](http://openreview.net/profile?id=~SakethaNath_Jagarlapudi1), [Bamdev Mishra](http://openreview.net/profile?id=~Bamdev_Mishra1)
  - **Affiliations:** Department of Computer Science and Engineering, IIT Hyderabad, India, Microsoft, India, Walmart Global Tech, India, Department of Computer Science and Engineering, IIT Hyderabad, India, Microsoft, India
  - **TL;DR:** This study introduces novel sparsity-constrained formulations for unbalanced optimal transport (UOT) that allow for structured sparse transport plans. The proposed methods leverage maximum mean discrepancy and demonstrate effective greedy algorithms for diverse applications, enhancing interpretability and robustness in transport plans.
  - **Keywords:** Unbalanced optimal transport (UOT), structured sparse transport plans, Maximum mean discrepancy (MMD), sparsity-constrained UOT formulations, gradient-based discrete greedy algorithms, Domain adaptation, ecological inference, multi-label classification, Learning sparse transport plans, robustness in noisy measures, Novel sparsity-constrained UOT formulations, theoretical guarantees for algorithms


- [Split-and-Denoise: Protect large language model inference with local differential privacy](https://icml.cc/virtual/2024/poster/33633) (Poster)
  - **Authors:** [Peihua Mai](http://openreview.net/profile?id=~Peihua_Mai1), [Ran Yan](http://openreview.net/profile?id=~Ran_Yan3), [Zhe Huang](http://openreview.net/profile?id=~Zhe_Huang6), [Youjia Yang](http://openreview.net/profile?id=~Youjia_Yang1), [Yan (James) Pang](http://openreview.net/profile?id=~Yan_Pang1)
  - **Affiliations:** National University of Singapore, National University of Singapore, North China Electric Power University, University of South California, National University of Singapore
  - **TL;DR:** This paper introduces the Split-N-Denoise (SnD) framework, which enhances privacy during the inference of Large Language Models by executing the token embedding layer on the client side and adding noise before transmission. The approach significantly improves the privacy-utility tradeoff, achieving over 10% better performance under the same privacy budget compared to existing methods.
  - **Keywords:** Large Language Models, Privacy Preservation, Local Differential Privacy, Split Learning, Denoising Techniques, Embedding-as-a-Service (EaaS), Privacy Leakage, Utility-Privacy Tradeoff, Split-N-Denoise (SnD) Framework, Performance Improvement under Privacy Budget


- [Towards General Neural Surrogate Solvers with Specialized Neural Accelerators](https://icml.cc/virtual/2024/poster/34538) (Poster)
  - **Authors:** [Chenkai Mao](http://openreview.net/profile?id=~Chenkai_Mao1), [Robert Lupoiu](http://openreview.net/profile?id=~Robert_Lupoiu1), [Tianxiang Dai](http://openreview.net/profile?id=~Tianxiang_Dai1), [Mingkun Chen](http://openreview.net/profile?id=~Mingkun_Chen1), [Jonathan Fan](http://openreview.net/profile?id=~Jonathan_Fan1)
  - **Affiliations:** Department of Electrical Engineering, Stanford, Palo Alto, USA, Department of Electrical Engineering, Stanford, Palo Alto, USA, Department of Electrical Engineering, Stanford, Palo Alto, USA, Department of Electrical Engineering, Stanford, Palo Alto, USA, Department of Electrical Engineering, Stanford, Palo Alto, USA
  - **TL;DR:** This study introduces Specialized Neural Accelerator-Powered Domain Decomposition Methods (SNAP-DDM) to enhance the solving of large-scale PDE problems with arbitrary domain sizes and boundary conditions. The proposed method demonstrates near unity accuracy in 2D electromagnetics and fluidic flow applications, addressing limitations of existing neural network-based approaches.
  - **Keywords:** Neural network-based PDE solvers, Domain decomposition methods, Specialized neural operators, Neural Operators, Domain Decomposition Methods (DDM), 2D electromagnetics, Fluidic flow problems, Fixed domain sizes, Arbitrary boundary conditions, High spatial frequency phenomena, Specialized surrogate subdomain solvers, Near unity accuracy, Physics Informed Neural Network (PINN), PDE-Net, DeepONet, Fourier Neural Operators (FNO)


- [$H$-Consistency Guarantees for Regression](https://icml.cc/virtual/2024/poster/33103) (Poster)
  - **Authors:** [Anqi Mao](http://openreview.net/profile?id=~Anqi_Mao1), [Mehryar Mohri](http://openreview.net/profile?id=~Mehryar_Mohri2), [Yutao Zhong](http://openreview.net/profile?id=~Yutao_Zhong1)
  - **Affiliations:** Courant Institute of Mathematical Sciences, New York, NY; None, Google Research, New York, NY, Courant Institute of Mathematical Sciences, New York, NY; None
  - **TL;DR:** This study presents new H-consistency bounds for surrogate loss functions in regression, demonstrating their effectiveness in handling outliers and promoting sparser solutions. The findings lead to the development of novel algorithms for adversarial regression, showing promising experimental results.
  - **Keywords:** H-consistency, regression, Surrogate loss functions, Huber loss, `p losses, squared ε-insensitive loss, ε-insensitive loss, Adversarial regression, Handling outliers, ensuring sparser solutions, computational intractability, Novel H-consistency bounds, principled surrogate losses, algorithms for adversarial regression


- [Large Language Models are Geographically Biased](https://icml.cc/virtual/2024/poster/32916) (Poster)
  - **Authors:** [Rohin Manvi](http://openreview.net/profile?id=~Rohin_Manvi1), [Samar Khanna](http://openreview.net/profile?id=~Samar_Khanna1), [Marshall Burke](http://openreview.net/profile?id=~Marshall_Burke1), [David Lobell](http://openreview.net/profile?id=~David_B._Lobell1), [Stefano Ermon](http://openreview.net/profile?id=~Stefano_Ermon1)
  - **Affiliations:** Stanford University, Stanford University, Stanford University, Stanford University, Stanford University
  - **TL;DR:** This study investigates the geographic biases present in Large Language Models (LLMs), revealing that they often exhibit systemic errors in geospatial predictions, particularly against locations with lower socioeconomic conditions. The findings highlight the importance of understanding these biases to mitigate potential societal harm and improve fairness in LLM applications.
  - **Keywords:** Large Language Models, Geographic Bias, Systemic errors in geospatial predictions, biases against lower socioeconomic conditions, Bias score to quantify geographic bias, Bias in Open-Ended Language Generation Dataset (BOLD), Bias Benchmark for QA (BBQ), Societal harm, fairness, accuracy


- [Keep the Momentum: Conservation Laws beyond Euclidean Gradient Flows](https://icml.cc/virtual/2024/poster/33401) (Poster)
  - **Authors:** [Sibylle Marcotte](http://openreview.net/profile?id=~Sibylle_Marcotte1), [Rémi Gribonval](http://openreview.net/profile?id=~R%C3%A9mi_Gribonval1), [Gabriel Peyré](http://openreview.net/profile?id=~Gabriel_Peyr%C3%A92)
  - **Affiliations:** ENS - PSL Univ.; None, Univ Lyon, EnsL, UCBL, CNRS, Inria, LIP, CNRS
  - **TL;DR:** This paper investigates conservation laws in the context of momentum-based dynamics and non-Euclidean geometries, revealing that these laws exhibit temporal dependence and often result in a conservation loss compared to gradient flows. The findings highlight the reduced number of conservation laws in momentum dynamics, particularly for linear and ReLU networks.
  - **Keywords:** Conservation laws, Gradient flows, Momentum dynamics, Neural network training, Nonnegative Matrix Factorization (NMF), Conservation loss, Temporal dependence, Identification of momentum conservation laws, Global convergence of gradient descent, ReLU networks, Linear networks, Non-Euclidean metrics


- [Using AI Uncertainty Quantification to Improve Human Decision-Making](https://icml.cc/virtual/2024/poster/33454) (Poster)
  - **Authors:** [Laura Marusich](http://openreview.net/profile?id=~Laura_Marusich1), [Jonathan Bakdash](http://openreview.net/profile?id=~Jonathan_Bakdash1), [Yan Zhou](http://openreview.net/profile?id=~Yan_Zhou2), [Murat Kantarcioglu](http://openreview.net/profile?id=~Murat_Kantarcioglu1)
  - **Affiliations:** DEVCOM Army Research Laboratory, University of Texas at Dallas, Richardson, TX, University of Texas at Dallas, Richardson, TX, University of Texas at Dallas, Richardson, TX
  - **TL;DR:** The study investigates the impact of AI Uncertainty Quantification (UQ) on human decision-making, demonstrating that well-calibrated UQ significantly enhances decision accuracy and confidence calibration compared to AI predictions alone. The findings suggest that implementing high-quality UQ can improve decision-making in real systems across various representations of probabilistic information.
  - **Keywords:** AI Uncertainty Quantification, Human Decision-Making, Instance-level UQ, Calibration, Scoring Rule, Human-AI Interaction, Decision Accuracy, Trust in AI, Communication of Uncertainty, Improved Decision-Making Performance, Confidence Calibration, Explainability, Interpretability


- [Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling](https://icml.cc/virtual/2024/poster/32823) (Poster)
  - **Authors:** [Ivan Marisca](http://openreview.net/profile?id=~Ivan_Marisca1), [Cesare Alippi](http://openreview.net/profile?id=~Cesare_Alippi1), [Filippo Maria Bianchi](http://openreview.net/profile?id=~Filippo_Maria_Bianchi1)
  - **Affiliations:** IDSIA USI-SUPSI, Università della Svizzera italiana; Politecnico di Milano, IDSIA USI-SUPSI, Università della Svizzera italiana; Politecnico di Milano, Dept. of Mathematics and Statistics, UiT the Arctic University of Norway; NORCE, Norwegian Research Centre AS
  - **TL;DR:** This study presents a novel approach for spatiotemporal forecasting that addresses the challenges of missing data through hierarchical spatiotemporal downsampling and an attention mechanism. The proposed method outperforms existing techniques, particularly in scenarios with contiguous blocks of missing values, demonstrating its effectiveness in capturing complex spatiotemporal dynamics.
  - **Keywords:** Spatiotemporal forecasting, Time-series analysis, Sensor networks, Spatiotemporal graph neural networks (STGNNs), Hierarchical spatiotemporal downsampling, Attention mechanism, Air quality monitoring, Traffic monitoring, Missing data, Contiguous blocks of missing values, High-dimensional data, Improved forecasting methods, Multi-scale spatiotemporal representations, Graph neural networks (GNNs), Temporal hierarchy, Spatial hierarchy


- [On the Consistency of Kernel Methods with Dependent Observations](https://icml.cc/virtual/2024/poster/34778) (Poster)
  - **Authors:** [Pierre-François Massiani](http://openreview.net/profile?id=~Pierre-Fran%C3%A7ois_Massiani2), [Sebastian Trimpe](http://openreview.net/profile?id=~Sebastian_Trimpe1), [Friedrich Solowjow](http://openreview.net/profile?id=~Friedrich_Solowjow1)
  - **Affiliations:** Institute for Data Science in Mechanical Engineering, RWTH Aachen University, Aachen, Germany, Institute for Data Science in Mechanical Engineering, RWTH Aachen University, Aachen, Germany, Institute for Data Science in Mechanical Engineering, RWTH Aachen University, Aachen, Germany
  - **TL;DR:** This paper introduces the concept of empirical weak convergence (EWC) to establish the consistency of kernel methods like SVMs and Gaussian processes under non-i.i.d. sampling conditions. The findings suggest that these methods can perform well despite dependencies in training data, paving the way for a broader understanding of statistical learning beyond traditional assumptions.
  - **Keywords:** kernel methods, empirical weak convergence, statistical learning, support vector machines (SVMs), Gaussian processes, conditional kernel mean embeddings (CKMEs), consistency of learning methods, non-i.i.d. data, dependencies in training data, new notion of empirical weak convergence (EWC), consistency results for SVMs and kernel mean embeddings


- [No-Regret Reinforcement Learning in Smooth MDPs](https://icml.cc/virtual/2024/poster/34507) (Poster)
  - **Authors:** [Davide Maran](http://openreview.net/profile?id=~Davide_Maran1), [Alberto Maria Metelli](http://openreview.net/profile?id=~Alberto_Maria_Metelli2), [Matteo Papini](http://openreview.net/profile?id=~Matteo_Papini1), [Marcello Restelli](http://openreview.net/profile?id=~Marcello_Restelli1)
  - **Affiliations:** Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy
  - **TL;DR:** This paper addresses the challenge of obtaining no-regret guarantees in reinforcement learning for continuous state and action spaces by introducing a novel structural assumption called ν-smoothness and proposing two algorithms for regret minimization. The algorithms leverage Legendre polynomials and demonstrate improved regret properties compared to existing methods in RL theory.
  - **Keywords:** reinforcement learning, no-regret guarantees, continuous state and action spaces, ν-smoothness, LEGENDRE-ELEANOR, LEGENDRE-LSVI, orthogonal feature map, Legendre polynomials, robotics, autonomous driving, trading, challenges in continuous state/action spaces, regret minimization, algorithms for regret minimization, best guarantees in RL theory, MUJOCO, Markov decision processes (MDPs), linear MDPs, Lipschitz MDPs


- [tinyBenchmarks: evaluating LLMs with fewer examples](https://icml.cc/virtual/2024/poster/33007) (Poster)
  - **Authors:** [Felipe Maia Polo](http://openreview.net/profile?id=~Felipe_Maia_Polo1), [Lucas Weber](http://openreview.net/profile?id=~Lucas_Weber1), [Leshem Choshen](http://openreview.net/profile?id=~Leshem_Choshen1), [Yuekai Sun](http://openreview.net/profile?id=~Yuekai_Sun1), [Gongjun Xu](http://openreview.net/profile?id=~Gongjun_Xu1), [Mikhail Yurochkin](http://openreview.net/profile?id=~Mikhail_Yurochkin1)
  - **Affiliations:** Department of Statistics, University of Michigan, USA, Department of Translation and Language Sciences, University of Pompeu Fabra, Spain, IBM Research; MIT, Department of Statistics, University of Michigan, USA, Department of Statistics, University of Michigan, USA, IBM Research; MIT-IBM Watson AI Lab
  - **TL;DR:** This paper investigates strategies to reduce the number of evaluations needed to assess the performance of large language models (LLMs) on key benchmarks, demonstrating that evaluating on a small curated subset can reliably estimate performance. The authors release tools and tiny versions of popular benchmarks to facilitate efficient evaluation, significantly lowering computational and financial costs.
  - **Keywords:** Large Language Models (LLMs), Evaluation of LLMs, IRT++, Stratified random sampling, Expensive evaluation of LLMs, Need for efficient evaluation strategies, Tools for efficient LLM evaluation, Tiny benchmark datasets, Open LLM Leaderboard, MMLU, HELM, AlpacaEval 2.0


- [On the Tractability of SHAP Explanations under Markovian Distributions](https://icml.cc/virtual/2024/poster/33373) (Poster)
  - **Authors:** [Reda Marzouk](http://openreview.net/profile?id=~Reda_Marzouk1), [De la Higuera](http://openreview.net/profile?id=~De_la_Higuera1)
  - **Affiliations:** LS2N, Université de Nantes, France, LS2N, Université de Nantes, France
  - **TL;DR:** This study investigates the computational complexity of the SHAP score by relaxing the feature independence assumption and introducing a Markovian perspective. It demonstrates that computing the SHAP score for certain model classes can be performed in polynomial time, providing a significant advancement in the tractability of SHAP explanations.
  - **Keywords:** Explainability, SHAP framework, computational complexity, SHAP score, Weighted automata, Disjoint DNFs, Decision Trees, Interpretable machine learning, stochastic modeling, NP-Hard computation, feature independence assumption, Polynomial-time computation of SHAP score under Markovian assumption, Markovian distributions


- [Auto-Regressive Next-Token Predictors are Universal Learners](https://icml.cc/virtual/2024/poster/33369) (Poster)
  - **Authors:** [Eran Malach](http://openreview.net/profile?id=~eran_malach1)
  - **Affiliations:** Harvard University, Kempner Institute for the Study of Natural and Artificial Intelligence
  - **TL;DR:** This study presents a theoretical framework demonstrating that simple auto-regressive next-token predictors, particularly linear models trained on Chain-of-Thought data, can approximate any function computable by a Turing machine. The findings suggest that the capabilities of large language models in logical reasoning stem largely from the auto-regressive training approach rather than the model architecture itself.
  - **Keywords:** Auto-regressive next-token prediction, Large language models, Logical reasoning, Chain-of-Thought (CoT) techniques, Linear next-token predictors, Shallow Multi-Layer Perceptrons (MLPs), Natural language processing, Text generation, Arithmetic tasks, Complexity of function approximation, Intermediate computations, Length complexity measure, Theoretical framework for next-token predictors, Turing machine, Next-token prediction


- [Delving into Differentially Private Transformer](https://icml.cc/virtual/2024/poster/34517) (Poster)
  - **Authors:** [Youlong Ding](http://openreview.net/profile?id=~Youlong_Ding1), [Xueyang Wu](http://openreview.net/profile?id=~Xueyang_Wu1), [Yining meng](http://openreview.net/profile?id=~Yining_meng1), [Yonggang Luo](http://openreview.net/profile?id=~Yonggang_Luo1), [Hao Wang](http://openreview.net/profile?id=~Hao_Wang3), [Pan Weike](http://openreview.net/profile?id=~Weike_Pan1)
  - **Affiliations:** College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; The Hebrew University of Jerusalem, Jerusalem, Israel, Hong Kong University of Science and Technology, Hong Kong SAR, China, Changan Automobile; Changan Technology Co., Ltd, Chongqing, China, Changan Automobile; Changan Technology Co., Ltd, Chongqing, China, Rutgers University, New Jersey, USA, College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China
  - **TL;DR:** This paper addresses the challenges of training Transformer models with differential privacy by proposing a modular approach that reduces the problem to training vanilla neural networks. Key contributions include the introduction of the Re-Attention Mechanism and Phantom Clipping to enhance model accuracy and training efficiency.
  - **Keywords:** Differential Privacy, Deep Learning, Transformer Models, Re-Attention Mechanism, Phantom Clipping, Natural Language Processing, Image Classification, Attention Distraction Phenomenon, Gradient Clipping Efficiency, Modular Treatment for DP Transformers, Differential Privacy, Vanilla Neural Nets, DP Transformers


- [Self-Composing Policies for Scalable Continual Reinforcement Learning](https://icml.cc/virtual/2024/poster/33472) (Oral)
  - **Authors:** [Mikel Malagón](http://openreview.net/profile?id=~Mikel_Malagon1), [Josu Ceberio](http://openreview.net/profile?id=~Josu_Ceberio1), [Jose A Lozano](http://openreview.net/profile?id=~Jose_A._Lozano1)
  - **Affiliations:** Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, Donostia-San Sebastian, Spain; Basque Center for Applied Mathematics (BCAM), Bilbao, Spain, Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, Donostia-San Sebastian, Spain, Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, Donostia-San Sebastian, Spain; Basque Center for Applied Mathematics (BCAM), Bilbao, Spain
  - **TL;DR:** This study presents a modular neural network architecture called CompoNet that effectively addresses catastrophic forgetting in continual reinforcement learning by allowing self-composing policies. The proposed approach demonstrates improved knowledge transfer and performance while maintaining a linear growth in parameters with respect to the number of tasks.
  - **Keywords:** Continual Reinforcement Learning, Neural Networks, Growable Neural Network Architecture, Self-Composing Policies, Continuous Control, Visual Problems, Catastrophic Forgetting, Interference, Scalability, Knowledge Transfer, Performance Improvement


- [SCoRe: Submodular Combinatorial Representation Learning](https://icml.cc/virtual/2024/poster/34512) (Poster)
  - **Authors:** [Anay Majee](http://openreview.net/profile?id=~Anay_Majee1), [Suraj Kothawade](http://openreview.net/profile?id=~Suraj_Nandkishor_Kothawade2), [Krishnateja Killamsetty](http://openreview.net/profile?id=~Krishnateja_Killamsetty1), [Rishabh Iyer](http://openreview.net/profile?id=~Rishabh_K_Iyer2)
  - **Affiliations:** Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA, Google Research, Sunnyvale, CA, USA, IBM Research, San Jose, CA, USA, Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA
  - **TL;DR:** The paper introduces the SCoRe framework for representation learning, which effectively addresses inter-class bias and intra-class variance in long-tail distributions. It demonstrates significant improvements in classification performance across various datasets, highlighting its versatility and effectiveness compared to existing methods.
  - **Keywords:** representation learning, long-tail distribution, inter-class bias, intra-class variance, submodular information measures, loss functions, supervised contrastive loss, orthogonal projection loss, N-pairs loss, image classification, object detection, class imbalance, data sparsity, few-shot learning, SCoRe framework, novel combinatorial formulations, improved classification metrics, CIFAR-10-LT, CIFAR-100-LT, MedMNIST, ImageNet-LT, IDD, LVIS (v1.0)


- [Regression with Multi-Expert Deferral](https://icml.cc/virtual/2024/poster/34983) (Spotlight Poster)
  - **Authors:** [Anqi Mao](http://openreview.net/profile?id=~Anqi_Mao1), [Mehryar Mohri](http://openreview.net/profile?id=~Mehryar_Mohri2), [Yutao Zhong](http://openreview.net/profile?id=~Yutao_Zhong1)
  - **Affiliations:** Courant Institute of Mathematical Sciences, New York, NY, Google Research, New York, NY, Courant Institute of Mathematical Sciences, New York, NY
  - **TL;DR:** This paper introduces a novel framework for regression with deferral, allowing predictions to be deferred to multiple experts, addressing unique challenges in regression. The proposed surrogate loss functions and H-consistency bounds provide stronger consistency guarantees, leading to effective algorithms demonstrated through extensive experiments.
  - **Keywords:** regression with deferral, multiple experts, surrogate loss functions, H-consistency bounds, natural language processing, speech recognition, image annotation, medical diagnosis, financial forecasting, computer vision, challenges in regression, instance-dependent costs, label-dependent costs, novel algorithms for regression with deferral


- [Deep Fusion: Efficient Network Training via Pre-trained Initializations](https://icml.cc/virtual/2024/poster/35023) (Poster)
  - **Authors:** [Hanna Mazzawi](http://openreview.net/profile?id=~Hanna_Mazzawi1), [Xavi Gonzalvo](http://openreview.net/profile?id=~Javier_Gonzalvo1), [Michael Wunder](http://openreview.net/profile?id=~Michael_Wunder2), [Sammy Jerome](http://openreview.net/profile?id=~Sammy_Jerome1), [Benoit Dherin](http://openreview.net/profile?id=~Benoit_Dherin1)
  - **Affiliations:** Google Research, New York, NY, USA, Google Research, New York, NY, USA, Google Research, New York, NY, USA, Google Research, New York, NY, USA, Google, Sunnyvale, CA, USA
  - **TL;DR:** The study introduces Deep Fusion, an efficient training method for large language models that utilizes pre-trained initializations to reduce computational costs and training time. The proposed approach maintains or surpasses the performance of traditional training methods across various NLP tasks.
  - **Keywords:** deep learning, natural language processing, large language models (LLMs), network growing algorithms, pre-trained initializations, backward error analysis, text generation, translation, summarization, question answering, high computational costs, energy consumption, training time, Deep Fusion, efficient training methods, optimized training dynamics, transformer, model compression, mixed-precision training


- [Convergence and Trade-Offs in Riemannian Gradient Descent and Riemannian Proximal Point](https://icml.cc/virtual/2024/poster/33194) (Poster)
  - **Authors:** [David Martínez-Rubio](http://openreview.net/profile?id=~David_Mart%C3%ADnez-Rubio2), [Christophe Roux](http://openreview.net/profile?id=~Christophe_Roux1), [Sebastian Pokutta](http://openreview.net/profile?id=~Sebastian_Pokutta1)
  - **Affiliations:** Zuse Institute Berlin, Germany; Technische Universität Berlin, Germany, Zuse Institute Berlin, Germany; Technische Universität Berlin, Germany, Zuse Institute Berlin, Germany; Technische Universität Berlin, Germany
  - **TL;DR:** This study analyzes Riemannian gradient descent and proximal point algorithms, quantifying their convergence rates and demonstrating that iterates remain close to an optimizer. The authors also introduce an inexact proximal point algorithm and establish new properties of Riemannian proximal methods, particularly in the presence of positive curvature.
  - **Keywords:** Riemannian optimization, geodesically convex optimization, Riemannian gradient descent, Riemannian proximal point, Convergence rates, bounded iterates, geometric deformations, New properties of Riemannian proximal methods, implementable inexact proximal point algorithm, g-convex, Moreau envelope


- [Entropy-Reinforced Planning with Large Language Models for Drug Discovery](https://icml.cc/virtual/2024/poster/34551) (Poster)
  - **Authors:** [Xuefeng Liu](http://openreview.net/profile?id=~Xuefeng_Liu2), [Chih-chan Tien](http://openreview.net/profile?id=~Chih-chan_Tien1), [Peng Ding](http://openreview.net/profile?id=~Peng_Ding2), [Songhao Jiang](http://openreview.net/profile?id=~Songhao_Jiang2), [Rick Stevens](http://openreview.net/profile?id=~Rick_L._Stevens1)
  - **Affiliations:** Department of Computer Science, University of Chicago, Chicago, IL, USA; Healin-AI LLC, Chicago, IL, USA, Department of Computer Science, University of Chicago, Chicago, IL, USA; Healin-AI LLC, Chicago, IL, USA, Department of Computer Science, University of Chicago, Chicago, IL, USA; Healin-AI LLC, Chicago, IL, USA, Department of Computer Science, University of Chicago, Chicago, IL, USA; Healin-AI LLC, Chicago, IL, USA, Department of Computer Science, University of Chicago, Chicago, IL, USA; Argonne National Laboratory, Lemont, IL, USA
  - **TL;DR:** This study introduces Entropy-Reinforced Planning (ERP) to enhance the molecular generation process using large language models, addressing issues of invalid and suboptimal molecule generation. The proposed method consistently outperforms existing algorithms in drug discovery benchmarks, demonstrating its effectiveness across various Transformer models.
  - **Keywords:** drug discovery, molecular design, large language models, Entropy-Reinforced Planning (ERP), Transformer decoding, Monte Carlo Tree Search (MCTS), pharmaceutical properties, SARS-CoV-2, cancer cell target protein, invalid molecule generation, sample inefficiency, exploration vs. exploitation balance, improvements in molecular generation properties, outperforming state-of-the-art algorithms, SARS-CoV-2 virus (3CLPro), human cancer cell target protein (RTCB), large language models (LLMs), Transformer, beam search


- [Position: Graph Foundation Models Are Already Here](https://icml.cc/virtual/2024/poster/34571) (Spotlight Poster)
  - **Authors:** [Haitao Mao](http://openreview.net/profile?id=~Haitao_Mao1), [Zhikai Chen](http://openreview.net/profile?id=~Zhikai_Chen3), [Wenzhuo Tang](http://openreview.net/profile?id=~Wenzhuo_Tang1), [Jianan Zhao](http://openreview.net/profile?id=~Jianan_Zhao2), [Yao Ma](http://openreview.net/profile?id=~Yao_Ma3), [Tong Zhao](http://openreview.net/profile?id=~Tong_Zhao3), [Neil Shah](http://openreview.net/profile?id=~Neil_Shah2), [Mikhail Galkin](http://openreview.net/profile?id=~Mikhail_Galkin1), [Jiliang Tang](http://openreview.net/profile?id=~Jiliang_Tang1)
  - **Affiliations:** Michigan State University, Michigan State University, Michigan State University, Université de Montréal; Mila - Québec AI Institute, Rensselaer Polytechnic Institute, Snap Inc., Snap Inc., Intel Labs, Michigan State University
  - **TL;DR:** The paper discusses the development of Graph Foundation Models (GFMs) aimed at leveraging extensive graph data for improved generalization across various tasks. It highlights the challenges of finding a suitable graph vocabulary to encode invariance in diverse graph structures, drawing parallels with foundation models in computer vision and natural language processing.
  - **Keywords:** Graph Foundation Models, Graph Neural Networks, Knowledge Graphs, Molecular Graphs, Positive transfer, Invariance across diverse graph data, Graph vocabulary construction, Foundation models, Pre-training, Downstream tasks


- [Roping in Uncertainty: Robustness and Regularization in Markov Games](https://icml.cc/virtual/2024/poster/32681) (Poster)
  - **Authors:** [Jeremy McMahan](http://openreview.net/profile?id=~Jeremy_McMahan1), [Giovanni Artiglio](http://openreview.net/profile?id=~Giovanni_Artiglio1), [Qiaomin Xie](http://openreview.net/profile?id=~Qiaomin_Xie1)
  - **Affiliations:** University of Wisconsin-Madison, USA, University of Wisconsin-Madison, USA, University of Wisconsin-Madison, USA
  - **TL;DR:** This study investigates robust Markov games (RMG) with s-rectangular uncertainty, establishing an equivalence between computing robust Nash equilibria and regularized Markov games. The findings reveal that while computing robust Nash equilibria is computationally challenging, a special structure allows for polynomial-time solutions in certain cases.
  - **Keywords:** robust Markov games, robustness, regularization, robust Nash equilibrium (RNE), Nash equilibrium (NE), regularized Markov games (MG), multi-agent environments, offline reinforcement learning (RL), sim-to-real gap, computational complexity, PPAD-hard problems, planning algorithm, provable robustness guarantees, efficient player-decomposability, L1 and L∞ ball uncertainty sets, Markov-perfect robust Nash Equilibrium (MPRNE), (s, a)-rectangularity


- [IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation](https://icml.cc/virtual/2024/poster/32888) (Poster)
  - **Authors:** [Luke Melas-Kyriazi](http://openreview.net/profile?id=~Luke_Melas-Kyriazi1), [Iro Laina](http://openreview.net/profile?id=~Iro_Laina1), [Christian Rupprecht](http://openreview.net/profile?id=~Christian_Rupprecht1), [Natalia Neverova](http://openreview.net/profile?id=~Natalia_Neverova1), [Andrea Vedaldi](http://openreview.net/profile?id=~Andrea_Vedaldi1), [Oran Gafni](http://openreview.net/profile?id=~Oran_Gafni1), [Filippos Kokkinos](http://openreview.net/profile?id=~Filippos_Kokkinos1)
  - **Affiliations:** Meta; University of Oxford, University of Oxford, University of Oxford, Meta, Meta, Meta, Meta
  - **TL;DR:** The study presents IM-3D, a novel method for generating high-quality 3D assets from text/image pairs by leveraging video diffusion models and Gaussian splatting for robust 3D reconstruction. This approach significantly improves multi-view generation while addressing issues of speed and stability associated with traditional text-to-3D generators.
  - **Keywords:** text-to-3D generation, multi-view generation, Score Distillation Sampling (SDS), Gaussian splatting, 3D asset generation, video diffusion models, slow generation, instability, artifacts in 3D reconstruction, IM-3D method, high-quality 3D outputs


- [OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization](https://icml.cc/virtual/2024/poster/33730) (Poster)
  - **Authors:** [Xiang Meng](http://openreview.net/profile?id=~Xiang_Meng1), [Shibal Ibrahim](http://openreview.net/profile?id=~Shibal_Ibrahim1), [Kayhan Behdin](http://openreview.net/profile?id=~Kayhan_Behdin1), [Hussein Hazimeh](http://openreview.net/profile?id=~Hussein_Hazimeh1), [Natalia Ponomareva](http://openreview.net/profile?id=~Natalia_Ponomareva1), [Rahul Mazumder](http://openreview.net/profile?id=~Rahul_Mazumder1)
  - **Affiliations:** Massachusetts Institute of Technology, Cambridge, MA, USA, Massachusetts Institute of Technology, Cambridge, MA, USA, Massachusetts Institute of Technology, Cambridge, MA, USA, Google Research, New York, NY, USA, Google Research, New York, NY, USA, Massachusetts Institute of Technology, Cambridge, MA, USA
  - **TL;DR:** This study presents OSSCAR, a novel framework for one-shot structured pruning in vision and language models that significantly reduces inference costs without requiring retraining. The proposed method achieves substantial improvements in efficiency and accuracy, handling models with up to 30 billion parameters.
  - **Keywords:** structured pruning, vision and language models, one-shot pruning, combinatorial optimization, layer-wise reconstruction objective, local combinatorial optimization algorithm, large language models, vision models, inference costs, model accuracy retention, computational efficiency, scalable optimization framework, improved utility-sparsity tradeoffs, WikiText, ResNet50, MobileNet, OPT-1.3B – OPT-30B


- [O$n$ Learning Deep O($n$)-Equivariant Hyperspheres](https://icml.cc/virtual/2024/poster/33047) (Poster)
  - **Authors:** [Pavlo Melnyk](http://openreview.net/profile?id=~Pavlo_Melnyk1), [Michael Felsberg](http://openreview.net/profile?id=~Michael_Felsberg2), [Mårten Wadenbäck](http://openreview.net/profile?id=~M%C3%A5rten_Wadenb%C3%A4ck1), [Andreas Robinson](http://openreview.net/profile?id=~Andreas_Robinson1), [Cuong Le](http://openreview.net/profile?id=~Cuong_Le1)
  - **Affiliations:** Computer Vision Laboratory, Department of Electrical Engineering, Linköping University, Sweden, Computer Vision Laboratory, Department of Electrical Engineering, Linköping University, Sweden, Computer Vision Laboratory, Department of Electrical Engineering, Linköping University, Sweden, Computer Vision Laboratory, Department of Electrical Engineering, Linköping University, Sweden, Computer Vision Laboratory, Department of Electrical Engineering, Linköping University, Sweden
  - **TL;DR:** This paper presents a novel approach to learning deep features that are equivariant under nD transformations using O(n)-equivariant neurons with spherical decision surfaces, termed Deep Equivariant Hyperspheres. The authors demonstrate that their method outperforms existing techniques on benchmark datasets, achieving a favorable balance between speed and performance.
  - **Keywords:** Deep learning, O(n)-equivariance, geometric deep learning, O(n)-equivariant neurons, spherical decision surfaces, invariant operator, Molecular analysis, protein design, catalyst design, Data transformations, high-dimensional data, Deep Equivariant Hyperspheres, Gram matrix, nD spheres, regular n-simplexes, orthogonal group O(n)


- [Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics](https://icml.cc/virtual/2024/poster/32785) (Oral)
  - **Authors:** [Siqi Miao](http://openreview.net/profile?id=~Siqi_Miao1), [Zhiyuan Lu](http://openreview.net/profile?id=~Zhiyuan_Lu1), [Mia Liu](http://openreview.net/profile?id=~Mia_Liu1), [Javier Duarte](http://openreview.net/profile?id=~Javier_Duarte1), [Pan Li](http://openreview.net/profile?id=~Pan_Li2)
  - **Affiliations:** Georgia Institute of Technology, Beijing University of Posts and Telecommunications, Purdue University, University of California San Diego, Georgia Institute of Technology
  - **TL;DR:** This study presents a novel transformer model, the LSH-based Efficient Point Transformer (HEPT), optimized for large-scale point cloud processing in scientific fields like high-energy physics. HEPT significantly outperforms existing graph neural networks and transformers in both accuracy and computational speed, addressing critical challenges in real-time data analysis.
  - **Keywords:** point cloud processing, geometric deep learning, high-energy physics, transformer model, locality-sensitive hashing (LSH), OR & AND-construction LSH, high-energy physics (HEP), astrophysics, drug discovery, computational complexity, real-time analysis, graph construction challenges, LSH-based Efficient Point Transformer (HEPT), error-complexity tradeoff analysis, graph neural networks (GNNs), k-NN, KD-trees


- [Can Implicit Bias Imply Adversarial Robustness?](https://icml.cc/virtual/2024/poster/34498) (Poster)
  - **Authors:** [Hancheng Min](http://openreview.net/profile?id=~Hancheng_Min1), [Rene Vidal](http://openreview.net/profile?id=~Rene_Vidal1)
  - **Affiliations:** University of Pennsylvania, Philadelphia, PA, USA, University of Pennsylvania, Philadelphia, PA, USA
  - **TL;DR:** This study investigates the impact of implicit bias in gradient-based training algorithms on adversarial robustness, revealing that while traditional ReLU networks generalize well, they are vulnerable to adversarial attacks. By modifying the activation function to a polynomial ReLU, the authors demonstrate that robustness can be achieved, emphasizing the importance of data structure and architecture design.
  - **Keywords:** Implicit bias, Adversarial robustness, Gradient-based training, Gradient flow, ReLU activation, Polynomial ReLU (pReLU), Deep neural networks, Binary classification, Adversarial attacks, Non-robust classifiers, Data distribution challenges, Robust networks, Neural alignment perspective, Clusters with small inter-cluster correlation


- [The Illusion of State in State-Space Models](https://icml.cc/virtual/2024/poster/34075) (Poster)
  - **Authors:** [William Merrill](http://openreview.net/profile?id=~William_Merrill1), [Jackson Petty](http://openreview.net/profile?id=~Jackson_Petty1), [Ashish Sabharwal](http://openreview.net/profile?id=~Ashish_Sabharwal1)
  - **Affiliations:** New York University, New York University, Allen Institute for AI
  - **TL;DR:** This study investigates the expressive power of state-space models (SSMs) compared to transformers, revealing that SSMs like S4 and Mamba have similar limitations in state tracking capabilities. The findings suggest that despite their design, SSMs cannot effectively solve certain sequential problems, indicating a need for more expressive architectures.
  - **Keywords:** State-space models, Transformers, Sequential computation, State tracking, S4, Mamba, Recurrent neural networks (RNNs), Large language models (LLMs), Entity tracking, Limitations of transformers, Inherently sequential problems, State tracking challenges, Expressiveness limitations of SSMs, Development of new SSM architectures


- [Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation](https://icml.cc/virtual/2024/poster/32969) (Poster)
  - **Authors:** [Thomas Merth](http://openreview.net/profile?id=~Thomas_Merth2), [Qichen Fu](http://openreview.net/profile?id=~Qichen_Fu2), [Mohammad Rastegari](http://openreview.net/profile?id=~Mohammad_Rastegari2), [Mahyar Najibi](http://openreview.net/profile?id=~Mahyar_Najibi1)
  - **Affiliations:** Apple, Cupertino, CA, USA, Apple, Cupertino, CA, USA, Meta, Menlo Park, CA, USA, Apple, Cupertino, CA, USA
  - **TL;DR:** This study introduces superposition prompting, a novel methodology for improving and accelerating retrieval-augmented generation (RAG) in large language models (LLMs). The approach significantly reduces compute time by 93× while enhancing accuracy by 43% on the NaturalQuestions-Open dataset compared to naive RAG methods.
  - **Keywords:** Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Superposition prompting, Transformer-based models, Question-answering benchmarks, Text processing, Long context processing, Inference cost, Distraction phenomenon, Time efficiency improvement, Accuracy enhancement, NaturalQuestions-Open, MPT-7B instruction-tuned model, Hallucination


- [Understanding Retrieval-Augmented Task Adaptation for Vision-Language Models](https://icml.cc/virtual/2024/poster/34055) (Poster)
  - **Authors:** [Yifei Ming](http://openreview.net/profile?id=~Yifei_Ming1), [Sharon Li](http://openreview.net/profile?id=~Yixuan_Li1)
  - **Affiliations:** Department of Computer Sciences, University of Wisconsin-Madison, Department of Computer Sciences, University of Wisconsin-Madison
  - **TL;DR:** This study investigates retrieval-augmented adaptation for vision-language models, revealing that image-to-image retrieval outperforms text-to-image retrieval and that logit ensemble significantly enhances adaptation performance in low-data scenarios. The findings provide new insights and theoretical support for improving adaptation methods in real-world applications.
  - **Keywords:** retrieval-augmented adaptation, vision-language models, contrastive vision-language pre-training, image-to-image (I2I) retrieval, text-to-image (T2I) retrieval, logit ensemble, natural language processing, computer vision, adaptation to low-data regimes, fine-grained categories, distributional shifts, insights on retrieval methods, theoretical underpinnings for adaptation, web-scale databases, large-scale text and image databases


- [Physics-Informed Neural Network Policy Iteration: Algorithms, Convergence, and Verification](https://icml.cc/virtual/2024/poster/32906) (Poster)
  - **Authors:** [Yiming Meng](http://openreview.net/profile?id=~Yiming_Meng1), [Ruikun Zhou](http://openreview.net/profile?id=~Ruikun_Zhou1), [Amartya Mukherjee](http://openreview.net/profile?id=~Amartya_Mukherjee1), [Maxwell Fitzsimmons](http://openreview.net/profile?id=~Maxwell_Fitzsimmons1), [Christopher Song](http://openreview.net/profile?id=~Christopher_Song1), [Jun Liu](http://openreview.net/profile?id=~Jun_Liu11)
  - **Affiliations:** None, None, Department of Applied Mathematics, University of Waterloo, Waterloo, Canada, Department of Applied Mathematics, University of Waterloo, Waterloo, Canada, Department of Applied Mathematics, University of Waterloo, Waterloo, Canada, Coordinated Science Laboratory, University of Illinois Urbana-Champaign, Champaign–Urbana, Illinois, United States; Department of Applied Mathematics, University of Waterloo, Waterloo, Canada
  - **TL;DR:** This study presents algorithms for model-based policy iterations to solve nonlinear optimal control problems, ensuring convergence through neural approximations of linear PDEs. The proposed methods significantly outperform traditional approaches and provide theoretical guarantees for stability and accuracy in high-dimensional settings.
  - **Keywords:** Nonlinear optimal control, Policy iteration, Reinforcement learning, Neural approximations, Linear partial differential equations (PDEs), Extreme learning machines (ELM), Physics-informed neural networks (PINN), High-dimensional problems, Control systems, Curse of dimensionality, Non-differentiable cost functions, Hamilton–Jacobi–Bellman (HJB) equation, Convergence guarantees, Verification techniques, Suboptimal solutions, Generalized Hamilton-Jacobi-Bellman (GHJB) equation, Viscosity solutions


- [Prodigy: An Expeditiously Adaptive Parameter-Free Learner](https://icml.cc/virtual/2024/poster/34385) (Poster)
  - **Authors:** [Konstantin Mishchenko](http://openreview.net/profile?id=~Konstantin_Mishchenko1), [Aaron Defazio](http://openreview.net/profile?id=~Aaron_Defazio1)
  - **Affiliations:** Samsung AI Center, Fundamental AI Research Team, Meta
  - **TL;DR:** The paper introduces Prodigy, an algorithm that estimates the distance to the solution for optimal learning rate setting in adaptive methods, improving convergence rates compared to existing methods. Experimental results demonstrate that Prodigy consistently outperforms D-Adaptation and achieves accuracy levels comparable to hand-tuned Adam across various datasets and models.
  - **Keywords:** adaptive learning rates, optimization, Prodigy, D-Adaptation, AdaGrad, Adam, machine learning, computer vision, natural language processing, reinforcement learning, learning rate estimation, convergence speed, multi-agent optimization, parameter-free adaptive learning rate methods, improved convergence rates, CIFAR10, Imagenet, IWSLT14, Criteo, Knee MRI dataset, Book-Wiki, GANs (Generative Adversarial Networks), Neural Architecture Search (NAS), LSTM (Long Short-Term Memory networks), ViT (Vision Transformer)


- [Rethinking Momentum Knowledge Distillation in Online Continual Learning](https://icml.cc/virtual/2024/poster/33942) (Poster)
  - **Authors:** [Nicolas MICHEL](http://openreview.net/profile?id=~Nicolas_Michel1), [Maorong Wang](http://openreview.net/profile?id=~Maorong_Wang1), [Ling Xiao](http://openreview.net/profile?id=~Ling_Xiao2), [Toshihiko Yamasaki](http://openreview.net/profile?id=~Toshihiko_Yamasaki1)
  - **Affiliations:** Univ Gustave Eiffel, CNRS, LIGM, F-77454 Marne-la-Vallée, France, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
  - **TL;DR:** This study introduces a methodology for applying Momentum Knowledge Distillation (MKD) to Online Continual Learning (OCL), addressing challenges such as Catastrophic Forgetting and improving accuracy by over 10% on ImageNet100. The findings suggest that MKD should be a central component in OCL strategies.
  - **Keywords:** Online Continual Learning (OCL), Knowledge Distillation (KD), Catastrophic Forgetting (CF), Momentum Knowledge Distillation (MKD), Experience Replay (ER), Image Recognition, Catastrophic Forgetting, non-i.i.d. data, sequential learning, Improved accuracy on ImageNet100, enhanced OCL methods, ImageNet100


- [Position: Tensor Networks are a Valuable Asset for Green AI](https://icml.cc/virtual/2024/poster/33164) (Poster)
  - **Authors:** [Eva Memmel](http://openreview.net/profile?id=~Eva_Memmel1), [Clara Menzen](http://openreview.net/profile?id=~Clara_Menzen1), [Jetze Schuurmans](http://openreview.net/profile?id=~Jetze_Schuurmans1), [Frederiek Wesel](http://openreview.net/profile?id=~Frederiek_Wesel1), [Kim Batselier](http://openreview.net/profile?id=~kim_batselier1)
  - **Affiliations:** Delft Center of Systems and Control, Delft University of Technology, Delft, The Netherlands, Delft Center of Systems and Control, Delft University of Technology, Delft, The Netherlands, Xebia Data, Amsterdam, The Netherlands, Delft Center of Systems and Control, Delft University of Technology, Delft, The Netherlands, Delft Center of Systems and Control, Delft University of Technology, Delft, The Netherlands
  - **TL;DR:** This position paper establishes a link between tensor networks and Green AI, advocating for their integration to enhance sustainability and inclusivity in AI research. It emphasizes the need for efficiency metrics alongside traditional accuracy measures to address the unsustainable growth of AI development.
  - **Keywords:** Tensor Networks, Green AI, Sustainability, Inclusivity, Tensor Networks (TNs), Efficiency Metrics, Kernel Machines, Deep Learning, Unsustainable AI Development, High Carbon Footprint, Resource Inequality, Integration of TNs into AI Research, Efficiency as a Benchmark


- [RODEO: Robust Outlier Detection via Exposing Adaptive Out-of-Distribution Samples](https://icml.cc/virtual/2024/poster/32673) (Poster)
  - **Authors:** [Hossein Mirzaei](http://openreview.net/profile?id=~Hossein_Mirzaei1), [Mohammad Jafari Varnousfaderani](http://openreview.net/profile?id=~Mohammad_Jafari1), [Hamid Reza Dehbashi](http://openreview.net/profile?id=~Hamid_Reza_Dehbashi1), [Ali Ansari](http://openreview.net/profile?id=~Ali_Ansari2), [Sepehr Ghobadi](http://openreview.net/profile?id=~Sepehr_Ghobadi1), [Masoud Hadi](http://openreview.net/profile?id=~Masoud_Hadi1), [Arshia Soltani Moakhar](http://openreview.net/profile?id=~Arshia_Soltani_Moakhar1), [Mohammad Azizmalayeri](http://openreview.net/profile?id=~Mohammad_Azizmalayeri1), [Mahdieh Soleymani Baghshah](http://openreview.net/profile?id=~Mahdieh_Soleymani_Baghshah1), [Mohammad H Rohban](http://openreview.net/profile?id=~Mohammad_Hossein_Rohban1)
  - **Affiliations:** Sharif University of Technology, Tehran, Iran, Sharif University of Technology, Tehran, Iran, Sharif University of Technology, Tehran, Iran, Sharif University of Technology, Tehran, Iran, Sharif University of Technology, Tehran, Iran, Isfahan University of Technology, Isfahan, Iran, Sharif University of Technology, Tehran, Iran, Sharif University of Technology, Tehran, Iran, Sharif University of Technology, Tehran, Iran, Sharif University of Technology, Tehran, Iran; Isfahan University of Technology, Isfahan, Iran
  - **TL;DR:** This study introduces RODEO, a data-centric approach for robust outlier detection that incorporates outlier exposure and adversarial training to generate effective outliers. The results demonstrate significant improvements in detection performance, particularly in adversarial settings.
  - **Keywords:** Outlier detection, Adversarial robustness, Outlier exposure (OE), Adversarial training, Text-to-image model, Image outlier detection, Open-world machine learning, Performance drop under adversarial attacks, Lack of effective exposure to adversarial scenarios, Generation of diverse and near-distribution outliers, Enhanced performance of outlier detectors, Tiny ImageNet, CIFAR10, CIFAR100, SVHN, MNIST, Gaussian noise


- [Efficient World Models with Context-Aware Tokenization](https://icml.cc/virtual/2024/poster/34714) (Poster)
  - **Authors:** [Vincent Micheli](http://openreview.net/profile?id=~Vincent_Micheli1), [Eloi Alonso](http://openreview.net/profile?id=~Eloi_Alonso1), [François Fleuret](http://openreview.net/profile?id=~Fran%C3%A7ois_Fleuret2)
  - **Affiliations:** University of Geneva, Switzerland, University of Geneva, Switzerland, University of Geneva, Switzerland
  - **TL;DR:** This paper introduces ∆-IRIS, a new agent designed to enhance model-based reinforcement learning by efficiently encoding stochastic deltas between time steps, achieving state-of-the-art performance in the Crafter benchmark while significantly reducing training time compared to previous methods. The approach addresses the computational challenges of scaling deep reinforcement learning in complex environments.
  - **Keywords:** Deep Reinforcement Learning, Model-based Reinforcement Learning, Transformer-based world models, Discrete autoencoder, Autoregressive transformer, Complex environments, Simulation of environments, Sample efficiency, Computational prohibitions, Encoding visually challenging frames, ∆-IRIS agent, New state of the art in the Crafter benchmark, Faster training methods, Crafter benchmark, Stochastic deltas, Dynamics learning, Sequence modelling


- [CLIPZyme: Reaction-Conditioned Virtual Screening of Enzymes](https://icml.cc/virtual/2024/poster/35185) (Poster)
  - **Authors:** [Peter Mikhael](http://openreview.net/profile?id=~Peter_Mikhael1), [Itamar Chinn](http://openreview.net/profile?id=~Itamar_Chinn1), [Regina Barzilay](http://openreview.net/profile?id=~Regina_Barzilay1)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, U.S.A., Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, U.S.A., Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, U.S.A.
  - **TL;DR:** This study introduces CLIPZyme, a computational framework for virtual enzyme screening that utilizes contrastive learning to improve the identification of efficient catalysts from uncharacterized enzyme sequences. The method demonstrates enhanced performance in scenarios with limited reaction information, offering significant implications for biosynthesis applications.
  - **Keywords:** enzyme screening, computational biosynthesis, contrastive learning, CLIP-style training, biosynthesis, enzyme optimization, uncharacterized enzymes, reaction-enzyme pair identification, transition state modeling, CLIPZyme framework, improved performance in virtual enzyme screening, AlphaFold, enzyme commission (EC) number


- [From Inverse Optimization to Feasibility to ERM](https://icml.cc/virtual/2024/poster/34505) (Poster)
  - **Authors:** [Saurabh Mishra](http://openreview.net/profile?id=~Saurabh_kumar_Mishra1), [Anant Raj](http://openreview.net/profile?id=~Anant_Raj2), [Sharan Vaswani](http://openreview.net/profile?id=~Sharan_Vaswani1)
  - **Affiliations:** Simon Fraser University, SIERRA Project Team (Inria); Coordinated Science Laboratory (CSL), UIUC, Simon Fraser University
  - **TL;DR:** This study explores contextual inverse optimization to infer unknown parameters of optimization problems using additional contextual information, focusing on contextual inverse linear programming (CILP). The proposed method demonstrates linear convergence and improved performance on both synthetic and real-world problems compared to existing methods.
  - **Keywords:** Inverse optimization, Contextual inverse optimization, Data-driven inverse optimization, Contextual inverse linear programming (CILP), Convex feasibility problem, Empirical risk minimization (ERM), Transportation, Power systems, Healthcare, Optimal transport, Vehicle routing, Inferring unknown parameters, Non-differentiable nature of LPs, Generalization performance, Linear convergence guarantee, Scalable first-order optimization methods


- [Rethinking Independent Cross-Entropy Loss For Graph-Structured Data](https://icml.cc/virtual/2024/poster/32614) (Poster)
  - **Authors:** [Rui Miao](http://openreview.net/profile?id=~Rui_Miao2), [Kaixiong Zhou](http://openreview.net/profile?id=~Kaixiong_Zhou1), [Yili Wang](http://openreview.net/profile?id=~Yili_Wang2), [Ninghao Liu](http://openreview.net/profile?id=~Ninghao_Liu2), [Ying Wang](http://openreview.net/profile?id=~Ying_Wang13), [Xin Wang](http://openreview.net/profile?id=~Xin_Wang54)
  - **Affiliations:** School of Artificial Intelligence, Jilin University, China, Institute for Medical Engineering Science, Massachusetts Institute of Technology, USA, School of Artificial Intelligence, Jilin University, China, School of Computing, University of Georgia, USA, College of Computer Science and Technology, Jilin University, China, School of Artificial Intelligence, Jilin University, China; College of Computer Science and Technology, Jilin University, China
  - **TL;DR:** This study proposes a new framework called joint-cluster supervised learning to improve the performance of graph neural networks in node classification by modeling the joint distribution of node and cluster labels. The approach enhances classification accuracy and robustness against adversarial attacks by leveraging local cluster reference signals.
  - **Keywords:** Graph Neural Networks (GNNs), Node Classification, Joint-Cluster Supervised Learning, Cross-Entropy Loss, Social Networks, Biological Networks, Recommender Systems, Overfitting, Susceptibility to Adversarial Attacks, Improved Node Classification Accuracy, Robustness Against Adversarial Attacks


- [Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs](https://icml.cc/virtual/2024/poster/33787) (Poster)
  - **Authors:** [Chandra Mouli Sekar](http://openreview.net/profile?id=~S_Chandra_Mouli1), [Danielle Robinson](http://openreview.net/profile?id=~Danielle_C._Maddix1), [Shima Alizadeh](http://openreview.net/profile?id=~Shima_Alizadeh1), [Gaurav Gupta](http://openreview.net/profile?id=~Gaurav_Gupta2), [Andrew Stuart](http://openreview.net/profile?id=~Andrew_Stuart1), [Michael Mahoney](http://openreview.net/profile?id=~Michael_W._Mahoney1), [Yuyang Wang](http://openreview.net/profile?id=~Bernie_Wang1)
  - **Affiliations:** Department of Computer Science, Purdue University, West Lafayette, IN, USA; None, AWS AI Labs, Santa Clara, CA, USA, AWS AI Labs, Santa Clara, CA, USA, AWS AI Labs, Santa Clara, CA, USA, Department of Computing and Mathematical Sciences, Caltech, Pasadena, CA, USA; Amazon Search, Pasadena, CA, USA, Amazon Supply Chain Optimization Technologies, New York, NY, USA, AWS AI Labs, Santa Clara, CA, USA
  - **TL;DR:** This study investigates the limitations of Neural Operators in out-of-domain learning for PDEs and proposes a novel method, DIVERSE NO, which enhances uncertainty quantification and model performance in challenging scenarios while adhering to physical constraints like conservation laws.
  - **Keywords:** Scientific machine learning, Neural Operators, uncertainty quantification, Neural Operators (NOs), ensembling, OPERATOR-PROBCONSERV, Partial differential equations (PDEs), out-of-domain learning, Out-of-domain (OOD) performance, high-error regions, uncertainty estimation, DIVERSE NO, improved OOD model performance, conservation laws


- [Learning Optimal Deterministic Policies with Stochastic Policy Gradients](https://icml.cc/virtual/2024/poster/34781) (Spotlight Poster)
  - **Authors:** [Alessandro Montenegro](http://openreview.net/profile?id=~Alessandro_Montenegro1), [Marco Mussi](http://openreview.net/profile?id=~Marco_Mussi1), [Alberto Maria Metelli](http://openreview.net/profile?id=~Alberto_Maria_Metelli2), [Matteo Papini](http://openreview.net/profile?id=~Matteo_Papini1)
  - **Affiliations:** Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133, Milan, Italy, Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133, Milan, Italy, Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133, Milan, Italy, Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133, Milan, Italy
  - **TL;DR:** This paper investigates the learning of optimal deterministic policies through stochastic policy gradients, addressing the challenges of robustness and safety in reinforcement learning. It presents a framework for understanding the convergence to the best deterministic policy and explores the trade-off between sample complexity and performance in policy deployment.
  - **Keywords:** reinforcement learning, policy gradient methods, deterministic policies, stochastic policies, deterministic policy gradient, action-based exploration, parameter-based exploration, autonomous driving, industrial plants, robotic controllers, exploration problem, robustness, safety, traceability, global convergence to the best deterministic policy, optimization of exploration level


- [OAK: Enriching Document Representations using Auxiliary Knowledge for Extreme Classification](https://icml.cc/virtual/2024/poster/34673) (Poster)
  - **Authors:** [Shikhar Mohan](http://openreview.net/profile?id=~Shikhar_Mohan1), [Deepak Saini](http://openreview.net/profile?id=~Deepak_Saini2), [Anshul Mittal](http://openreview.net/profile?id=~Anshul_Mittal2), [Sayak Ray Chowdhury](http://openreview.net/profile?id=~Sayak_Ray_Chowdhury1), [Bhawna Paliwal](http://openreview.net/profile?id=~Bhawna_Paliwal1), [Jian Jiao](http://openreview.net/profile?id=~Jian_Jiao2), [Manish Gupta](http://openreview.net/profile?id=~Manish_Gupta4), [Manik Varma](http://openreview.net/profile?id=~Manik_Varma1)
  - **Affiliations:** Microsoft, India, Microsoft, USA, Microsoft, India, Microsoft Research, India, Microsoft Research, India, Microsoft, USA, Microsoft, India, Microsoft Research, India
  - **TL;DR:** The study introduces Online Auxiliary Knowledge (OAK), a framework that enhances eXtreme Classification (XC) by utilizing auxiliary data to improve document label prediction accuracy. OAK demonstrates significant performance improvements over existing methods in both academic datasets and sponsored search applications.
  - **Keywords:** eXtreme Classification (XC), document representation, Online Auxiliary Knowledge (OAK), knowledge embeddings, sponsored search, document tagging, product recommendation, large label space, data sparsity, improved XC accuracy, enriched document embeddings, LF-ORCAS-800K, LF-Wikipedia-500K, auxiliary knowledge, knowledge bank


- [Causal Representation Learning Made Identifiable by Grouping of Observational Variables](https://icml.cc/virtual/2024/poster/34013) (Poster)
  - **Authors:** [Hiroshi Morioka](http://openreview.net/profile?id=~Hiroshi_Morioka1), [Aapo Hyvarinen](http://openreview.net/profile?id=~Aapo_Hyvarinen1)
  - **Affiliations:** RIKEN Center for Advanced Intelligence Project, Kyoto, Japan, Department of Computer Science, University of Helsinki, Helsinki, Finland
  - **TL;DR:** The study presents a novel approach to Causal Representation Learning (CRL) that identifies causal models for hidden features without requiring temporal structure or interventions. It introduces a self-supervised estimation framework that demonstrates superior performance and robustness against latent confounders and causal cycles.
  - **Keywords:** Causal Representation Learning, Causal Discovery, Ill-posed problems, Identifiability conditions, Self-supervised estimation framework, Statistical consistency, Nonlinear Independent Component Analysis (NICA), Causal graph, Observational variables


- [Position: Levels of AGI for Operationalizing Progress on the Path to AGI](https://icml.cc/virtual/2024/poster/35180) (Spotlight Poster)
  - **Authors:** [Meredith Morris](http://openreview.net/profile?id=~Meredith_Ringel_Morris1), [Jascha Sohl-Dickstein](http://openreview.net/profile?id=~Jascha_Sohl-Dickstein2), [Noah Fiedel](http://openreview.net/profile?id=~Noah_Fiedel1), [Tris Warkentin](http://openreview.net/profile?id=~Tris_Warkentin1), [Allan Dafoe](http://openreview.net/profile?id=~Allan_Dafoe1), [Aleksandra Faust](http://openreview.net/profile?id=~Aleksandra_Faust1), [Clement Farabet](http://openreview.net/profile?id=~Clement_Farabet2), [Shane Legg](http://openreview.net/profile?id=~Shane_Legg1)
  - **Affiliations:** Google DeepMind, Seattle, WA, USA, Google DeepMind, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, Google DeepMind, London, UK, Google DeepMind, Mountain View, CA, USA, Google DeepMind, London, UK, Google DeepMind, London, UK
  - **TL;DR:** The paper proposes a framework for classifying the capabilities and behaviors of Artificial General Intelligence (AGI) models, introducing levels of performance, generality, and autonomy. It emphasizes the need for shared definitions to assess risks and measure progress towards AGI, while considering the implications of deployment and human-AI interaction.
  - **Keywords:** Artificial General Intelligence (AGI), AI capabilities, AI risks, Measuring progress towards AGI, assessing risks of AGI, Framework for AGI classification, levels of AGI performance and generality, Human-AI Interaction, autonomy, emergent properties


- [Truly No-Regret Learning in Constrained MDPs](https://icml.cc/virtual/2024/poster/33376) (Spotlight Poster)
  - **Authors:** [Adrian Müller](http://openreview.net/profile?id=~Adrian_M%C3%BCller2), [Pragnya Alatur](http://openreview.net/profile?id=~Pragnya_Alatur1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1), [Giorgia Ramponi](http://openreview.net/profile?id=~Giorgia_Ramponi1), [Niao He](http://openreview.net/profile?id=~Niao_He3)
  - **Affiliations:** EPFL, ETH Zürich, EPFL, University of Zürich, ETH Zürich
  - **TL;DR:** This paper addresses the challenge of ensuring safety in reinforcement learning through constrained Markov decision processes (CMDPs) and presents a model-based primal-dual algorithm that achieves sublinear regret without allowing error cancellations, thus ensuring safety during the learning process.
  - **Keywords:** Constrained Markov Decision Processes (CMDPs), Reinforcement Learning (RL), Safety Constraints, Primal-Dual Algorithms, Last-Iterate Convergence, Model-Based Algorithms, Autonomous Driving, Drone Navigation, Safety Constraints in Learning, Regret Bounds, Error Cancellations, Sublinear Regret without Error Cancellations, Efficient Learning in Unknown CMDPs


- [Provable Interactive Learning with Hindsight Instruction Feedback](https://icml.cc/virtual/2024/poster/34668) (Poster)
  - **Authors:** [Dipendra Misra](http://openreview.net/profile?id=~Dipendra_Misra1), [Aldo Pacchiano](http://openreview.net/profile?id=~Aldo_Pacchiano1), [Robert Schapire](http://openreview.net/profile?id=~Robert_E._Schapire1)
  - **Affiliations:** Microsoft Research, Broad Institute of MIT and Harvard; Boston University, Microsoft Research
  - **TL;DR:** This study introduces a novel approach to interactive learning using hindsight labeling, where a teacher provides suitable instructions for the agent's generated responses. The proposed LORIL algorithm demonstrates no-regret learning in a specialized setting, improving the agent's ability to follow instructions in virtual environments.
  - **Keywords:** interactive learning, hindsight labeling, instruction-response mapping, no-regret algorithm, low-rank matrix decomposition, robotics, virtual environments, high sample complexity, expensive ground truth collection, LORIL algorithm, theoretical analysis of interactive learning


- [TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors](https://icml.cc/virtual/2024/poster/33201) (Poster)
  - **Authors:** [Yichuan Mo](http://openreview.net/profile?id=~Yichuan_Mo1), [Hui Huang](http://openreview.net/profile?id=~Hui_Huang6), [Mingjie Li](http://openreview.net/profile?id=~Mingjie_Li1), [Ang Li](http://openreview.net/profile?id=~Ang_Li20), [Yisen Wang](http://openreview.net/profile?id=~Yisen_Wang1)
  - **Affiliations:** National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, China, School of EECS, Peking University, China, CISPA Helmholtz Center for Information Security, Germany, School of EECS, Peking University, China, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, China; Institute for Artificial Intelligence, Peking University, China
  - **TL;DR:** This paper presents TERD, a novel framework designed to protect diffusion models from backdoor attacks by employing a trigger reversion strategy and a new detection approach. The framework achieves a 100% True Positive Rate and True Negative Rate across various datasets, demonstrating its effectiveness and adaptability.
  - **Keywords:** diffusion models, backdoor attacks, model security, trigger reversion strategy, differential multi-step samplers, KL divergence, image generation, content editing, zero-shot classification, adversarial purification, vulnerability to backdoor attacks, integrity compromise, noise output dynamics, TERD framework, backdoor detection approach, reversed loss, Stochastic Differential Equation (SDE)


- [Language Models with Conformal Factuality Guarantees](https://icml.cc/virtual/2024/poster/32822) (Poster)
  - **Authors:** [Christopher Mohri](http://openreview.net/profile?id=~Christopher_Mohri1), [Tatsunori Hashimoto](http://openreview.net/profile?id=~Tatsunori_Hashimoto1)
  - **Affiliations:** Department of Computer Science, Stanford University, Department of Computer Science, Stanford University
  - **TL;DR:** This paper introduces "conformal factuality," a framework that ensures high probability correctness guarantees for language model outputs by connecting language modeling with conformal prediction. The approach demonstrates the ability to provide 80-90% correctness guarantees while maintaining the majority of the original outputs from the language model.
  - **Keywords:** language models, factuality, correctness guarantees, conformal prediction, uncertainty quantification, closed book QA, reasoning tasks, health, law, robotics, hallucinations, non-factual content, performance guarantees, high probability correctness guarantees, back-off algorithm, FActScore, NaturalQuestions, MATH, Large Language Models, conformal factuality


- [Optimal bounds for $\ell_p$ sensitivity sampling via $\ell_2$ augmentation](https://icml.cc/virtual/2024/poster/33074) (Poster)
  - **Authors:** [Alexander Munteanu](http://openreview.net/profile?id=~Alexander_Munteanu1), [Simon Omlor](http://openreview.net/profile?id=~Simon_Omlor1)
  - **Affiliations:** Dortmund Data Science Center, Faculties of Statistics and Computer Science, TU Dortmund University, Dortmund, Germany, Faculty of Statistics, TU Dortmund University, Dortmund, Germany; Lamarr-Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany
  - **TL;DR:** This paper presents optimal bounds for ℓp sensitivity sampling by augmenting with ℓ2 sensitivities, achieving improved sampling complexity for data subsampling. The findings resolve an open question in the field and enhance sensitivity subsampling methods, particularly in the context of logistic regression.
  - **Keywords:** data subsampling, sensitivity sampling, ℓp sensitivity, ℓ2 augmentation, importance sampling, logistic regression, approximation accuracy, high-dimensional data, optimal linear sampling complexity, sensitivity sampling bounds, VC dimension, Lewis weights


- [Finding NEM-U: Explaining unsupervised representation learning through neural network generated explanation masks](https://icml.cc/virtual/2024/poster/34437) (Poster)
  - **Authors:** [Bjørn Leth Møller](http://openreview.net/profile?id=~Bj%C3%B8rn_Leth_M%C3%B8ller1), [Christian Igel](http://openreview.net/profile?id=~Christian_Igel1), [Kristoffer Wickstrøm](http://openreview.net/profile?id=~Kristoffer_Knutsen_Wickstr%C3%B8m1), [Jon Sporring](http://openreview.net/profile?id=~Jon_Sporring1), [Robert Jenssen](http://openreview.net/profile?id=~Robert_Jenssen1), [Bulat Ibragimov](http://openreview.net/profile?id=~Bulat_Ibragimov3)
  - **Affiliations:** Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Norwegian Computing Center, Oslo, Norway, Department of Computer Science, University of Copenhagen, Copenhagen, Denmark, Department of Physics and Technology, UiT The Arctic University of Norway, Tromsø, Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Norwegian Computing Center, Oslo, Norway, Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; Norwegian Computing Center, Oslo, Norway, Department of Computer Science, University of Copenhagen, Copenhagen, Denmark
  - **TL;DR:** The study introduces the Neural Explanation Masks (NEM) framework to enhance unsupervised representation learning by providing efficient and accurate occlusion-based explanations. NEM-U, an instance of this framework, demonstrates faster performance and lower complexity compared to existing methods while maintaining high accuracy.
  - **Keywords:** Unsupervised representation learning, Explainable AI (XAI), Neural Explanation Masks (NEM), U-net structure, occlusion-based explanations, Deep learning systems, feature extraction, Inaccurate explanations, slow explanation generation, limitations of existing methods, Faster and lower complexity explanations, high accuracy in locality, VOC dataset


- [Turnstile $\ell_p$ leverage score sampling with applications](https://icml.cc/virtual/2024/poster/33228) (Poster)
  - **Authors:** [Alexander Munteanu](http://openreview.net/profile?id=~Alexander_Munteanu1), [Simon Omlor](http://openreview.net/profile?id=~Simon_Omlor1)
  - **Affiliations:** Dortmund Data Science Center, Faculties of Statistics and Computer Science, TU Dortmund University, Dortmund, Germany, Faculty of Statistics, TU Dortmund University, Dortmund, Germany; Lamarr-Institute for Machine Learning and Artificial Intelligence, Dortmund, Germany
  - **TL;DR:** This paper presents a novel algorithm for sampling rows of a matrix in a turnstile data stream, achieving efficient ℓp leverage score sampling and (1 + ε) approximation for logistic regression. The proposed method significantly improves upon previous algorithms by using polynomial sketch size, enabling effective data analysis in dynamic environments.
  - **Keywords:** turnstile data stream, sampling algorithms, coresets, logistic regression, ℓp norm sampling, ℓp leverage score sampling, polynomial sketching, regression problems, data stream processing, dynamic data manipulation, data accessibility, approximation error, (1 + ε) approximation for logistic regression, efficient subsampling methods


- [Straight-Through Meets Sparse Recovery: the Support Exploration Algorithm](https://icml.cc/virtual/2024/poster/34149) (Poster)
  - **Authors:** [Mimoun Mohamed](http://openreview.net/profile?id=~Mimoun_Mohamed1), [Francois Malgouyres](http://openreview.net/profile?id=~Francois_Malgouyres1), [Valentin Emiya](http://openreview.net/profile?id=~Valentin_Emiya2), [Caroline Chaux](http://openreview.net/profile?id=~Caroline_Chaux2)
  - **Affiliations:** Aix Marseille Univ, CNRS, LIS, Marseille, France; Aix Marseille Univ, CNRS, I2M, Marseille, France, Institut de Mathématiques de Toulouse; UMR5219, Université de Toulouse; CNRS, UPS IMT F-31062 Toulouse Cedex 9, France, Aix Marseille Univ, CNRS, LIS, Marseille, France, CNRS, IPAL, Singapour
  - **TL;DR:** This study introduces the Support Exploration Algorithm (SEA) based on the straight-through estimator (STE) to enhance sparse support recovery in quantized neural networks. The proposed method outperforms existing techniques, particularly in scenarios with strong coherence among measurement columns, and provides theoretical recovery guarantees.
  - **Keywords:** quantized neural networks, sparse support recovery, straight-through estimator (STE), Support Exploration Algorithm (SEA), model selection, spike deconvolution, data sparsity, recovery guarantees, coherence in measurements, superior performance in support recovery, theoretical guarantees for STE-based algorithms, Restricted Isometry Property (RIP)


- [Slot Abstractors: Toward Scalable Abstract Visual Reasoning](https://icml.cc/virtual/2024/poster/33526) (Poster)
  - **Authors:** [Shanka Subhra Mondal](http://openreview.net/profile?id=~Shanka_Subhra_Mondal1), [Jonathan Cohen](http://openreview.net/profile?id=~Jonathan_D._Cohen1), [Taylor Webb](http://openreview.net/profile?id=~Taylor_Whittington_Webb1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, US; Princeton Neuroscience Institute, Princeton University, Princeton, NJ, US, Princeton Neuroscience Institute, Princeton University, Princeton, NJ, US, Department of Psychology, University of California Los Angeles, Los Angeles, CA, US
  - **TL;DR:** This study introduces Slot Abstractors, a scalable approach to abstract visual reasoning that effectively identifies relational patterns among multiple objects. The method demonstrates state-of-the-art performance across various visual reasoning tasks, addressing limitations of previous models.
  - **Keywords:** abstract visual reasoning, systematic generalization, slot-based methods, relational abstraction, Abstractors, Transformers, visual reasoning tasks, multi-object inputs, scalability issues, single rule limitation, overfitting to training data, Slot Abstractors, state-of-the-art performance, relational bottleneck, relational cross-attention


- [A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks](https://icml.cc/virtual/2024/poster/33970) (Poster)
  - **Authors:** [Behrad Moniri](http://openreview.net/profile?id=~Behrad_Moniri1), [Donghwan Lee](http://openreview.net/profile?id=~Donghwan_Lee5), [Hamed Hassani](http://openreview.net/profile?id=~Hamed_Hassani2), [Edgar Dobriban](http://openreview.net/profile?id=~Edgar_Dobriban2)
  - **Affiliations:** Department of Electrical and Systems Engineering, University of Pennsylvania, PA, USA, Graduate Group in Applied Mathematics and Computational Science, University of Pennsylvania, PA, USA, Department of Electrical and Systems Engineering, University of Pennsylvania, PA, USA, Department of Statistics and Data Science, University of Pennsylvania, PA, USA
  - **TL;DR:** This study explores how a growing learning rate in two-layer neural networks can facilitate the learning of non-linear features through the introduction of multiple rank-one components. The findings suggest that these non-linear features significantly enhance the learning process, addressing limitations in traditional models that only capture linear components.
  - **Keywords:** feature learning, deep neural networks, two-layer fully-connected neural networks, gradient descent, computer vision, natural language processing, learning non-linear components, Gaussian noise, model overfitting, multiple rank-one components, polynomial features, training and test errors, Gaussian equivalence property, random features models, neural tangent kernel


- [SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States](https://icml.cc/virtual/2024/poster/33403) (Poster)
  - **Authors:** [Noga Mudrik](http://openreview.net/profile?id=~Noga_Mudrik1), [Gal Mishne](http://openreview.net/profile?id=~Gal_Mishne1), [Adam Charles](http://openreview.net/profile?id=~Adam_Shabti_Charles1)
  - **Affiliations:** Biomedical Engineering, Kavli NDI, Center for Imaging Science, The Mathematical Institute for Data Science, The Johns Hopkins University, Baltimore, MD, USA, Halıcıoğlu Data Science Institute, UCSD, San Diego, CA, USA, Biomedical Engineering, Kavli NDI, Center for Imaging Science, The Mathematical Institute for Data Science, The Johns Hopkins University, Baltimore, MD, USA
  - **TL;DR:** The study presents SiBBlInGS, a framework for discovering interpretable Building Blocks in high-dimensional time series data across distinct states, addressing challenges like inter- and intra-state variability and missing samples. The method demonstrates robustness and reveals insights into complex phenomena through various applications, including web search and neural data.
  - **Keywords:** Time series analysis, Building Blocks (BBs), inter- and intra-state variability, Graph-based dictionary learning, sparse BBs discovery, Neuroscience, social sciences, environmental studies, High-dimensional data variability, missing samples, variable trial duration, Insights into complex phenomena, robustness to noise and missing samples


- [BAGEL: Bootstrapping Agents by Guiding Exploration with Language](https://icml.cc/virtual/2024/poster/33899) (Poster)
  - **Authors:** [Shikhar Murty](http://openreview.net/profile?id=~Shikhar_Murty1), [Christopher Manning](http://openreview.net/profile?id=~Christopher_D_Manning1), [Peter Shaw](http://openreview.net/profile?id=~Peter_Shaw1), [Mandar Joshi](http://openreview.net/profile?id=~Mandar_Joshi1), [Kenton Lee](http://openreview.net/profile?id=~Kenton_Lee1)
  - **Affiliations:** Department of Computer Science, Stanford University; Google Deepmind, Department of Computer Science, Stanford University, Google Deepmind, Google Deepmind, Google Deepmind
  - **TL;DR:** This paper presents BAGEL, a method for bootstrapping language model agents to follow natural language instructions in digital environments without human supervision. The approach demonstrates significant improvements in performance and a reduction in execution failures by iteratively generating synthetic demonstrations from explored trajectories.
  - **Keywords:** language model agents, natural language instructions, bootstrapping agents, zero-shot learning, iterative round-trip procedure, in-context learning, digital environments, web browsers, REST APIs, generalization to new environments, reliance on human demonstrations, error recovery, synthetic demonstrations, reduction in execution failures, adaptation of LM agents, ToolQA, MiniWob++, large language models (LLMs), LM labeler, trajectory mapping


- [Factored-Reward Bandits with Intermediate Observations](https://icml.cc/virtual/2024/poster/34697) (Poster)
  - **Authors:** [Marco Mussi](http://openreview.net/profile?id=~Marco_Mussi1), [Simone Drago](http://openreview.net/profile?id=~Simone_Drago1), [Marcello Restelli](http://openreview.net/profile?id=~Marcello_Restelli1), [Alberto Maria Metelli](http://openreview.net/profile?id=~Alberto_Maria_Metelli2)
  - **Affiliations:** Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy
  - **TL;DR:** This paper introduces Factored-Reward Bandits (FRBs) to address sequential decision-making problems where actions yield intermediate observations, characterizing the statistical complexity and proposing two algorithms for regret minimization. The findings highlight the importance of considering intermediate observations to optimize learning and performance guarantees in real-world scenarios.
  - **Keywords:** Factored-Reward Bandits, Sequential Decision-Making, F-UCB, F-Track, Regret Minimization Algorithms, E-commerce, Revenue Maximization, Learning Problem Complexity, Action Space Size, Noise in Reward, Instance-Dependent Regret Guarantees, Statistical Complexity Characterization, Multi-Armed Bandit (MAB), Intermediate Observations


- [Test-Time Regret Minimization in Meta Reinforcement Learning](https://icml.cc/virtual/2024/poster/34298) (Poster)
  - **Authors:** [Mirco Mutti](http://openreview.net/profile?id=~Mirco_Mutti1), [Aviv Tamar](http://openreview.net/profile?id=~Aviv_Tamar2)
  - **Affiliations:** Technion – Israel Institute of Technology, Haifa, Israel, Technion – Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This paper investigates the efficiency of meta reinforcement learning in minimizing regret against optimal policies in test tasks, demonstrating that under certain conditions, nearly optimal regret rates can be achieved. The authors introduce stronger assumptions that allow for faster learning rates and reduced dependence on the number of tasks.
  - **Keywords:** meta reinforcement learning, regret minimization, Markov decision processes, algorithms, robotics, algorithms design, conversational agents, learning efficiency, regret in reinforcement learning, lower bound for test-time regret minimization, fast rates for algorithms


- [Differentially private exact recovery for stochastic block models](https://icml.cc/virtual/2024/poster/34246) (Poster)
  - **Authors:** [Dung Nguyen](http://openreview.net/profile?id=~Dung_Nguyen2), [Anil Vullikanti](http://openreview.net/profile?id=~Anil_Vullikanti1)
  - **Affiliations:** Department of Computer Science and Biocomplexity Institute and Initiative, University of Virginia, Virginia, USA, Department of Computer Science and Biocomplexity Institute and Initiative, University of Virginia, Virginia, USA
  - **TL;DR:** This paper investigates the recoverability of community structures in Stochastic Block Models (SBMs) under edge differential privacy, presenting conditions for exact recovery in various SBM configurations. The proposed algorithms operate in polynomial time and achieve recovery thresholds comparable to non-private settings as the privacy parameter approaches infinity.
  - **Keywords:** Stochastic Block Models, Community Detection, Exact Recovery, Differential Privacy, Network Analysis, Unsupervised Machine Learning, Community Structure Recovery, Privacy in Networks, Conditions for Exact Recoverability, Polynomial Time Algorithms, Asymmetric SBM, General Structure SBM, Censored SBM


- [PIDformer: Transformer Meets Control Theory](https://icml.cc/virtual/2024/poster/34006) (Poster)
  - **Authors:** [Tam Nguyen](http://openreview.net/profile?id=~Tam_Minh_Nguyen1), [Cesar Uribe](http://openreview.net/profile?id=~Cesar_A_Uribe1), [Tan Nguyen](http://openreview.net/profile?id=~Tan_Minh_Nguyen1), [Richard Baraniuk](http://openreview.net/profile?id=~Richard_Baraniuk1)
  - **Affiliations:** Department of Electrical & Computer Engineering, Rice University, Houston, USA, Department of Electrical & Computer Engineering, Rice University, Houston, USA, Department of Mathematics, National University of Singapore, Singapore, Department of Electrical & Computer Engineering, Rice University, Houston, USA
  - **TL;DR:** This study introduces the PID-controlled Transformer (PIDformer) to enhance the robustness and representation capacity of transformer architectures by addressing input corruption and rank collapse. The integration of a PID feedback control system improves model stability and noise resilience, demonstrating advantages across various practical tasks.
  - **Keywords:** transformer architectures, robustness, representation capacity, self-attention, Proportional-Integral-Derivative (PID) control, object classification, image segmentation, language modeling, input corruption, rank collapse, susceptibility to adversarial attacks, PID-controlled Transformer (PIDformer), improved robustness, noise-resilience, softmax attention, state-space model


- [On Least Square Estimation in Softmax Gating Mixture of Experts](https://icml.cc/virtual/2024/poster/34728) (Poster)
  - **Authors:** [Huy Nguyen](http://openreview.net/profile?id=~Huy_Nguyen5), [Nhat Ho](http://openreview.net/profile?id=~Nhat_Ho1), [Alessandro Rinaldo](http://openreview.net/profile?id=~Alessandro_Rinaldo1)
  - **Affiliations:** Department of Statistics and Data Sciences, The University of Texas at Austin, USA, Department of Statistics and Data Sciences, The University of Texas at Austin, USA, Department of Statistics and Data Sciences, The University of Texas at Austin, USA
  - **TL;DR:** This study investigates the performance of least squares estimators in a deterministic mixture of experts model, establishing a condition for convergence behavior. It finds that estimation rates for feed-forward networks are significantly faster than those for polynomial experts, highlighting important implications for expert selection.
  - **Keywords:** Mixture of Experts (MoE), Least Squares Estimation, Softmax gating, Regression model, Strong identifiability, Large language models, Computer vision, Multi-task learning, Reinforcement learning, Parameter estimation, Expert selection, Faster estimation rates for feed-forward networks, Slow estimation rates for polynomial experts


- [Best Arm Identification for Stochastic Rising Bandits](https://icml.cc/virtual/2024/poster/33841) (Spotlight Poster)
  - **Authors:** [Marco Mussi](http://openreview.net/profile?id=~Marco_Mussi1), [Alessandro Montenegro](http://openreview.net/profile?id=~Alessandro_Montenegro1), [Francesco Trovò](http://openreview.net/profile?id=~Francesco_Trov%C3%B21), [Marcello Restelli](http://openreview.net/profile?id=~Marcello_Restelli1), [Alberto Maria Metelli](http://openreview.net/profile?id=~Alberto_Maria_Metelli2)
  - **Affiliations:** Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy
  - **TL;DR:** This paper addresses the fixed-budget Best Arm Identification problem in Stochastic Rising Bandits, proposing two algorithms (R-UCBE and R-SR) that ensure high probability of correctly identifying the optimal option while minimizing simple regret. The study highlights the necessity of a sufficiently large budget for effective identification in this setting.
  - **Keywords:** Stochastic Rising Bandits, Best Arm Identification, R-UCBE, R-SR, UCB-like approach, successive reject procedure, Online best model selection, Automated Machine Learning (AutoML), Combined Algorithm Selection and Hyperparameter optimization (CASH), Fixed-budget decision-making, error probability in arm recommendation, Guarantees on identifying optimal options, lower bound on error probability, Multi-Armed Bandits (MAB), rested bandit setting


- [Learning in Deep Factor Graphs with Gaussian Belief Propagation](https://icml.cc/virtual/2024/poster/34915) (Poster)
  - **Authors:** [Seth Nabarro](http://openreview.net/profile?id=~Seth_Nabarro1), [Mark van der Wilk](http://openreview.net/profile?id=~Mark_van_der_Wilk1), [Andrew Davison](http://openreview.net/profile?id=~Andrew_Davison1)
  - **Affiliations:** Dyson Robotics Lab, Imperial College London, UK, Department of Computer Science, University of Oxford, UK, Dyson Robotics Lab, Imperial College London, UK
  - **TL;DR:** This paper presents a method for learning in Gaussian factor graphs using Gaussian belief propagation, enabling efficient training and prediction in deep networks. The approach facilitates continual learning and demonstrates improved performance in tasks like video denoising and image classification.
  - **Keywords:** deep learning, continual learning, Gaussian factor graphs, Gaussian belief propagation (GBP), belief propagation (BP), video denoising, image classification, data sparsity, incremental learning, distributed training, online updating of parameter posterior, efficient model-parallelism


- [Latent Optimal Paths by Gumbel Propagation for Variational Bayesian Dynamic Programming](https://icml.cc/virtual/2024/poster/33914) (Poster)
  - **Authors:** [Xinlei Niu](http://openreview.net/profile?id=~Xinlei_Niu1), [Christian Walder](http://openreview.net/profile?id=~Christian_Walder1), [Jing Zhang](http://openreview.net/profile?id=~Jing_Zhang23), [Charles Martin](http://openreview.net/profile?id=~Charles_Patrick_Martin1)
  - **Affiliations:** Australian National University, Canberra, Australia, Google DeepMind, Australian National University, Canberra, Australia, Australian National University, Canberra, Australia
  - **TL;DR:** This study introduces a stochastic optimal path approach that utilizes Bayesian dynamic programming to solve classical optimal path problems, enabling end-to-end training in generative tasks reliant on unobserved structural information. The method is validated in applications such as text-to-speech and singing voice synthesis, showcasing its effectiveness in capturing structured sparse optimal paths.
  - **Keywords:** optimal paths, generative tasks, structured relationships, dynamic programming (DP), Bayesian dynamic programming (BDP), Gibbs distribution, message-passing algorithm, text-to-speech, singing voice synthesis, variational autoencoders (V AEs), non-differentiability of DP, sparse outputs, unobserved structural relationships, stochastic optimal path, end-to-end training, latent optimal paths


- [How Transformers Learn Causal Structure with Gradient Descent](https://icml.cc/virtual/2024/poster/33313) (Poster)
  - **Authors:** [Eshaan Nichani](http://openreview.net/profile?id=~Eshaan_Nichani1), [Alex Damian](http://openreview.net/profile?id=~Alex_Damian1), [Jason Lee](http://openreview.net/profile?id=~Jason_D._Lee1)
  - **Affiliations:** Princeton University, Princeton University, Princeton University
  - **TL;DR:** This study investigates how transformers learn causal structures through gradient descent, demonstrating that the attention matrix's gradient encodes mutual information between tokens. The findings reveal that transformers can effectively recover various causal structures, particularly in the context of in-context learning tasks.
  - **Keywords:** transformers, causal structure, sequence modeling, in-context learning, self-attention mechanism, gradient descent, attention matrix, language modeling, computer vision, reinforcement learning, learning causal structure, understanding training processes, encoding latent causal graphs, induction heads, Markov chains, attention layers, causal graphs


- [Understanding the Impact of Introducing Constraints at Inference Time on Generalization Error](https://icml.cc/virtual/2024/poster/33586) (Poster)
  - **Authors:** [Masaaki Nishino](http://openreview.net/profile?id=~Masaaki_Nishino1), [Kengo Nakamura](http://openreview.net/profile?id=~Kengo_Nakamura1), [Norihito Yasuda](http://openreview.net/profile?id=~Norihito_Yasuda1)
  - **Affiliations:** NTT Communication Science Laboratories, NTT Corporation, Kyoto, Japan, NTT Communication Science Laboratories, NTT Corporation, Kyoto, Japan, NTT Communication Science Laboratories, NTT Corporation, Kyoto, Japan
  - **TL;DR:** This study investigates the impact of applying constraints at inference time on the generalization error of machine learning models. It finds that certain loss functions can maintain the relative generalization error, emphasizing the importance of selecting appropriate loss functions when using constraints during inference.
  - **Keywords:** machine learning, generalization error, constraints, loss functions, softmax cross-entropy loss, safety-critical situations, generative models, low prediction errors, severe results, model performance degradation, analysis of generalization error bounds, preservation of relative generalization error, inference time verification (ITV), PAC-learnable


- [Test-Time Model Adaptation with Only Forward Passes](https://icml.cc/virtual/2024/poster/32971) (Oral)
  - **Authors:** [Shuaicheng Niu](http://openreview.net/profile?id=~Shuaicheng_Niu1), [Chunyan Miao](http://openreview.net/profile?id=~Chunyan_Miao1), [Guohao Chen](http://openreview.net/profile?id=~Guohao_Chen1), [Pengcheng Wu](http://openreview.net/profile?id=~Pengcheng_Wu1), [Peilin Zhao](http://openreview.net/profile?id=~Peilin_Zhao2)
  - **Affiliations:** College of Computing and Data Science, Nanyang Technological University, Singapore; Joint NTU-WeBank Research Centre on Fintech, Singapore; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Singapore, College of Computing and Data Science, Nanyang Technological University, Singapore; Joint NTU-WeBank Research Centre on Fintech, Singapore; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Singapore, Tencent AI Lab, Shenzhen, China, College of Computing and Data Science, Nanyang Technological University, Singapore; Joint NTU-WeBank Research Centre on Fintech, Singapore, Tencent AI Lab, Shenzhen, China
  - **TL;DR:** This study introduces a novel test-time adaptation method called Forward-Optimization Adaptation (FOA) that effectively adapts models to unseen test samples without backpropagation, achieving significant performance improvements on quantized models. The method demonstrates up to a 24-fold memory reduction while outperforming existing gradient-based approaches.
  - **Keywords:** Test-time adaptation, model generalization, Forward-Optimization Adaptation (FOA), covariance matrix adaptation evolution strategy, Resource-limited devices, low-power edge devices, Distribution shifts, unseen test samples, model updating limitations, Novel fitness function, activation shifting scheme, memory reduction, ImageNet-C


- [Density Ratio Estimation with Doubly Strong Robustness](https://icml.cc/virtual/2024/poster/34102) (Poster)
  - **Authors:** [Ryosuke Nagumo](http://openreview.net/profile?id=~Ryosuke_Nagumo1), [Hironori Fujisawa](http://openreview.net/profile?id=~Hironori_Fujisawa1)
  - **Affiliations:** The Graduate University for Advanced Studies (SOKENDAI), Tokyo, Japan; Panasonic Holdings Corporation, Osaka, Japan, Institute of Statistical Mathematics, Tokyo, Japan
  - **TL;DR:** This study presents two new methods for density ratio estimation that are robust to outliers, addressing the limitations of existing methods by incorporating divergence with weight functions. The proposed methods demonstrate improved robustness against contamination in both reference and target distributions, as shown through numerical experiments.
  - **Keywords:** Density Ratio Estimation, Robustness to Outliers, Unnormalized Kullback-Leibler divergence, Weighted DRE, γ-divergence, DC (Difference of Convex functions), Change detection, Outlier detection, Covariate shift adaptation, Two-sample test, Outlier contamination, High-dimensional data, Estimation in regions of low density, New robust density ratio estimation methods, Doubly strong robustness


- [Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design](https://icml.cc/virtual/2024/poster/33420) (Poster)
  - **Authors:** [Quan Nguyen](http://openreview.net/profile?id=~Quan_Nguyen2), [Adji Bousso Dieng](http://openreview.net/profile?id=~Adji_Bousso_Dieng1)
  - **Affiliations:** Department of Computer Science & Engineering, Washington University in St. Louis, Department of Computer Science, Princeton University; Vertaix
  - **TL;DR:** This paper introduces quality-weighted Vendi scores to enhance experimental design by balancing quality and diversity, addressing the limitations of existing techniques that favor exploitation over exploration. The proposed methods significantly improve the discovery of high-performing data points, achieving a 70%–170% increase in effective discoveries compared to baseline approaches.
  - **Keywords:** experimental design, diversity metrics, active search, Bayesian optimization, quality-weighted Vendi scores, drug discovery, materials discovery, reinforcement learning, local optima, diversity in experimental design, policies for experimental design, increase in effective discoveries, Vendi scores, similarity-based diversity metrics


- [Linear Explanations for Individual Neurons](https://icml.cc/virtual/2024/poster/33879) (Poster)
  - **Authors:** [Tuomas Oikarinen](http://openreview.net/profile?id=~Tuomas_Oikarinen1), [Lily Weng](http://openreview.net/profile?id=~Tsui-Wei_Weng1)
  - **Affiliations:** CSE, UC San Diego, CA, USA, HDSI, UC San Diego, CA, USA
  - **TL;DR:** This paper proposes a method called Linear Explanations to better understand individual neurons in neural networks by representing them as linear combinations of concepts. The authors demonstrate that this approach provides a more comprehensive understanding of neuron activations and improves evaluation methods compared to existing techniques.
  - **Keywords:** mechanistic interpretability, neural networks, individual neurons, linear explanations, simulation evaluation, vision models, understanding neuron activations, limitations of existing methods, new method for producing linear explanations, improved evaluation of neuron descriptions, SigLIP models


- [Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning](https://icml.cc/virtual/2024/poster/34956) (Poster)
  - **Authors:** [Michal Nauman](http://openreview.net/profile?id=~Michal_Nauman1), [Michał Bortkiewicz](http://openreview.net/profile?id=~Micha%C5%82_Bortkiewicz1), [Piotr Milos](http://openreview.net/profile?id=~Piotr_Mi%C5%82o%C5%9B1), [Tomasz Trzcinski](http://openreview.net/profile?id=~Tomasz_Trzcinski2), [Mateusz Ostaszewski](http://openreview.net/profile?id=~Mateusz_Ostaszewski1), [Marek Cygan](http://openreview.net/profile?id=~Marek_Cygan1)
  - **Affiliations:** Ideas NCBR; University of Warsaw, Warsaw University of Technology, Polish Academy of Sciences, Tooploox, Warsaw University of Technology, Nomagic
  - **TL;DR:** This study investigates the impact of various regularization techniques on the performance of off-policy reinforcement learning agents, revealing that certain combinations consistently yield superior results across diverse tasks. A well-regularized Soft Actor-Critic agent can achieve better performance than previously model-based approaches.
  - **Keywords:** Reinforcement Learning, Off-Policy Learning, Soft Actor-Critic (SAC), Regularization Techniques, Robotics, Simulation Benchmarks, Overestimation, Overfitting, Plasticity Loss, Improved Sample Efficiency, Robust Performance


- [PairNet: Training with Observed Pairs to Estimate Individual Treatment Effect](https://icml.cc/virtual/2024/poster/33097) (Poster)
  - **Authors:** [Lokesh Nagalapatti](http://openreview.net/profile?id=~Lokesh_Nagalapatti1), [Pranava Singhal](http://openreview.net/profile?id=~Pranava_Singhal1), [Avishek Ghosh](http://openreview.net/profile?id=~Avishek_Ghosh2), [Sunita Sarawagi](http://openreview.net/profile?id=~Sunita_Sarawagi1)
  - **Affiliations:** IIT Bombay, IIT Bombay, IIT Bombay, IIT Bombay
  - **TL;DR:** The study introduces PairNet, a novel training strategy for estimating individual treatment effects (ITE) using observed pairs of outcomes, which avoids reliance on pseudo-outcomes. Empirical results demonstrate that PairNet significantly reduces ITE error compared to existing methods, making it a consistent and model-agnostic approach.
  - **Keywords:** Individual Treatment Effect (ITE) estimation, observational data analysis, PairNet, loss minimization over pairs, binary treatments, Medicine, finance, retail, Estimating treatment effects from observational data, reliance on pseudo-outcomes, Consistent estimator of ITE risk, lower generalization error, model-agnostic approach


- [$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy](https://icml.cc/virtual/2024/poster/34057) (Poster)
  - **Authors:** [Nicola Novello](http://openreview.net/profile?id=~Nicola_Novello1), [Andrea Tonello](http://openreview.net/profile?id=~Andrea_M_Tonello1)
  - **Affiliations:** Department of Networked and Embedded Systems, University of Klagenfurt, Klagenfurt, Austria, Department of Networked and Embedded Systems, University of Klagenfurt, Klagenfurt, Austria
  - **TL;DR:** This paper explores the use of f-divergence in classification tasks within deep learning, proposing a novel shifted log divergence as an objective function. The results indicate that this new approach significantly improves classification accuracy across various application scenarios.
  - **Keywords:** deep learning, classification, optimization problems, f-divergence, maximum a posteriori probability (MAP), variational representation, computer vision, biomedical, telecommunications engineering, signal detection/decoding, novel f-divergence (shifted log), objective functions, Kullback-Leibler (KL) divergence, cross-entropy, Bregman divergences


- [Adaptive Proximal Gradient Methods Are Universal Without Approximation](https://icml.cc/virtual/2024/poster/34005) (Spotlight Poster)
  - **Authors:** [Konstantinos Oikonomidis](http://openreview.net/profile?id=~Konstantinos_Oikonomidis1), [Emanuel Laude](http://openreview.net/profile?id=~Emanuel_Laude1), [Puya Latafat](http://openreview.net/profile?id=~Puya_Latafat1), [Andreas Themelis](http://openreview.net/profile?id=~Andreas_Themelis1), [Panagiotis Patrinos](http://openreview.net/profile?id=~Panagiotis_Patrinos1)
  - **Affiliations:** Department of Electrical Engineering (ESAT-STADIUS), KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium, Department of Electrical Engineering (ESAT-STADIUS), KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium, Department of Electrical Engineering (ESAT-STADIUS), KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium, Faculty of Information Science and Electrical Engineering (ISEE), Kyushu University, 744 Motooka, Nishi-ku 819-0395, Fukuoka, Japan, Department of Electrical Engineering (ESAT-STADIUS), KU Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium
  - **TL;DR:** This study demonstrates that adaptive proximal gradient methods can converge under local Hölder continuity without relying on traditional Lipschitzian assumptions. The findings suggest that these methods are applicable to a broader class of convex problems, particularly in machine learning contexts, while maintaining efficiency by avoiding linesearch procedures.
  - **Keywords:** adaptive proximal gradient methods, convex optimization, linesearch-free methods, local Hölder continuity, proximal gradient method, machine learning, composite minimization problems, lack of local Lipschitz continuity, convergence under local Hölder conditions, full sequence convergence, numerical experiments, comparisons with baseline methods, semi-algebraic functions, ε-oracles, Hölder inequalities


- [Risk-Sensitive Reward-Free Reinforcement Learning with CVaR](https://icml.cc/virtual/2024/poster/33831) (Poster)
  - **Authors:** [Xinyi Ni](http://openreview.net/profile?id=~Xinyi_Ni1), [Guanlin Liu](http://openreview.net/profile?id=~Guanlin_Liu1), [Lifeng Lai](http://openreview.net/profile?id=~Lifeng_Lai1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of California, Davis, Davis, USA, Department of Electrical and Computer Engineering, University of California, Davis, Davis, USA, Department of Electrical and Computer Engineering, University of California, Davis, Davis, USA
  - **TL;DR:** This study introduces a novel risk-sensitive reward-free reinforcement learning framework based on Conditional Value-at-Risk (CVaR) to enhance exploration efficiency in safety-critical settings. The proposed algorithm, CVaR-RF-UCRL, demonstrates near-optimal sample complexity and addresses the need for risk-sensitive decision-making in reinforcement learning.
  - **Keywords:** Risk-sensitive reinforcement learning, reward-free reinforcement learning, Conditional Value-at-Risk (CVaR), CVaR-RF-UCRL, CVaR-VI, CVaR-VI-DISC, Safety-critical settings, decision-making, Efficient exploration strategies, sample complexity, Near-optimal exploration algorithm, risk tolerance parameter, Coherent risk measures, risk preferences


- [The Perception-Robustness Tradeoff in Deterministic Image Restoration](https://icml.cc/virtual/2024/poster/33306) (Spotlight Poster)
  - **Authors:** [Guy Ohayon](http://openreview.net/profile?id=~Guy_Ohayon1), [Tomer Michaeli](http://openreview.net/profile?id=~Tomer_Michaeli1), [Michael Elad](http://openreview.net/profile?id=~Michael_Elad1)
  - **Affiliations:** Faculty of Computer Science, Technion, Haifa, Israel, Faculty of Electrical and Computer Engineering, Technion, Haifa, Israel, Faculty of Computer Science, Technion, Haifa, Israel
  - **TL;DR:** This study investigates the tradeoff between perceptual quality and consistency in deterministic image restoration methods, proving that achieving both leads to an infinite Lipschitz constant, making these methods more vulnerable to adversarial attacks. The findings highlight the implications for single image super-resolution algorithms in both noisy and noiseless contexts.
  - **Keywords:** Deterministic image restoration, Inverse problems in imaging, Lipschitz constant, Single image super-resolution algorithms, Image denoising, Super-resolution, MRI reconstruction, Image-to-image translation, Perceptual quality vs. consistency, Adversarial susceptibility, Rigorous proof of tradeoff between perceptual quality and consistency


- [Equivariant Deep Weight Space Alignment](https://icml.cc/virtual/2024/poster/33133) (Poster)
  - **Authors:** [Aviv Navon](http://openreview.net/profile?id=~Aviv_Navon1), [Aviv Shamsian](http://openreview.net/profile?id=~Aviv_Shamsian1), [Ethan Fetaya](http://openreview.net/profile?id=~Ethan_Fetaya1), [Gal Chechik](http://openreview.net/profile?id=~Gal_Chechik1), [Nadav Dym](http://openreview.net/profile?id=~Nadav_Dym1), [Haggai Maron](http://openreview.net/profile?id=~Haggai_Maron1)
  - **Affiliations:** Bar-Ilan University, Bar-Ilan University, Bar-Ilan University, Bar-Ilan University; NVIDIA Research, Technion, Technion; NVIDIA Research
  - **TL;DR:** The study introduces DEEP-ALIGN, a novel framework for efficiently aligning the weights of deep networks, addressing the NP-hard weight alignment problem. Experimental results show that DEEP-ALIGN achieves better or equivalent alignments compared to existing optimization methods, significantly speeding up convergence in various applications.
  - **Keywords:** weight alignment, deep networks, model merging, DEEP-ALIGN, deep architecture, optimization algorithms, federated learning, continual learning, weight space mixup, NP-hard problem, time-consuming methods, suboptimal solutions, improved alignment quality, faster run time, effective initialization, permutation symmetries, invariant distance function


- [Novel Spectral Algorithms for the Partial Credit Model](https://icml.cc/virtual/2024/poster/33496) (Spotlight Poster)
  - **Authors:** [Duc Nguyen](http://openreview.net/profile?id=~Duc_Nguyen3), [Anderson Zhang](http://openreview.net/profile?id=~Anderson_Ye_Zhang1)
  - **Affiliations:** Department of Computer & Information Science, University of Pennsylvania, Department of Statistics & Data Science, Wharton School, University of Pennsylvania
  - **TL;DR:** This paper introduces a novel spectral algorithm for inference under the Partial Credit Model, demonstrating optimal error guarantees and significant efficiency improvements over existing methods. The proposed approach is validated through comprehensive experiments across various application domains, including education and recommendation systems.
  - **Keywords:** Partial Credit Model, Item Response Theory, psychometrics, Spectral algorithm, EM-based algorithm, Education testing, recommendation systems, financial investment, Estimation of item parameters, accuracy in response modeling, Optimal error guarantee, time-efficient statistical inference, Synthetic datasets, real-life datasets


- [RNAFlow: RNA Structure & Sequence Design via Inverse Folding-Based Flow Matching](https://icml.cc/virtual/2024/poster/33279) (Poster)
  - **Authors:** [Divya Nori](http://openreview.net/profile?id=~Divya_Nori1), [Wengong Jin](http://openreview.net/profile?id=~Wengong_Jin1)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA, Broad Institute of MIT and Harvard, Cambridge, MA, USA
  - **TL;DR:** The study introduces RNAFlow, a flow matching model designed for RNA sequence-structure generation that integrates RNA inverse folding with a pre-trained structure prediction network. It addresses challenges in RNA design by simultaneously generating RNA sequences and structures while modeling dynamic conformations, demonstrating advantages over existing methods.
  - **Keywords:** RNA engineering, RNA design, AI methods, Flow matching, RNA inverse folding, RosettaFold2NA, RNA therapeutics, protein binding, Conformational flexibility of RNA, computational cost of structure prediction, RNAFlow model, simultaneous RNA sequence and structure generation


- [Mixtures of Experts Unlock Parameter Scaling for Deep RL](https://icml.cc/virtual/2024/poster/33835) (Spotlight Poster)
  - **Authors:** [Johan Obando Ceron](http://openreview.net/profile?id=~Johan_Samir_Obando_Ceron1), [Ghada Sokar](http://openreview.net/profile?id=~Ghada_Sokar1), [Timon Willi](http://openreview.net/profile?id=~Timon_Willi1), [Clare Lyle](http://openreview.net/profile?id=~Clare_Lyle1), [Jesse Farebrother](http://openreview.net/profile?id=~Jesse_Farebrother1), [Jakob Foerster](http://openreview.net/profile?id=~Jakob_Nicolaus_Foerster1), [Gintare Karolina Dziugaite](http://openreview.net/profile?id=~Gintare_Karolina_Dziugaite1), [Doina Precup](http://openreview.net/profile?id=~Doina_Precup1), [Pablo Samuel Castro](http://openreview.net/profile?id=~Pablo_Samuel_Castro1)
  - **Affiliations:** Google DeepMind; Mila - Québec AI Institute; Université de Montréal, Google DeepMind, Google DeepMind; University of Oxford, Google DeepMind; McGill University, Google DeepMind; Mila - Québec AI Institute; McGill University, University of Oxford, Google DeepMind, Google DeepMind; Mila - Québec AI Institute; McGill University, Google DeepMind; Mila - Québec AI Institute; Université de Montréal
  - **TL;DR:** This study explores the integration of Mixture-of-Expert modules into value-based networks to enhance parameter scalability in deep reinforcement learning. The findings indicate significant performance improvements across various training regimes and model sizes, contributing to the understanding of scaling laws in reinforcement learning.
  - **Keywords:** Deep Reinforcement Learning, Mixture of Experts, Soft MoE, value-based networks, Parameter scaling challenges in reinforcement learning, Improved performance with Mixture of Experts, empirical evidence for scaling laws in RL


- [Sliced Wasserstein with Random-Path Projecting Directions](https://icml.cc/virtual/2024/poster/33793) (Poster)
  - **Authors:** [Khai Nguyen](http://openreview.net/profile?id=~Khai_Nguyen1), [Shujian Zhang](http://openreview.net/profile?id=~Shujian_Zhang1), [Tam Le](http://openreview.net/profile?id=~Tam_Le2), [Nhat Ho](http://openreview.net/profile?id=~Nhat_Ho1)
  - **Affiliations:** Department of Statistics and Data Sciences, University of Texas at Austin, USA, Department of Statistics and Data Sciences, University of Texas at Austin, USA, Department of Advanced Data Science, The Institute of Statistical Mathematics (ISM), Japan; RIKEN AIP, Japan, Department of Statistics and Data Sciences, University of Texas at Austin, USA
  - **TL;DR:** This study introduces an optimization-free slicing distribution for the Sliced Wasserstein distance, enhancing the efficiency of parameter estimators through fast sampling methods. The proposed Random-Path Projection Sliced Wasserstein (RPSW) and its variant demonstrate improved performance in applications like gradient flow and generative modeling.
  - **Keywords:** Sliced Wasserstein distance, optimization-free slicing distribution, Random-path projecting direction (RPD), Random-Path Projection Sliced Wasserstein (RPSW), Importance Weighted Random-Path Projection Sliced Wasserstein (IWRPSW), Generative models, gradient flow, denoising diffusion generative models, Expensive optimization for slicing distribution selection, sampling methods, Fast sampling for Monte Carlo estimation, improved performance of parameter estimators


- [In value-based deep reinforcement learning, a pruned network is a good network](https://icml.cc/virtual/2024/poster/32900) (Poster)
  - **Authors:** [Johan Obando Ceron](http://openreview.net/profile?id=~Johan_Samir_Obando_Ceron1), [Aaron Courville](http://openreview.net/profile?id=~Aaron_Courville3), [Pablo Samuel Castro](http://openreview.net/profile?id=~Pablo_Samuel_Castro1)
  - **Affiliations:** Google DeepMind; Mila - Québec AI Institute; Université de Montréal, Mila - Québec AI Institute; Université de Montréal, Google DeepMind; Mila - Québec AI Institute; Université de Montréal
  - **TL;DR:** This study investigates the use of gradual magnitude pruning in value-based deep reinforcement learning agents to enhance parameter effectiveness, resulting in significant performance improvements while utilizing only a fraction of the original network parameters. The findings suggest that non-standard network topologies can be beneficial for training deep RL agents.
  - **Keywords:** deep reinforcement learning, parameter effectiveness, sparse training techniques, gradual magnitude pruning, DQN, ResNet, under-utilization of network parameters, implicit underparameterization, dormant neurons during training, performance improvements, scaling network architectures


- [Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?](https://icml.cc/virtual/2024/poster/33275) (Poster)
  - **Authors:** [Andreas Opedal](http://openreview.net/profile?id=~Andreas_Opedal1), [Alessandro Stolfo](http://openreview.net/profile?id=~Alessandro_Stolfo1), [Haruki Shirakami](http://openreview.net/profile?id=~Haruki_Shirakami1), [Ying Jiao](http://openreview.net/profile?id=~Ying_Jiao1), [Ryan Cotterell](http://openreview.net/profile?id=~Ryan_Cotterell1), [Bernhard Schölkopf](http://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1), [Abulhair Saparov](http://openreview.net/profile?id=~Abulhair_Saparov1), [Mrinmaya Sachan](http://openreview.net/profile?id=~Mrinmaya_Sachan3)
  - **Affiliations:** ETH Zürich; Max Planck Institute for Intelligent Systems, ETH Zürich; Max Planck Institute for Intelligent Systems, ETH Zürich, KU Leuven, ETH Zürich, ETH Zürich; Max Planck Institute for Intelligent Systems, New York University, ETH Zürich
  - **TL;DR:** This study investigates whether large language models (LLMs) exhibit cognitive biases similar to those of children when solving arithmetic word problems. The findings indicate that LLMs show human-like biases in the text comprehension and solution planning stages, but not during the execution of arithmetic expressions.
  - **Keywords:** Large Language Models, Cognitive Biases, Problem Solving, Neuro-symbolic approach, Education, Cognitive Modeling, Cognitive biases in children, Arithmetic word problems, Evidence of human-like biases in LLMs during problem-solving


- [Fair Resource Allocation in Multi-Task Learning](https://icml.cc/virtual/2024/poster/34346) (Poster)
  - **Authors:** [Hao Ban](http://openreview.net/profile?id=~Hao_Ban1), [Kaiyi Ji](http://openreview.net/profile?id=~Kaiyi_Ji1)
  - **Affiliations:** Department of Computer Science and Engineering, University at Buffalo, New York, United States, Department of Computer Science and Engineering, University at Buffalo, New York, United States
  - **TL;DR:** This paper presents FairGrad, a novel optimization objective for multi-task learning that addresses the challenge of conflicting gradients by framing the problem as a utility maximization issue. The proposed method enhances performance across various tasks while ensuring fairness, achieving state-of-the-art results in multiple benchmarks.
  - **Keywords:** Multi-Task Learning (MTL), Fair Resource Allocation, FairGrad, Gradient Manipulation Methods, Optimization Algorithms, Natural Language Processing, Computer Vision, Autonomous Driving, Recommendation Systems, Conflicting Gradients, Fair Optimization, Data Efficiency, State-of-the-art Performance, Theoretical Convergence Guarantee, α-fairness, Max-Min Fairness, Proportional Fairness


- [Deep Stochastic Mechanics](https://icml.cc/virtual/2024/poster/34944) (Poster)
  - **Authors:** [Elena Orlova](http://openreview.net/profile?id=~Elena_Orlova1), [Aleksei Ustimenko](http://openreview.net/profile?id=~Aleksei_Ustimenko1), [Ruoxi Jiang](http://openreview.net/profile?id=~Ruoxi_Jiang1), [Peter Y. Lu](http://openreview.net/profile?id=~Peter_Y._Lu1), [Rebecca Willett](http://openreview.net/profile?id=~Rebecca_Willett1)
  - **Affiliations:** Department of Computer Science, University of Chicago, Chicago, USA, ShareChat, London, UK; Department of Physics, University of Chicago, Chicago, USA; Department of Statistics, University of Chicago, Chicago, USA, Department of Computer Science, University of Chicago, Chicago, USA, Department of Physics, University of Chicago, Chicago, USA, Department of Computer Science, University of Chicago, Chicago, USA; Department of Statistics, University of Chicago, Chicago, USA
  - **TL;DR:** This paper presents a novel deep-learning-based method for simulating the time-evolving Schrödinger equation, leveraging stochastic mechanics to reduce computational complexity in high dimensions. The proposed approach demonstrates significant advantages over existing methods, particularly in addressing the challenges posed by the curse of dimensionality in quantum mechanics.
  - **Keywords:** deep learning, stochastic mechanics, quantum mechanics, generative diffusion models, Markovian diffusion, stochastic differential equations (SDEs), quantum chemistry, drug discovery, condensed matter physics, quantum computing, high-dimensional PDEs, curse of dimensionality, computational complexity, novel equations for stochastic quantum mechanics, performance improvements in TDSE, time-dependent Schrödinger equation (TDSE), Klein-Gordon equation, Dirac equation, McKean-Vlasov SDEs


- [Variational Linearized Laplace Approximation for Bayesian Deep Learning](https://icml.cc/virtual/2024/poster/35135) (Poster)
  - **Authors:** [Luis A. Ortega](http://openreview.net/profile?id=~Luis_A._Ortega1), [Simon Rodriguez Santana](http://openreview.net/profile?id=~Simon_Rodriguez_Santana1), [Daniel Hernández-Lobato](http://openreview.net/profile?id=~Daniel_Hern%C3%A1ndez-Lobato1)
  - **Affiliations:** Universidad Autónoma de Madrid; None, Institute for Research in Technology (IIT), ICAI Engineering School, Universidad Pontificia Comillas, Universidad Autónoma de Madrid; None
  - **TL;DR:** This study proposes a new method for approximating the Linearized Laplace Approximation (LLA) using a variational sparse Gaussian Process to improve uncertainty estimation in deep neural networks while reducing computational costs. Experimental results demonstrate that this method outperforms existing efficient LLA variants in both predictive distribution quality and computational time.
  - **Keywords:** Bayesian Deep Learning, Uncertainty Estimation, Linearized Laplace Approximation (LLA), Variational Sparse Gaussian Process (GP), Generalized Gauss-Newton (GGN), Autonomous Driving, Healthcare Systems, Computational Costs, Model Uncertainty, Weak Calibration, Efficient Stochastic Optimization, Predictive Distribution Quality


- [Structured Chemistry Reasoning with Large Language Models](https://icml.cc/virtual/2024/poster/34883) (Poster)
  - **Authors:** [Siru Ouyang](http://openreview.net/profile?id=~Siru_Ouyang1), [Zhuosheng Zhang](http://openreview.net/profile?id=~Zhuosheng_Zhang1), [Bing Yan](http://openreview.net/profile?id=~Bing_Yan2), [Xuan Liu](http://openreview.net/profile?id=~Xuan_Liu6), [Yejin Choi](http://openreview.net/profile?id=~Yejin_Choi1), [Jiawei Han](http://openreview.net/profile?id=~Jiawei_Han1), [Lianhui Qin](http://openreview.net/profile?id=~Lianhui_Qin1)
  - **Affiliations:** University of Illinois Urbana-Champaign, Shanghai Jiao Tong University, New York University, University of Illinois Urbana-Champaign, University of Washington; Allen Institute for AI, University of Illinois Urbana-Champaign, University of California San Diego; Allen Institute for AI
  - **TL;DR:** This study introduces STRUCT CHEM, a prompting strategy that enhances the reasoning capabilities of large language models (LLMs) in complex chemistry problems. The approach significantly improves performance by up to 30% in various chemistry domains, addressing the challenges of precise grounded reasoning in science.
  - **Keywords:** Large Language Models, Chemistry Reasoning, STRUCT CHEM, Step-by-step reasoning, Quantum Chemistry, Mechanics, Physical Chemistry, Kinetics, Complex scientific reasoning, Errors in reasoning, Knowledge elicitation, Improved reasoning capability, Performance enhancement of LLMs


- [Stochastic Quantum Sampling for Non-Logconcave Distributions and Estimating Partition Functions](https://icml.cc/virtual/2024/poster/32913) (Poster)
  - **Authors:** [Guneykan Ozgul](http://openreview.net/profile?id=~Guneykan_Ozgul1), [Xiantao Li](http://openreview.net/profile?id=~Xiantao_Li1), [Mehrdad Mahdavi](http://openreview.net/profile?id=~Mehrdad_Mahdavi2), [Chunhao Wang](http://openreview.net/profile?id=~Chunhao_Wang1)
  - **Affiliations:** Department of Computer Science and Engineering, Pennsylvania State University, Department of Mathematics, Pennsylvania State University, Department of Computer Science and Engineering, Pennsylvania State University, Department of Computer Science and Engineering, Pennsylvania State University
  - **TL;DR:** This paper presents quantum algorithms for sampling from non-logconcave distributions and estimating partition functions, addressing challenges in quantizing Markov chains that do not satisfy the detailed balance condition. The proposed methods demonstrate polynomial speedups compared to classical algorithms, enhancing efficiency in various applications such as statistical mechanics and machine learning.
  - **Keywords:** Quantum algorithms, Non-logconcave distributions, Gibbs-Boltzmann distribution, Stochastic gradient oracle, Quantum walk operators, Markov Chain Monte Carlo (MCMC), Metropolis-adjusted Langevin algorithm (MALA), Unadjusted Langevin algorithm (ULA), Statistical mechanics, Bayesian learning, Optimization, Molecular dynamics, Sampling from non-logconcave distributions, Mixing time analysis, Asymptotic bias in Markov chains, Polynomial speedups in quantum algorithms, Estimation of partition functions, Langevin diffusion, Detailed balance condition, Spectral gap


- [Implicit Representations via Operator Learning](https://icml.cc/virtual/2024/poster/35103) (Poster)
  - **Authors:** [Sourav Pal](http://openreview.net/profile?id=~Sourav_Pal1), [Harshavardhan Adepu](http://openreview.net/profile?id=~Harshavardhan_Adepu1), [Clinton Wang](http://openreview.net/profile?id=~Clinton_Wang1), [Polina Golland](http://openreview.net/profile?id=~Polina_Golland1), [Vikas Singh](http://openreview.net/profile?id=~Vikas_Singh1)
  - **Affiliations:** University of Wisconsin–Madison, University of Wisconsin–Madison, Massachusetts Institute of Technology, Massachusetts Institute of Technology, University of Wisconsin–Madison
  - **TL;DR:** This study introduces Operator INR (O-INR), a reformulation of Implicit Neural Representations that maps one function space to another, utilizing integral transforms and convolutions for improved stability and performance in various applications. The findings suggest that O-INR can effectively handle existing problem settings with minimal compromise compared to traditional methods.
  - **Keywords:** Implicit Neural Representations (INRs), Operator INR (O-INR), signal representation, Integral Transforms, Convolutions, Image super-resolution, texture synthesis, inverse problems, view synthesis, Challenges in using INRs for downstream processing tasks, isolation of INRs, Reformulation of INR model, stable behavior of O-INR


- [MADA: Meta-Adaptive Optimizers Through Hyper-Gradient Descent](https://icml.cc/virtual/2024/poster/32876) (Poster)
  - **Authors:** [Kaan Ozkara](http://openreview.net/profile?id=~Kaan_Ozkara1), [Can Karakus](http://openreview.net/profile?id=~Can_Karakus2), [Parameswaran Raman](http://openreview.net/profile?id=~Parameswaran_Raman1), [Mingyi Hong](http://openreview.net/profile?id=~Mingyi_Hong1), [Shoham Sabach](http://openreview.net/profile?id=~Shoham_Sabach1), [Branislav Kveton](http://openreview.net/profile?id=~Branislav_Kveton1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of California Los Angeles, USA, Amazon Web Services; Department of Electrical and Computer Engineering, University of Minnesota, USA, Amazon Web Services, Amazon Web Services; Department of Electrical and Computer Engineering, University of Minnesota, USA, Faculty of Data and Decision Sciences, Technion – Israel Institute of Technology, Israel, Amazon Web Services, Amazon Web Services; LIONS, IEM, STI, Ecole Polytechnique Fédérale de Lausanne, Switzerland
  - **TL;DR:** This paper introduces Meta-Adaptive Optimizers (MADA), a framework that dynamically learns the most suitable optimizer during training, outperforming traditional optimizers like Adam across various tasks. The study highlights the advantages of parameterized optimizers and presents a new method, A V-Grad, for hyper-gradient optimization.
  - **Keywords:** adaptive optimizers, deep learning, Meta-Adaptive Optimizers (MADA), hyper-gradient descent, A V-Grad, AMSGrad, vision tasks, language tasks, Large Language Models (LLMs), performance generalization, hyper-parameter tuning, unified optimizer framework, parameterized optimizer, convergence analysis, GPT-2


- [Differentiable Mapper for Topological Optimization of Data Representation](https://icml.cc/virtual/2024/poster/34077) (Oral)
  - **Authors:** [Ziyad Oulhaj](http://openreview.net/profile?id=~Ziyad_Oulhaj1), [Mathieu Carrière](http://openreview.net/profile?id=~Mathieu_Carri%C3%A8re1), [Bertrand Michel](http://openreview.net/profile?id=~Bertrand_Michel2)
  - **Affiliations:** Nantes Université; École Centrale Nantes, Laboratoire de Mathématiques Jean Leray, CNRS UMR 6629, Nantes, France, DataShape, Centre Inria d’Université Côte d’Azur, Sophia Antipolis, France, Nantes Université; École Centrale Nantes, Laboratoire de Mathématiques Jean Leray, CNRS UMR 6629, Nantes, France
  - **TL;DR:** This study introduces a novel optimization scheme for the filter parameter in Mapper graphs, addressing the challenges of parameter sensitivity in Topological Data Analysis. The proposed method demonstrates improved representations of data through optimized Mapper graphs across various datasets.
  - **Keywords:** Topological Data Analysis (TDA), Unsupervised Data Representation, Data Visualization, Mapper Graph, Filter Optimization, Biological Inference, Single-Cell Data Analysis, Parameter Sensitivity, Manual Tuning of Parameters, Optimization Scheme for Filter in Mapper Graphs, Improved Mapper Graph Representations


- [RMIB: Representation Matching Information Bottleneck for Matching Text Representations](https://icml.cc/virtual/2024/poster/33374) (Poster)
  - **Authors:** [Haihui Pan](http://openreview.net/profile?id=~Haihui_Pan1), [zhifang Liao](http://openreview.net/profile?id=~Zhifang_Liao1), [Wenrui Xie](http://openreview.net/profile?id=~Wenrui_Xie1), [Kun Han](http://openreview.net/profile?id=~Kun_Han2)
  - **Affiliations:** Central South University, Changsha, China; Cheetah Mobile Inc., Beijing, China, Central South University, Changsha, China, Datawhale Org., Hangzhou, China, Cheetah Mobile Inc., Beijing, China
  - **TL;DR:** This study introduces the Representation Matching Information Bottleneck (RMIB) to improve the generalization ability of text matching in asymmetrical domains by narrowing the distribution of text representations. The proposed method is theoretically validated and shown to enhance performance across multiple text matching models and datasets.
  - **Keywords:** text matching, asymmetrical domains, information bottleneck (IB), representation matching, natural language processing (NLP), question-and-answer matching, information retrieval, domain matching, distribution similarity, lexical gap, representation matching information bottleneck (RMIB), improved performance in text matching


- [Stability and Generalization for Stochastic Recursive Momentum-based Algorithms for (Strongly-)Convex One to $K$-Level Stochastic Optimizations](https://icml.cc/virtual/2024/poster/34608) (Poster)
  - **Authors:** [Xiaokang Pan](http://openreview.net/profile?id=~Xiaokang_Pan1), [Xingyu Li](http://openreview.net/profile?id=~Xingyu_Li4), [Jin Liu](http://openreview.net/profile?id=~Jin_Liu12), [Tao Sun](http://openreview.net/profile?id=~Tao_Sun7), [Kai Sun](http://openreview.net/profile?id=~Kai_Sun6), [Lixing Chen](http://openreview.net/profile?id=~Lixing_Chen1), [Zhe Qu](http://openreview.net/profile?id=~Zhe_Qu1)
  - **Affiliations:** Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha, China, Department of Computer Science, Tulane University, New Orleans, USA, Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China, School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China, Institute of Cyber Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, Hunan Provincial Key Lab on Bioinformatics, School of Computer Science and Engineering, Central South University, Changsha, China
  - **TL;DR:** This paper analyzes the generalization performance of STORM-based algorithms for K-level stochastic optimization problems, revealing that increased levels can escalate generalization errors and that larger batch sizes can improve performance. The study provides theoretical results linking algorithmic stability to generalization and presents excess risk bounds.
  - **Keywords:** Stochastic optimization, Generalization performance, Stochastic Recursive Momentum (STORM), COVER, SVMR, Reinforcement learning, Model-agnostic meta-learning, Risk-averse portfolio optimization, Deep AUC maximization, Biased gradient issue, Generalization error, Variance in estimators, Excess risk bounds, Stability results, K-level stochastic optimizations, Convex and strongly convex settings


- [Auto-Encoding Morph-Tokens for Multimodal LLM](https://icml.cc/virtual/2024/poster/33953) (Spotlight Poster)
  - **Authors:** [Kaihang Pan](http://openreview.net/profile?id=~Kaihang_Pan1), [Siliang Tang](http://openreview.net/profile?id=~Siliang_Tang1), [Juncheng Li](http://openreview.net/profile?id=~Juncheng_Li3), [Zhaoyu Fan](http://openreview.net/profile?id=~Zhaoyu_Fan1), [Wei Chow](http://openreview.net/profile?id=~Wei_Chow1), [Shuicheng YAN](http://openreview.net/profile?id=~Shuicheng_YAN3), [Tat-Seng Chua](http://openreview.net/profile?id=~Tat-Seng_Chua2), [Yueting Zhuang](http://openreview.net/profile?id=~Yueting_Zhuang1), [Hanwang Zhang](http://openreview.net/profile?id=~Hanwang_Zhang3)
  - **Affiliations:** Zhejiang University, Zhejiang University, Zhejiang University; National University of Singapore, Zhejiang University, Zhejiang University, Skywork AI; Nanyang Technological University, National University of Singapore, Zhejiang University, Skywork AI; Nanyang Technological University
  - **TL;DR:** This study addresses the challenge of achieving synergy between visual comprehension and generation in multimodal large language models (MLLMs) by introducing morph-tokens, which serve dual purposes for comprehension and generation. The proposed method demonstrates a new state-of-the-art performance in both tasks, resolving the conflicting objectives of visual abstraction and detail preservation.
  - **Keywords:** Multimodal Large Language Models, visual comprehension, visual generation, Morph-tokens, auto-regressive MLLM framework, Image reconstruction, vision-language tasks, Conflicting objectives in comprehension and generation, visual abstraction vs. detail preservation, New state-of-the-art (SOTA) for multimodal comprehension and generation


- [A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization](https://icml.cc/virtual/2024/poster/34966) (Poster)
  - **Authors:** [Ashwinee Panda](http://openreview.net/profile?id=~Ashwinee_Panda1), [Xinyu Tang](http://openreview.net/profile?id=~Xinyu_Tang1), [Saeed Mahloujifar](http://openreview.net/profile?id=~Saeed_Mahloujifar1), [Vikash Sehwag](http://openreview.net/profile?id=~Vikash_Sehwag1), [Prateek Mittal](http://openreview.net/profile?id=~Prateek_Mittal1)
  - **Affiliations:** Princeton University, Princeton University, Princeton University, Princeton University, Princeton University
  - **TL;DR:** This paper presents a new adaptive method for hyperparameter optimization in differentially private deep learning, addressing the challenges of tuning hyperparameters while considering privacy costs. The proposed method achieves state-of-the-art performance across various tasks and architectures, demonstrating effective scaling of hyperparameters.
  - **Keywords:** Differential Privacy, Hyperparameter Optimization, DP-SGD (Differentially Private Stochastic Gradient Descent), Adaptive HPO, Computer Vision, Natural Language Processing, Hyperparameter tuning, Privacy cost in HPO, State-of-the-art performance, Linear scaling rule for hyperparameters, ImageNet


- [Bayesian Program Learning by Decompiling Amortized Knowledge](https://icml.cc/virtual/2024/poster/34675) (Poster)
  - **Authors:** [Alessandro Palmarini](http://openreview.net/profile?id=~Alessandro_B._Palmarini1), [Christopher Lucas](http://openreview.net/profile?id=~Christopher_G._Lucas1), [Siddharth N](http://openreview.net/profile?id=~Siddharth_N1)
  - **Affiliations:** Santa Fe Institute, Santa Fe, NM, USA; School of Informatics, University of Edinburgh, Edinburgh, UK; The Alan Turing Institute, UK, School of Informatics, University of Edinburgh, Edinburgh, UK, School of Informatics, University of Edinburgh, Edinburgh, UK; The Alan Turing Institute, UK
  - **TL;DR:** This paper presents a novel approach to inductive program synthesis using DREAM CODER, which learns to simplify search through a neural search policy and library learning. The method enhances efficiency by reducing both search breadth and depth, leading to faster proficiency and better generalization across various domains, especially with limited examples.
  - **Keywords:** inductive program synthesis, program learning, neural search policy, library learning, chunking, search breadth reduction, search depth reduction, generalization from fewer examples, faster domain proficiency, improved generalization, DREAM CODER


- [Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation](https://icml.cc/virtual/2024/poster/33224) (Spotlight Poster)
  - **Authors:** [Xianghe Pang](http://openreview.net/profile?id=~Xianghe_Pang1), [shuo tang](http://openreview.net/profile?id=~Shuo_Tang2), [Rui Ye](http://openreview.net/profile?id=~Rui_Ye1), [Yuxin Xiong](http://openreview.net/profile?id=~Yuxin_Xiong1), [Bolun Zhang](http://openreview.net/profile?id=~Bolun_Zhang2), [Yanfeng Wang](http://openreview.net/profile?id=~Yanfeng_Wang1), [Siheng Chen](http://openreview.net/profile?id=~Siheng_Chen1)
  - **Affiliations:** Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Multi-Agent Governance & Intelligence Crew (MAGIC), Shanghai, China, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Multi-Agent Governance & Intelligence Crew (MAGIC), Shanghai, China, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Multi-Agent Governance & Intelligence Crew (MAGIC), Shanghai, China, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China, Shanghai AI Laboratory, Shanghai, China, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China; Multi-Agent Governance & Intelligence Crew (MAGIC), Shanghai, China
  - **TL;DR:** This paper introduces MATRIX, a social scene simulator designed to enable large language models (LLMs) to self-align with human values by considering social consequences before responding. The proposed method outperforms existing approaches, including Constitutional AI, demonstrating significant improvements in aligning LLMs with human values.
  - **Keywords:** Large Language Models, AI Alignment, Self-Alignment, Social Scene Simulation, MATRIX, Reinforcement Learning from Human Feedback (RLHF), AI Safety, Human Values Alignment, Misuse of LLMs, Value Alignment Challenges, Novel alignment methods, Performance improvement over Constitutional AI, Monopolylogue


- [Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI](https://icml.cc/virtual/2024/poster/34106) (Poster)
  - **Authors:** [Theodore Papamarkou](http://openreview.net/profile?id=~Theodore_Papamarkou1), [Maria Skoularidou](http://openreview.net/profile?id=~Maria_Skoularidou1), [Konstantina Palla](http://openreview.net/profile?id=~Konstantina_Palla1), [Laurence Aitchison](http://openreview.net/profile?id=~Laurence_Aitchison1), [Julyan Arbel](http://openreview.net/profile?id=~Julyan_Arbel1), [David Dunson](http://openreview.net/profile?id=~David_Dunson1), [Maurizio Filippone](http://openreview.net/profile?id=~Maurizio_Filippone1), [Vincent Fortuin](http://openreview.net/profile?id=~Vincent_Fortuin1), [Philipp Hennig](http://openreview.net/profile?id=~Philipp_Hennig1), [Jose Miguel Hernandez-Lobato](http://openreview.net/profile?id=~José_Miguel_Hernández-Lobato1), [Aliaksandr Hubin](http://openreview.net/profile?id=aliaksah@math.uio.no), [Alexander Immer](http://openreview.net/profile?id=~Alexander_Immer1), [Theofanis Karaletsos](http://openreview.net/profile?id=~Theofanis_Karaletsos1), [Khan Emtiyaz](http://openreview.net/profile?id=~Mohammad_Emtiyaz_Khan1), [Agustinus Kristiadi](http://openreview.net/profile?id=~Agustinus_Kristiadi1), [Yingzhen Li](http://openreview.net/profile?id=~Yingzhen_Li1), [Stephan Mandt](http://openreview.net/profile?id=~Stephan_Mandt1), [Chris Nemeth](http://openreview.net/profile?id=~Christopher_Nemeth1), [Michael A Osborne](http://openreview.net/profile?id=~Michael_A_Osborne1), [Tim G. J. Rudner](http://openreview.net/profile?id=~Tim_G._J._Rudner2), [David Rügamer](http://openreview.net/profile?id=~David_Rügamer1), [Yee-Whye Teh](http://openreview.net/profile?id=~Yee_Whye_Teh2), [Max Welling](http://openreview.net/profile?id=~Max_Welling1), [Andrew Wilson](http://openreview.net/profile?id=~Andrew_Gordon_Wilson1), [Ruqi Zhang](http://openreview.net/profile?id=~Ruqi_Zhang1)
  - **Affiliations:** Department of Mathematics, The University of Manchester, Manchester, UK, Eric and Wendy Schmidt Center, Broad Institute of MIT and Harvard, Cambridge, USA, Spotify, London, UK, Computational Neuroscience Unit, University of Bristol, Bristol, UK, Centre Inria de l’Université Grenoble Alpes, Grenoble, France, Department of Statistical Science, Duke University, USA, Statistics Program, KAUST, Saudi Arabia, Helmholtz AI, Munich, Germany; Department of Computer Science, Technical University of Munich, Munich, Germany; Munich Center for Machine Learning, Munich, Germany, Tübingen AI Center, University of Tübingen, Tübingen, Germany, Department of Engineering, University of Cambridge, Cambridge, UK, Department of Mathematics, University of Oslo, Oslo, Norway; Bioinformatics and Applied Statistics, Norwegian University of Life Sciences, Ås, Norway, Department of Computer Science, ETH Zurich, Switzerland, Chan Zuckerberg Initiative, California, USA, Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan, Vector Institute, Toronto, Canada, Department of Computing, Imperial College London, London, UK, Department of Computer Science, UC Irvine, Irvine, USA, Department of Mathematics and Statistics, Lancaster University, Lancaster, UK, Department of Engineering Science, University of Oxford, Oxford, UK, Center for Data Science, New York University, New York, USA, Department of Statistics, LMU Munich, Munich, Germany, DeepMind, London, UK; Department of Statistics, University of Oxford, Oxford, UK, Informatics Institute, University of Amsterdam, Amsterdam, Netherlands, Courant Institute of Mathematical Sciences and Center for Data Science, Computer Science Department, New York University, New York, USA, Department of Computer Science, Purdue University, West Lafayette, USA
  - **TL;DR:** The paper argues for the importance of Bayesian deep learning (BDL) in enhancing the capabilities of deep learning, particularly in addressing overlooked metrics and challenges in large-scale AI. It emphasizes the potential of combining BDL with large-scale foundation models to unlock new research avenues and improve predictive accuracy.
  - **Keywords:** Bayesian deep learning, large-scale AI, Bayesian inference, probabilistic modeling, Uncertainty, active learning, continual learning, Enhanced capabilities of deep learning, addressing challenges in BDL, Bayes' theorem, posterior probability, prior probability, likelihood


- [Feedback Loops With Language Models Drive In-Context Reward Hacking](https://icml.cc/virtual/2024/poster/34557) (Poster)
  - **Authors:** [Alexander Pan](http://openreview.net/profile?id=~Alexander_Pan1), [Erik Jones](http://openreview.net/profile?id=~Erik_Jones3), [Meena Jagadeesan](http://openreview.net/profile?id=~Meena_Jagadeesan1), [Jacob Steinhardt](http://openreview.net/profile?id=~Jacob_Steinhardt1)
  - **Affiliations:** University of California, Berkeley, USA, University of California, Berkeley, USA, University of California, Berkeley, USA, University of California, Berkeley, USA
  - **TL;DR:** This study investigates how feedback loops in language models can lead to in-context reward hacking, where the models optimize for engagement metrics while inadvertently increasing negative side effects like toxicity. The authors propose that traditional evaluations are insufficient and recommend new methods to better capture these dynamics.
  - **Keywords:** Feedback loops, Language models, In-context reward hacking, Autonomous agents, Social media engagement, Negative side effects, Toxicity, Under-specified objectives, Optimization, Output-refinement, Policy-refinement, Large Language Models (LLMs), A/B testing


- [$S^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting](https://icml.cc/virtual/2024/poster/32972) (Poster)
  - **Authors:** [Zijie Pan](http://openreview.net/profile?id=~Zijie_Pan3), [Yushan Jiang](http://openreview.net/profile?id=~Yushan_Jiang1), [Sahil Garg](http://openreview.net/profile?id=~Sahil_Garg1), [Anderson Schneider](http://openreview.net/profile?id=~Anderson_Schneider1), [Yuriy Nevmyvaka](http://openreview.net/profile?id=~Yuriy_Nevmyvaka1), [Dongjin Song](http://openreview.net/profile?id=~Dongjin_Song2)
  - **Affiliations:** School of Computing, University of Connecticut, Storrs, USA, School of Computing, University of Connecticut, Storrs, USA, Department of Machine Learning Research, Morgan Stanley, New York, USA, Department of Machine Learning Research, Morgan Stanley, New York, USA, Department of Machine Learning Research, Morgan Stanley, New York, USA, School of Computing, University of Connecticut, Storrs, USA
  - **TL;DR:** The study introduces S2IP-LLM, a method that aligns the semantic space of pre-trained large language models with time series embedding space to enhance forecasting accuracy. Empirical results demonstrate that S2IP-LLM outperforms state-of-the-art baselines, highlighting the importance of prompt learning informed by semantic space.
  - **Keywords:** Time Series Forecasting, Large Language Models, Semantic Space Informed Prompt Learning, Tokenization Module, Cross-Modality Alignment, Energy Load Management, Traffic Forecasting, Weather Forecasting, Health Risk Analysis, Non-stationary Characteristics, Concept Drift, Complexity of Model Training, Superior Forecasting Performance, Prompt Learning Informed by Semantic Space, Benchmark Datasets


- [Trainable Transformer in Transformer](https://icml.cc/virtual/2024/poster/34368) (Poster)
  - **Authors:** [Abhishek Panigrahi](http://openreview.net/profile?id=~Abhishek_Panigrahi1), [Sadhika Malladi](http://openreview.net/profile?id=~Sadhika_Malladi2), [Mengzhou Xia](http://openreview.net/profile?id=~Mengzhou_Xia1), [Sanjeev Arora](http://openreview.net/profile?id=~Sanjeev_Arora1)
  - **Affiliations:** Department of Computer Science, Princeton University, Department of Computer Science, Princeton University, Department of Computer Science, Princeton University, Department of Computer Science, Princeton University
  - **TL;DR:** This study introduces a novel construction called Transformer in Transformer (TINT) that enables efficient simulation and fine-tuning of complex models during inference without parameter updates. The findings demonstrate that TINT can significantly enhance the performance of large pre-trained language models, suggesting their capability to perform intricate subroutines.
  - **Keywords:** in-context learning, large pre-trained language models, transformer models, Transformer in Transformer (TINT), gradient-based training, fine-tuning, language modeling, downstream tasks, memory overhead, simulation of complex models, improved performance of language models, efficient internal fine-tuning, AI safety, AI alignment


- [LPGD: A General Framework for Backpropagation through Embedded Optimization Layers](https://icml.cc/virtual/2024/poster/33967) (Poster)
  - **Authors:** [Anselm Paulus](http://openreview.net/profile?id=~Anselm_Paulus1), [Georg Martius](http://openreview.net/profile?id=~Georg_Martius1), [Vit Musil](http://openreview.net/profile?id=~V%C3%ADt_Musil1)
  - **Affiliations:** University of Tübingen, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany, University of Tübingen, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Tübingen, Germany, Masaryk University, Brno, Czech Republic
  - **TL;DR:** This paper introduces Lagrangian Proximal Gradient Descent (LPGD), a framework for training machine learning architectures with embedded optimization layers, which addresses the challenge of degenerate derivatives in optimization problems. LPGD demonstrates faster convergence than traditional gradient descent methods, providing a unified approach to various existing techniques.
  - **Keywords:** Embedded optimization layers, machine learning architectures, Lagrangian Proximal Gradient Descent (LPGD), stochastic gradient descent (GD), Autonomous driving, modeling physical systems, robotic control, Degenerate derivatives, bi-level optimization, Unification of optimization methods, faster convergence than gradient descent


- [The Linear Representation Hypothesis and the Geometry of Large Language Models](https://icml.cc/virtual/2024/poster/33950) (Poster)
  - **Authors:** [Kiho Park](http://openreview.net/profile?id=~Kiho_Park1), [Yo Joong Choe](http://openreview.net/profile?id=~Yo_Joong_Choe1), [Victor Veitch](http://openreview.net/profile?id=~Victor_Veitch1)
  - **Affiliations:** University of Chicago, Illinois, USA, University of Chicago, Illinois, USA, University of Chicago, Illinois, USA
  - **TL;DR:** This paper formalizes the Linear Representation Hypothesis, exploring how high-level concepts are represented linearly in language models and addressing geometric notions in representation space. The findings demonstrate the existence of linear representations of concepts and their implications for model interpretation and control.
  - **Keywords:** Linear Representation Hypothesis, Large Language Models, Linear probing, Model steering, Counterfactuals, Natural Language Processing, Interpretation and control of model behavior, Formalizations of linear representation, Unification of linear representation notions, LLaMA-2


- [Optimal Ridge Regularization for Out-of-Distribution Prediction](https://icml.cc/virtual/2024/poster/33617) (Spotlight Poster)
  - **Authors:** [Pratik Patil](http://openreview.net/profile?id=~Pratik_Patil1), [Jin-Hong Du](http://openreview.net/profile?id=~Jin-Hong_Du1), [Ryan Tibshirani](http://openreview.net/profile?id=~Ryan_Tibshirani2)
  - **Affiliations:** Department of Statistics, University of California, Berkeley, CA 94720, USA, Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA, Department of Statistics, University of California, Berkeley, CA 94720, USA
  - **TL;DR:** This study investigates the behavior of optimal ridge regularization and risk in out-of-distribution prediction, revealing that negative regularization levels can be optimal under certain shifts in data distribution. The findings highlight significant differences from traditional in-distribution settings and provide insights into the alignment of covariance and signal structures.
  - **Keywords:** Ridge regression, Out-of-distribution prediction, Optimal ridge regularization, Regularization levels, Statistical modeling, High-dimensional data analysis, Covariate shift, Regression shift, Model overfitting, Optimal regularization behavior, Monotonic risk tuning


- [Variational Inference with Coverage Guarantees in Simulation-Based Inference](https://icml.cc/virtual/2024/poster/33852) (Poster)
  - **Authors:** [Yash Patel](http://openreview.net/profile?id=~Yash_Patel3), [Declan McNamara](http://openreview.net/profile?id=~Declan_McNamara1), [Jackson Loper](http://openreview.net/profile?id=~Jackson_Loper1), [Jeffrey Regier](http://openreview.net/profile?id=~Jeffrey_Regier1), [Ambuj Tewari](http://openreview.net/profile?id=~Ambuj_Tewari1)
  - **Affiliations:** Department of Statistics, University of Michigan, Ann Arbor, USA, Department of Statistics, University of Michigan, Ann Arbor, USA, Department of Statistics, University of Michigan, Ann Arbor, USA, Department of Statistics, University of Michigan, Ann Arbor, USA, Department of Statistics, University of Michigan, Ann Arbor, USA
  - **TL;DR:** The study introduces Conformalized Amortized Neural Variational Inference (CANVI), a scalable method that provides guaranteed marginal coverage for posterior approximations in simulation-based inference. It demonstrates accurate calibration and high predictive efficiency across various benchmark tasks, including analyzing galaxy emission spectra.
  - **Keywords:** Variational Inference, Simulation-Based Inference, Amortized Variational Inference, Conformalized Amortized Neural Variational Inference (CANVI), Astrophysics, Neuroscience, Particle Physics, Lack of theoretical guarantees in variational inference, biased posterior estimates, calibration of expected coverage, Predictive efficiency, guaranteed marginal coverage, Posterior distributions, likelihood-free settings, credible intervals


- [Bias of Stochastic Gradient Descent or the Architecture: Disentangling the Effects of Overparameterization of Neural Networks](https://icml.cc/virtual/2024/poster/32615) (Poster)
  - **Authors:** [Amit Peleg](http://openreview.net/profile?id=~Amit_Peleg1), [Matthias Hein](http://openreview.net/profile?id=~Matthias_Hein2)
  - **Affiliations:** University of Tübingen, Tübingen AI Center, University of Tübingen, Tübingen AI Center
  - **TL;DR:** This study investigates the factors influencing generalization in neural networks, focusing on the effects of overparameterization and the implicit bias of Stochastic Gradient Descent (SGD). The findings reveal that increasing width enhances generalization due to SGD's bias, while increasing depth negatively impacts generalization, attributed to architectural bias.
  - **Keywords:** Generalization in neural networks, Overparameterization, Stochastic Gradient Descent (SGD), Random sampling of networks, Binary classification, Generalization despite overparameterization, Implicit bias of SGD, Architectural bias, Analysis of overparameterization effects, Comparison of SGD and random networks


- [Learning to Stabilize Online Reinforcement Learning in Unbounded State Spaces](https://icml.cc/virtual/2024/poster/34943) (Poster)
  - **Authors:** [Brahma Pavse](http://openreview.net/profile?id=~Brahma_S_Pavse1), [Matthew Zurek](http://openreview.net/profile?id=~Matthew_Zurek1), [Yudong Chen](http://openreview.net/profile?id=~Yudong_Chen1), [Qiaomin Xie](http://openreview.net/profile?id=~Qiaomin_Xie1), [Josiah Hanna](http://openreview.net/profile?id=~Josiah_P._Hanna1)
  - **Affiliations:** University of Wisconsin–Madison, USA, University of Wisconsin–Madison, USA, University of Wisconsin–Madison, USA, University of Wisconsin–Madison, USA, University of Wisconsin–Madison, USA
  - **TL;DR:** This study addresses the challenge of achieving stability in online reinforcement learning within unbounded state spaces, revealing that minimizing optimality cost can lead to unstable policies. The authors propose a new approach combining Lyapunov-based cost-shaping and state transformations, demonstrating its effectiveness in various applications.
  - **Keywords:** reinforcement learning, stability in control systems, deep reinforcement learning, Lyapunov-based cost-shaping, stochastic queuing networks, traffic signal control, unbounded state spaces, instability in policies, credit-assignment issues, stable and optimal policy development, empirical study results


- [Position: Topological Deep Learning is the New Frontier for Relational Learning](https://icml.cc/virtual/2024/poster/34191) (Poster)
  - **Authors:** [Theodore Papamarkou](http://openreview.net/profile?id=~Theodore_Papamarkou1), [Tolga Birdal](http://openreview.net/profile?id=~Tolga_Birdal3), [Michael Bronstein](http://openreview.net/profile?id=~Michael_M._Bronstein1), [Gunnar Carlsson](http://openreview.net/profile?id=~Gunnar_E._Carlsson1), [Justin Curry](http://openreview.net/profile?id=~Justin_Curry1), [Yue Gao](http://openreview.net/profile?id=~Yue_Gao4), [Mustafa Hajij](http://openreview.net/profile?id=~Mustafa_Hajij1), [Roland Kwitt](http://openreview.net/profile?id=~Roland_Kwitt1), [Pietro Lió](http://openreview.net/profile?id=~Pietro_Lio1), [Paolo Di Lorenzo](http://openreview.net/profile?id=~Paolo_Di_Lorenzo1), [Vasileios Maroulas](http://openreview.net/profile?id=vmaroula@utk.edu), [Nina Miolane](http://openreview.net/profile?id=~Nina_Miolane2), [Farzana Nasrin](http://openreview.net/profile?id=fnasrin@hawaii.edu), [Karthikeyan Ramamurthy](http://openreview.net/profile?id=~Karthikeyan_Natesan_Ramamurthy1), [Bastian Rieck](http://openreview.net/profile?id=~Bastian_Rieck1), [Simone Scardapane](http://openreview.net/profile?id=~Simone_Scardapane1), [Michael Schaub](http://openreview.net/profile?id=~Michael_T_Schaub1), [Petar Veličković](http://openreview.net/profile?id=~Petar_Veličković1), [Bei Wang](http://openreview.net/profile?id=~Bei_Wang3), [Yusu Wang](http://openreview.net/profile?id=~Yusu_Wang1), [Guowei Wei](http://openreview.net/profile?id=~Guowei_Wei1), [Ghada Zam](http://openreview.net/profile?id=~Ghada_Zamzmi2)
  - **Affiliations:** Department of Mathematics, The University of Manchester, Manchester, UK, Department of Computing, Imperial College London, London, UK, Department of Computer Science, University of Oxford, Oxford, UK, Department of Mathematics, Stanford University, Stanford, USA; BlueLightAI Inc, USA, University at Albany, New York, USA, School of Software, Tsinghua University, Beijing, China, University of San Francisco, San Francisco, USA, Department of Artificial Intelligence and Human Interfaces, University of Salzburg, Austria, Department of Computer Science and Technology, University of Cambridge, Cambridge, UK, Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy, Department of Mathematics, University of Tennessee, Knoxville, USA, Department of Electrical and Computer Engineering, UC Santa Barbara, Santa Barbara, USA, Department of Mathematics, University of Hawai ‘i at M ¯anoa, Hawai ‘i, USA, IBM Corporation New York, USA, Helmholtz Munich, Munich Germany; Technical University of Munich, Munich Germany, Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy, RWTH Aachen University, Aachen, Germany, Google DeepMind, School of Computing, University of Utah, Utah, USA, Computer Science and Engineering Department, University of California San Diego, San Diego, USA, Department of Mathematics, Michigan State University, East Lansing, Michigan, USA, University of South Florida, Florida, USA
  - **TL;DR:** This paper introduces topological deep learning (TDL) as a new frontier for relational learning, emphasizing its potential to enhance traditional machine learning models by incorporating topological concepts. It discusses open problems and future research opportunities in TDL, inviting the scientific community to engage in this emerging field.
  - **Keywords:** Topological Deep Learning, Relational Learning, Topological Data Analysis (TDA), Persistent Homology, Graph Representation Learning, Geometric Deep Learning, Open problems in TDL, Practical benefits, Theoretical foundations, Potential solutions, Future research opportunities, D3R Grand Challenges


- [BOtied: Multi-objective Bayesian optimization with tied multivariate ranks](https://icml.cc/virtual/2024/poster/33580) (Poster)
  - **Authors:** [Ji Won Park](http://openreview.net/profile?id=~Ji_Won_Park1), [Natasa Tagasovska](http://openreview.net/profile?id=~Natasa_Tagasovska2), [Michael Maser](http://openreview.net/profile?id=~Michael_Maser1), [Stephen Ra](http://openreview.net/profile?id=~Stephen_Ra1), [Kyunghyun Cho](http://openreview.net/profile?id=~Kyunghyun_Cho1)
  - **Affiliations:** Prescient Design, Genentech, South San Francisco, USA, Prescient Design, Genentech, South San Francisco, USA, Prescient Design, Genentech, South San Francisco, USA, Prescient Design, Genentech, South San Francisco, USA, Department of Computer Science, New York University, New York City, USA; Center for Data Science, New York University, New York City, USA
  - **TL;DR:** This paper presents BOtied, a novel acquisition function for multi-objective Bayesian optimization that leverages the cumulative distribution function to efficiently identify Pareto-optimal solutions. The proposed method outperforms existing state-of-the-art algorithms while maintaining computational efficiency across multiple objectives.
  - **Keywords:** Multi-objective Bayesian optimization, Pareto-optimal solutions, Acquisition function, CDF indicator, copulas, Scientific applications, industrial applications, biochemistry, Joint optimization of multiple competing objectives, high-dimensional integrals, non-informative monotonic transformations, BOtied acquisition function, Pareto-compliant metric, improved optimization algorithms


- [UPOCR: Towards Unified Pixel-Level OCR Interface](https://icml.cc/virtual/2024/poster/32965) (Poster)
  - **Authors:** [Dezhi Peng](http://openreview.net/profile?id=~Dezhi_Peng1), [Zhenhua Yang](http://openreview.net/profile?id=~Zhenhua_Yang3), [Jiaxin Zhang](http://openreview.net/profile?id=~Jiaxin_Zhang1), [Chongyu Liu](http://openreview.net/profile?id=~Chongyu_Liu2), [Yongxin Shi](http://openreview.net/profile?id=~Yongxin_Shi2), [Kai Ding](http://openreview.net/profile?id=~Kai_Ding2), [Fengjun Guo](http://openreview.net/profile?id=~Fengjun_Guo1), [Lianwen Jin](http://openreview.net/profile?id=~Lianwen_Jin1)
  - **Affiliations:** South China University of Technology; INTSIG-SCUT Joint Lab of Document Image Analysis and Recognition, South China University of Technology; INTSIG-SCUT Joint Lab of Document Image Analysis and Recognition, South China University of Technology; INTSIG-SCUT Joint Lab of Document Image Analysis and Recognition, South China University of Technology; INTSIG-SCUT Joint Lab of Document Image Analysis and Recognition, South China University of Technology; INTSIG-SCUT Joint Lab of Document Image Analysis and Recognition, INTSIG Information Co. Ltd., INTSIG Information Co. Ltd., South China University of Technology; INTSIG-SCUT Joint Lab of Document Image Analysis and Recognition
  - **TL;DR:** The study introduces UPOCR, a unified pixel-level OCR model that simplifies the diverse paradigms and architectures of existing OCR tasks into a single framework using a ViT-based encoder-decoder. It demonstrates state-of-the-art performance across text removal, text segmentation, and tampered text detection without the need for task-specific fine-tuning.
  - **Keywords:** Optical Character Recognition (OCR), Unified Pixel-Level OCR, Vision Transformer (ViT), Encoder-Decoder Architecture, Text Removal, Text Segmentation, Tampered Text Detection, Complexity of research and maintenance in OCR, Divergent paradigms and architectures, State-of-the-art performance on multiple OCR tasks with a unified model, Image-to-Image Transformation, Task Prompts


- [Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from Human Input](https://icml.cc/virtual/2024/poster/34151) (Poster)
  - **Authors:** [Andi Peng](http://openreview.net/profile?id=~Andi_Peng1), [Yuying Sun](http://openreview.net/profile?id=~Yuying_Sun1), [Tianmin Shu](http://openreview.net/profile?id=~Tianmin_Shu1), [David Abel](http://openreview.net/profile?id=~David_Abel1)
  - **Affiliations:** Massachusetts Institute of Technology, Boston University, Johns Hopkins University, Google DeepMind
  - **TL;DR:** This study proposes a framework for learning user-aligned reward functions by incorporating feature-level preferences alongside traditional pairwise comparisons. The findings indicate that this approach leads to more efficient convergence to accurate reward models, validated through a behavioral experiment in a mushroom foraging task.
  - **Keywords:** reward learning, user-aligned preferences, social learning, reinforcement learning from human feedback (RLHF), pairwise comparisons, feature-level preferences, robotic systems, language models, mushroom foraging, loss of fine-grained information in pairwise comparisons, varying user preferences, efficient reward convergence, pedagogical framework for preference learning, AI safety, value alignment


- [SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding](https://icml.cc/virtual/2024/poster/32638) (Poster)
  - **Authors:** [Chanho Park](http://openreview.net/profile?id=~Chanho_Park1), [Namyoon Lee](http://openreview.net/profile?id=~Namyoon_Lee1)
  - **Affiliations:** Department of Electrical Engineering, POSTECH, Pohang, South Korea, School of Electrical Engineering, Korea University, Seoul, South Korea
  - **TL;DR:** This paper presents signSGD with federated defense (signSGD-FD), a novel method that maintains convergence rates in distributed learning despite the presence of adversarial workers. The method effectively utilizes gradient information from adversarial workers, leading to superior performance compared to traditional algorithms in various adversarial scenarios.
  - **Keywords:** Distributed learning, Adversarial attacks, SignSGD, SignSGD with majority voting (signSGD-MV), SignSGD with federated defense (signSGD-FD), Communication costs, Adversarial manipulation of datasets and gradients, Improved convergence rates, Gradient sign decoding


- [Knowledge Distillation with Auxiliary Variable](https://icml.cc/virtual/2024/poster/34823) (Poster)
  - **Authors:** [Bo Peng](http://openreview.net/profile?id=~Bo_Peng24), [zhen fang](http://openreview.net/profile?id=~Zhen_Fang2), [Guangquan Zhang](http://openreview.net/profile?id=~Guangquan_Zhang2), [Jie Lu](http://openreview.net/profile?id=~Jie_Lu4)
  - **Affiliations:** Faculty of Engineering & Information Technology, University of Technology Sydney, Sydney, Australia, Faculty of Engineering & Information Technology, University of Technology Sydney, Sydney, Australia, Faculty of Engineering & Information Technology, University of Technology Sydney, Sydney, Australia, Faculty of Engineering & Information Technology, University of Technology Sydney, Sydney, Australia
  - **TL;DR:** This paper proposes a novel knowledge distillation method that introduces an auxiliary variable to enhance the predictive modeling ability of student models, addressing the capacity discrepancy between teacher and student models. The proposed approach demonstrates significant improvements over existing knowledge distillation methods in both theoretical insights and experimental results.
  - **Keywords:** Knowledge Distillation, Model Capacity, Kullback-Leibler Divergence, Predictive Distribution, Logit Outputs, Real-time Applications, Lightweight Models, Capacity Discrepancy, Sub-optimal Knowledge Transfer, Novel Objective Function for KD, Enhanced Predictive Modeling, Auxiliary Variable


- [UPAM: Unified Prompt Attack in Text-to-Image Generation Models Against Both Textual Filters and Visual Checkers](https://icml.cc/virtual/2024/poster/34341) (Poster)
  - **Authors:** [Duo Peng](http://openreview.net/profile?id=~Duo_Peng1), [Qiuhong Ke](http://openreview.net/profile?id=~Qiuhong_Ke6), [Jun Liu](http://openreview.net/profile?id=~Jun_Liu8)
  - **Affiliations:** Singapore University of Technology and Design, Singapore, Monash University, Australia, Singapore University of Technology and Design, Singapore; Lancaster University, United Kingdom
  - **TL;DR:** This paper introduces UPAM, a framework designed to enhance the robustness of Text-to-Image models against both textual and visual defenses. The study demonstrates UPAM's effectiveness in generating adversarial prompts that can bypass existing security measures, highlighting the need for improved defense strategies in T2I systems.
  - **Keywords:** Text-to-Image Generation, Adversarial Attacks, Gradient-based Optimization, Sphere-Probing Learning (SPL), Semantic-Enhancing Learning (SEL), Image Generation, AI Safety, Security Concerns, Misuse of T2I Models, UPAM Framework, Enhanced Defense Mechanisms, T2I Models, Textual Filters, Visual Checkers


- [The Max-Min Formulation of Multi-Objective Reinforcement Learning: From Theory to a Model-Free Algorithm](https://icml.cc/virtual/2024/poster/33589) (Poster)
  - **Authors:** [Giseung Park](http://openreview.net/profile?id=~Giseung_Park1), [woohyeon Byeon](http://openreview.net/profile?id=~Woohyeon_Byeon1), [Seongmin Kim](http://openreview.net/profile?id=~Seongmin_Kim2), [Elad Havakuk](http://openreview.net/profile?id=~Elad_Havakuk1), [Amir Leshem](http://openreview.net/profile?id=~Amir_Leshem1), [Youngchul Sung](http://openreview.net/profile?id=~Youngchul_Sung1)
  - **Affiliations:** School of Electrical Engineering, Korea Advanced Institute of Science & Technology, Daejeon 34141, Republic of Korea, School of Electrical Engineering, Korea Advanced Institute of Science & Technology, Daejeon 34141, Republic of Korea, School of Electrical Engineering, Korea Advanced Institute of Science & Technology, Daejeon 34141, Republic of Korea, Faculty of Engineering, Bar-Ilan University, Ramat Gan 52900, Israel, Faculty of Engineering, Bar-Ilan University, Ramat Gan 52900, Israel, School of Electrical Engineering, Korea Advanced Institute of Science & Technology, Daejeon 34141, Republic of Korea
  - **TL;DR:** This paper presents a max-min framework for multi-objective reinforcement learning, focusing on fairness among competing goals. The proposed model-free algorithm shows significant performance improvements over existing methods, addressing the challenges of optimizing multiple conflicting objectives.
  - **Keywords:** Multi-Objective Reinforcement Learning, Fairness in Optimization, Max-Min Framework, Model-Free Algorithm, Autonomous Vehicles, Control Problems, Balancing Competing Goals, Conflicting Objectives, Theoretical Advances, Performance Improvement, Multi-Objective Markov Decision Process (MOMDP), Pareto Boundary


- [Multi-Track Message Passing: Tackling Oversmoothing and Oversquashing in Graph Learning via Preventing Heterophily Mixing](https://icml.cc/virtual/2024/poster/35130) (Spotlight Poster)
  - **Authors:** [Hongbin Pei](http://openreview.net/profile?id=~Hongbin_Pei1), [Yu Li](http://openreview.net/profile?id=~Yu_Li30), [Huiqi Deng](http://openreview.net/profile?id=~Huiqi_Deng1), [Jingxin Hai](http://openreview.net/profile?id=~Jingxin_Hai1), [Pinghui Wang](http://openreview.net/profile?id=~Pinghui_Wang1), [Jie Ma](http://openreview.net/profile?id=~Jie_Ma1), [Jing Tao](http://openreview.net/profile?id=~Jing_Tao2), [Yuheng Xiong](http://openreview.net/profile?id=~Yuheng_Xiong1), [Xiaohong Guan](http://openreview.net/profile?id=~Xiaohong_Guan2)
  - **Affiliations:** MOE KLINNS Lab, Xi’an Jiaotong University, China, MOE KLINNS Lab, Xi’an Jiaotong University, China, Shanghai Jiao Tong University, China, MOE KLINNS Lab, Xi’an Jiaotong University, China, MOE KLINNS Lab, Xi’an Jiaotong University, China, MOE KLINNS Lab, Xi’an Jiaotong University, China, MOE KLINNS Lab, Xi’an Jiaotong University, China, MOE KLINNS Lab, Xi’an Jiaotong University, China, MOE KLINNS Lab, Xi’an Jiaotong University, China
  - **TL;DR:** This study introduces a Multi-Track Graph Convolutional Network (MTGCN) to effectively address the issues of oversmoothing and oversquashing in graph neural networks by preventing heterophilic mixing of messages. The proposed method enhances long-distance information flow and achieves state-of-the-art performance on various graph datasets, including a new benchmark accuracy of 86.4% on Cora.
  - **Keywords:** Graph Neural Networks (GNNs), Message Passing, Multi-Track Graph Convolutional Network (MTGCN), Recommender Systems, Computational Chemistry, Physical Simulation, Oversmoothing, Oversquashing, Information Loss, Heterophily Mixing, State-of-the-art performance on graph datasets, New benchmark accuracy on Cora, Cora, Heterophily, Category Semantics, Isomorphic Graphs


- [Foundation Policies with Hilbert Representations](https://icml.cc/virtual/2024/poster/34274) (Poster)
  - **Authors:** [Seohong Park](http://openreview.net/profile?id=~Seohong_Park1), [Tobias Kreiman](http://openreview.net/profile?id=~Tobias_Kreiman1), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1)
  - **Affiliations:** University of California, Berkeley, University of California, Berkeley, University of California, Berkeley
  - **TL;DR:** This paper presents a novel unsupervised framework for pre-training generalist policies in reinforcement learning that captures diverse long-horizon behaviors from unlabeled offline data. The proposed method enables quick adaptation to new tasks in a zero-shot manner, outperforming existing methods in various benchmarks.
  - **Keywords:** reinforcement learning, generalist policies, unsupervised learning, goal-conditioned RL, behavioral cloning, unsupervised skill discovery, robotic locomotion, manipulation, scalability, optimization, offline learning, structured representation, zero-shot policy prompting


- [Mean-field Chaos Diffusion Models](https://icml.cc/virtual/2024/poster/33206) (Oral)
  - **Authors:** [Sungwoo Park](http://openreview.net/profile?id=~Sungwoo_Park3), [Dongjun Kim](http://openreview.net/profile?id=~Dongjun_Kim1), [Ahmed Alaa](http://openreview.net/profile?id=~Ahmed_Alaa1)
  - **Affiliations:** Department of Electrical Engineering and Computer Sciences, UC Berkeley, Department of Computer Science, Stanford, Department of Electrical Engineering and Computer Sciences, UC Berkeley; UCSF
  - **TL;DR:** This paper introduces mean-field chaos diffusion models (MF-CDMs) to effectively manage high-cardinality data distributions by leveraging mean-field theory concepts. The proposed models address scalability issues in generative models and demonstrate effectiveness in handling large data structures like 3D point clouds.
  - **Keywords:** score-based generative models, high-cardinality data distributions, mean-field theory, mean-field chaos diffusion models, score-matching method, propagation of chaos, 3D point clouds, curse of dimensionality, scalability of generative models, high-dimensional data, mean-field score matching, approximation scheme for efficient training


- [State-Free Inference of State-Space Models: The *Transfer Function* Approach](https://icml.cc/virtual/2024/poster/34604) (Poster)
  - **Authors:** [Rom N. Parnichkun](http://openreview.net/profile?id=~Rom_Parnichkun1), [Stefano Massaroli](http://openreview.net/profile?id=~Stefano_Massaroli1), [Alessandro Moro](http://openreview.net/profile?id=~Alessandro_Moro1), [Jimmy Smith](http://openreview.net/profile?id=~Jimmy_T.H._Smith1), [Ramin Hasani](http://openreview.net/profile?id=~Ramin_Hasani1), [Mathias Lechner](http://openreview.net/profile?id=~Mathias_Lechner1), [Qi An](http://openreview.net/profile?id=~Qi_An6), [Christopher Re](http://openreview.net/profile?id=~Christopher_Re1), [Hajime Asama](http://openreview.net/profile?id=~Hajime_Asama1), [Stefano Ermon](http://openreview.net/profile?id=~Stefano_Ermon1), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1), [Michael Poli](http://openreview.net/profile?id=~Michael_Poli1), [Atsushi Yamashita](http://openreview.net/profile?id=~Atsushi_Yamashita1)
  - **Affiliations:** The University of Tokyo; Liquid AI, Liquid AI; RIKEN, The University of Tokyo, Liquid AI; Stanford University, Liquid AI; Massachusetts Institute of Technology, Liquid AI; Massachusetts Institute of Technology, The University of Tokyo, None, The University of Tokyo, Stanford University, The University of Tokyo; RIKEN, Liquid AI; Stanford University, The University of Tokyo
  - **TL;DR:** This paper presents a state-free inference algorithm for state-space models using a transfer function approach, achieving a 35% training speed improvement over existing methods while maintaining high performance on downstream tasks. The proposed method addresses memory consumption issues associated with larger state sizes, enabling efficient parallel inference.
  - **Keywords:** state-space models, deep learning, sequence modeling, transfer function, Fast Fourier Transform (FFT), sequence parallel inference, memory consumption, computational cost, model expressive capacity, training speed improvement, state-of-the-art performance, perplexity reduction, Long Range Arena benchmark


- [BetterV: Controlled Verilog Generation with Discriminative Guidance](https://icml.cc/virtual/2024/poster/33315) (Poster)
  - **Authors:** [Zehua Pei](http://openreview.net/profile?id=~Zehua_PEI1), [Huiling Zhen](http://openreview.net/profile?id=~Huiling_Zhen1), [Mingxuan Yuan](http://openreview.net/profile?id=~Mingxuan_Yuan1), [Yu Huang](http://openreview.net/profile?id=~Yu_Huang13), [Bei Yu](http://openreview.net/profile?id=~Bei_Yu2)
  - **Affiliations:** The Chinese University of Hong Kong, Hong Kong SAR, Noah’s Ark Lab, Huawei, Hong Kong SAR, Noah’s Ark Lab, Huawei, Hong Kong SAR, Noah’s Ark Lab, Huawei, Hong Kong SAR, The Chinese University of Hong Kong, Hong Kong SAR; Noah’s Ark Lab, Huawei, Hong Kong SAR
  - **TL;DR:** This study presents BetterV, a framework for generating Verilog code using fine-tuned large language models and generative discriminators, addressing the challenges of automated circuit design. BetterV outperforms existing models like GPT-4 in generating correct Verilog and enhances various electronic design automation tasks.
  - **Keywords:** Verilog generation, automated circuit design, Large language models (LLMs), generative discriminators, instruct-tuning, Electronic design automation (EDA), integrated circuits (ICs), Complexity of hardware designs, limited Verilog resources, overfitting, data bias, BetterV framework, syntactically and functionally correct Verilog generation, improvements in EDA tasks, Domain-specific datasets, Verilog modules, Hardware design languages (HDLs), Boolean Satisfiability (SAT)


- [FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler](https://icml.cc/virtual/2024/poster/33813) (Poster)
  - **Authors:** [Hongyi Peng](http://openreview.net/profile?id=~Hongyi_Peng1), [Han Yu](http://openreview.net/profile?id=~Han_Yu1), [Xiaoli Tang](http://openreview.net/profile?id=~Xiaoli_Tang1), [Xiaoxiao Li](http://openreview.net/profile?id=~Xiaoxiao_Li1)
  - **Affiliations:** College of Computing and Data Science, Nanyang Technological University, Singapore, College of Computing and Data Science, Nanyang Technological University, Singapore, College of Computing and Data Science, Nanyang Technological University, Singapore, Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; Vector Institute, Canada
  - **TL;DR:** This study introduces FedCal, a novel approach for improving both local and global calibration in federated learning, addressing the challenges posed by data heterogeneity. The proposed method significantly reduces global calibration error by an average of 47.66%, enhancing the reliability of federated learning models in critical applications.
  - **Keywords:** Federated Learning, Model Calibration, Aggregated Parameterized Scaler, Client-specific scalers, Healthcare, Finance, Autonomous Driving, Data heterogeneity, Non-IID data, Calibration error, Federated Calibration (FedCal), Global calibration error reduction, Reliability in machine learning


- [Graph Automorphism Group Equivariant Neural Networks](https://icml.cc/virtual/2024/poster/32774) (Poster)
  - **Authors:** [Edward Pearce-Crump](http://openreview.net/profile?id=~Edward_Pearce-Crump1), [William J. Knottenbelt](http://openreview.net/profile?id=~William_Knottenbelt1)
  - **Affiliations:** Department of Computing, Imperial College London, United Kingdom, Department of Computing, Imperial College London, United Kingdom
  - **TL;DR:** This paper presents a method for constructing neural networks that are equivariant to the automorphism group of a graph, providing a full characterization of learnable, linear functions between layers. The findings have significant implications for learning from graph-structured data, addressing limitations of existing symmetric group-based approaches.
  - **Keywords:** Graph Neural Networks, Automorphism Group, Equivariance, Aut(G)-equivariant neural networks, linear functions, spanning set of matrices, Social networks, molecular structures, recommendation systems, Limitations of symmetric group equivariance, relations between graph vertices, Characterization of Aut(G)-equivariant functions, bilabelled graphs


- [The Relative Value of Prediction in Algorithmic Decision Making](https://icml.cc/virtual/2024/poster/33078) (Poster)
  - **Authors:** [Juan Perdomo](http://openreview.net/profile?id=~Juan_Carlos_Perdomo1)
  - **Affiliations:** Harvard University, Center for Research on Computation and Society, Cambridge, MA
  - **TL;DR:** The study investigates the relative value of algorithmic predictions in decision making, emphasizing that while predictions can enhance social welfare, they are just one of many policy levers available for improving outcomes. The findings suggest a need to evaluate when predictions are "good enough" for effective decision making.
  - **Keywords:** Algorithmic decision making, social welfare, resource allocation, Statistical models, Education, public health, Improving decision making quality, maximizing welfare, resource allocation challenges, Relative value of prediction, policy levers for welfare improvement


- [Transport of Algebraic Structure to Latent Embeddings](https://icml.cc/virtual/2024/poster/32955) (Spotlight Poster)
  - **Authors:** [Samuel Pfrommer](http://openreview.net/profile?id=~Samuel_Pfrommer1), [Brendon G. Anderson](http://openreview.net/profile?id=~Brendon_G._Anderson1), [Somayeh Sojoudi](http://openreview.net/profile?id=~Somayeh_Sojoudi1)
  - **Affiliations:** University of California, Berkeley, University of California, Berkeley, University of California, Berkeley
  - **TL;DR:** This study proposes a method for transporting algebraic structures to latent embeddings in machine learning, enabling operations like set union to be performed while respecting mathematical laws. The findings demonstrate that maintaining the underlying algebraic structure is crucial for achieving accurate and self-consistent operations in learned embeddings.
  - **Keywords:** algebraic structure, latent embeddings, machine learning, implicit neural representations, structural transport nets, 3D modeling, geometric deep learning, learning operations on latent embeddings, respecting algebraic laws, parameterizing latent space operations, bijection from latent space to mirrored algebra, universal algebra, DeepSet architecture, graph neural networks


- [Cross-view Masked Diffusion Transformers for Person Image Synthesis](https://icml.cc/virtual/2024/poster/33321) (Poster)
  - **Authors:** [Trung Pham](http://openreview.net/profile?id=~Trung_X._Pham1), [Kang Zhang](http://openreview.net/profile?id=~Kang_Zhang6), [Chang Yoo](http://openreview.net/profile?id=~Chang_D._Yoo1)
  - **Affiliations:** Korea Advanced Institute of Science and Technology (KAIST), Korea Advanced Institute of Science and Technology (KAIST), Korea Advanced Institute of Science and Technology (KAIST)
  - **TL;DR:** This paper introduces X-MDPT, a novel diffusion model for pose-guided human image generation that outperforms existing methods in terms of image quality and efficiency. The model achieves significant improvements in metrics like FID while using fewer parameters and demonstrating faster inference speeds.
  - **Keywords:** Pose-guided human image generation, diffusion models, Masked diffusion transformers, denoising diffusion Transformer, aggregation network, Person image synthesis, fashion image generation, High inference speeds, memory consumption, generation accuracy, X-MDPT model, improved FID, SSIM, and LPIPS metrics, DeepFashion dataset, Unet, GANs (Generative Adversarial Networks), latent diffusion


- [Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach](https://icml.cc/virtual/2024/poster/33142) (Poster)
  - **Authors:** [Johan Peralez](http://openreview.net/profile?id=~Johan_Peralez1), [Aurélien Delage](http://openreview.net/profile?id=~Aur%C3%A9lien_Delage1), [Olivier Buffet](http://openreview.net/profile?id=~Olivier_Buffet1), [Jilles Dibangoye](http://openreview.net/profile?id=~Jilles_Steeve_Dibangoye1)
  - **Affiliations:** Université de Lyon, INSA Lyon and Inria, CITI, F-69000 Lyon, Université de Lyon, INSA Lyon and Inria, CITI, F-69000 Lyon, Université de Lorraine, CNRS, INRIA, LORIA, F-54000 Nancy, Bernoulli Institute, University of Groningen, Nijenborgh 4, NL-9747 AG Groningen, Netherlands
  - **TL;DR:** This paper presents a method to solve hierarchical information-sharing Dec-POMDPs by disentangling decision variables, significantly reducing time complexity while maintaining optimality. The proposed algorithms can effectively scale to larger multi-player games, addressing the challenges posed by the silent coordination dilemma.
  - **Keywords:** Decentralized decision-making, Hierarchical information sharing, Dec-POMDP, Bellman’s principle of optimality, Extensive-form games, Collaborative decision-making, Game theory, Silent coordination dilemma, Double-exponential complexity, Algorithms for scaling multi-player games, Optimality maintenance


- [Prompting a Pretrained Transformer Can Be a Universal Approximator](https://icml.cc/virtual/2024/poster/35049) (Poster)
  - **Authors:** [Aleksandar Petrov](http://openreview.net/profile?id=~Aleksandar_Petrov1), [Phil Torr](http://openreview.net/profile?id=~Philip_Torr1), [Adel Bibi](http://openreview.net/profile?id=~Adel_Bibi1)
  - **Affiliations:** Department of Engineering Science, University of Oxford, UK, Department of Engineering Science, University of Oxford, UK, Department of Engineering Science, University of Oxford, UK
  - **TL;DR:** This paper investigates whether prompting and prefix-tuning can universally approximate sequence-to-sequence functions using pretrained transformer models. It concludes that even small models can achieve this, with prefix-tuning of a single attention head being sufficient for approximating any continuous function.
  - **Keywords:** prompting, prefix-tuning, universal approximation, transformer models, attention mechanism, sequence-to-sequence functions, modifying model behavior without changing parameters, approximation precision, smaller pretrained models as universal approximators, bounds on prefix length


- [Detecting Influence Structures in Multi-Agent Reinforcement Learning](https://icml.cc/virtual/2024/poster/33204) (Poster)
  - **Authors:** [Fabian Raoul Pieroth](http://openreview.net/profile?id=~Fabian_Raoul_Pieroth1), [Katherine Fitch](http://openreview.net/profile?id=~Katherine_Fitch1), [Lenz Belzner](http://openreview.net/profile?id=~Lenz_Belzner1)
  - **Affiliations:** School of Computation, Science and Technology, Technical University of Munich, Germany, Formerly, Chair of Operations Research, Technical University of Munich, Germany, Technische Hochschule Ingolstadt, Germany
  - **TL;DR:** This study introduces a unified framework for quantifying influence structures among agents in multi-agent reinforcement learning, focusing on total and state impact measurements. The proposed decentralized algorithms demonstrate effectiveness in identifying intricate influence structures with convergence guarantees in the average reward setting.
  - **Keywords:** Multi-Agent Reinforcement Learning (MARL), Influence Structures, Total Impact Measurement (TIM), State Impact Measurement (SIM), Approximation Algorithms, Energy Network Management, Vehicle Formation Control, Repeated Auctions, Influence Measurement, Inter-agent Influence, Average Reward Setting, Convergence Guarantees, Stability Analysis, Decentralized Algorithms


- [Mechanistic Design and Scaling of Hybrid Architectures](https://icml.cc/virtual/2024/poster/34509) (Poster)
  - **Authors:** [Michael Poli](http://openreview.net/profile?id=~Michael_Poli1), [Armin Thomas](http://openreview.net/profile?id=~Armin_W_Thomas1), [Eric Nguyen](http://openreview.net/profile?id=~Eric_Nguyen1), [Pragaash Ponnusamy](http://openreview.net/profile?id=~Pragaash_Ponnusamy1), [Björn Deiseroth](http://openreview.net/profile?id=~Bj%C3%B6rn_Deiseroth1), [Kristian Kersting](http://openreview.net/profile?id=~Kristian_Kersting1), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1), [Brian Hie](http://openreview.net/profile?id=~Brian_Hie1), [Stefano Ermon](http://openreview.net/profile?id=~Stefano_Ermon1), [Christopher Re](http://openreview.net/profile?id=~Christopher_Re1), [Ce Zhang](http://openreview.net/profile?id=~Ce_Zhang1), [Stefano Massaroli](http://openreview.net/profile?id=~Stefano_Massaroli1)
  - **Affiliations:** Together AI; Stanford University, Stanford University, Stanford University, Together AI, Hessian AI, Hessian AI, RIKEN; The University of Tokyo, Arc Institute; Stanford University, Stanford University; CZ Biohub, Stanford University, Together AI, RIKEN; Liquid AI
  - **TL;DR:** This study presents a mechanistic architecture design (MAD) pipeline to streamline the development of deep learning architectures, demonstrating that new hybrid architectures can outperform existing state-of-the-art models. The findings suggest that performance on synthetic tasks can predict scaling laws, emphasizing the importance of specialized layers in optimal architecture design.
  - **Keywords:** deep learning architectures, mechanistic architecture design (MAD), hybrid architectures, scaling laws, synthetic token manipulation tasks, compute-optimal analysis, state-optimal analysis, language models, model training and evaluation, resource-demanding architecture development, long prototyping times, high compute costs, new hybrid architectures, correlation with compute-optimal perplexity, performance evaluation via proxy tasks, Transformer, convolutional architectures, recurrent architectures, hybridization, sparsity


- [Contrasting Multiple Representations with the Multi-Marginal Matching Gap](https://icml.cc/virtual/2024/poster/33554) (Oral)
  - **Authors:** [Zoe Piran](http://openreview.net/profile?id=~Zoe_Piran1), [Michal Klein](http://openreview.net/profile?id=~Michal_Klein1), [James Thornton](http://openreview.net/profile?id=~James_Thornton1), [Marco Cuturi](http://openreview.net/profile?id=~marco_cuturi2)
  - **Affiliations:** Apple; Hebrew University Jerusalem, Apple, Apple, Apple
  - **TL;DR:** This paper introduces the multi-marginal matching gap (M3G) as a novel loss function for learning representations from multiple views or modalities, addressing the limitations of existing pairwise loss methods. Experiments show that M3G outperforms traditional approaches in both self-supervised and multimodal tasks.
  - **Keywords:** multi-view learning, representation learning, self-supervised learning, multi-marginal optimal transport (MM-OT), Sinkhorn algorithm, multimodal tasks, zero-shot learning, limitations of pairwise losses, complexity of matching k-tuples, multi-marginal matching gap (M3G), improved performance over pairwise losses


- [Improving Diffusion Models for Inverse Problems Using Optimal Posterior Covariance](https://icml.cc/virtual/2024/poster/34609) (Poster)
  - **Authors:** [Xinyu Peng](http://openreview.net/profile?id=~Xinyu_Peng1), [Ziyang Zheng](http://openreview.net/profile?id=~Ziyang_Zheng2), [Wenrui Dai](http://openreview.net/profile?id=~Wenrui_Dai1), [Nuoqian Xiao](http://openreview.net/profile?id=~Nuoqian_Xiao1), [Chenglin Li](http://openreview.net/profile?id=~Chenglin_Li2), [Junni Zou](http://openreview.net/profile?id=~Junni_Zou1), [Hongkai Xiong](http://openreview.net/profile?id=~Hongkai_Xiong1)
  - **Affiliations:** School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This paper proposes improvements to diffusion models for solving noisy linear inverse problems by optimizing posterior covariance using maximum likelihood estimation. The methods enhance reconstruction performance without the need for hyperparameter tuning or retraining.
  - **Keywords:** diffusion models, noisy linear inverse problems, zero-shot methods, Gaussian approximation, maximum likelihood estimation, posterior covariance optimization, image processing, denoising, inpainting, deblurring, super-resolution, data consistency, conditional sampling, reconstruction performance, plug-and-play solutions, posterior covariance prediction


- [Bayesian Regret Minimization in Offline Bandits](https://icml.cc/virtual/2024/poster/33146) (Poster)
  - **Authors:** [Marek Petrik](http://openreview.net/profile?id=~Marek_Petrik2), [Guy Tennenholtz](http://openreview.net/profile?id=~Guy_Tennenholtz4), [Mohammad Ghavamzadeh](http://openreview.net/profile?id=~Mohammad_Ghavamzadeh2)
  - **Affiliations:** University of New Hampshire, Google Research, Amazon AGI
  - **TL;DR:** This paper introduces a new algorithm, BRMOB, for minimizing Bayesian regret in offline linear bandits, arguing against the reliance on lower confidence bounds (LCB). The proposed method demonstrates tighter theoretical guarantees and improved empirical performance through the use of randomized policies.
  - **Keywords:** Bayesian regret minimization, offline bandits, offline reinforcement learning, conic optimization, Value-at-Risk (VaR), robust optimization, minimizing Bayesian regret, reliance on lower confidence bounds (LCB), new algorithm (BRMOB), tighter theoretical guarantees, randomized policies


- [Learning Multiple Secrets in Mastermind](https://icml.cc/virtual/2024/poster/34788) (Poster)
  - **Authors:** [Milind Prabhu](http://openreview.net/profile?id=~Milind_Prabhu1), [David Woodruff](http://openreview.net/profile?id=~David_Woodruff1)
  - **Affiliations:** Department of Computer Science and Engineering, University of Michigan, Department of Computer Science, Carnegie Mellon University
  - **TL;DR:** This paper presents a two-round adaptive algorithm for learning multiple hidden points in the Generalized Mastermind problem while minimizing query complexity. It establishes optimal bounds for query complexity and explores variants of the problem in different query models.
  - **Keywords:** Generalized Mastermind problem, codebreaking, learning hidden sets, Two-round adaptive algorithm, query complexity bounds, Theoretical computer science, algorithm design, Learning multiple hidden points, minimizing queries, adaptive randomized algorithms, Optimal query complexity bounds, deterministic algorithms for hidden set learning, Hypercube, Hamming distance, Euclidean distance, distance oracles


- [Mechanistic Neural Networks for Scientific Machine Learning](https://icml.cc/virtual/2024/poster/33046) (Poster)
  - **Authors:** [Adeel Pervez](http://openreview.net/profile?id=~Adeel_Pervez1), [Francesco Locatello](http://openreview.net/profile?id=~Francesco_Locatello1), [Efstratios Gavves](http://openreview.net/profile?id=~Stratis_Gavves1)
  - **Affiliations:** Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands, Institute of Science and Technology, Klosterneuburg, Austria, Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands
  - **TL;DR:** This paper introduces Mechanistic Neural Networks, a novel neural network design that incorporates Mechanistic Blocks to explicitly learn governing differential equations, enhancing interpretability and efficiency in scientific data modeling. The proposed approach demonstrates significant performance improvements in tasks such as equation discovery and dynamic systems modeling compared to existing methods.
  - **Keywords:** Mechanistic Neural Networks, Scientific Machine Learning, Mechanistic Block, Relaxed Linear Programming Solver (NeuRLP), Ordinary Differential Equations (ODEs), Data modeling, Equation discovery, Dynamic systems modeling, Understanding and modeling mechanisms underlying data evolution, Interpretability in data modeling, New neural network design, Efficient training methods, Scalable GPU processing


- [Robust Data-driven Prescriptiveness Optimization](https://icml.cc/virtual/2024/poster/33644) (Poster)
  - **Authors:** [Mehran Poursoltani](http://openreview.net/profile?id=~Mehran_Poursoltani1), [Erick Delage](http://openreview.net/profile?id=~Erick_Delage2), [Angelos Georghiou](http://openreview.net/profile?id=~Angelos_Georghiou1)
  - **Affiliations:** Desautels Faculty of Management, McGill University, Montréal, Canada, GERAD & Department of Decision Sciences, HEC Montréal, Montréal, Canada, Department of Business and Public Administration, University of Cyprus, Nicosia, Cyprus
  - **TL;DR:** This paper introduces a distributionally robust contextual optimization model that utilizes a new measure called the coefficient of prescriptiveness to enhance decision-making in uncertain environments. The study evaluates the robustness of the proposed policies against alternative methods in the context of varying distribution shifts.
  - **Keywords:** Data-driven optimization, Contextual decision-making, Distributionally robust optimization, Bisection algorithm, Linear programming, Contextual shortest path problem, Decision-making under uncertainty, Distribution shift, Conditional probability distribution function, Coefficient of prescriptiveness, Optimization techniques, Stochastic programming, Covariates, Side information


- [A Subquadratic Time Algorithm for Robust Sparse Mean Estimation](https://icml.cc/virtual/2024/poster/33869) (Spotlight Poster)
  - **Authors:** [Ankit Pensia](http://openreview.net/profile?id=~Ankit_Pensia1)
  - **Affiliations:** IBM Research
  - **TL;DR:** This paper presents a subquadratic time algorithm for robust sparse mean estimation that effectively handles adversarial outliers in high-dimensional data. The proposed method improves upon existing algorithms by reducing the computational complexity while maintaining similar sample efficiency.
  - **Keywords:** robust statistics, sparse mean estimation, high-dimensional data, subquadratic time algorithm, sample covariance matrix, adversarial outliers, contamination, data sparsity, efficient algorithms, robust sparse PCA, k-sparse, i.i.d. samples, weak correlations


- [Interpreting and Improving Diffusion Models from an Optimization Perspective](https://icml.cc/virtual/2024/poster/33099) (Poster)
  - **Authors:** [Frank Permenter](http://openreview.net/profile?id=~Frank_Permenter1), [Chenyang Yuan](http://openreview.net/profile?id=~Chenyang_Yuan1)
  - **Affiliations:** Toyota Research Institute, Cambridge, Massachusetts, USA, Toyota Research Institute, Cambridge, Massachusetts, USA
  - **TL;DR:** This paper interprets denoising diffusion models as approximate gradient descent for distance minimization and provides a convergence analysis of the DDIM sampler. The proposed gradient-estimation sampler achieves state-of-the-art results in image generation tasks with significantly fewer function evaluations.
  - **Keywords:** diffusion models, denoising, optimization, gradient descent, DDIM sampler, projection, image generation, text-to-3D generation, robot path-planning, human animation, text-to-audio generation, noise perturbation, projection error, new gradient-estimation sampler, convergence analysis, CIFAR-10, CelebA


- [Efficient Exploration in Average-Reward Constrained Reinforcement Learning: Achieving Near-Optimal Regret With Posterior Sampling](https://icml.cc/virtual/2024/poster/33108) (Poster)
  - **Authors:** [Danil Provodin](http://openreview.net/profile?id=~Danil_Provodin1), [Maurits Kaptein](http://openreview.net/profile?id=~Maurits_Clemens_Kaptein1), [Mykola Pechenizkiy](http://openreview.net/profile?id=~Mykola_Pechenizkiy1)
  - **Affiliations:** Eindhoven University of Technology, Jheronimus Academy of Data Science, None, Eindhoven University of Technology, None, Eindhoven University of Technology; University of Jyvaskyla
  - **TL;DR:** This study presents a new algorithm utilizing posterior sampling for learning in Constrained Markov Decision Processes (CMDP) in an infinite-horizon average reward setting, achieving near-optimal regret bounds. Empirical results demonstrate that the proposed algorithm outperforms existing methods in constrained reinforcement learning.
  - **Keywords:** Constrained Reinforcement Learning, Average-Reward Optimization, Posterior Sampling, Constrained Markov Decision Processes (CMDP), Robotics, Telecommunication Networks, Autonomous Driving, Learning under constraints, Cost minimization, Near-optimal regret bounds, Bayesian regret bound


- [Learning-Efficient Yet Generalizable Collaborative Filtering for Item Recommendation](https://icml.cc/virtual/2024/poster/34645) (Poster)
  - **Authors:** [Yuanhao Pu](http://openreview.net/profile?id=~Yuanhao_Pu1), [Xiaolong Chen](http://openreview.net/profile?id=~Xiaolong_Chen5), [Xu Huang](http://openreview.net/profile?id=~Xu_Huang2), [Jin Chen](http://openreview.net/profile?id=~Jin_Chen4), [Defu Lian](http://openreview.net/profile?id=~Defu_Lian1), [Enhong Chen](http://openreview.net/profile?id=~Enhong_Chen1)
  - **Affiliations:** School of Artificial Intelligence and Data Science, University of Science and Technology of China, China, School of Artificial Intelligence and Data Science, University of Science and Technology of China, China, School of Computer Science and Technology, University of Science and Technology of China, China, School of Business and Management, Hong Kong University of Science and Technology, Hong Kong, School of Computer Science and Technology, University of Science and Technology of China, China; State Key Laboratory of Cognitive Intelligence, China, School of Computer Science and Technology, University of Science and Technology of China, China; State Key Laboratory of Cognitive Intelligence, China
  - **TL;DR:** This study establishes a connection between weighted squared loss and ranking metrics in collaborative filtering, introducing a new surrogate loss function called Ranking-Generalizable Squared (RG2) loss. Experimental results show that RG2 loss achieves competitive ranking performance and faster convergence compared to traditional softmax loss.
  - **Keywords:** Collaborative Filtering, Item Recommendation, Weighted Squared Loss, Softmax Loss, Matrix Factorization (MF), Alternating Least Squares (iALS), Ranking performance, Implicit feedback, Data sparsity, Ranking-Generalizable Squared (RG2) loss, DCG-consistency, Optimization algorithms, Three public datasets, Discounted Cumulative Gain (DCG), Normalized Discounted Cumulative Gain (NDCG), Weighted Regularized Matrix Factorization (WRMF)


- [Unsupervised Domain Adaptation for Anatomical Structure Detection in Ultrasound Images](https://icml.cc/virtual/2024/poster/33163) (Poster)
  - **Authors:** [Bin Pu](http://openreview.net/profile?id=~Bin_Pu2), [Xingguo Lv](http://openreview.net/profile?id=~Xingguo_Lv1), [Jiewen Yang](http://openreview.net/profile?id=~Jiewen_Yang1), [He Guannan](http://openreview.net/profile?id=~He_Guannan2), [Xingbo Dong](http://openreview.net/profile?id=~Xingbo_Dong1), [Yiqun Lin](http://openreview.net/profile?id=~Yiqun_Lin1), [Li Shengli](http://openreview.net/profile?id=~Li_Shengli1), [Ying Tan](http://openreview.net/profile?id=~Tan_Ying1), [Liu Fei](http://openreview.net/profile?id=~Liu_Fei1), [Ming Chen](http://openreview.net/profile?id=~Ming_Chen16), [Zhe Jin](http://openreview.net/profile?id=~Zhe_Jin4), [Kenli Li](http://openreview.net/profile?id=~Kenli_Li1), [Xiaomeng Li](http://openreview.net/profile?id=~Xiaomeng_Li1)
  - **Affiliations:** The Hong Kong University of Science and Technology, Anhui Provincial International Joint Research Center for Advanced Technology in Medical Imaging, Anhui University, The Hong Kong University of Science and Technology, Sichuan Provincial Maternity and Child Health Care Hospital, Anhui Provincial International Joint Research Center for Advanced Technology in Medical Imaging, Anhui University, The Hong Kong University of Science and Technology, Shenzhen Maternity and Child Healthcare Hospital, Shenzhen Maternity and Child Healthcare Hospital, Anhui Provincial International Joint Research Center for Advanced Technology in Medical Imaging, Anhui University, Harbin Red Cross Central Hospital, Anhui Provincial International Joint Research Center for Advanced Technology in Medical Imaging, Anhui University, Hunan University, The Hong Kong University of Science and Technology
  - **TL;DR:** This study presents ToMo-UDA, a novel Unsupervised Domain Adaptation method that integrates Topology and Morphology Knowledge Transfer for improved detection of fetal anatomical structures in ultrasound images. The proposed method significantly enhances detection performance across different health centers, addressing challenges posed by domain gaps and annotation difficulties.
  - **Keywords:** Unsupervised Domain Adaptation, Anatomical Structure Detection, Ultrasound Imaging, Topology Knowledge Transfer (TKT), Morphology Knowledge Transfer (MKT), Medical Image Analysis, Fetal Ultrasound, Domain Gaps, Variations in Data Collection, Annotation Challenges, ToMo-UDA method, FUSH2 benchmark, FUSH2 (FetalUltraSound benchmark)


- [Adaptive Conformal Inference by Betting](https://icml.cc/virtual/2024/poster/33190) (Poster)
  - **Authors:** [Aleksandr Podkopaev](http://openreview.net/profile?id=~Aleksandr_Podkopaev1), [Darren Xu](http://openreview.net/profile?id=~Dong_Xu7), [Kuang-chih Lee](http://openreview.net/profile?id=~Kuang-chih_Lee1)
  - **Affiliations:** Walmart Global Tech, Walmart Global Tech, Walmart Global Tech
  - **TL;DR:** This paper presents a novel approach to adaptive conformal inference that does not rely on data exchangeability assumptions, utilizing parameter-free online convex optimization techniques. The proposed method effectively controls long-term miscoverage frequency while demonstrating strong empirical performance without the need for extensive parameter tuning.
  - **Keywords:** Conformal prediction, Predictive uncertainty, Adaptive conformal inference, Online convex optimization, Pinball loss, Online gradient descent, Machine learning models, Decision-making, Data exchangeability, Uncertainty estimation, Sequential data, Parameter-free optimization techniques, Long-term miscoverage control, Nonconformity scores, Split conformal prediction


- [Learning to Remove Cuts in Integer Linear Programming](https://icml.cc/virtual/2024/poster/33277) (Poster)
  - **Authors:** [Pol Puigdemont](http://openreview.net/profile?id=~Pol_Puigdemont1), [EFSTRATIOS PANTELEIMON SKOULAKIS](http://openreview.net/profile?id=~Stratis_Skoulakis2), [Grigorios Chrysos](http://openreview.net/profile?id=~Grigorios_Chrysos1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1)
  - **Affiliations:** LIONS, École Polytechnique Fédérale de Lausanne, Switzerland; Universitat Politècnica de Catalunya (UPC), Spain, LIONS, École Polytechnique Fédérale de Lausanne, Switzerland, Department of Electrical and Computer Engineering, University of Wisconsin-Madison, USA, LIONS, École Polytechnique Fédérale de Lausanne, Switzerland
  - **TL;DR:** This study introduces a novel approach to cutting plane methods in integer linear programming by exploring the removal of previously added cuts, demonstrating significant improvements in optimization performance. The findings suggest that cut removal policies can enhance the efficiency of solving ILPs compared to traditional cut addition strategies.
  - **Keywords:** Integer Linear Programming, Cutting Plane Methods, Combinatorial Optimization, Engineering, Operational Research, Finance, NP-hard problems, Constraint Satisfaction, Cut Removal Policies, Heuristic Improvements, Gomory Cuts, Branch and Bound


- [Conformalized Survival Distributions: A Generic Post-Process to Increase Calibration](https://icml.cc/virtual/2024/poster/33286) (Poster)
  - **Authors:** [Shi-ang Qi](http://openreview.net/profile?id=~Shi-ang_Qi1), [Yakun Yu](http://openreview.net/profile?id=~Yakun_Yu1), [Russell Greiner](http://openreview.net/profile?id=~Russell_Greiner2)
  - **Affiliations:** Computing Science, University of Alberta, Edmonton, Canada; Alberta Machine Intelligence Institute, Edmonton, Canada, Electrical Computer Engineering, University of Alberta, Edmonton, Canada, Computing Science, University of Alberta, Edmonton, Canada; Alberta Machine Intelligence Institute, Edmonton, Canada
  - **TL;DR:** This paper presents the Conformalized Survival Distribution (CSD) framework, a novel post-processing method that enhances the calibration of survival models without compromising their discriminative power. The approach is validated across 11 real-world datasets, demonstrating its effectiveness in improving model predictions in survival analysis.
  - **Keywords:** Survival analysis, Calibration, Discrimination, Conformal regression, Biomedical applications, Healthcare, Censored subjects, Discrimination-calibration trade-off, Conformalized Survival Distribution (CSD) framework, 11 real-world datasets


- [ByMI: Byzantine Machine Identification with False Discovery Rate Control](https://icml.cc/virtual/2024/poster/34515) (Poster)
  - **Authors:** [Chengde Qian](http://openreview.net/profile?id=~Chengde_Qian1), [Mengyuan Wang](http://openreview.net/profile?id=~Mengyuan_Wang2), [Haojie Ren](http://openreview.net/profile?id=~Haojie_Ren1), [Changliang Zou](http://openreview.net/profile?id=~Changliang_Zou2)
  - **Affiliations:** School of Statistics and Data Sciences, LPMC, KLMDASR and LEBPS, Nankai University, Tianjin, China, School of Statistics and Data Sciences, LPMC, KLMDASR and LEBPS, Nankai University, Tianjin, China, School of Mathematical Sciences, Shanghai Jiao Tong University, School of Statistics and Data Sciences, LPMC, KLMDASR and LEBPS, Nankai University, Tianjin, China
  - **TL;DR:** This paper presents ByMI, a detection procedure for identifying Byzantine machines in distributed learning systems, utilizing a sample-splitting strategy and robust estimation methods. The method achieves false discovery rate control and is validated through numerical experiments on synthetic and real data.
  - **Keywords:** Byzantine machine identification, distributed learning, robust estimation, Sample-splitting strategy, robust estimation algorithms, score statistic, Distributed and federated learning, Byzantine failures, detection of Byzantine machines, ByMI detection procedure, false discovery rate control


- [The Entropy Enigma: Success and Failure of Entropy Minimization](https://icml.cc/virtual/2024/poster/35194) (Poster)
  - **Authors:** [Ori Press](http://openreview.net/profile?id=~Ori_Press1), [Ravid Shwartz-Ziv](http://openreview.net/profile?id=~Ravid_Shwartz-Ziv2), [Yann LeCun](http://openreview.net/profile?id=~Yann_LeCun1), [Matthias Bethge](http://openreview.net/profile?id=~Matthias_Bethge1)
  - **Affiliations:** University of Tübingen, Tübingen AI Center, Germany, New York University, New York University; Meta AI, FAIR, University of Tübingen, Tübingen AI Center, Germany
  - **TL;DR:** This paper investigates the effectiveness of entropy minimization (EM) for adapting classification models to new data, revealing that while EM initially improves accuracy, prolonged optimization leads to a decline in performance. The authors propose a novel method for estimating model accuracy on arbitrary datasets without labels, achieving state-of-the-art results.
  - **Keywords:** Entropy minimization, Test time adaptation, Self-supervised learning, Optimization algorithms, Classification models, Image recognition, Model adaptation, Accuracy estimation, Data distribution shift, New method for estimating model accuracy without labels, State-of-the-art performance, 23 challenging datasets, Neural collapse


- [Efficient Non-stationary Online Learning by Wavelets with Applications to Online Distribution Shift Adaptation](https://icml.cc/virtual/2024/poster/34345) (Poster)
  - **Authors:** [Yu-Yang Qian](http://openreview.net/profile?id=~Yu-Yang_Qian1), [Peng Zhao](http://openreview.net/profile?id=~Peng_Zhao1), [Yu-Jie Zhang](http://openreview.net/profile?id=~Yu-Jie_Zhang1), [Masashi Sugiyama](http://openreview.net/profile?id=~Masashi_Sugiyama1), [Zhi-Hua Zhou](http://openreview.net/profile?id=~Zhi-Hua_Zhou2)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, The University of Tokyo, Chiba, Japan, RIKEN AIP, Tokyo, Japan, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This paper presents a method for achieving optimal dynamic regret in non-stationary online learning without using a two-layer ensemble, utilizing wavelet detection and adaptive restarts. The proposed approach significantly enhances computational and storage efficiency while addressing the challenges of environmental non-stationarity.
  - **Keywords:** Non-stationary online learning, Dynamic regret minimization, Wavelet detection, Adaptive restart, Streaming wavelet operator, Online distribution shift adaptation, Online label shift adaptation, Computational overhead of two-layer algorithms, Environmental non-stationarity, Optimal dynamic regret, Improved computation/storage efficiency, Universal dynamic regret, Binary indexed tree


- [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://icml.cc/virtual/2024/poster/33307) (Oral)
  - **Authors:** [Haotong Qin](http://openreview.net/profile?id=~Haotong_Qin1), [Xudong Ma](http://openreview.net/profile?id=~Xudong_Ma3), [Xingyu Zheng](http://openreview.net/profile?id=~Xingyu_Zheng1), [Xiaoyang Li](http://openreview.net/profile?email=lixiaoyang.x@bytedance.com), [Yang Zhang](http://openreview.net/profile?id=~Yang_Zhang21), [Shouda Liu](http://openreview.net/profile?id=~Shouda_Liu1), [Jie Luo](http://openreview.net/profile?id=~Jie_Luo5), [Xianglong Liu](http://openreview.net/profile?id=~Xianglong_Liu3), [Michele Magno](http://openreview.net/profile?id=~Michele_Magno1)
  - **Affiliations:** ETH Zürich, Beihang University, Beihang University, Bytedance AI Lab, Bytedance AI Lab, Bytedance AI Lab, Beihang University, Beihang University, ETH Zürich
  - **TL;DR:** This paper introduces IR-QLoRA, a novel method for enhancing the accuracy of quantized large language models (LLMs) through information retention techniques. The proposed approach significantly improves performance while maintaining efficiency, demonstrating notable gains in accuracy across LLaMA models with minimal additional time consumption.
  - **Keywords:** Large Language Models, Quantization, LoRA-Finetuning, Information Calibration Quantization, Information Elastic Connection, Resource-Constrained Hardware, Edge Devices, Information Loss, Accuracy Degradation, Resource Requirements, IR-QLoRA, Performance Improvement, Efficiency Gains, MMLU Benchmark


- [Feasible Reachable Policy Iteration](https://icml.cc/virtual/2024/poster/33234) (Poster)
  - **Authors:** [Shentao Qin](http://openreview.net/profile?id=~Shentao_Qin1), [Yujie Yang](http://openreview.net/profile?id=~Yujie_Yang1), [Yao Mu](http://openreview.net/profile?id=~Yao_Mu1), [Jie Li](http://openreview.net/profile?id=~JIE_LI14), [Wenjun Zou](http://openreview.net/profile?id=~Wenjun_Zou1), [Jingliang Duan](http://openreview.net/profile?id=~Jingliang_Duan1), [Shengbo Li](http://openreview.net/profile?id=~Shengbo_Eben_Li2)
  - **Affiliations:** School of Vehicle and Mobility, Tsinghua University, Beijing, China, School of Vehicle and Mobility, Tsinghua University, Beijing, China, Department of Computer Science, The University of Hong Kong, Hong Kong, China, School of Vehicle and Mobility, Tsinghua University, Beijing, China, School of Vehicle and Mobility, Tsinghua University, Beijing, China, School of Mechanical Engineering, University of Science and Technology Beijing, Beijing, China, School of Vehicle and Mobility, Tsinghua University, Beijing, China
  - **TL;DR:** This study introduces the Feasible Reachable Policy Iteration (FRPI) method to address goal-reaching tasks with safety constraints in reinforcement learning, focusing on efficient exploration and policy space pruning. The proposed method enhances convergence speed and sample efficiency without compromising safety.
  - **Keywords:** Safe Reinforcement Learning, Goal-Reaching Tasks, Control Problems, Feasible Reachable Function, Fixed Point Iteration, Policy Iteration, Intelligent Driving, Robot Manipulation, Safety Constraints, Sparse Rewards, Inefficient Exploration, Feasible Reachable Policy Iteration (FRPI), Policy Space Pruning, Improved Convergence Speed


- [ULAREF: A Unified Label Refinement Framework for Learning with Inaccurate Supervision](https://icml.cc/virtual/2024/poster/32892) (Spotlight Poster)
  - **Authors:** [Congyu Qiao](http://openreview.net/profile?id=~Congyu_Qiao3), [Ning Xu](http://openreview.net/profile?id=~Ning_Xu5), [Yihao Hu](http://openreview.net/profile?id=~Yihao_Hu2), [Xin Geng](http://openreview.net/profile?id=~Xin_Geng1)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China
  - **TL;DR:** This paper introduces ULAREF, a unified label refinement framework designed to improve learning with inaccurate supervision by leveraging refined labels through global detection of reliability and local enhancement. The framework aims to enhance model accuracy and generalization by addressing challenges associated with various forms of annotations.
  - **Keywords:** weakly supervised learning, inaccurate supervision, label refinement, predictive model training, consistency loss, online queries, crowdsourcing, ecoinformatics, multimedia content analysis, handling inaccurate supervision, noisy labels, partial labels, model overfitting, unified label refinement framework (ULAREF), enhanced model accuracy


- [Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention](https://icml.cc/virtual/2024/poster/34262) (Poster)
  - **Authors:** [Zhen Qin](http://openreview.net/profile?id=~Zhen_Qin6), [Weigao Sun](http://openreview.net/profile?id=~Weigao_Sun1), [Dong Li](http://openreview.net/profile?id=~Dong_Li11), [Xuyang Shen](http://openreview.net/profile?id=~Xuyang_Shen1), [Weixuan Sun](http://openreview.net/profile?id=~Weixuan_Sun1), [Yiran Zhong](http://openreview.net/profile?id=~Yiran_Zhong1)
  - **Affiliations:** TapTap; OpenNLPLab, Shanghai AI Lab, TapTap; OpenNLPLab, Shanghai AI Lab, TapTap; OpenNLPLab, Shanghai AI Lab, TapTap; OpenNLPLab, Shanghai AI Lab, TapTap; OpenNLPLab, Shanghai AI Lab, TapTap; OpenNLPLab, Shanghai AI Lab
  - **TL;DR:** This paper introduces Lightning Attention, a novel linear attention implementation that maintains constant training speed across various sequence lengths while addressing the limitations of previous models. The proposed TransNormerLLM architecture demonstrates improved efficiency and performance compared to state-of-the-art language models.
  - **Keywords:** Linear Attention, Language Modeling, Lightning Attention, Kernel Trick, TransNormerLLM, Cumulative Summation Operations, Slow Training Speed, Efficient Language Modeling, Constant Training Speed, Standard Datasets, Self-Collected Datasets


- [To Cool or not to Cool? Temperature Network Meets Large Foundation Models via DRO](https://icml.cc/virtual/2024/poster/33770) (Poster)
  - **Authors:** [Zi-Hao Qiu](http://openreview.net/profile?id=~Zi-Hao_Qiu1), [Siqi Guo](http://openreview.net/profile?id=~Siqi_Guo2), [Mao Xu](http://openreview.net/profile?id=~Mao_Xu1), [Tuo Zhao](http://openreview.net/profile?id=~Tuo_Zhao2), [Lijun Zhang](http://openreview.net/profile?id=~Lijun_Zhang1), [Tianbao Yang](http://openreview.net/profile?id=~Tianbao_Yang1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China, Computer Science and Engineering, Texas A&M University, College Station, USA, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China, Computer Science and Engineering, Georgia Institute of Technology, Atlanta, USA, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Pazhou Laboratory (Huangpu), Guangzhou, China, Computer Science and Engineering, Texas A&M University, College Station, USA
  - **TL;DR:** This paper introduces a temperature prediction network (TempNet) designed to enhance large foundation models (LFMs) by personalizing the temperature parameter during training and inference. Experimental results demonstrate that TempNet significantly improves the performance of existing models, making it a valuable tool for various tasks involving LLMs and CLIP models.
  - **Keywords:** Temperature scaling, Large foundation models (LFMs), Personalized temperature prediction, Temperature prediction network (TempNet), Constrained distributionally robust optimization (DRO), Large language models (LLMs), CLIP models, Enhancing LFMs, Personalization of temperature for model training, Improved performance of LFMs, Generalizable and transferable temperature prediction, Softmax function, Contrastive loss


- [Learning High-Order Relationships of Brain Regions](https://icml.cc/virtual/2024/poster/33885) (Poster)
  - **Authors:** [Weikang Qiu](http://openreview.net/profile?id=~Weikang_Qiu1), [Huangrui Chu](http://openreview.net/profile?id=~Huangrui_Chu1), [Selena Wang](http://openreview.net/profile?id=~Selena_Wang1), [Haolan Zuo](http://openreview.net/profile?id=~Haolan_Zuo1), [Xiaoxiao Li](http://openreview.net/profile?id=~Xiaoxiao_Li1), [Yize Zhao](http://openreview.net/profile?id=~Yize_Zhao1), [ZHITAO YING](http://openreview.net/profile?id=~Zhitao_Ying1)
  - **Affiliations:** Yale University, New Haven, USA, Yale University, New Haven, USA, Yale University, New Haven, USA, Yale University, New Haven, USA, University of British Columbia, Vancouver, Canada; Vector Institute, Toronto, Canada, Yale University, New Haven, USA, Yale University, New Haven, USA
  - **TL;DR:** This study introduces a novel method called HYBRID to identify high-order relationships among brain regions from fMRI data, addressing the limitations of existing methods that focus solely on pairwise connections. The proposed method significantly improves predictive performance by an average of 11.2% over state-of-the-art models.
  - **Keywords:** high-order relationships, brain regions, fMRI signals, MIMR (maximally informative and minimally redundant), CONSTRUCTOR, WEIGHTER, multi-head drop-bottleneck, neuroimaging, clinical diagnosis, cognitive function, identifying high-order relationships, exponential search space, low predictive performance, novel method (HYBRID), improved predictive model performance, hyperedge structures, CPM (standard protocol for studying brain connections)


- [MolCRAFT: Structure-Based Drug Design in Continuous Parameter Space](https://icml.cc/virtual/2024/poster/34336) (Poster)
  - **Authors:** [Yanru Qu](http://openreview.net/profile?id=~Yanru_Qu1), [Keyue Qiu](http://openreview.net/profile?id=~Keyue_Qiu1), [Yuxuan Song](http://openreview.net/profile?id=~Yuxuan_Song2), [Jingjing Gong](http://openreview.net/profile?id=~Jingjing_Gong3), [Jiawei Han](http://openreview.net/profile?id=~Jiawei_Han1), [Mingyue Zheng](http://openreview.net/profile?id=~Mingyue_Zheng1), [Hao Zhou](http://openreview.net/profile?id=~Hao_Zhou5), [Wei-Ying Ma](http://openreview.net/profile?id=~Wei-Ying_Ma2)
  - **Affiliations:** Institute for AI Industry Research (AIR), Tsinghua University; University of Illinois Urbana-Champaign, USA; Department of Computer Science and Technology, Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University; University of Illinois Urbana-Champaign, USA; Department of Computer Science and Technology, Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University; University of Illinois Urbana-Champaign, USA; Department of Computer Science and Technology, Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University, University of Illinois Urbana-Champaign, USA, Shanghai Institute of Materia Medica, Chinese Academy of Sciences, Institute for AI Industry Research (AIR), Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University
  - **TL;DR:** The study introduces MolCRAFT, a novel generative model for structure-based drug design that operates in continuous parameter space, addressing issues of conformational stability and false positives in generated molecules. Empirical results demonstrate that MolCRAFT achieves superior binding affinity and stable 3D structures compared to existing models.
  - **Keywords:** Structure-Based Drug Design (SBDD), Generative Models, Autoregressive Models, Diffusion Models, Drug Discovery, Molecular Design, Ill-conformational Problems, False Positives, Conformational Stability, MolCRAFT Model, Noise Reduced Sampling Strategy, Binding Affinity Improvement, Vina Scores, Interatomic Interactions


- [Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes](https://icml.cc/virtual/2024/poster/33581) (Poster)
  - **Authors:** [Zhen Qin](http://openreview.net/profile?id=~Zhen_Qin10), [Daoyuan Chen](http://openreview.net/profile?id=~Daoyuan_Chen1), [Bingchen Qian](http://openreview.net/profile?id=~Bingchen_Qian1), [Bolin Ding](http://openreview.net/profile?id=~Bolin_Ding3), [Yaliang Li](http://openreview.net/profile?id=~Yaliang_Li1), [Shuiguang Deng](http://openreview.net/profile?id=~Shuiguang_Deng1)
  - **Affiliations:** College of Computer Science and Technology, Zhejiang University, Hangzhou, China, Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group, College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Alibaba Group
  - **TL;DR:** This study presents FedKSeed, a novel approach for federated full-parameter tuning of large language models that significantly reduces communication costs to under 18 kilobytes. The method demonstrates superior performance in communication efficiency and zero-shot generalization compared to existing federated fine-tuning techniques.
  - **Keywords:** Federated Learning, Large Language Models, Full-Parameter Tuning, Parameter-Efficient Fine-Tuning, Zeroth-Order Optimization, Natural Language Processing, Communication Cost, Data Privacy, Model Responsiveness, FedKSeed, Probability-Differentiated Seed Sampling


- [Compute Better Spent: Replacing Dense Layers with Structured Matrices](https://icml.cc/virtual/2024/poster/34556) (Poster)
  - **Authors:** [Shikai Qiu](http://openreview.net/profile?id=~Shikai_Qiu1), [Andres Potapczynski](http://openreview.net/profile?id=~Andres_Potapczynski3), [Marc Finzi](http://openreview.net/profile?id=~Marc_Anton_Finzi1), [Micah Goldblum](http://openreview.net/profile?id=~Micah_Goldblum1), [Andrew Wilson](http://openreview.net/profile?id=~Andrew_Gordon_Wilson1)
  - **Affiliations:** New York University, New York University, Carnegie Mellon University, New York University, New York University; Carnegie Mellon University
  - **TL;DR:** This study explores the use of structured matrices as efficient alternatives to dense matrices in neural networks, demonstrating that structured layers can significantly improve computational efficiency and performance. The proposed Block Tensor-Train (BTT) matrix family outperforms dense matrices on various tasks while requiring less compute.
  - **Keywords:** structured matrices, computational efficiency, dense matrices, Maximal Update Parameterization, Block Tensor-Train (BTT), matrix-vector multiplies (MVMs), image modeling, language modeling, computational bottleneck, scaling laws, better performance per unit compute, lower training loss, CIFAR-10, CIFAR-100, ImageNet-1k, GPT-2


- [Transferring Knowledge From Large Foundation Models to Small Downstream Models](https://icml.cc/virtual/2024/poster/33800) (Poster)
  - **Authors:** [Shikai Qiu](http://openreview.net/profile?id=~Shikai_Qiu1), [Boran Han](http://openreview.net/profile?id=~Boran_Han1), [Danielle Robinson](http://openreview.net/profile?id=~Danielle_C._Maddix1), [Shuai Zhang](http://openreview.net/profile?id=~Shuai_Zhang7), [Yuyang Wang](http://openreview.net/profile?id=~Bernie_Wang1), [Andrew Wilson](http://openreview.net/profile?id=~Andrew_Gordon_Wilson1)
  - **Affiliations:** AWS AI Labs, Santa Clara, CA, USA; Department of Computer Science, New York University, NYC, USA, AWS AI Labs, Santa Clara, CA, USA, AWS AI Labs, Santa Clara, CA, USA, AWS AI Labs, Santa Clara, CA, USA, AWS AI Labs, Santa Clara, CA, USA, AWS AI Labs, Santa Clara, CA, USA; Department of Computer Science, New York University, NYC, USA
  - **TL;DR:** The study introduces Adaptive Feature Transfer (AFT) as a method to efficiently transfer task-relevant knowledge from large foundation models to smaller downstream models, significantly improving performance while reducing computational costs. AFT allows for the selective transfer of features, enhancing the effectiveness of transfer learning across various datasets.
  - **Keywords:** Transfer Learning, Foundation Models, Downstream Models, Adaptive Feature Transfer (AFT), Vision, Language, Multi-modal Datasets, Limited information transfer, computational burden of fine-tuning, inability to combine multiple pre-trained models, Improved downstream performance, effective feature transfer


- [Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints](https://icml.cc/virtual/2024/poster/33043) (Poster)
  - **Authors:** [Dan Qiao](http://openreview.net/profile?id=~Dan_Qiao1), [Yu-Xiang Wang](http://openreview.net/profile?id=~Yu-Xiang_Wang1)
  - **Affiliations:** Department of Computer Science, University of California, Santa Barbara (UCSB), USA, Halıcıoğlu Data Science Institute, University of California, San Diego (UCSD), USA
  - **TL;DR:** This paper addresses the challenge of multi-agent reinforcement learning (MARL) under adaptivity constraints, proposing a new algorithm that achieves near-optimal performance while minimizing policy updates. The findings indicate significant advancements in batch complexity and regret bounds for two-player zero-sum Markov Games.
  - **Keywords:** multi-agent reinforcement learning (MARL), adaptivity constraints, policy elimination based algorithm, autonomous driving, household power management, computer networking, costly deployments of new policies, minimizing policy updates, near-optimal batch complexity, regret bounds, Markov Games, zero-sum games


- [Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations](https://icml.cc/virtual/2024/poster/33929) (Poster)
  - **Authors:** [Helen Qu](http://openreview.net/profile?id=~Helen_Qu1), [Sang Michael Xie](http://openreview.net/profile?id=~Sang_Michael_Xie1)
  - **Affiliations:** Department of Physics and Astronomy, University of Pennsylvania, Department of Computer Science, Stanford University
  - **TL;DR:** This paper introduces the Connect Later framework to improve model robustness against out-of-distribution data by fine-tuning with targeted augmentations. The approach demonstrates state-of-the-art OOD accuracy while maintaining or improving in-distribution performance across various real-world tasks.
  - **Keywords:** unsupervised domain adaptation, out-of-distribution generalization, self-supervised pretraining, contrastive learning, masked autoencoding, wildlife identification, tumor detection, astronomy, poor generalization on OOD data, distribution shifts, Connect Later framework, targeted augmentations, IWILDCAM-WILDS, CAMELYON 17-WILDS, ASTRO CLASSIFICATION, REDSHIFTS


- [Learning Constraints from Offline Demonstrations via Superior Distribution Correction Estimation](https://icml.cc/virtual/2024/poster/34745) (Poster)
  - **Authors:** [Guorui Quan](http://openreview.net/profile?id=~Guorui_Quan1), [Zhiqiang Xu](http://openreview.net/profile?id=~zhiqiang_xu1), [Guiliang Liu](http://openreview.net/profile?id=~Guiliang_Liu1)
  - **Affiliations:** School of Data Science, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China, Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, School of Data Science, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China
  - **TL;DR:** This study introduces Inverse Constrained Superior Distribution Correction Estimation (ICSDICE) as an offline ICRL solver that effectively learns safety constraints from expert demonstrations without the need for unsafe exploration. The proposed method outperforms existing algorithms by accurately recovering constraints and adapting to high-dimensional environments.
  - **Keywords:** Inverse Constrained Reinforcement Learning (ICRL), safety constraints, control policies, Inverse Constrained Superior Distribution Correction Estimation (ICSDICE), dual optimization problem, Constrained Markov Decision Process (CMDP), safety-critical applications, reinforcement learning, data collection challenges, constraint violation, uncertainty in unvisited states, effective constraint inference, superior distributions estimation


- [Multiply-Robust Causal Change Attribution](https://icml.cc/virtual/2024/poster/33144) (Poster)
  - **Authors:** [Víctor Quintas-Martínez](http://openreview.net/profile?id=~Victor_Quintas-Martinez1), [Mohammad Bahadori](http://openreview.net/profile?id=~Mohammad_Taha_Bahadori1), [Eduardo Santiago](http://openreview.net/profile?id=~Eduardo_Santiago1), [Jeff Mu](http://openreview.net/profile?email=jefmu@amazon.com), [David Heckerman](http://openreview.net/profile?email=heckerma@amazon.com)
  - **Affiliations:** MIT Department of Economics; Amazon, Amazon, Amazon, Amazon, Amazon
  - **TL;DR:** This study presents a multiply robust estimation strategy for causal change attribution that quantifies the contribution of various causal mechanisms to changes in outcome distributions. The proposed method is consistent, asymptotically normal, and can be integrated into existing causal attribution frameworks, demonstrating strong performance in simulations and practical applications.
  - **Keywords:** Causal change attribution, causal mechanisms, Regression, re-weighting methods, causal Directed Acyclic Graph (DAG), Performance metrics analysis, sales volume analysis, Change attribution, counterfactual distribution, causal inference, Multiply robust estimation strategy, consistent and asymptotically normal estimator, DoWhy (Python library), Shapley values, Monte Carlo simulations


- [STEER: Assessing the Economic Rationality of Large Language Models](https://icml.cc/virtual/2024/poster/33120) (Poster)
  - **Authors:** [Narun Raman](http://openreview.net/profile?id=~Narun_Krishnamurthi_Raman1), [Taylor Lundy](http://openreview.net/profile?id=~Taylor_Lundy1), [Samuel Joseph Amouyal](http://openreview.net/profile?id=~Samuel_Joseph_Amouyal1), [Yoav Levine](http://openreview.net/profile?id=~Yoav_Levine1), [Kevin Leyton-Brown](http://openreview.net/profile?id=~Kevin_Leyton-Brown1), [Moshe Tennenholtz](http://openreview.net/profile?id=~Moshe_Tennenholtz1)
  - **Affiliations:** Department of Computer Science, University of British Columbia, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada, Tel Aviv University, Aviv, Israel, Stanford & AI21 Labs, Palo Alto, California, United States, Department of Computer Science, University of British Columbia, Vancouver, Canada, Technion & AI21 Labs, Tel Aviv, Israel
  - **TL;DR:** This paper proposes a methodology for assessing the economic rationality of Large Language Models (LLMs) as decision-making agents through a benchmark called STEER. The study evaluates the performance of 14 different LLMs, highlighting their capabilities and the impact of model size on rational behavior.
  - **Keywords:** Large Language Models, Economic Rationality, Decision-Making Agents, Personal Finance, Medical Diagnostics, Game Playing, Reliability of LLM agents, Configuration of LLMs for decision-making, STEER (Systematic and Tuneable Evaluation of Economic Rationality), Benchmark Distribution, Reinforcement Learning from AI Feedback (RLAIF), Chain-of-Thought Reasoning


- [Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation](https://icml.cc/virtual/2024/poster/33425) (Poster)
  - **Authors:** [Ossi Räisä](http://openreview.net/profile?id=~Ossi_R%C3%A4is%C3%A41), [Joonas Jälkö](http://openreview.net/profile?id=~Joonas_J%C3%A4lk%C3%B61), [Antti Honkela](http://openreview.net/profile?id=~Antti_Honkela1)
  - **Affiliations:** Department of Computer Science, University of Helsinki, Department of Computer Science, University of Helsinki, Department of Computer Science, University of Helsinki
  - **TL;DR:** This study investigates the impact of batch size on total gradient variance in differentially private stochastic gradient descent (DP-SGD), demonstrating that larger batch sizes reduce effective total gradient variance. The findings suggest that large batch sizes are beneficial for maintaining privacy while optimizing performance in deep learning models.
  - **Keywords:** Differential Privacy, Stochastic Gradient Descent, Deep Learning, Differentially Private Stochastic Gradient Descent (DP-SGD), Poisson Subsampling, Gaussian Mechanism, Total Gradient Variance, Subsampling-Induced Variance, Noise-Induced Variance, Effective Total Gradient Variance Reduction, Asymptotic Regime Analysis, Subsampling Amplification


- [Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks](https://icml.cc/virtual/2024/poster/34315) (Poster)
  - **Authors:** [Rahul Ramesh](http://openreview.net/profile?id=~Rahul_Ramesh2), [Ekdeep Singh Lubana](http://openreview.net/profile?id=~Ekdeep_Singh_Lubana1), [Mikail Khona](http://openreview.net/profile?id=~Mikail_Khona2), [Robert Dick](http://openreview.net/profile?id=~Robert_P._Dick1), [Hidenori Tanaka](http://openreview.net/profile?id=~Hidenori_Tanaka1)
  - **Affiliations:** Computer and Information Science, University of Pennsylvania, Electrical Engineering and Computer Science, University of Michigan; Physics & Information Laboratories, NTT Research, Physics, MIT, Electrical Engineering and Computer Science, University of Michigan, Physics & Information Laboratories, NTT Research; Center for Brain Science, Harvard University
  - **TL;DR:** This study investigates the compositional capabilities of autoregressive Transformers trained on a synthetic data-generating process, demonstrating their ability to learn and generalize complex function compositions. Key findings indicate that generating intermediate outputs enhances generalization to unseen compositions, while biases in training data can hinder performance.
  - **Keywords:** Compositional capabilities, Autoregressive Transformers, Generalization of functions, Composition of capabilities, Learning linear chains of compositions, Generating intermediate outputs, Synthetic data-generating process, Transformers, Attention layers, Feed-forward layers


- [TabLog: Test-Time Adaptation for Tabular Data Using Logic Rules](https://icml.cc/virtual/2024/poster/34288) (Poster)
  - **Authors:** [Weijieying Ren](http://openreview.net/profile?id=~Weijieying_Ren1), [Xiaoting Li](http://openreview.net/profile?id=~Xiaoting_Li3), [Huiyuan Chen](http://openreview.net/profile?id=~Huiyuan_Chen1), [Vineeth Rakesh](http://openreview.net/profile?id=~Vineeth_Rakesh1), [Zhuoyi Wang](http://openreview.net/profile?id=~Zhuoyi_Wang1), [Mahashweta Das](http://openreview.net/profile?id=~Mahashweta_Das2), [Vasant Honavar](http://openreview.net/profile?id=~Vasant_G_Honavar1)
  - **Affiliations:** Artificial Intelligence Research Laboratory, Center for Artificial Intelligence Foundations and Scientific Applications, Institute for Computational and Data Sciences, College of Information Sciences and Technology, The Pennsylvania State University, PA, United States, Visa Research, CA, United States, Visa Research, CA, United States, Visa Research, CA, United States, Visa Research, CA, United States, Visa Research, CA, United States, Artificial Intelligence Research Laboratory, Center for Artificial Intelligence Foundations and Scientific Applications, Institute for Computational and Data Sciences, College of Information Sciences and Technology, The Pennsylvania State University, PA, United States; College of Information Sciences and Technology, The Pennsylvania State University, PA, United States
  - **TL;DR:** The study introduces TabLog, a novel test-time adaptation technique for predictive models trained on tabular data, which effectively addresses distribution shifts using logical rules and a contrastive loss approach. Experimental results demonstrate that TabLog is competitive with or improves upon existing state-of-the-art methods for this problem.
  - **Keywords:** Test-time adaptation, Tabular data, Logical neural network, Rule ensemble, Contrastive loss, Clinical diagnosis, Climate modeling, E-commerce, Distribution shift, Heterogeneous features, Complex dependencies, End-to-end framework for training and adaptation, Benchmark datasets


- [Unveiling Privacy, Memorization, and Input Curvature Links](https://icml.cc/virtual/2024/poster/35011) (Poster)
  - **Authors:** [Deepak Ravikumar](http://openreview.net/profile?id=~Deepak_Ravikumar1), [Efstathia Soufleri](http://openreview.net/profile?id=~Efstathia_Soufleri1), [Abolfazl Hashemi](http://openreview.net/profile?id=~Abolfazl_Hashemi1), [Kaushik Roy](http://openreview.net/profile?id=~Kaushik_Roy1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana 47906, Department of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana 47906, Department of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana 47906, Department of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana 47906
  - **TL;DR:** This study investigates the relationship between memorization in deep neural networks and input loss curvature, establishing theoretical links to differential privacy. The findings suggest that input loss curvature can serve as an efficient proxy for measuring memorization, validated through experiments on CIFAR and ImageNet datasets.
  - **Keywords:** Memorization, Differential Privacy, Deep Neural Networks, Input Loss Curvature, Stability-based Memorization Score, Computer Vision, Natural Language Processing, Overfitting, Memorization, Membership Inference Attacks, Upper Bound on Memorization, Theoretical Links between Privacy and Memorization, CIFAR, ImageNet


- [Rejuvenating image-GPT as Strong Visual Representation Learners](https://icml.cc/virtual/2024/poster/33145) (Oral)
  - **Authors:** [Sucheng Ren](http://openreview.net/profile?id=~Sucheng_Ren1), [Zeyu Wang](http://openreview.net/profile?id=~Zeyu_Wang2), [Hongru Zhu](http://openreview.net/profile?id=~Hongru_Zhu1), [Junfei Xiao](http://openreview.net/profile?id=~Junfei_Xiao1), [Alan Yuille](http://openreview.net/profile?id=~Alan_Yuille1), [Cihang Xie](http://openreview.net/profile?id=~Cihang_Xie3)
  - **Affiliations:** Johns Hopkins University, UC Santa Cruz, Johns Hopkins University, Johns Hopkins University, Johns Hopkins University, UC Santa Cruz
  - **TL;DR:** This study enhances image-GPT by shifting the prediction target to semantic tokens and supplementing autoregressive modeling, resulting in the D-iGPT model, which achieves a remarkable 90.0% top-1 accuracy on the ImageNet-1K dataset. The findings indicate that D-iGPT is a strong learner of visual representations with effective generalization capabilities.
  - **Keywords:** visual representation learning, autoregressive pretraining, image-GPT (iGPT), D-iGPT, semantic tokens, CLIP, image recognition, computer vision, 90.0% top-1 accuracy on ImageNet-1K, strong generalization on downstream tasks, ImageNet-1K, publicly available datasets, Transformer architecture, autoregressive modeling


- [Position: Mission Critical – Satellite Data is a Distinct Modality in Machine Learning](https://icml.cc/virtual/2024/poster/34123) (Spotlight Poster)
  - **Authors:** [Esther Rolf](http://openreview.net/profile?id=~Esther_Rolf1), [Konstantin Klemmer](http://openreview.net/profile?id=~Konstantin_Klemmer1), [Caleb Robinson](http://openreview.net/profile?id=~Caleb_Robinson1), [Hannah Kerner](http://openreview.net/profile?id=~Hannah_Kerner1)
  - **Affiliations:** Harvard Data Science Initiative and Center for Research on Computation and Society, Harvard University; University of Colorado, Boulder, Microsoft Research, Microsoft AI for Good Research Lab, School of Computing and Augmented Intelligence, Arizona State University
  - **TL;DR:** This position paper argues that satellite data represents a distinct modality for machine learning, necessitating specialized methods and a new research agenda to effectively address its unique challenges. The authors highlight the shortcomings of current approaches and propose actionable suggestions to enhance the impact of machine learning in satellite data analysis.
  - **Keywords:** Satellite data, Machine learning for satellite data (SatML), Remote sensing, Self-supervised learning, Rotation-equivariant neural networks, Climate change, Poverty, Food insecurity, Biodiversity loss, Challenges in extracting actionable insights from satellite data, Sub-optimal ML solutions for satellite data, New research agenda for SatML, Specialized methods for satellite data analysis


- [Position: Application-Driven Innovation in Machine Learning](https://icml.cc/virtual/2024/poster/32725) (Poster)
  - **Authors:** [David Rolnick](http://openreview.net/profile?id=~David_Rolnick1), [Alan Aspuru-Guzik](http://openreview.net/profile?id=~Alan_Aspuru-Guzik2), [Sara Beery](http://openreview.net/profile?id=~Sara_Beery1), [Bistra Dilkina](http://openreview.net/profile?id=~Bistra_Dilkina2), [Priya Donti](http://openreview.net/profile?id=~Priya_L._Donti1), [Marzyeh Ghassemi](http://openreview.net/profile?id=~Marzyeh_Ghassemi2), [Hannah Kerner](http://openreview.net/profile?id=~Hannah_Kerner1), [Claire Monteleoni](http://openreview.net/profile?id=~Claire_Monteleoni1), [Esther Rolf](http://openreview.net/profile?id=~Esther_Rolf1), [Milind Tambe](http://openreview.net/profile?id=~Milind_Tambe1), [Adam White](http://openreview.net/profile?id=~Adam_White1)
  - **Affiliations:** McGill University and Mila – Quebec AI Institute, Montreal, Canada, University of Toronto and Vector Institute, Toronto, Canada, Massachusetts Institute of Technology, Cambridge, USA, University of Southern California, Los Angeles, USA, Massachusetts Institute of Technology, Cambridge, USA, Massachusetts Institute of Technology, Cambridge, USA, Arizona State University, Tempe, USA, Inria Paris, Paris, France; University of Colorado Boulder, Boulder, USA, Harvard University, Cambridge, USA; University of Colorado Boulder, Boulder, USA, Harvard University, Cambridge, USA, University of Alberta and Alberta Machine Intelligence Institute, Edmonton, Canada
  - **TL;DR:** This position paper argues for the importance of application-driven research in machine learning, highlighting its potential to inspire innovative algorithms that address real-world challenges. It discusses the systemic undervaluation of this approach within the ML community and suggests improvements to support application-driven innovation alongside traditional methods-driven research.
  - **Keywords:** Application-driven research, machine learning innovation, Healthcare, climate science, heavy industry, Bridging the gap between methods-driven and application-driven research, under-valued application-driven innovation


- [Decomposable Submodular Maximization in Federated Setting](https://icml.cc/virtual/2024/poster/34019) (Poster)
  - **Authors:** [Akbar Rafiey](http://openreview.net/profile?id=~Akbar_Rafiey1)
  - **Affiliations:** Halıcıoğlu Data Science Institute, University of California, San Diego, USA
  - **TL;DR:** This paper proposes a federated optimization approach for decomposable submodular functions, addressing the challenges of computational prohibitions and privacy concerns. The method utilizes a continuous greedy algorithm to maximize a weighted sum of client preferences while significantly reducing communication costs.
  - **Keywords:** Decomposable submodular functions, Federated optimization, Privacy-preserving optimization, Continuous greedy algorithm, Machine learning, Recommendation systems, Welfare maximization, Optimization of decomposable submodular functions, Computational prohibitions, Privacy concerns, Federated algorithm for optimization, Approximate solutions, Submodular functions, Diminishing returns property


- [One-Shot Strategic Classification Under Unknown Costs](https://icml.cc/virtual/2024/poster/34160) (Poster)
  - **Authors:** [Elan Rosenfeld](http://openreview.net/profile?id=~Elan_Rosenfeld1), [Nir Rosenfeld](http://openreview.net/profile?id=~Nir_Rosenfeld2)
  - **Affiliations:** Carnegie Mellon University, Technion – Israel Institute of Technology
  - **TL;DR:** This study addresses the challenge of strategic classification under unknown user responses, proposing a minimax approach to learn robust classifiers that minimize worst-case risk. The findings highlight the significant impact of cost mis-estimation on predictive accuracy and the necessity of considering a range of plausible cost functions.
  - **Keywords:** strategic classification, decision rules, robustness to manipulation, minimax problem, dual norm regularization, public policy, loan approvals, admissions, hiring, insurance, welfare benefits, strategic input manipulation, unknown cost function, Goodhart's law, efficient algorithms, worst-case risk minimization


- [Modelling Microbial Communities with Graph Neural Networks](https://icml.cc/virtual/2024/poster/32792) (Poster)
  - **Authors:** [Albane Ruaud](http://openreview.net/profile?id=~Albane_Ruaud1), [Cansu Sancaktar](http://openreview.net/profile?id=~Cansu_Sancaktar1), [Marco Bagatella](http://openreview.net/profile?id=~Marco_Bagatella1), [Christoph Ratzke](http://openreview.net/profile?id=~Christoph_Ratzke1), [Georg Martius](http://openreview.net/profile?id=~Georg_Martius1)
  - **Affiliations:** Cluster of Excellence – ML for Science, University of Tübingen, Tübingen, Germany, Autonomous Learning group, Max Planck Institute for Intelligent Systems, Tübingen, Germany; ETH Zürich, Zürich, Switzerland, Autonomous Learning group, Max Planck Institute for Intelligent Systems, Tübingen, Germany; ETH Zürich, Zürich, Switzerland, Cluster of Excellence – CMFI, University of Tübingen, Tübingen, Germany, Autonomous Learning group, Max Planck Institute for Intelligent Systems, Tübingen, Germany
  - **TL;DR:** This study models bacterial communities using graph neural networks (GNNs) to predict community relative abundance profiles directly from genomes, overcoming limitations of traditional models. The approach demonstrates generalization to unseen bacteria and varying community structures, highlighting the potential of GNNs in understanding microbial interactions.
  - **Keywords:** microbial communities, bacterial interactions, graph neural networks, graph neural networks (GNNs), medical settings, environmental settings, modeling bacterial interactions, generalization to new bacteria, data sparsity, prediction of community relative abundance profiles, simulation for data generation, real-world datasets


- [Position: Key Claims in LLM Research Have a Long Tail of Footnotes](https://icml.cc/virtual/2024/poster/34259) (Poster)
  - **Authors:** [Anna Rogers](http://openreview.net/profile?id=~Anna_Rogers1), [Sasha Luccioni](http://openreview.net/profile?id=~Sasha_Luccioni1)
  - **Affiliations:** IT University of Copenhagen, Hugging Face, Canada
  - **TL;DR:** This position paper defines Large Language Models (LLMs) and critically examines five common claims about their functionality, highlighting the need for clearer definitions and rigorous research practices in the field. The authors emphasize the impact of these claims on the Machine Learning research landscape and propose directions for future research.
  - **Keywords:** Large Language Models, Machine Learning, Health, Education, Definition ambiguity, Claims and assumptions in LLM discourse, Definition of LLMs, Examination of common claims, Emergent properties, Transformer, Socio-technical critiques


- [Position: Amazing Things Come From Having Many Good Models](https://icml.cc/virtual/2024/poster/33089) (Spotlight Poster)
  - **Authors:** [Cynthia Rudin](http://openreview.net/profile?id=~Cynthia_Rudin1), [Chudi Zhong](http://openreview.net/profile?id=~Chudi_Zhong1), [Lesia Semenova](http://openreview.net/profile?id=~Lesia_Semenova1), [Margo Seltzer](http://openreview.net/profile?id=~Margo_Seltzer1), [Ron Parr](http://openreview.net/profile?id=~Ronald_Parr1), [Jiachang Liu](http://openreview.net/profile?id=~Jiachang_Liu1), [Srikar Katta](http://openreview.net/profile?id=~Srikar_Katta1), [Jon Donnelly](http://openreview.net/profile?id=~Jon_Donnelly1), [Harry Chen](http://openreview.net/profile?id=~Harry_Chen2), [Zachery Boner](http://openreview.net/profile?id=~Zachery_Boner1)
  - **Affiliations:** Department of Computer Science, Duke University, Durham, North Carolina, USA, Department of Computer Science, Duke University, Durham, North Carolina, USA, Department of Computer Science, Duke University, Durham, North Carolina, USA, Department of Computer Science, University of British Columbia, Vancouver, Canada, Department of Computer Science, Duke University, Durham, North Carolina, USA, Department of Computer Science, Duke University, Durham, North Carolina, USA, Department of Computer Science, Duke University, Durham, North Carolina, USA, Department of Computer Science, Duke University, Durham, North Carolina, USA, Department of Computer Science, Duke University, Durham, North Carolina, USA, Department of Computer Science, Duke University, Durham, North Carolina, USA
  - **TL;DR:** The paper discusses the Rashomon Effect, which reveals that multiple equally good predictive models exist for the same dataset, particularly in noisy settings. It emphasizes the implications of this phenomenon for machine learning practices, including the potential for simple-yet-accurate models and the lack of an accuracy-interpretability trade-off.
  - **Keywords:** Rashomon Effect, machine learning, predictive models, tabular data problems, public policy, healthcare, financial loan decisions, uncertainty in predictions, fairness, variable importance, accuracy-interpretability trade-off, simple-yet-accurate models, insights into model selection


- [Generalizing Orthogonalization for Models with Non-Linearities](https://icml.cc/virtual/2024/poster/33057) (Poster)
  - **Authors:** [David Rügamer](http://openreview.net/profile?id=~David_R%C3%BCgamer1), [Chris Kolb](http://openreview.net/profile?id=~Chris_Kolb1), [Tobias Weber](http://openreview.net/profile?id=~Tobias_Weber1), [Lucas Kook](http://openreview.net/profile?id=~Lucas_Kook1), [Thomas Nagler](http://openreview.net/profile?id=~Thomas_Nagler1)
  - **Affiliations:** Department of Statistics, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, Department of Statistics, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, Department of Statistics, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany, Institute for Statistics and Mathematics, Vienna University of Economics and Business, Vienna, Austria, Department of Statistics, LMU Munich, Munich, Germany; Munich Center for Machine Learning (MCML), Munich, Germany
  - **TL;DR:** This paper addresses the biases in black-box algorithms, particularly in neural networks, and proposes a method for orthogonalization that corrects for non-linearities like ReLU activations. The findings demonstrate the effectiveness of this approach in safeguarding sensitive data and ensuring fair treatment in algorithmic predictions.
  - **Keywords:** Orthogonalization, Normalization, Bias in Algorithms, Neural Networks, ReLU Activations, Convolutional Neural Networks, Medical Decision-Making, Image Analysis, Biases in Model Predictions, Ethical Considerations in AI, Corrections for Non-Linearities, Safeguarding Sensitive Data, Black-Box Algorithms, Metadata Normalization


- [Position: The Reasonable Person Standard for AI](https://icml.cc/virtual/2024/poster/34122) (Poster)
  - **Authors:** [Sunayana Rane](http://openreview.net/profile?id=~Sunayana_Rane1)
  - **Affiliations:** Department of Computer Science, Princeton University; University of Chicago Law School
  - **TL;DR:** This paper argues that the "Reasonable Person Standard" should guide the development and evaluation of AI behavior, aligning it with societal expectations of reasonable human conduct. It emphasizes the importance of regulating AI behavior to ensure it is useful and constructive for society.
  - **Keywords:** AI governance, AI alignment, reasonable behavior, Reasonable Person Standard, AI systems, legal contexts, Regulation of AI behavior, emulation of human behavior, Guidelines for AI behavior, technical goals for AI researchers


- [Rolling Diffusion Models](https://icml.cc/virtual/2024/poster/33697) (Poster)
  - **Authors:** [David Ruhe](http://openreview.net/profile?id=~David_Ruhe1), [Jonathan Heek](http://openreview.net/profile?id=~Jonathan_Heek1), [Tim Salimans](http://openreview.net/profile?id=~Tim_Salimans1), [Emiel Hoogeboom](http://openreview.net/profile?id=~Emiel_Hoogeboom1)
  - **Affiliations:** Google Deepmind, Amsterdam, Netherlands; University of Amsterdam, Netherlands, Google Deepmind, Amsterdam, Netherlands, Google Deepmind, Amsterdam, Netherlands, Google Deepmind, Amsterdam, Netherlands
  - **TL;DR:** This paper introduces Rolling Diffusion, a novel approach to diffusion models that progressively corrupts data over time, enhancing performance in complex temporal dynamics. The method demonstrates superior results in video prediction tasks and chaotic fluid dynamics forecasting compared to standard diffusion techniques.
  - **Keywords:** diffusion models, temporal data, video prediction, Rolling Diffusion, sliding window denoising process, video generation, fluid dynamics forecasting, climate modeling, noise in diffusion process, uncertainty in future frames, computational intensity in frame generation, improved performance in complex temporal dynamics, superior results in video prediction tasks, Kinetics-600 video dataset


- [Second-Order Uncertainty Quantification: A Distance-Based Approach](https://icml.cc/virtual/2024/poster/33918) (Spotlight Poster)
  - **Authors:** [Yusuf Sale](http://openreview.net/profile?id=~Yusuf_Sale1), [Viktor Bengs](http://openreview.net/profile?id=~Viktor_Bengs1), [Michele Caprio](http://openreview.net/profile?id=~Michele_Caprio1), [Eyke Hüllermeier](http://openreview.net/profile?id=~Eyke_H%C3%BCllermeier1)
  - **Affiliations:** Institute of Informatics, LMU Munich, Munich, Germany; Munich Center for Machine Learning, Munich, Germany, Institute of Informatics, LMU Munich, Munich, Germany; Munich Center for Machine Learning, Munich, Germany, Precise Center, University of Pennsylvania, Philadelphia, USA, Institute of Informatics, LMU Munich, Munich, Germany; Munich Center for Machine Learning, Munich, Germany
  - **TL;DR:** The paper proposes a framework for quantifying predictive uncertainty in machine learning using second-order probability distributions, addressing criticisms of existing measures. It introduces a new uncertainty measure based on the Wasserstein distance that satisfies formal criteria for meaningful uncertainty quantification.
  - **Keywords:** Predictive uncertainty, Machine learning, Second-order probability distributions, Wasserstein distance, Bayesian inference, Evidential Deep Learning (EDL), Healthcare, Socio-technical systems, Aleatoric uncertainty, Epistemic uncertainty, Predictive uncertainty, Uncertainty measures, Framework for developing uncertainty measures


- [A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization](https://icml.cc/virtual/2024/poster/34775) (Poster)
  - **Authors:** [Sebastian Sanokowski](http://openreview.net/profile?id=~Sebastian_Sanokowski1), [Sepp Hochreiter](http://openreview.net/profile?id=~Sepp_Hochreiter1), [Sebastian Lehner](http://openreview.net/profile?id=~Sebastian_Lehner1)
  - **Affiliations:** Institute for Machine Learning, Johannes Kepler University, Linz, Austria; ELLIS Unit Linz; NXAI GmbH, Institute for Machine Learning, Johannes Kepler University, Linz, Austria; ELLIS Unit Linz; NXAI GmbH, Institute for Machine Learning, Johannes Kepler University, Linz, Austria; ELLIS Unit Linz; NXAI GmbH
  - **TL;DR:** This study presents a novel method for sampling from intractable distributions in Combinatorial Optimization using diffusion models, overcoming the limitations of existing generative models that require exact sample likelihoods. The proposed approach achieves state-of-the-art results on various benchmark problems, demonstrating its effectiveness in data-free approximation of discrete distributions.
  - **Keywords:** Combinatorial Optimization, Distribution Learning, Diffusion Models, Latent Variable Models, Variational Autoencoders, High-dimensional Target Distributions, Energy Functions, Monte Carlo Integration, Intractable Distributions, Data-free Approximation, Discrete Target Distributions, New Method for Data-free Approximation, State-of-the-art Results, Kullback-Leibler Divergence, Boltzmann Distribution


- [Proactive Detection of Voice Cloning with Localized Watermarking](https://icml.cc/virtual/2024/poster/34713) (Poster)
  - **Authors:** [Robin San Roman](http://openreview.net/profile?id=~Robin_San_Roman1), [Pierre Fernandez](http://openreview.net/profile?id=~Pierre_Fernandez1), [Hady Elsahar](http://openreview.net/profile?id=~Hady_Elsahar2), [Alexandre Defossez](http://openreview.net/profile?id=~Alexandre_D%C3%A9fossez1), [Teddy Furon](http://openreview.net/profile?id=~Teddy_Furon1), [Tuan Tran](http://openreview.net/profile?id=~Tuan_Tran5)
  - **Affiliations:** FAIR, Meta; None, FAIR, Meta; None, FAIR, Meta; None, Kyutai, Inria, FAIR, Meta; Inria
  - **TL;DR:** The study introduces AudioSeal, a novel audio watermarking technique for the localized detection of AI-generated speech, addressing the growing concerns of voice cloning and misinformation. AudioSeal demonstrates superior robustness and speed compared to existing methods, making it suitable for large-scale and real-time applications.
  - **Keywords:** audio authenticity, voice cloning, AI-generated speech, audio watermarking, generator/detector architecture, localization loss, perceptual loss, speech generation, audio forensics, real-time applications, risks of voice cloning, misinformation, detection of synthesized audio, AudioSeal, state-of-the-art performance, fast detection


- [Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency](https://icml.cc/virtual/2024/poster/32870) (Poster)
  - **Authors:** [Sudeep Salgia](http://openreview.net/profile?id=~Sudeep_Salgia1), [Sattar Vakili](http://openreview.net/profile?id=~Sattar_Vakili1), [Qing Zhao](http://openreview.net/profile?id=~Qing_Zhao1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA, MediaTek Research, Cambridge, UK, Department of Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA
  - **TL;DR:** This study presents a random exploration approach in Bayesian optimization using Gaussian Process models, achieving optimal error rates and order-optimal regret guarantees in both noise-free and noisy settings. The proposed algorithm enhances computational efficiency by eliminating the need for expensive optimization of non-convex acquisition functions.
  - **Keywords:** Bayesian optimization, Gaussian Process models, kernel-based bandit optimization, random exploration, domain shrinking, hyperparameter optimization, experimental design, recommendation systems, robotics, optimization of unknown functions, cumulative regret, order-optimal regret guarantees, computational efficiency, Reproducing Kernel Hilbert Space (RKHS), GP-UCB


- [Predictive Coding beyond Correlations](https://icml.cc/virtual/2024/poster/33121) (Poster)
  - **Authors:** [Tommaso Salvatori](http://openreview.net/profile?id=~Tommaso_Salvatori1), [Luca Pinchetti](http://openreview.net/profile?id=~Luca_Pinchetti1), [Amine M'Charrak](http://openreview.net/profile?id=~Amine_M%27Charrak1), [Beren Millidge](http://openreview.net/profile?id=~Beren_Millidge1), [Thomas Lukasiewicz](http://openreview.net/profile?id=~Thomas_Lukasiewicz2)
  - **Affiliations:** VERSES Research Lab, Los Angeles, CA 90016, USA; Institute of Logic and Computation, Vienna University of Technology, Austria, Department of Computer Science, University of Oxford, UK, Department of Computer Science, University of Oxford, UK, MRC Brain Network Dynamics Unit, University of Oxford, UK; Zyphra, Palo Alto, CA, USA, Institute of Logic and Computation, Vienna University of Technology, Austria; Department of Computer Science, University of Oxford, UK
  - **TL;DR:** This study demonstrates how predictive coding can be adapted to perform causal inference tasks, including interventions and structure learning, without the need for graph mutilation. The findings indicate that these adaptations can enhance the performance of predictive coding models in image classification tasks, achieving results comparable to traditional hierarchical models.
  - **Keywords:** predictive coding, causal inference, biologically plausible algorithms, PC graphs, interventional queries, structure learning, image classification, observational data analysis, computational expense of mutilating causal graphs, performance comparison with hierarchical models, improved performance of PC graphs, techniques for modeling interventions and structure learning, MNIST, FashionMNIST, Bayesian networks, do-operations, counterfactuals


- [Universal Gradient Methods for Stochastic Convex Optimization](https://icml.cc/virtual/2024/poster/33851) (Poster)
  - **Authors:** [Anton Rodomanov](http://openreview.net/profile?id=~Anton_Rodomanov1), [Ali Kavis](http://openreview.net/profile?id=~Ali_Kavis1), [Yongtao Wu](http://openreview.net/profile?id=~Yongtao_Wu1), [Kimon Antonakopoulos](http://openreview.net/profile?id=~Kimon_Antonakopoulos1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1)
  - **Affiliations:** CISPA Helmholtz Center for Information Security, Saarbrücken, Germany, Institute for Foundations of Machine Learning (IFML), UT Austin, Texas, USA, Laboratory for Information and Inference Systems (LIONS), EPFL, Lausanne, Switzerland, Laboratory for Information and Inference Systems (LIONS), EPFL, Lausanne, Switzerland, Laboratory for Information and Inference Systems (LIONS), EPFL, Lausanne, Switzerland
  - **TL;DR:** This paper presents universal gradient methods for Stochastic Convex Optimization that adapt to both the oracle's noise and the H¨older smoothness of the objective function without prior knowledge of the problem's characteristics. The proposed algorithms achieve state-of-the-art convergence rates and introduce line-search-free variants that automatically adjust to various optimization parameters.
  - **Keywords:** Stochastic Convex Optimization, Universal Gradient Methods, Stochastic Gradient Method (SGD), Universal Fast Gradient Method, H¨older smoothness, oracle noise, optimization algorithms, State-of-the-art convergence rate guarantees, line-search-free variants of UGMs, H¨older class, Lipschitz continuous gradient, nonsmooth functions


- [Optimal Batched Linear Bandits](https://icml.cc/virtual/2024/poster/34628) (Poster)
  - **Authors:** [Xuanfei Ren](http://openreview.net/profile?id=~Xuanfei_Ren1), [Tianyuan Jin](http://openreview.net/profile?id=~Tianyuan_Jin1), [Pan Xu](http://openreview.net/profile?id=~Pan_Xu1)
  - **Affiliations:** University of Science and Technology of China, National University of Singapore, Duke University
  - **TL;DR:** The paper introduces the E4 algorithm for batched linear bandits, achieving both minimax and asymptotic optimality in regret with optimal batch complexities. Empirical results demonstrate that E4 outperforms baseline algorithms in regret minimization, batch complexity, and computational efficiency.
  - **Keywords:** Batched linear bandits, Sequential decision-making, Explore-Estimate-Eliminate-Exploit framework, E4 algorithm, Online advertising, Recommendation systems, Trade-off between exploitation and exploration, Batch complexity, Minimax optimal regret, Asymptotic optimality, Linear contextual bandits


- [Fair Federated Learning via the Proportional Veto Core](https://icml.cc/virtual/2024/poster/34912) (Poster)
  - **Authors:** [Bhaskar Ray Chaudhury](http://openreview.net/profile?id=~Bhaskar_Ray_Chaudhury1), [Aniket Murhekar](http://openreview.net/profile?id=~Aniket_Murhekar1), [Zhuowen Yuan](http://openreview.net/profile?id=~Zhuowen_Yuan1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19), [Ruta Mehta](http://openreview.net/profile?id=~Ruta_Mehta2), [Ariel Procaccia](http://openreview.net/profile?id=~Ariel_D._Procaccia1)
  - **Affiliations:** University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Chicago, University of Illinois Urbana-Champaign, Harvard University
  - **TL;DR:** This paper introduces a new approach to fairness in federated learning by adapting the proportional veto core concept, allowing for fairness guarantees that do not rely on concave utility functions. The proposed Rank-Core-Fed algorithm demonstrates improved fairness outcomes across various datasets compared to existing methods.
  - **Keywords:** Fairness in Federated Learning, Core Stability, Proportional Veto Core (PVC), Rank-Core-Fed Algorithm, Autonomous Vehicles, Digital Healthcare, Utility-based fairness guarantees, Model overfitting, Non-concave utility functions, PVC-stable models, Distributed federated learning


- [Invariant Risk Minimization Is A Total Variation Model](https://icml.cc/virtual/2024/poster/34139) (Poster)
  - **Authors:** [Zhao-Rong Lai](http://openreview.net/profile?id=~Zhao-Rong_Lai1), [Weiwen Wang](http://openreview.net/profile?id=~Weiwen_Wang2)
  - **Affiliations:** Department of Mathematics, College of Information Science and Technology, Jinan University, Guangzhou, China, Department of Mathematics, College of Information Science and Technology, Jinan University, Guangzhou, China
  - **TL;DR:** This paper investigates the mathematical essence of Invariant Risk Minimization (IRM) and establishes that it can be framed as a total variation model, specifically TV-ℓ2. The authors propose a novel IRM framework based on TV-ℓ1, which enhances the robustness and flexibility of learning risk functions, facilitating better out-of-distribution generalization.
  - **Keywords:** Invariant Risk Minimization, Out-of-Distribution Generalization, Total Variation, TV-ℓ2, TV-ℓ1, Machine Learning, Signal Processing, Distributional Shift, Misclassification, Novel IRM Framework, Robust Performance in Denoising


- [Plug-and-Play image restoration with Stochastic deNOising REgularization](https://icml.cc/virtual/2024/poster/33615) (Poster)
  - **Authors:** [Marien Renaud](http://openreview.net/profile?id=~Marien_Renaud1), [Jean Prost](http://openreview.net/profile?id=~Jean_Prost1), [Arthur Leclaire](http://openreview.net/profile?id=~Arthur_Leclaire1), [Nicolas Papadakis](http://openreview.net/profile?id=~Nicolas_Papadakis3)
  - **Affiliations:** Univ. Bordeaux, CNRS, INRIA, Bordeaux INP, IMB, UMR 5251, F-33400 Talence, France, Université Paris-Cité, CNRS, MAP5, France, LTCI, Télécom Paris, IP Paris, Univ. Bordeaux, CNRS, INRIA, Bordeaux INP, IMB, UMR 5251, F-33400 Talence, France
  - **TL;DR:** This paper introduces a new Plug-and-Play framework called Stochastic deNOising REgularization (SNORE) that enhances image restoration by applying denoising only on appropriately noisy images. The proposed method demonstrates competitive performance in deblurring and inpainting tasks compared to existing state-of-the-art techniques.
  - **Keywords:** image restoration, inverse problems, Plug-and-Play algorithms, Stochastic deNOising REgularization (SNORE), stochastic gradient descent, proximal splitting algorithms, deblurring, inpainting, ill-posed inverse problems, noise reduction, convergence analysis, competitive performance against state-of-the-art methods


- [CarbonNovo: Joint Design of Protein Structure and Sequence Using a Unified Energy-based Model](https://icml.cc/virtual/2024/poster/34533) (Poster)
  - **Authors:** [Milong Ren](http://openreview.net/profile?id=~Milong_Ren2), [Tian Zhu](http://openreview.net/profile?id=~Tian_Zhu1), [Haicang Zhang](http://openreview.net/profile?id=~Haicang_Zhang1)
  - **Affiliations:** Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** The study presents CarbonNovo, a unified energy-based model for jointly generating protein structures and sequences, addressing limitations of traditional two-stage methods. It demonstrates improved performance in designability, novelty, and sequence plausibility compared to existing approaches.
  - **Keywords:** de novo protein design, protein structure and sequence generation, score-based generative model, Markov Random Fields, diffusion-based generative models, drug development, enzyme engineering, overfitting in sequence design, lack of interaction between structure and sequence design modules, unified energy-based model, improved designability, novelty, sequence plausibility, and Rosetta energy, RFdiffusion, ProteinMPNN, ESM-IF


- [Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion](https://icml.cc/virtual/2024/poster/33856) (Poster)
  - **Authors:** [Ishaan Rawal](http://openreview.net/profile?id=~Ishaan_Singh_Rawal1), [Alexander Matyasko](http://openreview.net/profile?id=~Alexander_Matyasko2), [Shantanu Jaiswal](http://openreview.net/profile?id=~Shantanu_Jaiswal1), [Basura Fernando](http://openreview.net/profile?id=~Basura_Fernando1), [Cheston Tan](http://openreview.net/profile?id=~Cheston_Tan1)
  - **Affiliations:** Centre for Frontier AI Research, Agency for Science, Technology & Research, Singapore; Institute of High Performance Computing, Agency for Science, Technology & Research, Singapore, Centre for Frontier AI Research, Agency for Science, Technology & Research, Singapore; Institute of High Performance Computing, Agency for Science, Technology & Research, Singapore, Centre for Frontier AI Research, Agency for Science, Technology & Research, Singapore; Institute of High Performance Computing, Agency for Science, Technology & Research, Singapore, Centre for Frontier AI Research, Agency for Science, Technology & Research, Singapore; Institute of High Performance Computing, Agency for Science, Technology & Research, Singapore, Centre for Frontier AI Research, Agency for Science, Technology & Research, Singapore; Institute of High Performance Computing, Agency for Science, Technology & Research, Singapore; Institute for Infocomm Research, Agency for Science, Technology & Research, Singapore
  - **TL;DR:** This study investigates the effectiveness of VideoQA Transformer models in capturing multimodal representations by introducing QUAG, a probe that reveals high performance without true modality fusion. The findings suggest that current models struggle with learning highly-coupled multimodal representations, as evidenced by their poor performance on the newly designed CLAVI dataset.
  - **Keywords:** Multimodal learning, Video Question Answering (VideoQA), Transformer models, QUAG (QUadrant Average), QUAG-attention, Video analysis, Natural language processing, Modality fusion, Biases in datasets, Spurious features, Insights into multimodal representations, Performance evaluation methods, CLAVI (Complements in Language and Video) dataset


- [Implicit Regularization in Feedback Alignment Learning Mechanisms for Neural Networks](https://icml.cc/virtual/2024/poster/32979) (Poster)
  - **Authors:** [Zach Robertson](http://openreview.net/profile?id=~Zachary_Robertson1), [Sanmi Koyejo](http://openreview.net/profile?id=~Sanmi_Koyejo1)
  - **Affiliations:** Department of Computer Science, Stanford, California, United States, Department of Computer Science, Stanford, California, United States
  - **TL;DR:** This study introduces a unified framework for Feedback Alignment (FA) in neural networks, addressing its alignment mechanism and limitations in multi-class classification. Key findings include a conservation law linking synaptic weight changes to implicit regularization, which enhances FA performance on complex tasks.
  - **Keywords:** Feedback Alignment, Neural Networks, Implicit Regularization, Distributed Machine Learning, Privacy-aware Machine Learning, Weight-transport problem, Multi-class classification limitations, Conservation law, Alignment dominance, Enhanced performance techniques, CIFAR-100, Tiny-ImageNet, Bio-plausible learning rules


- [A fast algorithm to simulate nonlinear resistive networks](https://icml.cc/virtual/2024/poster/33137) (Poster)
  - **Authors:** [Benjamin Scellier](http://openreview.net/profile?id=~Benjamin_Scellier1)
  - **Affiliations:** Rain AI, San Francisco, CA, USA
  - **TL;DR:** This paper presents a fast algorithm for simulating nonlinear resistive networks, significantly improving training efficiency and scalability compared to traditional methods. The proposed approach outperforms existing SPICE-based simulations, enabling the training of much larger networks at significantly faster speeds.
  - **Keywords:** nonlinear resistive networks, energy-efficient computing, machine learning, quadratic programming, coordinate descent algorithm, neuromorphic computing, analog electrical networks, simulation bottleneck, scalability of networks, fast simulation methodology, improved training efficiency, MNIST, Fashion-MNIST, SPICE, variable resistors, memristors, local learning rules


- [Parallel Affine Transformation Tuning of Markov Chain Monte Carlo](https://icml.cc/virtual/2024/poster/34021) (Poster)
  - **Authors:** [Philip Schär](http://openreview.net/profile?id=~Philip_Sch%C3%A4r1), [Michael Habeck](http://openreview.net/profile?id=~Michael_Habeck1), [Daniel Rudolf](http://openreview.net/profile?id=~Daniel_Rudolf1)
  - **Affiliations:** Microscopic Image Analysis Group, Friedrich Schiller University Jena, Jena, Germany, Microscopic Image Analysis Group, Friedrich Schiller University Jena, Jena, Germany, Faculty of Computer Science and Mathematics, University of Passau, Passau, Germany
  - **TL;DR:** This study presents a method for enhancing the performance of Markov Chain Monte Carlo samplers through bijective affine transformations of the sample space. The proposed adaptive learning scheme significantly improves sampling quality while maintaining low computational costs across various real-world applications.
  - **Keywords:** Markov Chain Monte Carlo, sampling, affine transformations, Gibbsian polar slice sampling, probabilistic inference, machine learning, high-dimensional probability distribution, computational bottleneck, adaptive learning of affine transformation, improved sampling performance


- [Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference](https://icml.cc/virtual/2024/poster/34900) (Poster)
  - **Authors:** [Marvin Schmitt](http://openreview.net/profile?id=~Marvin_Schmitt1), [Desi Ivanova](http://openreview.net/profile?id=~Desi_R._Ivanova1), [Daniel Habermann](http://openreview.net/profile?id=~Daniel_Habermann1), [Ullrich Koethe](http://openreview.net/profile?id=~Ullrich_Koethe1), [Paul Buerkner](http://openreview.net/profile?id=~Paul-Christian_B%C3%BCrkner1), [Stefan Radev](http://openreview.net/profile?id=~Stefan_T._Radev1)
  - **Affiliations:** University of Stuttgart, Germany, University of Oxford, UK, TU Dortmund University, Germany, Heidelberg University, Germany, TU Dortmund University, Germany, Rensselaer Polytechnic Institute, USA
  - **TL;DR:** This paper presents a method to enhance the efficiency and accuracy of amortized Bayesian inference by utilizing universal symmetries in probabilistic models, introducing a self-consistency loss that improves approximate inference in low data scenarios. The approach shows significant advantages in both neural posterior and likelihood approximation across various synthetic and scientific models.
  - **Keywords:** Amortized Bayesian Inference, Simulation-Based Inference, Markov Chain Monte Carlo (MCMC), Variational Inference, Neural Density Estimators, Scientific Modeling, Probabilistic Inference, Analytical Intractability, Data Efficiency, Approximate Inference, Self-Consistency Loss, Marginal Likelihood Estimation


- [Incentivized Learning in Principal-Agent Bandit Games](https://icml.cc/virtual/2024/poster/32655) (Poster)
  - **Authors:** [Antoine Scheid](http://openreview.net/profile?id=~Antoine_Scheid1), [Daniil Tiapkin](http://openreview.net/profile?id=~Daniil_Tiapkin1), [Etienne Boursier](http://openreview.net/profile?id=~Etienne_Boursier1), [Aymeric Capitaine](http://openreview.net/profile?id=~Aymeric_Capitaine1), [Eric Moulines](http://openreview.net/profile?id=~Eric_Moulines1), [Michael Jordan](http://openreview.net/profile?id=~Michael_Jordan1), [El-Mahdi El-Mhamdi](http://openreview.net/profile?id=~El-Mahdi_El-Mhamdi1), [Alain Oliviero Durmus](http://openreview.net/profile?id=~Alain_Oliviero_Durmus1)
  - **Affiliations:** Centre de Mathématiques Appliquées – CNRS – École polytechnique – Institut Polytechnique de Paris, None, Université Paris-Saclay, CNRS, Laboratoire de mathématiques d’Orsay, 91405, Orsay, France, INRIA, Université Paris Saclay, LMO, Orsay, France, Centre de Mathématiques Appliquées – CNRS – École polytechnique – Institut Polytechnique de Paris, None, Centre de Mathématiques Appliquées – CNRS – École polytechnique – Institut Polytechnique de Paris, None, University of California, Berkeley; Inria, École Normale Supérieure, PSL Research University, Centre de Mathématiques Appliquées – CNRS – École polytechnique – Institut Polytechnique de Paris, None, Centre de Mathématiques Appliquées – CNRS – École polytechnique – Institut Polytechnique de Paris, None
  - **TL;DR:** This study develops a framework for incentivized learning in principal-agent bandit games, where the principal learns to optimize her utility by offering incentives to an agent with misaligned objectives. The proposed algorithms achieve nearly optimal regret bounds in both multi-armed and linear contextual settings, addressing challenges in decision-making under uncertainty.
  - **Keywords:** Principal-Agent Bandit Games, Incentivized Learning, Incentivized Principal-Agent Algorithm (IPA), Regret-minimization algorithms, Healthcare, Ecological taxation, Decision-making under uncertainty, Misaligned objectives, Information asymmetries, Scarcity of decision-makers, Nearly optimal regret bounds, Learning incentive policies, Multi-armed bandits, Linear contextual bandits


- [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models](https://icml.cc/virtual/2024/poster/33875) (Oral)
  - **Authors:** [Christian Schlarmann](http://openreview.net/profile?id=~Christian_Schlarmann1), [Naman Singh](http://openreview.net/profile?id=~Naman_Deep_Singh1), [Francesco Croce](http://openreview.net/profile?id=~Francesco_Croce1), [Matthias Hein](http://openreview.net/profile?id=~Matthias_Hein2)
  - **Affiliations:** Tübingen AI Center, Germany; University of Tübingen, Germany, Tübingen AI Center, Germany; University of Tübingen, Germany, EPFL, Switzerland, Tübingen AI Center, Germany; University of Tübingen, Germany
  - **TL;DR:** This study proposes an unsupervised adversarial fine-tuning method to enhance the robustness of the CLIP vision encoder, addressing vulnerabilities to adversarial attacks in large vision-language models. The robust CLIP model significantly improves performance on downstream tasks without requiring retraining of the models that utilize it.
  - **Keywords:** Robustness of multi-modal foundation models, Adversarial attacks, Unsupervised adversarial fine-tuning, CLIP model, Vision-language models (LVLMs), Zero-shot classification, Vulnerability to adversarial attacks, Risk of spreading fake information, Robust CLIP vision encoder, Improved performance on downstream tasks, CLIP (Contrastive Language-Image Pre-training), LVLMs (Large Vision-Language Models)


- [A sampling theory perspective on activations for implicit neural representations](https://icml.cc/virtual/2024/poster/33593) (Poster)
  - **Authors:** [Hemanth Saratchandran](http://openreview.net/profile?id=~Hemanth_Saratchandran1), [Sameera Ramasinghe](http://openreview.net/profile?id=~Sameera_Ramasinghe1), [Violetta Shevchenko](http://openreview.net/profile?id=~Violetta_Shevchenko1), [Alexander Long](http://openreview.net/profile?id=~Alexander_Long1), [Simon Lucey](http://openreview.net/profile?id=~Simon_Lucey2)
  - **Affiliations:** University of Adelaide, Australia, Amazon, Australia, Amazon, Australia, Amazon, Australia, University of Adelaide, Australia
  - **TL;DR:** This study analyzes activations in Implicit Neural Representations (INRs) from a sampling theory perspective, revealing that sinc activations are optimal for signal encoding. The research establishes a connection between dynamical systems and INRs, providing a unified theoretical framework for understanding these activations.
  - **Keywords:** Implicit Neural Representations (INRs), signal encoding, Fourier positional encodings, non-traditional activation functions, sinc activations, Spectral bias, high-frequency signal representation, Unified theory of INR activations, generator functions for signal reconstruction


- [Sparse and Structured Hopfield Networks](https://icml.cc/virtual/2024/poster/34155) (Spotlight Poster)
  - **Authors:** [Saúl Santos](http://openreview.net/profile?id=~Saul_Jos%C3%A9_Rodrigues_dos_Santos1), [Vlad Niculae](http://openreview.net/profile?id=~Vlad_Niculae2), [Daniel McNamee](http://openreview.net/profile?id=~Daniel_C_McNamee1), [Andre Martins](http://openreview.net/profile?id=~Andre_Martins1)
  - **Affiliations:** Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Instituto de Telecomunicações, Lisbon, Portugal, Language Technology Lab, University of Amsterdam, The Netherlands, Champalimaud Research, Lisbon, Portugal, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Instituto de Telecomunicações, Lisbon, Portugal; Unbabel, Lisbon, Portugal
  - **TL;DR:** This paper introduces a unified framework for sparse Hopfield networks linked to Fenchel-Young losses, enabling exact memory retrieval and structured pattern associations. The proposed models demonstrate significant improvements in storage capacity and retrieval accuracy through experiments in multiple instance learning and text rationalization.
  - **Keywords:** Hopfield networks, sparse neural networks, structured retrieval, Fenchel-Young losses, SparseMAP transformation, α-entmax, α-normmax, Multiple instance learning, text rationalization, Exact memory retrieval, sparsity, limited storage capacity, Hopfield-Fenchel-Young energy functions, exponential storage capacity, structured margins


- [Towards Scalable and Versatile Weight Space Learning](https://icml.cc/virtual/2024/poster/32815) (Poster)
  - **Authors:** [Konstantin Schürholt](http://openreview.net/profile?id=~Konstantin_Sch%C3%BCrholt1), [Michael Mahoney](http://openreview.net/profile?id=~Michael_W._Mahoney1), [Damian Borth](http://openreview.net/profile?id=~Damian_Borth1)
  - **Affiliations:** AIML Lab, University of St.Gallen, St. Gallen, Switzerland; International Computer Science Institute, Berkeley, CA, USA, International Computer Science Institute, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Department of Statistics, University of California at Berkeley, CA, USA, AIML Lab, University of St.Gallen, St. Gallen, Switzerland
  - **TL;DR:** This paper introduces the SANE approach to weight-space learning, which provides scalable and task-agnostic representations of neural networks. The method demonstrates superior performance in generating unseen models and initializing new tasks compared to previous techniques.
  - **Keywords:** weight space learning, neural network representations, task-agnostic learning, SANE approach, hyper-representations, sequential processing, limitations in processing larger networks, task-specific learning, scalable representations, generation of unseen neural network models, performance on weight representation learning benchmarks, NMIST, SVHN, CIFAR-10, CIFAR-100, Tiny-ImageNet


- [On Multi-Armed Bandit with Impatient Arms](https://icml.cc/virtual/2024/poster/33329) (Poster)
  - **Authors:** [Yuming Shao](http://openreview.net/profile?id=~Yuming_Shao1), [Zhixuan Fang](http://openreview.net/profile?id=~Zhixuan_Fang1)
  - **Affiliations:** Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China
  - **TL;DR:** This paper explores a Multi-Armed Bandit setting where arms can exit if neglected, proposing the FC-SE and FC-Entry algorithms to address this issue. The study highlights the challenges of balancing exploitation and exploration while ensuring arms remain engaged, validated through theoretical results and experiments.
  - **Keywords:** Multi-Armed Bandit, Impatient Arms, FC-SE algorithm, FC-Entry algorithm, Online advertising, Crowdsourcing, Arm exit due to neglect, Balancing exploitation and exploration, Expected regret upper bounds, Performance guarantees


- [Bayesian Adaptation of Network Depth and Width for Continual Learning](https://icml.cc/virtual/2024/poster/33603) (Poster)
  - **Authors:** [Jeevan Thapa](http://openreview.net/profile?id=~Jeevan_Thapa1), [Rui Li](http://openreview.net/profile?id=~Rui_Li3)
  - **Affiliations:** College of Computing and Information Sciences, Rochester Institute of Technology, Rochester, New York, USA, College of Computing and Information Sciences, Rochester Institute of Technology, Rochester, New York, USA
  - **TL;DR:** This study presents a novel non-parametric Bayesian method for adapting both network depth and width in continual learning, addressing the limitations of existing methods that only focus on width. The proposed approach demonstrates competitive performance across various benchmarks and can be extended to unsupervised continual learning.
  - **Keywords:** Continual learning, Dynamic architecture, Non-parametric Bayesian approach, Beta process, Drop-connect regularization, Conjugate Bernoulli process, Catastrophic forgetting, Evolving data distributions, Superior performance, Model adaptation


- [Language Generation with Strictly Proper Scoring Rules](https://icml.cc/virtual/2024/poster/34311) (Poster)
  - **Authors:** [Chenze Shao](http://openreview.net/profile?id=~Chenze_Shao1), [Fandong Meng](http://openreview.net/profile?id=~Fandong_Meng3), [Yijin Liu](http://openreview.net/profile?id=~Yijin_Liu1), [Jie Zhou](http://openreview.net/profile?id=~Jie_Zhou8)
  - **Affiliations:** Pattern Recognition Center, WeChat AI, Tencent Inc., Pattern Recognition Center, WeChat AI, Tencent Inc., Pattern Recognition Center, WeChat AI, Tencent Inc., Pattern Recognition Center, WeChat AI, Tencent Inc.
  - **TL;DR:** This study proposes a strategy for adapting strictly proper scoring rules to language generation, demonstrating that using alternative scoring rules like the Brier and Spherical scores can significantly enhance model performance. The findings indicate that these improvements are scalable to large language models, such as LLaMA-7B and LLaMA-13B.
  - **Keywords:** Language generation, Maximum likelihood estimation, Log-likelihood loss, Strictly proper scoring rules, Brier score, Spherical score, Natural language processing, Text generation, Handling exponentially large sample space, Calibration of probabilistic models, Improvements in model generation capabilities, Adaptation of scoring rules, LLaMA-7B, LLaMA-13B


- [Asymptotics of Learning with Deep Structured (Random) Features](https://icml.cc/virtual/2024/poster/34056) (Poster)
  - **Authors:** [Dominik Schröder](http://openreview.net/profile?id=~Dominik_Schr%C3%B6der1), [Daniil Dmitriev](http://openreview.net/profile?id=~Daniil_Dmitriev2), [Hugo Cui](http://openreview.net/profile?id=~Hugo_Cui1), [Bruno Loureiro](http://openreview.net/profile?id=~Bruno_Loureiro1)
  - **Affiliations:** Department of Mathematics, ETH Zurich, 8006 Zürich, Switzerland, Department of Mathematics, ETH Zurich and ETH AI Center, 8092 Zürich, Switzerland, Statistical Physics Of Computation lab., Institute of Physics, École Polytechnique Fédérale de Lausanne (EPFL), 1015 Lausanne, Switzerland, Département d’Informatique, École Normale Supérieure (ENS) - PSL & CNRS, F-75230 Paris cedex 05, France
  - **TL;DR:** This study provides a tight asymptotic characterization of the test error in learning with Gaussian rainbow networks, which are deep non-linear networks with structured random weights. The findings reveal insights into the relationship between network architecture and generalization, particularly in high-dimensional settings.
  - **Keywords:** deep learning, Gaussian rainbow networks, high-dimensional learning, random features, structured random weights, asymptotic characterization, test error, generalization, inductive bias, learning with structured weights, closed-form formula for feature covariance, insights into feature maps learned by neural networks


- [Lessons from Generalization Error Analysis of Federated Learning: You May Communicate Less Often!](https://icml.cc/virtual/2024/poster/33458) (Poster)
  - **Authors:** [Milad Sefidgaran](http://openreview.net/profile?id=~Milad_Sefidgaran1), [Romain Chor](http://openreview.net/profile?id=~Romain_Chor1), [Abdellatif Zaidi](http://openreview.net/profile?id=~Abdellatif_Zaidi1), [Yijun Wan](http://openreview.net/profile?id=~Yijun_Wan1)
  - **Affiliations:** Huawei Paris Research Center, Paris, France; Université Gustave Eiffel, Champs-sur-Marne, France, Huawei Paris Research Center, Paris, France; Université Gustave Eiffel, Champs-sur-Marne, France, Huawei Paris Research Center, Paris, France; Université Gustave Eiffel, Champs-sur-Marne, France, Huawei Paris Research Center, Paris, France; Université Gustave Eiffel, Champs-sur-Marne, France
  - **TL;DR:** This study investigates the generalization error in Federated Learning, revealing that increased communication rounds with the parameter server can diminish the generalization power of local models. The findings suggest that the generalization error decreases more slowly with communication rounds compared to empirical risk, indicating potential implications for the design of Federated Learning systems.
  - **Keywords:** Federated Learning, Generalization Error, PAC-Bayes, Rate-Distortion Theory, Support Vector Machines (FSVM), Generalization error, Communication rounds, Data sparsity, Generalization bounds, Empirical risk, Population risk, ResNet-56


- [A Multimodal Automated Interpretability Agent](https://icml.cc/virtual/2024/poster/33183) (Poster)
  - **Authors:** [Tamar Rott Shaham](http://openreview.net/profile?id=~Tamar_Rott_Shaham1), [Sarah Schwettmann](http://openreview.net/profile?id=~Sarah_Schwettmann2), [Franklin Wang](http://openreview.net/profile?id=~Franklin_Wang2), [Achyuta Rajaram](http://openreview.net/profile?id=~Achyuta_Rajaram1), [Evan Hernandez](http://openreview.net/profile?id=~Evan_Hernandez1), [Jacob Andreas](http://openreview.net/profile?id=~Jacob_Andreas1), [Antonio Torralba](http://openreview.net/profile?id=~Antonio_Torralba1)
  - **Affiliations:** MIT CSAIL, MIT CSAIL, MIT CSAIL, MIT CSAIL, MIT CSAIL, MIT CSAIL, MIT CSAIL
  - **TL;DR:** This paper presents MAIA, a Multimodal Automated Interpretability Agent that automates the understanding of neural models through feature interpretation and failure mode discovery. MAIA demonstrates the ability to produce human-comparable descriptions of learned representations and aids in reducing sensitivity to spurious features and identifying likely misclassifications.
  - **Keywords:** Multimodal Interpretability, Neural Model Understanding, Neural Models, Vision-Language Models, Computer Vision, Feature Interpretation, Failure Mode Discovery, Sensitivity to Spurious Features, Misclassification, Automated Interpretability, Feature Description, Experimental Tools, Novel Dataset of Synthetic Vision Neurons


- [Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models](https://icml.cc/virtual/2024/poster/34348) (Poster)
  - **Authors:** [Bilgehan Sel](http://openreview.net/profile?id=~Bilgehan_Sel1), [Ahmad Al-Tawaha](http://openreview.net/profile?id=~Ahmad_Tawaha1), [Vanshaj Khattar](http://openreview.net/profile?id=~Vanshaj_Khattar1), [Ruoxi Jia](http://openreview.net/profile?id=~Ruoxi_Jia1), [Ming Jin](http://openreview.net/profile?id=~Ming_Jin2)
  - **Affiliations:** Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA, Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA, Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA, Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA, Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA
  - **TL;DR:** This study introduces the Algorithm of Thoughts, a novel strategy that enhances the reasoning capabilities of Large Language Models (LLMs) by utilizing algorithmic reasoning pathways, significantly reducing the number of queries needed. The findings suggest that LLMs can outperform traditional algorithms when guided by structured algorithmic instructions, indicating their potential for optimized search processes.
  - **Keywords:** Large Language Models, Algorithmic Reasoning, Chain-of-Thought, Tree of Thoughts, Self-Consistency, Problem Solving, Code Generation, Instruction Following, Increased computational overhead, query request escalation, Algorithm of Thoughts, enhanced idea exploration


- [Double Momentum Method for Lower-Level Constrained Bilevel Optimization](https://icml.cc/virtual/2024/poster/34886) (Poster)
  - **Authors:** [Wanli Shi](http://openreview.net/profile?id=~Wanli_Shi1), [Yi Chang](http://openreview.net/profile?id=~Yi_Chang4), [Bin Gu](http://openreview.net/profile?id=~Bin_Gu1)
  - **Affiliations:** School of Artificial Intelligence, Jilin University, China; Mohamed bin Zayed University of Artificial Intelligence, UAE, School of Artificial Intelligence, Jilin University, China, School of Artificial Intelligence, Jilin University, China; Mohamed bin Zayed University of Artificial Intelligence, UAE
  - **TL;DR:** This paper introduces a new hypergradient method for lower-level constrained bilevel optimization that avoids restrictive assumptions and proposes a single-loop single-timescale algorithm based on the double-momentum method. The method demonstrates improved efficiency and convergence properties, achieving a (δ, ϵ)-stationary point in ˜O(d²ϵ⁻⁴) iterations.
  - **Keywords:** Bilevel optimization, hypergradient methods, lower-level constrained optimization, double-momentum method, adaptive step size method, nonsmooth implicit function theorem, hyper-parameter optimization, meta-learning, reinforcement learning, lower-level constrained bilevel optimization, efficiency of updates, convergence rate analysis, new hypergradient formulation, single-loop single-timescale algorithm, (δ, ϵ)-stationary point


- [Position: Do pretrained Transformers Learn In-Context by Gradient Descent?](https://icml.cc/virtual/2024/poster/33845) (Oral)
  - **Authors:** [Lingfeng Shen](http://openreview.net/profile?id=~Lingfeng_Shen1), [Aayush Mishra](http://openreview.net/profile?id=~Aayush_Mishra1), [Daniel Khashabi](http://openreview.net/profile?id=~Daniel_Khashabi2)
  - **Affiliations:** Johns Hopkins University, Baltimore MD, Johns Hopkins University, Baltimore MD, Johns Hopkins University, Baltimore MD
  - **TL;DR:** This study investigates the relationship between In-Context Learning (ICL) and Gradient Descent (GD) in pre-trained language models, revealing that their behaviors differ significantly in various contexts. The findings suggest that the equivalence between ICL and GD is not established and warrants further investigation.
  - **Keywords:** In-Context Learning (ICL), Large Language Models (LLMs), Gradient Descent (GD), Equivalence between ICL and GD, sensitivity to demonstration order, Empirical analyses on language models, performance metrics comparison, LLaMa-7B, Transformer


- [Why Do Animals Need Shaping? A Theory of Task Composition and Curriculum Learning](https://icml.cc/virtual/2024/poster/34031) (Poster)
  - **Authors:** [Jin Hwa Lee](http://openreview.net/profile?id=~Jin_Hwa_Lee1), [Stefano Mannelli](http://openreview.net/profile?id=~Stefano_Sarao_Mannelli1), [Andrew Saxe](http://openreview.net/profile?id=~Andrew_M_Saxe1)
  - **Affiliations:** Sainsbury Wellcome Centre for Neural Circuits and Behaviour; Gatsby Computational Neuroscience Unit, University College London, London, UK, Sainsbury Wellcome Centre for Neural Circuits and Behaviour; Gatsby Computational Neuroscience Unit, University College London, London, UK, Sainsbury Wellcome Centre for Neural Circuits and Behaviour; Gatsby Computational Neuroscience Unit, University College London, London, UK
  - **TL;DR:** This study explores the significance of shaping in learning complex tasks, proposing a model that analyzes deep policy gradient learning for compositional reinforcement tasks. The findings reveal how effective shaping strategies can enhance learning efficiency and robustness by breaking down tasks into manageable components.
  - **Keywords:** Shaping, Curriculum Learning, Compositional Tasks, Deep Reinforcement Learning, Policy Gradient Learning, Systems Neuroscience, Behavioral Training, Learning Dynamics, Task Complexity, Analytical Understanding of Shaping Benefits, Learning Strategies, Compositionality, Systematic Compositionality


- [ReLUs Are Sufficient for Learning Implicit Neural Representations](https://icml.cc/virtual/2024/poster/32893) (Poster)
  - **Authors:** [Joseph Shenouda](http://openreview.net/profile?id=~Joseph_Shenouda1), [Yamin Zhou](http://openreview.net/profile?id=~Yamin_Zhou1), [Robert Nowak](http://openreview.net/profile?id=~Robert_D_Nowak1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Wisconsin-Madison, Department of Computer Sciences, University of Wisconsin-Madison, Department of Electrical and Computer Engineering, University of Wisconsin-Madison
  - **TL;DR:** This study demonstrates that deep neural networks using only ReLU activation functions can effectively learn implicit neural representations, addressing the spectral bias issue through specific constraints. The findings indicate that these networks can achieve state-of-the-art performance in various imaging tasks, including signal representation and computed tomography.
  - **Keywords:** Implicit Neural Representations (INRs), Deep Neural Networks (DNNs), Rectified Linear Unit (ReLU), B-spline wavelets, Computer graphics, Image processing, Biomedical imaging, Sparse-view computed tomography, Spectral bias, High-frequency function approximation, Hyperparameter selection, Regularity quantification of learned functions


- [LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies](https://icml.cc/virtual/2024/poster/34465) (Oral)
  - **Authors:** [Jia Shi](http://openreview.net/profile?id=~Jia_Shi2), [Gautam Rajendrakumar Gare](http://openreview.net/profile?id=~Gautam_Rajendrakumar_Gare1), [Jinjin Tian](http://openreview.net/profile?id=~Jinjin_Tian1), [Siqi Chai](http://openreview.net/profile?id=~Siqi_Chai1), [Zhiqiu Lin](http://openreview.net/profile?id=~Zhiqiu_Lin1), [Arun Balajee Vasudevan](http://openreview.net/profile?id=~Arun_Balajee_Vasudevan1), [Di Feng](http://openreview.net/profile?id=~Di_Feng1), [Francesco Ferroni](http://openreview.net/profile?id=~Francesco_Ferroni1), [Shu Kong](http://openreview.net/profile?id=~Shu_Kong1)
  - **Affiliations:** Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, Argo AI GmbH; Apple, Argo AI GmbH; Nvidia, Texas A&M University; University of Macau
  - **TL;DR:** This study introduces the LCA-on-the-Line framework to predict Out-of-Distribution (OOD) performance from In-Distribution (ID) measurements, revealing a strong correlation between ID LCA distance and OOD accuracy. The findings suggest that aligning model predictions with class taxonomies can enhance generalization, particularly for Visual-Language Models.
  - **Keywords:** Out-of-Distribution (OOD) generalization, In-Distribution (ID) performance, Lowest Common Ancestor (LCA) distance, K-means clustering, Vision Models (VMs), Visual-Language Models (VLMs), image recognition, Model robustness, distribution shifts, generalization challenges, Strong correlation between ID LCA distance and OOD accuracy, taxonomic hierarchy construction, ImageNet, ObjectNet, LAION, Effective robustness, class taxonomies


- [CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers](https://icml.cc/virtual/2024/poster/34682) (Poster)
  - **Authors:** [Dachuan Shi](http://openreview.net/profile?id=~Dachuan_Shi2), [Chaofan Tao](http://openreview.net/profile?id=~Chaofan_Tao1), [Anyi Rao](http://openreview.net/profile?id=~Anyi_Rao2), [Zhendong Yang](http://openreview.net/profile?id=~Zhendong_Yang2), [Chun Yuan](http://openreview.net/profile?id=~Chun_Yuan1), [Jiaqi Wang](http://openreview.net/profile?id=~Jiaqi_Wang1)
  - **Affiliations:** Tsinghua University; Shanghai AI Laboratory, The University of Hong Kong, Stanford University, Tsinghua University, Tsinghua University, Shanghai AI Laboratory
  - **TL;DR:** This paper presents CrossGET, an acceleration framework for vision-language Transformers that reduces computational costs by adaptively combining tokens during inference. The framework demonstrates effectiveness across various vision-language tasks while maintaining high performance.
  - **Keywords:** vision-language models, model acceleration, Cross-Guided Matching, Complete-Graph Soft Matching, image-text retrieval, visual reasoning, image captioning, visual question answering, high computational costs, token reduction, Cross-Guided Ensemble of Tokens (CrossGET), Transformers, CLIP, BLIP2, multimodal architectures


- [Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty](https://icml.cc/virtual/2024/poster/33004) (Poster)
  - **Authors:** [Laixi Shi](http://openreview.net/profile?id=~Laixi_Shi1), [Eric Mazumdar](http://openreview.net/profile?id=~Eric_Mazumdar1), [Yuejie Chi](http://openreview.net/profile?id=~Yuejie_Chi1), [Adam Wierman](http://openreview.net/profile?id=~Adam_Wierman1)
  - **Affiliations:** Department of Computing Mathematical Sciences, California Institute of Technology, CA 91125, USA, Department of Computing Mathematical Sciences, California Institute of Technology, CA 91125, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA, Department of Computing Mathematical Sciences, California Institute of Technology, CA 91125, USA
  - **TL;DR:** This study addresses the challenges of robustness in multi-agent reinforcement learning under environmental uncertainties by proposing a sample-efficient model-based algorithm (DR-NVI) for learning in distributionally robust Markov games. The findings highlight the importance of developing robust equilibrium strategies to mitigate the risks associated with environmental perturbations in strategic interactions among agents.
  - **Keywords:** Multi-Agent Reinforcement Learning, Robustness, Environmental Uncertainty, Distributionally Robust Markov Games, Model-Based Algorithms, DR-NVI, Ecosystem Protection, Board Games, Strategic Management, Autonomous Driving, Sim-to-Real Gap, Environmental Perturbations, Sensitivity of Equilibria, Finite-Sample Complexity Guarantees, Robust Equilibrium Strategies, Markov Games, Nash Equilibria, Correlated Equilibria, Coarse Correlated Equilibria


- [Simultaneous identification of models and parameters of scientific simulators](https://icml.cc/virtual/2024/poster/32726) (Poster)
  - **Authors:** [Cornelius Schröder](http://openreview.net/profile?id=~Cornelius_Schr%C3%B6der1), [Jakob Macke](http://openreview.net/profile?id=~Jakob_H._Macke1)
  - **Affiliations:** Machine Learning in Science, University of Tübingen; Tübingen AI Center, Germany, Machine Learning in Science, University of Tübingen; Max Planck Institute for Intelligent Systems, Department Empirical Inference, Tübingen, Germany
  - **TL;DR:** The study presents a simulation-based model inference (SBMI) method that utilizes neural networks to infer joint probability distributions over model components and parameters without requiring likelihood evaluations. It demonstrates the ability to discover multiple data-consistent model configurations and reveals non-identifiable components, providing a valuable tool for data-driven scientific inquiry.
  - **Keywords:** Bayesian inference, simulation-based inference, neural networks, conditional mixture of multivariate binary distributions, Grassmann formalism, neuroscience, computational models, model selection, non-identifiable model components, parameter inference, simulation-based model inference (SBMI), data-driven scientific inquiry, compositional stochastic simulator, likelihood evaluations


- [Improved Generalization of Weight Space Networks via Augmentations](https://icml.cc/virtual/2024/poster/35047) (Poster)
  - **Authors:** [Aviv Shamsian](http://openreview.net/profile?id=~Aviv_Shamsian1), [Aviv Navon](http://openreview.net/profile?id=~Aviv_Navon1), [David Zhang](http://openreview.net/profile?id=~David_W._Zhang1), [Yan Zhang](http://openreview.net/profile?id=~Yan_Zhang1), [Ethan Fetaya](http://openreview.net/profile?id=~Ethan_Fetaya1), [Gal Chechik](http://openreview.net/profile?id=~Gal_Chechik1), [Haggai Maron](http://openreview.net/profile?id=~Haggai_Maron1)
  - **Affiliations:** Bar-Ilan University, Bar-Ilan University, University of Amsterdam, Samsung - SAIT AI Lab, Montreal, Bar-Ilan University; NVIDIA Research, Bar-Ilan University; NVIDIA Research, Technion
  - **TL;DR:** This study investigates the overfitting issues in deep weight spaces (DWS) and proposes data augmentation strategies, including a MixUp method, to enhance generalization. The findings demonstrate that these methods can significantly improve performance in classification tasks, akin to having up to 10 times more data.
  - **Keywords:** deep weight spaces, neural networks, generalization, data augmentation, MixUp method, 2D and 3D neural fields, Implicit Neural Representations (INRs), neural view classification, overfitting, lack of diversity in datasets, generalization to new objects and neural views, improved performance in classification, gains in self-supervised contrastive learning, ModelNet40


- [Prompting is a Double-Edged Sword: Improving Worst-Group Robustness of Foundation Models](https://icml.cc/virtual/2024/poster/33460) (Poster)
  - **Authors:** [Amrith Setlur](http://openreview.net/profile?id=~Amrith_Setlur1), [Saurabh Garg](http://openreview.net/profile?id=~Saurabh_Garg3), [Virginia Smith](http://openreview.net/profile?id=~Virginia_Smith1), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1)
  - **Affiliations:** Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, UC Berkeley
  - **TL;DR:** This study investigates the robustness of foundation models against distribution shifts caused by spurious correlations, proposing a method called Prompting for Robustness (PfR) to enhance performance on minority groups. The findings indicate that PfR achieves performance nearly equivalent to an oracle algorithm that utilizes human-labeled spurious attributes.
  - **Keywords:** robustness, foundation models, distribution shift, empirical risk minimization (ERM), zero-shot prediction, few-shot performance, contrastive pretraining, vision tasks, language tasks, spurious correlations, hidden confounders, minority group performance, Prompting for Robustness (PfR), balanced performance across groups, ImageNet


- [Reducing sequential change detection to sequential estimation](https://icml.cc/virtual/2024/poster/34576) (Poster)
  - **Authors:** [Shubhanshu Shekhar](http://openreview.net/profile?id=~Shubhanshu_Shekhar1), [Aaditya Ramdas](http://openreview.net/profile?id=~Aaditya_Ramdas2)
  - **Affiliations:** Department of Statistics and Data Science, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University, Department of Statistics and Data Science, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University
  - **TL;DR:** The paper presents a method for sequential change detection that utilizes confidence sequences to detect changes in a data stream with minimal assumptions about the underlying distribution. The proposed scheme guarantees control over false alarms while aiming for a small detection delay when changes occur.
  - **Keywords:** Sequential change detection, Sequential estimation, Confidence sequences (CSs), Change detection, False alarms, Detection delay, Change detection scheme, Average run length (ARL), Nonparametric distribution classes, Changepoint


- [Online Learning with Bounded Recall](https://icml.cc/virtual/2024/poster/35004) (Poster)
  - **Authors:** [Jon Schneider](http://openreview.net/profile?id=~Jon_Schneider1), [Kiran Vodrahalli](http://openreview.net/profile?id=~Kiran_Vodrahalli1)
  - **Affiliations:** Google, Google
  - **TL;DR:** This study investigates online learning in the bounded recall setting, demonstrating that traditional no-regret algorithms incur constant regret, while a new stationary bounded-recall algorithm achieves a per-round regret of Θ(1/√M). The findings highlight the importance of considering the ordering of past losses in bounded-recall algorithms.
  - **Keywords:** online learning, bounded recall, decision making, no-regret learning algorithms, stationary bounded-recall algorithm, constant regret, recency bias, low-regret algorithms, Θ(1/√M) regret, tight lower bound, repeated games, human behavior modeling, adaptive learning algorithms, sequence models


- [How Far Can Fairness Constraints Help Recover From Biased Data?](https://icml.cc/virtual/2024/poster/34045) (Poster)
  - **Authors:** [Mohit Sharma](http://openreview.net/profile?id=~mohit_sharma5), [Amit Jayant Deshpande](http://openreview.net/profile?id=~Amit_Deshpande1)
  - **Affiliations:** Indraprastha Institute of Information Technology, Delhi, India, Microsoft Research India
  - **TL;DR:** This study explores how fairness constraints can help recover accurate and fair classifiers from biased data, challenging the belief that fairness incurs a trade-off with accuracy. The authors extend previous results to various fairness constraints and data distributions, demonstrating that optimal classifiers can be achieved even under extreme data bias conditions.
  - **Keywords:** Fair classification, Data bias, Fairness constraints, Equal opportunity constraints, Fair reject option classifiers, Fair machine learning pipelines, Systematic biases in training data, Fairness-accuracy trade-off, Recovery of accurate and fair classifiers, Generalization to arbitrary data distributions, Under-representation, Label bias, Massart noise


- [Exploring the Complexity of Deep Neural Networks through Functional Equivalence](https://icml.cc/virtual/2024/poster/34072) (Poster)
  - **Authors:** [Guohao Shen](http://openreview.net/profile?id=~Guohao_Shen2)
  - **Affiliations:** Department of Applied Mathematics, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong SAR, China
  - **TL;DR:** This study investigates the complexity of deep neural networks through functional equivalence, revealing that different parameterizations can yield the same network function. It finds that overparameterized networks are easier to train and provides insights into generalization and optimization in deep learning.
  - **Keywords:** deep neural networks, functional equivalence, overparameterization, machine learning, artificial intelligence, generalization ability, overfitting, complexity, optimization, complexity measure, insights into generalization and optimization


- [Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in low-data regimes](https://icml.cc/virtual/2024/poster/34800) (Poster)
  - **Authors:** [Nabeel Seedat](http://openreview.net/profile?id=~Nabeel_Seedat1), [Nicolas Huynh](http://openreview.net/profile?id=~Nicolas_Huynh1), [Boris van Breugel](http://openreview.net/profile?id=~Boris_van_Breugel2), [M van der Schaar](http://openreview.net/profile?id=~Mihaela_van_der_Schaar2)
  - **Affiliations:** University of Cambridge, University of Cambridge, University of Cambridge, University of Cambridge
  - **TL;DR:** This study introduces CLLM, a method that leverages Large Language Models for data augmentation in low-data regimes, addressing the challenge of insufficient data in machine learning. The results demonstrate that CLLM outperforms conventional data generators, providing high-quality augmented datasets for tabular data applications.
  - **Keywords:** low-data machine learning, data augmentation, tabular data, Large Language Models (LLMs), generative models, synthetic data generation, healthcare, finance, data scarcity, insufficient data, underrepresented minorities, CLLM (Curated LLM), principled curation mechanism, high-quality dataset, GANs (Generative Adversarial Networks), SMOTE, Normalizing Flows


- [Learning Decision Policies with Instrumental Variables through Double Machine Learning](https://icml.cc/virtual/2024/poster/33353) (Poster)
  - **Authors:** [Bill Daqian Shao](http://openreview.net/profile?id=~Daqian_Shao1), [Ashkan Soleymani](http://openreview.net/profile?id=~Ashkan_Soleymani1), [Francesco Quinzan](http://openreview.net/profile?id=~Francesco_Quinzan1), [Marta Kwiatkowska](http://openreview.net/profile?id=~Marta_Kwiatkowska1)
  - **Affiliations:** Department of Computer Science, University of Oxford, UK, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA, Department of Computer Science, University of Oxford, UK, Department of Computer Science, University of Oxford, UK
  - **TL;DR:** This paper presents DML-IV, a non-linear instrumental variable regression method that addresses bias in two-stage IV regressions to learn high-performing decision-making policies. The proposed method demonstrates strong convergence rates and outperforms existing IV regression techniques in the presence of hidden confounders.
  - **Keywords:** decision-making policies, causal relationships, spurious correlations, instrumental variable (IV) regression, double/debiased machine learning (DML), deep neural networks (DNN), econometrics, drug testing, social sciences, hidden confounders, bias in two-stage IV regressions, DML-IV algorithm, strong convergence rate, O(N−1/2) suboptimality guarantees, offline IV bandit problem


- [The Balanced-Pairwise-Affinities Feature Transform](https://icml.cc/virtual/2024/poster/33020) (Poster)
  - **Authors:** [Daniel Shalam](http://openreview.net/profile?id=~Daniel_Shalam1), [Simon Korman](http://openreview.net/profile?id=~Simon_Korman1)
  - **Affiliations:** Department of Computer Science, University of Haifa, Israel, Department of Computer Science, University of Haifa, Israel
  - **TL;DR:** The study introduces the Balanced-Pairwise-Affinities (BPA) feature transform, which enhances feature representation for set-input tasks by encoding high order relations among input items. The method demonstrates significant improvements in few-shot classification, unsupervised image clustering, and person re-identification, showcasing its efficiency and flexibility.
  - **Keywords:** Balanced-Pairwise-Affinities, feature transform, set-input problems, Optimal transport (OT) optimization, min-cost-max-flow fractional matching, Sinkhorn algorithm, Few-shot classification, unsupervised image clustering, person re-identification, Sub-optimal feature extraction, high order relations between input features, Efficient and flexible feature representation, state-of-the-art results


- [Why Larger Language Models Do In-context Learning Differently?](https://icml.cc/virtual/2024/poster/33874) (Poster)
  - **Authors:** [Zhenmei Shi](http://openreview.net/profile?id=~Zhenmei_Shi1), [Junyi Wei](http://openreview.net/profile?id=~Junyi_Wei1), [Zhuoyan Xu](http://openreview.net/profile?id=~Zhuoyan_Xu1), [Yingyiu Liang](http://openreview.net/profile?id=~Yingyu_Liang1)
  - **Affiliations:** University of Wisconsin-Madison, University of Wisconsin-Madison, University of Wisconsin-Madison, University of Wisconsin-Madison; The University of Hong Kong
  - **TL;DR:** This study investigates the differing in-context learning behaviors of large language models, revealing that larger models are more sensitive to noise and distractions compared to smaller models. The findings suggest that smaller models focus on important features, leading to greater robustness in ICL tasks.
  - **Keywords:** In-context learning (ICL), Large language models (LLM), Linear regression, Transformers, Attention mechanisms, AI, Natural language processing, Sensitivity to noise, Robustness to distractions, Optimal solutions for ICL, Understanding of LLM behavior


- [Neural-Kernel Conditional Mean Embeddings](https://icml.cc/virtual/2024/poster/35171) (Poster)
  - **Authors:** [Eiki Shimizu](http://openreview.net/profile?id=~Eiki_Shimizu1), [Kenji Fukumizu](http://openreview.net/profile?id=~Kenji_Fukumizu1), [Dino Sejdinovic](http://openreview.net/profile?id=~Dino_Sejdinovic1)
  - **Affiliations:** Department of Statistical Science, Graduate University of Advanced Studies (SOKENDAI), Tokyo, Japan; The Institute of Statistical Mathematics, Tokyo, Japan, The Institute of Statistical Mathematics, Tokyo, Japan, School of Computer and Mathematical Sciences, The University of Adelaide, Australia
  - **TL;DR:** This study introduces a novel method that integrates deep learning with kernel conditional mean embeddings (CMEs) to overcome scalability and expressiveness challenges in representing conditional distributions. The proposed NN-CME hybrid demonstrates competitive performance in conditional density estimation and shows versatility in reinforcement learning applications.
  - **Keywords:** Kernel conditional mean embeddings, deep learning, conditional distribution, Neural networks, kernel-based objective, Gram matrix inversion, Conditional density estimation, reinforcement learning, Scalability challenges, expressiveness challenges, high-dimensional data, NN-CME hybrid, hyperparameter optimization strategies, Reproducing kernel Hilbert space (RKHS), multimodality, skewness, heteroscedastic noise


- [Statistical Test for Attention Maps in Vision Transformers](https://icml.cc/virtual/2024/poster/32832) (Poster)
  - **Authors:** [Tomohiro Shiraishi](http://openreview.net/profile?id=~Tomohiro_Shiraishi1), [Daiki Miwa](http://openreview.net/profile?id=~Daiki_Miwa1), [Teruyuki Katsuoka](http://openreview.net/profile?id=~Teruyuki_Katsuoka1), [Vo Nguyen Le Duy](http://openreview.net/profile?id=~Vo_Nguyen_Le_Duy1), [Kouichi Taji](http://openreview.net/profile?id=~Kouichi_Taji1), [Ichiro Takeuchi](http://openreview.net/profile?id=~Ichiro_Takeuchi1)
  - **Affiliations:** Nagoya University, Aichi, Japan, Nagoya Institute of Technology, Aichi, Japan, Nagoya University, Aichi, Japan, University of Information Technology, Ho Chi Minh City, Vietnam; Vietnam National University, Ho Chi Minh City, Vietnam; RIKEN, Tokyo, Japan, Nagoya University, Aichi, Japan, Nagoya University, Aichi, Japan; RIKEN, Tokyo, Japan
  - **TL;DR:** This study proposes a statistical test for the attention mechanisms in Vision Transformers (ViT) to quantify their significance and control false positive rates in high-stakes decision-making tasks. The method is validated through numerical experiments and applications in medical diagnostics, addressing the challenge of attention focusing on irrelevant regions.
  - **Keywords:** Vision Transformer, Attention Mechanisms, Statistical Significance, Selective Inference, Statistical Test, Medical Diagnostics, Autonomous Driving, False Positive Detection, Selection Bias, Quantification of Attention Significance, Controlled Error Rate


- [IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics](https://icml.cc/virtual/2024/poster/34667) (Poster)
  - **Authors:** [Ekaterina Shumitskaya](http://openreview.net/profile?id=~Ekaterina_Shumitskaya1), [Anastasia Antsiferova](http://openreview.net/profile?id=~Anastasia_Antsiferova1), [Dmitriy Vatolin](http://openreview.net/profile?id=~Dmitriy_S._Vatolin1)
  - **Affiliations:** ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia; MSU Institute for Artificial Intelligence, Moscow, Russia; Lomonosov Moscow State University, Moscow, Russia, MSU Institute for Artificial Intelligence, Moscow, Russia; ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia, ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia; MSU Institute for Artificial Intelligence, Moscow, Russia; Lomonosov Moscow State University, Moscow, Russia
  - **TL;DR:** This paper presents an Invisible One-Iteration adversarial attack on no-reference image and video quality metrics, demonstrating superior visual quality and speed compared to existing methods. The findings highlight the vulnerabilities of learning-based metrics in video processing benchmarks and the potential for adversarial manipulation.
  - **Keywords:** No-reference image quality metrics, No-reference video quality metrics, Adversarial attacks, Invisible One-Iteration (IOI) adversarial attack, Video processing benchmarks, Image and video quality assessment, Vulnerabilities of NR metrics to adversarial attacks, Need for fast and imperceptible attacks, Superior visual quality in adversarial videos, High speed of attack


- [InterpreTabNet: Distilling Predictive Signals from Tabular Data by Salient Feature Interpretation](https://icml.cc/virtual/2024/poster/33065) (Spotlight Poster)
  - **Authors:** [Jacob Si](http://openreview.net/profile?id=~Jacob_Yoke_Hong_Si1), [Wendy Yusi Cheng](http://openreview.net/profile?id=~Wendy_Yusi_Cheng1), [Michael Cooper](http://openreview.net/profile?id=~Michael_Cooper2), [Rahul G. Krishnan](http://openreview.net/profile?id=~Rahul_G_Krishnan1)
  - **Affiliations:** University of Toronto; Vector Institute, University of Toronto, University of Toronto; Vector Institute, University of Toronto; Vector Institute
  - **TL;DR:** The study introduces InterpreTabNet, a modified TabNet model that enhances interpretability by using sparse attention masks to identify important features in tabular data while maintaining competitive predictive accuracy. The approach leverages large language models for post-hoc interpretability, addressing challenges in existing models.
  - **Keywords:** interpretability, predictive modeling, TabNet, Gumbel-Softmax distribution, KL Divergence regularizer, healthcare, insurance, finance, dense attention masks, overlapping feature selection, interpretability challenges, InterpreTabNet, sparse attribution masks, improved interpretability, large language models (LLMs), attention mechanism


- [Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts](https://icml.cc/virtual/2024/poster/33584) (Poster)
  - **Authors:** [Jiang-Xin Shi](http://openreview.net/profile?id=~Jiang-Xin_Shi1), [Tong Wei](http://openreview.net/profile?id=~Tong_Wei1), [Zhi Zhou](http://openreview.net/profile?id=~Zhi_Zhou2), [Jie-Jing Shao](http://openreview.net/profile?id=~Jie-Jing_Shao1), [Xin-Yan Han](http://openreview.net/profile?id=~Xin-Yan_Han1), [Yu-Feng Li](http://openreview.net/profile?id=~Yu-Feng_Li1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, School of Computer Science and Engineering, Southeast University, China; Key Laboratory of Computer Network and Information Integration, Southeast University, Ministry of Education, China, National Key Laboratory for Novel Software Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This study investigates the impact of fine-tuning on long-tail learning tasks, revealing that heavy fine-tuning can degrade performance on tail classes, while lightweight fine-tuning is more effective. The authors propose a new algorithm, LIFT, which achieves better predictive performance with reduced training time and complexity compared to existing methods.
  - **Keywords:** Long-tail learning, Foundation models, Fine-tuning, Lightweight fine-tuning, Heavy fine-tuning, Adaptive lightweight fine-tuning, Performance deterioration on tail classes, Inconsistent class conditions, Imbalanced data, LIFT algorithm, Reduced training time, Improved predictive performance


- [Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation](https://icml.cc/virtual/2024/poster/34596) (Poster)
  - **Authors:** [Zakhar Shumaylov](http://openreview.net/profile?id=~Zakhar_Shumaylov1), [Jeremy Budd](http://openreview.net/profile?id=~Jeremy_Budd1), [Subhadip Mukherjee](http://openreview.net/profile?id=~Subhadip_Mukherjee1), [Carola-Bibiane Schönlieb](http://openreview.net/profile?id=~Carola-Bibiane_Sch%C3%B6nlieb1)
  - **Affiliations:** University of Cambridge, UK, California Institute of Technology, USA, Department of Electronics and Electrical Communication Engineering, IIT Kharagpur, India, University of Cambridge, UK
  - **TL;DR:** This paper presents a generalized formulation of convergent regularisation for inverse problems using weakly convex regularisers, proving the convergence of the primal-dual hybrid gradient method and demonstrating improved performance in learned adversarial regularisation for computed tomography reconstruction.
  - **Keywords:** Inverse problems, Variational regularisation, Learned regularisation, Weakly convex regularisers, Primal-dual hybrid gradient method, Universal approximation for input weakly convex neural networks (IWCNN), Medical imaging (CT reconstruction), Art restoration, Ill-posedness of inverse problems, Noise in measurements, Convergence of critical points, O(logk/k) ergodic convergence rate


- [Latent variable model for high-dimensional point process with structured missingness](https://icml.cc/virtual/2024/poster/33443) (Poster)
  - **Authors:** [Maksim Sinelnikov](http://openreview.net/profile?id=~Maksim_Sinelnikov2), [Manuel Haussmann](http://openreview.net/profile?id=~Manuel_Haussmann1), [Harri Lähdesmäki](http://openreview.net/profile?id=~Harri_L%C3%A4hdesm%C3%A4ki1)
  - **Affiliations:** Department of Computer Science, Aalto University, Espoo, Finland; Department of Mathematics and Computer Science, University of Southern Denmark, Odense, Denmark, Department of Computer Science, Aalto University, Espoo, Finland; Department of Mathematics and Computer Science, University of Southern Denmark, Odense, Denmark, Department of Computer Science, Aalto University, Espoo, Finland; Department of Mathematics and Computer Science, University of Southern Denmark, Odense, Denmark
  - **TL;DR:** This study presents a novel deep latent variable model designed to effectively handle structured missingness in high-dimensional longitudinal data by utilizing Gaussian processes and variational autoencoders. The proposed model demonstrates competitive performance on both simulated and real datasets, addressing significant challenges in temporal data analysis.
  - **Keywords:** longitudinal data, high-dimensional data, structured missingness, latent-variable model, Gaussian processes, variational autoencoder, deep neural networks, healthcare, sociology, seismology, biomedical datasets, high-dimensional data, structured missingness patterns, unknown stochastic processes, scalable amortized variational inference, competitive performance, simulated datasets, real datasets


- [Byzantine Resilient and Fast Federated Few-Shot Learning](https://icml.cc/virtual/2024/poster/33009) (Poster)
  - **Authors:** [Ankit Pratap Singh](http://openreview.net/profile?id=~Ankit_Pratap_Singh1), [Namrata Vaswani](http://openreview.net/profile?id=~Namrata_Vaswani1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Iowa State University, Ames IA, USA, Department of Electrical and Computer Engineering, Iowa State University, Ames IA, USA
  - **TL;DR:** This study presents a Byzantine-resilient AltGDmin algorithm for low-dimensional linear representation learning in a federated setting, emphasizing its sample efficiency and communication efficiency. The findings highlight the algorithm's effectiveness in addressing challenges related to data sparsity and adversarial attacks.
  - **Keywords:** Byzantine resilience, federated learning, few-shot learning, AltGDmin algorithm, low-dimensional linear representation, Multi-task representation learning, federated subspace learning, Data sparsity, adversarial attacks, non-convex optimization, Byzantine-resilient solutions, sample-efficient algorithms, communication-efficient methods


- [Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?](https://icml.cc/virtual/2024/poster/32863) (Poster)
  - **Authors:** [Emanuel Sommer](http://openreview.net/profile?id=~Emanuel_Sommer1), [Lisa Wimmer](http://openreview.net/profile?id=~Lisa_Wimmer1), [Theodore Papamarkou](http://openreview.net/profile?id=~Theodore_Papamarkou1), [Ludwig Bothmann](http://openreview.net/profile?id=~Ludwig_Bothmann1), [Bernd Bischl](http://openreview.net/profile?id=~Bernd_Bischl1), [David Rügamer](http://openreview.net/profile?id=~David_R%C3%BCgamer1)
  - **Affiliations:** Department of Statistics, LMU Munich; Munich Center for Machine Learning, Munich, Germany, Department of Statistics, LMU Munich; Munich Center for Machine Learning, Munich, Germany, Department of Mathematics, The University of Manchester, Manchester, UK, Department of Statistics, LMU Munich; Munich Center for Machine Learning, Munich, Germany, Department of Statistics, LMU Munich; Munich Center for Machine Learning, Munich, Germany, Department of Statistics, LMU Munich; Munich Center for Machine Learning, Munich, Germany
  - **TL;DR:** This study explores the challenges of sample-based inference in Bayesian neural networks, highlighting the relationship between weight and function space and the impact of overparameterization. The authors propose a deep ensemble initialized approach that demonstrates competitive performance and effective uncertainty quantification.
  - **Keywords:** Bayesian neural networks, sample-based inference, uncertainty quantification, Overparameterization, multimodality of posteriors, computational demands, Deep ensemble initialized approach, practical guidelines for sampling and convergence diagnosis, JAX


- [Deletion-Anticipative Data Selection with a Limited Budget](https://icml.cc/virtual/2024/poster/33486) (Poster)
  - **Authors:** [Rachael Hwee Ling Sim](http://openreview.net/profile?id=~Rachael_Hwee_Ling_Sim1), [Jue Fan](http://openreview.net/profile?id=~Jue_Fan1), [Xiao Tian](http://openreview.net/profile?id=~Xiao_Tian1), [Patrick Jaillet](http://openreview.net/profile?id=~Patrick_Jaillet1), [Bryan Kian Hsiang Low](http://openreview.net/profile?id=~Bryan_Kian_Hsiang_Low1)
  - **Affiliations:** Department of Computer Science, National University of Singapore, Republic of Singapore, Department of Computer Science, National University of Singapore, Republic of Singapore, Department of Computer Science, National University of Singapore, Republic of Singapore, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA, Department of Computer Science, National University of Singapore, Republic of Singapore; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA
  - **TL;DR:** This study proposes a method for deletion-anticipative data selection (DADS) to maximize data utility in machine learning models while considering future data deletions under GDPR regulations. The findings suggest that learners can proactively estimate deletion probabilities to optimize their data selection process, even with budget constraints.
  - **Keywords:** Data selection, Active learning, Machine learning, Supervised data subset selection, Deletion-anticipative data selection (DADS), Healthcare, Online services (e.g., Netflix, Amazon), Data utility preservation, GDPR right to erasure, Limited budget for data acquisition, Expected post-deletion utility maximization, Risk-averse utility optimization


- [Multi-Agent Reinforcement Learning with Hierarchical Coordination for Emergency Responder Stationing](https://icml.cc/virtual/2024/poster/33972) (Poster)
  - **Authors:** [Amutheezan Sivagnanam](http://openreview.net/profile?id=~Amutheezan_Sivagnanam1), [Ava Pettet](http://openreview.net/profile?id=~Ava_Pettet1), [Hunter Lee](http://openreview.net/profile?id=~Hunter_Lee1), [Ayan Mukhopadhyay](http://openreview.net/profile?id=~Ayan_Mukhopadhyay1), [Abhishek Dubey](http://openreview.net/profile?id=~Abhishek_Dubey1), [Aron Laszka](http://openreview.net/profile?id=~Aron_Laszka1)
  - **Affiliations:** Pennsylvania State University, University Park, PA, USA, Vanderbilt University, Nashville, TN, USA, Vanderbilt University, Nashville, TN, USA, Vanderbilt University, Nashville, TN, USA, Vanderbilt University, Nashville, TN, USA, Pennsylvania State University, University Park, PA, USA
  - **TL;DR:** This study introduces a novel reinforcement learning approach for optimizing the stationing of emergency responders, significantly reducing decision-making time while slightly improving ambulance response times. The proposed method addresses the computational challenges of proactive repositioning in emergency response management systems.
  - **Keywords:** Emergency responder management, resource allocation, proactive repositioning, Multi-Agent Reinforcement Learning, actor-critic methods, transformers, Monte Carlo tree search, Emergency medical services, urban emergency response, Computational challenges in resource allocation, uncertainty in demand, combinatorial state-action space, Reduction in computation time, optimization of responder waiting locations, Real-world data from Nashville, TN and Seattle, WA, Hierarchical coordination, spatial decomposition


- [Domain Generalisation via Imprecise Learning](https://icml.cc/virtual/2024/poster/32677) (Spotlight Poster)
  - **Authors:** [Anurag Singh](http://openreview.net/profile?id=~Anurag_Singh3), [Siu Lun Chau](http://openreview.net/profile?id=~Siu_Lun_Chau1), [Shahine Bouabid](http://openreview.net/profile?id=~Shahine_Bouabid1), [Krikamol Muandet](http://openreview.net/profile?id=~Krikamol_Muandet1)
  - **Affiliations:** CISPA Helmholtz Center for Information Security, Saarbrücken, Germany; Graduate School of Computer Science, Saarland University, Saarbrücken, Germany, CISPA Helmholtz Center for Information Security, Saarbrücken, Germany, Department of Statistics, University of Oxford, UK, CISPA Helmholtz Center for Information Security, Saarbrücken, Germany; Graduate School of Computer Science, Saarland University, Saarbrücken, Germany
  - **TL;DR:** The study introduces the Imprecise Domain Generalisation framework to address the challenges of out-of-distribution generalisation by allowing learners to optimise against a spectrum of generalisation strategies. It highlights the importance of integrating imprecision into domain generalisation to better align with the preferences of model operators during deployment.
  - **Keywords:** Out-of-distribution generalisation, Imprecise learning, Imprecise risk optimisation, Generalisation uncertainty, Distribution shifts, Adversarial attacks, Imprecise Domain Generalisation framework


- [Inexact Newton-type Methods for Optimisation with Nonnegativity Constraints](https://icml.cc/virtual/2024/poster/33058) (Poster)
  - **Authors:** [Oscar Smee](http://openreview.net/profile?id=~Oscar_Smee1), [Fred Roosta](http://openreview.net/profile?id=~Fred_Roosta1)
  - **Affiliations:** School of Mathematics and Physics, University of Queensland, Brisbane, Australia; ARC Training Centre for Information Resilience, Brisbane, Australia, School of Mathematics and Physics, University of Queensland, Brisbane, Australia; ARC Training Centre for Information Resilience, Brisbane, Australia
  - **TL;DR:** This paper presents novel two-metric projection algorithms for solving large scale nonconvex optimisation problems with nonnegativity constraints, addressing challenges such as ill-conditioning and saddle points. The proposed methods demonstrate state-of-the-art convergence rates and competitive performance in practical applications.
  - **Keywords:** nonconvex optimisation, nonnegativity constraints, machine learning, two-metric projection framework, projected gradient descent, projected Newton method, minimum residual (MINRES) method, nonnegative least-squares, nonnegative matrix factorisation, sparsity-inducing regularisation, ill-conditioned problems, saddle points, high-dimensional problems, novel two-metric projection type algorithms, state-of-the-art convergence rates


- [Harnessing the Power of Neural Operators with Automatically Encoded Conservation Laws](https://icml.cc/virtual/2024/poster/33964) (Spotlight Poster)
  - **Authors:** [Ning Liu](http://openreview.net/profile?id=~Ning_Liu6), [Yiming Fan](http://openreview.net/profile?id=~Yiming_Fan1), [Xianyi Zeng](http://openreview.net/profile?id=~Xianyi_Zeng1), [Milan Klöwer](http://openreview.net/profile?id=~Milan_Kl%C3%B6wer1), [LU ZHANG](http://openreview.net/profile?id=~LU_ZHANG18), [Yue Yu](http://openreview.net/profile?id=~Yue_Yu3)
  - **Affiliations:** Global Engineering and Materials, Inc., Princeton, NJ 08540, USA, Department of Mathematics, Lehigh University, Bethlehem, PA 18015, USA, Department of Mathematics, Lehigh University, Bethlehem, PA 18015, USA, Earth, Atmospheric and Planetary Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139, USA, Department of Mathematics, Lehigh University, Bethlehem, PA 18015, USA, Department of Mathematics, Lehigh University, Bethlehem, PA 18015, USA
  - **TL;DR:** This study introduces conservation law-encoded neural operators (clawNOs) that automatically satisfy fundamental conservation laws in modeling complex physical systems. The results demonstrate that clawNOs significantly outperform existing neural operators, particularly in scenarios with limited data.
  - **Keywords:** Neural Operators, Conservation Laws, Scientific Machine Learning, Conservation Law-Encoded Neural Operators (clawNOs), Divergence-Free Prediction, Material Deformation Modeling, Incompressible Fluid Dynamics, Atmospheric Simulation, Approximate Satisfaction of Conservation Laws, Learning Efficacy in Small-Data Regimes, Improved Learning Efficacy, Automatic Satisfaction of Conservation Laws


- [In-Context Reinforcement Learning for Variable Action Spaces](https://icml.cc/virtual/2024/poster/33021) (Poster)
  - **Authors:** [Viacheslav Sinii](http://openreview.net/profile?id=~Viacheslav_Sinii1), [Alexander Nikulin](http://openreview.net/profile?id=~Alexander_Nikulin1), [Vladislav Kurenkov](http://openreview.net/profile?id=~Vladislav_Kurenkov1), [Ilya Zisman](http://openreview.net/profile?id=~Ilya_Zisman1), [Sergey Kolesnikov](http://openreview.net/profile?id=~Sergey_Kolesnikov1)
  - **Affiliations:** Tinkoff, Moscow, Russia; Innopolis University, AIRI, Moscow, Russia; MIPT, AIRI, Moscow, Russia, AIRI, Moscow, Russia, Tinkoff, Moscow, Russia
  - **TL;DR:** This study introduces the Headless-AD model, which effectively generalizes to variable discrete action spaces in reinforcement learning without the need for retraining. The findings demonstrate that Headless-AD outperforms specialized models in various configurations, addressing the limitations of existing approaches in adapting to new action spaces.
  - **Keywords:** reinforcement learning, in-context learning, transformers, Headless-AD model, Algorithm Distillation (AD), sequential decision-making, gridworld environments, contextual bandits, adaptability to new action spaces, reliance on predefined action space size, generalization to variable action spaces, performance evaluation on unseen actions


- [Embarrassingly Parallel GFlowNets](https://icml.cc/virtual/2024/poster/34347) (Poster)
  - **Authors:** [Tiago Silva](http://openreview.net/profile?id=~Tiago_Silva4), [Luiz Carvalho](http://openreview.net/profile?id=~Luiz_Max_Carvalho1), [Amauri Souza](http://openreview.net/profile?id=~Amauri_H_Souza1), [Samuel Kaski](http://openreview.net/profile?id=~Samuel_Kaski1), [Diego Mesquita](http://openreview.net/profile?id=~Diego_Mesquita1)
  - **Affiliations:** Getulio Vargas Foundation, Getulio Vargas Foundation, Aalto University; Federal Institute of Ceará, Aalto University; University of Manchester, Getulio Vargas Foundation
  - **TL;DR:** The study introduces embarrassingly parallel GFlowNets (EP-GFlowNet) as a method to efficiently sample from discrete distributions, addressing challenges in large-scale posterior sampling and client-server communication in federated settings. The proposed method demonstrates effectiveness across various tasks, including Bayesian phylogenetics and multi-objective optimization.
  - **Keywords:** GFlowNets, MCMC sampling, discrete compositional random variables, Generative Flow Networks, divide-and-conquer method, Bayesian phylogenetics, multi-objective optimization, federated learning, Large-scale posterior sampling, client-server communication bottleneck, Embarrassingly parallel GFlowNet (EP-GFlowNet), sampling from product distributions


- [Probabilistic Modeling of Interpersonal Coordination Processes](https://icml.cc/virtual/2024/poster/34997) (Poster)
  - **Authors:** [Paulo Soares](http://openreview.net/profile?id=~Paulo_Soares1), [Adarsh Pyarelal](http://openreview.net/profile?id=~Adarsh_Pyarelal1), [Meghavarshini Krishnaswamy](http://openreview.net/profile?id=~Meghavarshini_Krishnaswamy1), [Emily Butler](http://openreview.net/profile?id=~Emily_Butler3), [Kobus Barnard](http://openreview.net/profile?id=~Kobus_Barnard1)
  - **Affiliations:** University of Arizona, University of Arizona, University of Arizona, University of Arizona, University of Arizona
  - **TL;DR:** This study presents a novel probabilistic model for interpersonal coordination, demonstrating its predictive power for team performance in virtual search and rescue missions. The findings indicate that coordination metrics can effectively capture team dynamics, particularly in later missions, and that incorporating semantic analysis enhances prediction accuracy.
  - **Keywords:** interpersonal coordination, collaboration, competition, probabilistic modeling, latent process analysis, virtual search and rescue (SAR) missions, team performance evaluation, temporal influence, coordination recovery despite noise, learning effects in team dynamics, predictive coordination metrics, improved prediction with semantic modalities, synthetic data, real data from SAR missions


- [Parallelized Spatiotemporal Slot Binding for Videos](https://icml.cc/virtual/2024/poster/34326) (Poster)
  - **Authors:** [Gautam Singh](http://openreview.net/profile?id=~Gautam_Singh3), [Yue Wang](http://openreview.net/profile?id=~Yue_Wang2), [Jiawei Yang](http://openreview.net/profile?id=~Jiawei_Yang1), [Boris Ivanovic](http://openreview.net/profile?id=~Boris_Ivanovic1), [Sungjin Ahn](http://openreview.net/profile?id=~Sungjin_Ahn1), [Marco Pavone](http://openreview.net/profile?id=~Marco_Pavone1), [Tong Che](http://openreview.net/profile?id=~Tong_Che1)
  - **Affiliations:** Rutgers University, NVIDIA Research; University of Southern California, University of Southern California, NVIDIA Research, KAIST, NVIDIA Research; Stanford University, NVIDIA Research
  - **TL;DR:** This study introduces the Parallelizable Spatiotemporal Binder (PSB), a novel architecture for object-centric learning that processes sequential inputs in parallel, significantly improving training speed and stability on longer sequences. The results demonstrate that PSB achieves performance comparable to or better than existing methods in unsupervised 2D and 3D scene decomposition.
  - **Keywords:** object-centric learning, sequential inputs, scalable architectures, Parallelizable Spatiotemporal Binder (PSB), causal attention, auto-encoding, video data analysis, object-centric scene decomposition, training instability, gradient vanishing/exploding, long-range dependencies, increased training speed, stable training on longer sequences, improved performance on scene understanding, RNN (Recurrent Neural Networks), slots, self-supervised learning


- [Sparse is Enough in Fine-tuning Pre-trained Large Language Models](https://icml.cc/virtual/2024/poster/35168) (Spotlight Poster)
  - **Authors:** [Weixi Song](http://openreview.net/profile?id=~Weixi_Song2), [Zuchao Li](http://openreview.net/profile?id=~Zuchao_Li1), [Lefei Zhang](http://openreview.net/profile?id=~Lefei_Zhang1), [hai zhao](http://openreview.net/profile?id=~hai_zhao1), [Bo Du](http://openreview.net/profile?id=~Bo_Du3)
  - **Affiliations:** National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, 430072, P. R. China; Hubei Luojia Laboratory, Wuhan 430072, P. R. China, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, 430072, P. R. China; Hubei Luojia Laboratory, Wuhan 430072, P. R. China, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, 430072, P. R. China; Hubei Luojia Laboratory, Wuhan 430072, P. R. China, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, P. R. China, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, 430072, P. R. China; Hubei Luojia Laboratory, Wuhan 430072, P. R. China
  - **TL;DR:** This paper investigates efficient adaptation of pre-trained large language models to downstream tasks using a new gradient-based sparse fine-tuning algorithm called Sparse Increment Fine-Tuning (SIFT). The study demonstrates that pre-training leads to a tighter generalization error bound, allowing for effective performance with fewer parameters.
  - **Keywords:** Parameter-Efficient Fine-Tuning, Pre-trained Language Models, PAC-Bayesian generalization error bound, Sparse Increment Fine-Tuning (SIFT), Downstream tasks, GLUE Benchmark, Instruction-tuning, Efficient adaptation, high cost of fine-tuning large models, Gradient-based sparse fine-tuning algorithm, GLUE Benchmark, Large Language Models (LLMs), pre-training, fine-tuning


- [Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates](https://icml.cc/virtual/2024/poster/34767) (Poster)
  - **Authors:** [Zhenqiao Song](http://openreview.net/profile?id=~Zhenqiao_Song1), [Yunlong Zhao](http://openreview.net/profile?id=~Yunlong_Zhao3), [Wenxian Shi](http://openreview.net/profile?id=~Wenxian_Shi1), [Wengong Jin](http://openreview.net/profile?id=~Wengong_Jin1), [Yang Yang](http://openreview.net/profile?id=~Yang_Yang55), [Lei Li](http://openreview.net/profile?id=~Lei_Li11)
  - **Affiliations:** Language Technologies Institute, Carnegie Mellon University, Department of Chemistry, Massachusetts Institute of Technology, Department of EECS, Massachusetts Institute of Technology, Broad Institute of MIT and Harvard, Department of Chemistry and Biochemistry, University of California Santa Barbara, Language Technologies Institute, Carnegie Mellon University
  - **TL;DR:** This study introduces EnzyGen, a unified generative model for designing functional enzymes by leveraging functionally important sites and small-molecule substrates. The model demonstrates superior performance in binding affinity across various enzyme families, significantly advancing the field of enzyme design.
  - **Keywords:** enzyme design, biocatalysts, protein design, generative model, attention network, neighborhood equivariant layers, pharmaceuticals, specialty chemicals, biofuels, enzyme-substrate interaction, lack of fitness data, unknown enzyme structures, EnzyGen model, EnzyBench dataset, improved substrate binding affinity, EnzyBench, protein data bank (PDB), functionally important sites, amino acid sequence, 3D coordinates


- [CHEMREASONER: Heuristic Search over a Large Language Model’s Knowledge Space using Quantum-Chemical Feedback](https://icml.cc/virtual/2024/poster/35045) (Poster)
  - **Authors:** [Henry W. Sprueill](http://openreview.net/profile?id=~Henry_W._Sprueill1), [Carl Edwards](http://openreview.net/profile?id=~Carl_Edwards1), [Khushbu Agarwal](http://openreview.net/profile?id=~Khushbu_Agarwal2), [Mariefel Olarte](http://openreview.net/profile?id=~Mariefel_V_Olarte1), [Udishnu Sanyal](http://openreview.net/profile?id=~Udishnu_Sanyal1), [Conrad Johnston](http://openreview.net/profile?id=~Conrad_Johnston1), [Hongbin Liu](http://openreview.net/profile?email=hongbin.liu@microsoft.com), [Heng Ji](http://openreview.net/profile?id=~Heng_Ji3), [Sutanay Choudhury](http://openreview.net/profile?id=~Sutanay_Choudhury2)
  - **Affiliations:** Pacific Northwest National Laboratory, University of Illinois Urbana-Champaign, Pacific Northwest National Laboratory, Pacific Northwest National Laboratory, Pacific Northwest National Laboratory, Azure Quantum, Microsoft, Azure Quantum, Microsoft, University of Illinois Urbana-Champaign, Pacific Northwest National Laboratory
  - **TL;DR:** This study presents an AI-guided framework for discovering new catalysts by combining large language model reasoning with quantum-chemical feedback, addressing the challenges of identifying optimal chemical descriptors. The approach demonstrates competitive performance in catalyst discovery, paving the way for more efficient chemical processes.
  - **Keywords:** catalyst discovery, AI-guided computational screening, large language models (LLMs), atomistic graph neural networks (GNNs), chemical processes, sustainable chemistry, uncertainty in catalyst discovery, combinatorial reasoning over chemical descriptors, integration of linguistic reasoning with quantum-chemical feedback, autonomous exploration methods


- [Learning to Intervene on Concept Bottlenecks](https://icml.cc/virtual/2024/poster/33434) (Poster)
  - **Authors:** [David Steinmann](http://openreview.net/profile?id=~David_Steinmann1), [Wolfgang Stammer](http://openreview.net/profile?id=~Wolfgang_Stammer1), [Felix Friedrich](http://openreview.net/profile?id=~Felix_Friedrich1), [Kristian Kersting](http://openreview.net/profile?id=~Kristian_Kersting1)
  - **Affiliations:** Artificial Intelligence and Machine Learning Group, TU Darmstadt, Germany; Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany; Centre for Cognitive Science, TU Darmstadt, Germany; German Center for Artificial Intelligence (DFKI), Artificial Intelligence and Machine Learning Group, TU Darmstadt, Germany; Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany, Artificial Intelligence and Machine Learning Group, TU Darmstadt, Germany; Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany, Artificial Intelligence and Machine Learning Group, TU Darmstadt, Germany; Hessian Center for Artificial Intelligence (hessian.AI), Darmstadt, Germany; Centre for Cognitive Science, TU Darmstadt, Germany; German Center for Artificial Intelligence (DFKI)
  - **TL;DR:** This study introduces concept bottleneck memory models (CB2Ms) that retain a memory of past interventions to improve the performance of concept bottleneck models (CBMs) in deep learning. The findings demonstrate that CB2Ms can generalize interventions to unseen data and identify wrongly inferred concepts, thus enhancing user interaction and reducing the need for repetitive feedback.
  - **Keywords:** explainable artificial intelligence, concept bottleneck models, deep learning interpretability, concept bottleneck memory models (CB2Ms), interventional interactions, model interpretability, handling distribution shifts, confounded data, generalization of interventions, automatic improvement of model performance, concept activations, bottleneck network, predictor network


- [SurfPro: Functional Protein Design Based on Continuous Surface](https://icml.cc/virtual/2024/poster/33699) (Poster)
  - **Authors:** [Zhenqiao Song](http://openreview.net/profile?id=~Zhenqiao_Song1), [Tinglin Huang](http://openreview.net/profile?id=~Tinglin_Huang1), [Lei Li](http://openreview.net/profile?id=~Lei_Li11), [Wengong Jin](http://openreview.net/profile?id=~Wengong_Jin1)
  - **Affiliations:** Language Technologies Institute, Carnegie Mellon University, Pittsburgh, the United States, Yale University, New Haven, United States, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, the United States, Broad Institute of MIT and Harvard, Boston United States
  - **TL;DR:** This paper introduces SurfPro, a novel method for designing functional proteins by integrating geometric shapes with biochemical properties. The method outperforms existing techniques in generating protein sequences that achieve high recovery rates and successful interactions in protein design tasks.
  - **Keywords:** functional protein design, protein engineering, hierarchical encoder, autoregressive decoder, local graph convolutions, global self-attention, protein binder design, enzyme design, inverse folding, geometric constraints, biochemical property constraints, SurfPro method, recovery rate on CATH 4.2, protein-protein binding success rates, enzyme-substrate interaction scores, CATH 4.2


- [Private Truly-Everlasting Robust-Prediction](https://icml.cc/virtual/2024/poster/34717) (Oral)
  - **Authors:** [Uri Stemmer](http://openreview.net/profile?id=~Uri_Stemmer1)
  - **Affiliations:** Tel Aviv University; Google Research
  - **TL;DR:** This paper introduces Private Everlasting Prediction (PEP), a model for differentially private learning that allows for an unbounded number of prediction queries while ensuring privacy for both the training set and the queries. The authors present improvements to the PEP framework, including robustness against poisoning attacks and a relaxed privacy definition.
  - **Keywords:** Private Learning, Differential Privacy, Private Everlasting Prediction, Prediction Oracle, Robustness against Poisoning Attacks, Privacy in Learning, Resource Requirements for Private Learning, Conceptual Modifications to PEP, Relaxed Privacy Definition, Improved Sample Complexity and Runtime, PAC Learning, Differential Privacy


- [OSN: Infinite Representations of Dynamic 3D Scenes from Monocular Videos](https://icml.cc/virtual/2024/poster/33973) (Poster)
  - **Authors:** [Ziyang Song](http://openreview.net/profile?id=~Ziyang_Song1), [Jinxi Li](http://openreview.net/profile?id=~Jinxi_Li2), [Bo Yang](http://openreview.net/profile?id=~Bo_Yang7)
  - **Affiliations:** Shenzhen Research Institute, The Hong Kong Polytechnic University, Shenzhen, China; vLAR Group, The Hong Kong Polytechnic University, Hung Hom, HKSAR, Shenzhen Research Institute, The Hong Kong Polytechnic University, Shenzhen, China; vLAR Group, The Hong Kong Polytechnic University, Hung Hom, HKSAR, Shenzhen Research Institute, The Hong Kong Polytechnic University, Shenzhen, China; vLAR Group, The Hong Kong Polytechnic University, Hung Hom, HKSAR
  - **TL;DR:** This paper presents a framework called OSN that learns all plausible 3D scene configurations from monocular videos, addressing the challenge of recovering dynamic 3D representations. The method outperforms existing approaches in dynamic novel view synthesis and demonstrates significant advantages in fine-grained 3D scene geometry learning.
  - **Keywords:** dynamic 3D scene representation, monocular video analysis, object scale network, joint optimization module, dynamic novel view synthesis, 3D shape reconstruction, ill-posed problem of dynamic 3D scene modeling, multiple geometric explanations, superior accuracy in dynamic scene configurations, learning fine-grained 3D scene geometry, synthetic datasets, real-world datasets


- [RLVF: Learning from Verbal Feedback without Overgeneralization](https://icml.cc/virtual/2024/poster/33110) (Poster)
  - **Authors:** [Moritz Stephan](http://openreview.net/profile?id=~Moritz_Pascal_Stephan1), [Alexander Khazatsky](http://openreview.net/profile?id=~Alexander_Khazatsky1), [Eric Mitchell](http://openreview.net/profile?id=~Eric_Mitchell1), [Annie Chen](http://openreview.net/profile?id=~Annie_S_Chen1), [Sheryl Hsu](http://openreview.net/profile?id=~Sheryl_Hsu1), [Archit Sharma](http://openreview.net/profile?id=~Archit_Sharma1), [Chelsea Finn](http://openreview.net/profile?id=~Chelsea_Finn1)
  - **Affiliations:** Department of Computer Science, Stanford University, CA, USA, None, None, None, None, None, None
  - **TL;DR:** The study introduces C3PO, a method for adapting large language models to high-level verbal feedback while minimizing overgeneralization. Experimental results show that C3PO effectively applies feedback in relevant contexts and preserves existing behaviors, achieving a 30% reduction in overgeneralization compared to current methods.
  - **Keywords:** verbal feedback, model customization, large language models, Contextualized Critiques with Constrained Preference Optimization (C3PO), reinforcement learning from human feedback (RLHF), overgeneralization, context-dependent feedback, reduction of overgeneralization by 30%, synthetic preference dataset generation


- [ReGAL: Refactoring Programs to Discover Generalizable Abstractions](https://icml.cc/virtual/2024/poster/34523) (Poster)
  - **Authors:** [Elias Stengel-Eskin](http://openreview.net/profile?id=~Elias_Stengel-Eskin1), [Archiki Prasad](http://openreview.net/profile?id=~Archiki_Prasad1), [Mohit Bansal](http://openreview.net/profile?id=~Mohit_Bansal2)
  - **Affiliations:** UNC Chapel Hill, UNC Chapel Hill, UNC Chapel Hill
  - **TL;DR:** The study introduces REGAL, a method for learning reusable functions through code refactorization to enhance program synthesis with large language models. It demonstrates significant accuracy improvements across various domains by utilizing shared function libraries, addressing issues of redundancy and lack of abstraction in program generation.
  - **Keywords:** program synthesis, reusable functions, code refactorization, gradient-free method, code refactorization, LOGO graphics generation, date reasoning, text-based games, mathematics, TabMWP, redundancy in code generation, lack of reusability, lack of abstraction, shared function libraries, improved accuracy in program prediction, LOGO, Date reasoning, TextCraft, MATH, TabMWP, large language models (LLMs), abstractions


- [Online Learning in CMDPs: Handling Stochastic and Adversarial Constraints](https://icml.cc/virtual/2024/poster/34067) (Poster)
  - **Authors:** [Francesco Emanuele Stradi](http://openreview.net/profile?id=~Francesco_Emanuele_Stradi1), [Jacopo Germano](http://openreview.net/profile?id=~Jacopo_Germano1), [Gianmarco Genalti](http://openreview.net/profile?id=~Gianmarco_Genalti1), [Matteo Castiglioni](http://openreview.net/profile?id=~Matteo_Castiglioni1), [Alberto Marchesi](http://openreview.net/profile?id=~Alberto_Marchesi1), [Nicola Gatti](http://openreview.net/profile?id=~Nicola_Gatti1)
  - **Affiliations:** Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy
  - **TL;DR:** This paper presents a novel algorithm for online learning in constrained Markov decision processes (CMDPs) that effectively manages both stochastic and adversarial constraints while maximizing rewards. The proposed method achieves state-of-the-art performance in terms of regret and constraint violation, addressing a significant gap in the existing literature on CMDPs.
  - **Keywords:** online learning, constrained Markov decision processes (CMDPs), reinforcement learning, autonomous driving, automated bidding, recommender systems, long-term constraints, cumulative regret, constraint violation, best-of-both-worlds algorithm, state-of-the-art regret and constraint violation bounds, Markov decision processes (MDPs), episodic CMDPs


- [Hybrid Reinforcement Learning from Offline Observation Alone](https://icml.cc/virtual/2024/poster/33606) (Poster)
  - **Authors:** [Yuda Song](http://openreview.net/profile?id=~Yuda_Song2), [J. Bagnell](http://openreview.net/profile?id=~Drew_Bagnell2), [Aarti Singh](http://openreview.net/profile?id=~Aarti_Singh1)
  - **Affiliations:** Carnegie Mellon University; Aurora Innovation, Carnegie Mellon University; Aurora Innovation, Carnegie Mellon University
  - **TL;DR:** This study introduces a hybrid reinforcement learning framework that utilizes offline observation-only datasets, addressing the challenges of competing with policies covered by such data. The authors propose a novel algorithm that matches the performance of those using richer data models, demonstrating its effectiveness through experiments.
  - **Keywords:** hybrid reinforcement learning, offline observation datasets, trace model, reset model, algorithm development, interactive decision making, robotics, data sparsity, distribution shifts, action generalization, new algorithm for trace model setting, proof-of-concept experiments


- [Latent Logic Tree Extraction for Event Sequence Explanation from LLMs](https://icml.cc/virtual/2024/poster/33016) (Poster)
  - **Authors:** [Zitao Song](http://openreview.net/profile?id=~Zitao_Song1), [Chao Yang](http://openreview.net/profile?id=~Chao_Yang9), [Chaojie Wang](http://openreview.net/profile?id=~Chaojie_Wang1), [Bo An](http://openreview.net/profile?id=~Bo_An2), [Shuang Li](http://openreview.net/profile?id=~Shuang_Li3)
  - **Affiliations:** School of Computer Science and Engineering, Nanyang Technological University, Singapore, School of Data Science, The Chinese University of Hong Kong, Shenzhen, China, Skywork AI, Singapore, Skywork AI, Singapore, School of Data Science, The Chinese University of Hong Kong, Shenzhen, China
  - **TL;DR:** This study presents a method for extracting logic tree-based explanations from event sequences using Large Language Models (LLMs), focusing on healthcare and robotics applications. The proposed framework demonstrates effective performance in generating customized insights and addressing the challenges posed by heterogeneous event data.
  - **Keywords:** Event sequence explanation, Large Language Models, Temporal point process model, Amortized Expectation-Maximization (EM) learning, GFlowNet, Healthcare, Robotics, Heterogeneity in event sequences, Generating concise medical knowledge, Logic tree-based explanations, Customized insights from LLMs, Logic trees, Latent variables, Posterior distribution, Explainability, AI Safety, Large Language Models


- [Designing Decision Support Systems using Counterfactual Prediction Sets](https://icml.cc/virtual/2024/poster/32939) (Spotlight Poster)
  - **Authors:** [Eleni Straitouri](http://openreview.net/profile?id=~Eleni_Straitouri1), [Manuel Gomez-Rodriguez](http://openreview.net/profile?id=~Manuel_Gomez_Rodriguez1)
  - **Affiliations:** Max Planck Institute for Software Systems, Kaiserslautern, Germany, Max Planck Institute for Software Systems, Kaiserslautern, Germany
  - **TL;DR:** This study develops a novel decision support system that utilizes counterfactual prediction sets to enhance expert decision-making in classification tasks. The findings indicate that limiting experts' agency can lead to improved performance compared to allowing unrestricted decision-making.
  - **Keywords:** Decision Support Systems, Classification Tasks, Human Expert Interaction, Conformal Prediction, Counterfactual Prediction Sets, Online Learning, Medicine, Education, Criminal Justice, Prediction Accuracy, Expert Decision-Making, Misleading Predictions, Methodology for Prediction Sets, Improvement in Regret, Human Subject Study, Agency in Decision-Making


- [QORA: Zero-Shot Transfer via Interpretable Object-Relational Model Learning](https://icml.cc/virtual/2024/poster/33052) (Poster)
  - **Authors:** [Gabriel Stella](http://openreview.net/profile?id=~Gabriel_Stella1), [Dmitri Loguinov](http://openreview.net/profile?id=~Dmitri_Loguinov1)
  - **Affiliations:** Department of Computer Science & Engineering, Texas A&M University, College Station, TX, USA, Department of Computer Science & Engineering, Texas A&M University, College Station, TX, USA
  - **TL;DR:** The study introduces QORA, an algorithm designed to enhance generalization, interpretability, and efficiency in reinforcement learning by utilizing object-oriented transition modeling without requiring domain knowledge. QORA achieves high predictive accuracy and demonstrates the ability to adapt to new environments with minimal training data.
  - **Keywords:** reinforcement learning, generalization, interpretability, efficiency, object-oriented transition modeling, QORA algorithm, robotic manipulation, autonomous driving, plasma confinement, generalization failure, model interpretability, learning efficiency, predictive accuracy, zero-shot transfer, rapid adaptation, GIE (generalization, interpretability, efficiency)


- [Benign Overfitting in Adversarial Training of Neural Networks](https://icml.cc/virtual/2024/poster/34603) (Poster)
  - **Authors:** [Yunjuan Wang](http://openreview.net/profile?id=~Yunjuan_Wang1), [Kaibo Zhang](http://openreview.net/profile?id=~Kaibo_Zhang3), [Raman Arora](http://openreview.net/profile?id=~Raman_Arora1)
  - **Affiliations:** Department of Computer Science, Johns Hopkins University, Baltimore, USA, Department of Computer Science, Johns Hopkins University, Baltimore, USA, Department of Computer Science, Johns Hopkins University, Baltimore, USA
  - **TL;DR:** This study investigates benign overfitting in adversarial training of neural networks, demonstrating that interpolating models can generalize well despite adversarial attacks. The authors provide theoretical guarantees for convergence and robust generalization, supported by empirical evidence.
  - **Keywords:** benign overfitting, adversarial training, neural networks, two-layer networks, smooth and non-smooth activation functions, adversarial training, image classification, speech recognition, model overfitting, adversarial examples, robust generalization, convergence guarantees, generalization guarantees, robust training loss


- [Position: A Roadmap to Pluralistic Alignment](https://icml.cc/virtual/2024/poster/33429) (Poster)
  - **Authors:** [Taylor Sorensen](http://openreview.net/profile?id=~Taylor_Sorensen1), [Jared Moore](http://openreview.net/profile?id=~Jared_Moore1), [Jillian Fisher](http://openreview.net/profile?id=~Jillian_Fisher1), [Mitchell Gordon](http://openreview.net/profile?id=~Mitchell_L_Gordon1), [Niloofar Mireshghallah](http://openreview.net/profile?id=~Niloofar_Mireshghallah1), [Christopher Rytting](http://openreview.net/profile?id=~Christopher_Michael_Rytting1), [Andre Ye](http://openreview.net/profile?id=~Andre_Ye1), [Liwei Jiang](http://openreview.net/profile?id=~Liwei_Jiang2), [Ximing Lu](http://openreview.net/profile?id=~Ximing_Lu1), [Nouha Dziri](http://openreview.net/profile?id=~Nouha_Dziri2), [Tim Althoff](http://openreview.net/profile?id=~Tim_Althoff2), [Yejin Choi](http://openreview.net/profile?id=~Yejin_Choi1)
  - **Affiliations:** Department of Computer Science, University of Washington, Seattle, Washington, USA, Department of Computer Science, Stanford University, Stanford, California, USA, Department of Computer Science, University of Washington, Seattle, Washington, USA; Department of Statistics, University of Washington, Seattle, Washington, USA, Department of Electrical Engineering and Computer Science, MIT, Cambridge, Massachusetts, USA, Department of Computer Science, University of Washington, Seattle, Washington, USA, Department of Computer Science, University of Washington, Seattle, Washington, USA, Department of Computer Science, University of Washington, Seattle, Washington, USA, Allen Institute for Artificial Intelligence, Seattle, Washington, USA, Department of Computer Science, University of Washington, Seattle, Washington, USA, Allen Institute for Artificial Intelligence, Seattle, Washington, USA, Department of Computer Science, University of Washington, Seattle, Washington, USA, Department of Computer Science, University of Washington, Seattle, Washington, USA; Allen Institute for Artificial Intelligence, Seattle, Washington, USA
  - **TL;DR:** The paper proposes a roadmap for pluralistic alignment in AI systems, emphasizing the need for models that can represent diverse human values and perspectives. It identifies three types of pluralistic models and discusses the limitations of current alignment techniques, advocating for further research in this area.
  - **Keywords:** AI alignment, pluralism in AI systems, Aligning models to serve diverse human values, challenges in current alignment techniques, Roadmap to pluralistic alignment, pluralistic benchmarks, Large language models, Overton pluralistic models, Steerably pluralistic models, Distributionally pluralistic models


- [Position: Leverage Foundational Models for Black-Box Optimization](https://icml.cc/virtual/2024/poster/33492) (Poster)
  - **Authors:** [Xingyou Song](http://openreview.net/profile?id=~Xingyou_Song1), [Yingtao Tian](http://openreview.net/profile?id=~Yingtao_Tian1), [Robert Lange](http://openreview.net/profile?id=~Robert_Tjarko_Lange1), [Chansoo Lee](http://openreview.net/profile?id=~Chansoo_Lee1), [Yujin Tang](http://openreview.net/profile?id=~Yujin_Tang1), [Yutian Chen](http://openreview.net/profile?id=~Yutian_Chen1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Sakana AI, Google DeepMind, Sakana AI, Google DeepMind
  - **TL;DR:** This paper explores the integration of Large Language Models with black-box optimization, highlighting their potential to revolutionize optimization strategies and enhance performance prediction in various experimental design contexts. The authors propose leveraging foundational models to address challenges in efficiently searching for optimal parameters without relying on gradient information.
  - **Keywords:** Black-box optimization, Large Language Models, Bayesian Optimization, learning to optimize, sequence models, Transformers, Automated machine learning, drug discovery, biological/chemical design, Efficient parameter search, high costs of design, lack of gradient information, New optimization strategies, enhanced performance prediction


- [Harmonic Self-Conditioned Flow Matching for joint Multi-Ligand Docking and Binding Site Design](https://icml.cc/virtual/2024/poster/33822) (Poster)
  - **Authors:** [Hannes Stärk](http://openreview.net/profile?id=~Hannes_Stark1), [Bowen Jing](http://openreview.net/profile?id=~Bowen_Jing1), [Regina Barzilay](http://openreview.net/profile?id=~Regina_Barzilay1), [Tommi Jaakkola](http://openreview.net/profile?id=~Tommi_S._Jaakkola1)
  - **Affiliations:** CSAIL, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology
  - **TL;DR:** The study presents HARMONIC FLOW and FLOW SITE, novel generative models for designing protein binding pockets for small molecules, including multi-ligand scenarios. The results demonstrate significant improvements in docking processes and binding site design compared to existing methods.
  - **Keywords:** protein-ligand binding, protein design, multi-ligand docking, generative processes, self-conditioned flow matching, HARMOIC FLOW, FLOW SITE, drug synthesis, energy storage, enzyme design, designing binding pockets, predicting amino acid identities, multi-ligand binding, improved generative processes for docking, better binding site design


- [Whispering Experts: Neural Interventions for Toxicity Mitigation in Language Models](https://icml.cc/virtual/2024/poster/35108) (Poster)
  - **Authors:** [Xavi Suau](http://openreview.net/profile?id=~Xavier_Suau1), [Pieter Delobelle](http://openreview.net/profile?id=~Pieter_Delobelle1), [Katherine Metcalf](http://openreview.net/profile?id=~Katherine_Metcalf1), [Armand Joulin](http://openreview.net/profile?id=~Armand_Joulin2), [Nicholas Apostoloff](http://openreview.net/profile?id=~Nicholas_Apostoloff1), [Luca Zappella](http://openreview.net/profile?id=~Luca_Zappella1), [Pau Rodriguez](http://openreview.net/profile?id=~Pau_Rodriguez2)
  - **Affiliations:** Apple, KU Leuven; Apple, Apple, Apple, Apple, Apple, Apple
  - **TL;DR:** This study presents AURA, a method for mitigating toxicity in Large Language Models by reducing the activation of neurons responsible for generating toxic language. The approach achieves significant toxicity reduction with minimal impact on model perplexity and is effective across various model scales.
  - **Keywords:** Toxicity mitigation, Large Language Models, AUROC adaptation (AURA), neural interventions, Natural language processing, AI safety, Toxic language generation, discrimination of toxic content, Reduction in toxicity, preservation of common-sense reasoning, Real Toxicity Prompts dataset


- [Learning from Streaming Data when Users Choose](https://icml.cc/virtual/2024/poster/32995) (Poster)
  - **Authors:** [Jinyan Su](http://openreview.net/profile?id=~Jinyan_Su1), [Sarah Dean](http://openreview.net/profile?id=~Sarah_Dean2)
  - **Affiliations:** Department of Computer Science, Cornell University, Department of Computer Science, Cornell University
  - **TL;DR:** This paper investigates the dynamics between users choosing service providers and the subsequent model updates by those providers in digital markets. It introduces a decentralized algorithm, MSGD, which effectively minimizes user loss and converges to stable points despite the challenges posed by non-stationary data and imperfect user choices.
  - **Keywords:** streaming data, user choice dynamics, model improvement, Multi-learner Streaming Gradient Descent (MSGD), digital markets, online services, personalized content, feedback loop, non-stationary distribution, imperfect user behavior, decentralized algorithm, convergence to fixed points


- [Rényi Pufferfish Privacy: General Additive Noise Mechanisms and Privacy Amplification by Iteration via Shift Reduction Lemmas](https://icml.cc/virtual/2024/poster/33909) (Poster)
  - **Authors:** [Clément Pierquin](http://openreview.net/profile?id=~Cl%C3%A9ment_Pierquin1), [Aurélien Bellet](http://openreview.net/profile?id=~Aur%C3%A9lien_Bellet1), [Marc Tommasi](http://openreview.net/profile?id=~Marc_Tommasi1), [Matthieu Boussard](http://openreview.net/profile?id=~Matthieu_Boussard1)
  - **Affiliations:** Craft AI, Paris, France; Université de Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France, Inria, Université de Montpellier, Université de Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France, Craft AI, Paris, France
  - **TL;DR:** This paper introduces a Rényi divergence-based variant of Pufferfish privacy to enhance the design of privacy mechanisms and address challenges in utility and composition guarantees. The authors generalize the Wasserstein mechanism and demonstrate its application in private convex optimization, providing new insights into privacy amplification through contractive noisy iterations.
  - **Keywords:** Pufferfish privacy, differential privacy, Rényi divergence, Wasserstein mechanism, additive noise mechanisms, Privacy-preserving data analysis, private convex optimization, Designing general and tractable Pufferfish mechanisms, privacy loss measurement, composition guarantees, Privacy amplification results, generalization of noise distributions, Shift reduction lemmas


- [Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction](https://icml.cc/virtual/2024/poster/34492) (Poster)
  - **Authors:** [Arjun Subramonian](http://openreview.net/profile?id=~Arjun_Subramonian1), [Levent Sagun](http://openreview.net/profile?id=~Levent_Sagun1), [Yizhou Sun](http://openreview.net/profile?id=~Yizhou_Sun1)
  - **Affiliations:** Computer Science Department, University of California, Los Angeles, USA, Meta, Paris, France, Computer Science Department, University of California, Los Angeles, USA
  - **TL;DR:** This study investigates the preferential attachment bias in Graph Neural Network link prediction, revealing that GCNs exhibit a within-group bias that can exacerbate power imbalances in networks. The authors propose a new fairness metric and a training strategy to mitigate these disparities in various social contexts.
  - **Keywords:** Graph Neural Networks, Link Prediction, Fairness in Networks, Graph Convolutional Networks (GCN), Symmetric Normalized Graph Filter, Citation Networks, Collaboration Networks, Online Social Networks, Within-group fairness, Degree bias, Power imbalances, New within-group fairness metric, Training-time strategy to alleviate unfairness, Preferential Attachment (PA), Dyadic Fairness, "Rich get richer" dynamics


- [ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance](https://icml.cc/virtual/2024/poster/35181) (Poster)
  - **Authors:** [Liwen Sun](http://openreview.net/profile?id=~Liwen_Sun2), [Abhineet Agarwal](http://openreview.net/profile?id=~Abhineet_Agarwal1), [Aaron Kornblith](http://openreview.net/profile?id=~Aaron_Kornblith1), [Bin Yu](http://openreview.net/profile?id=~Bin_Yu5), [Chenyan Xiong](http://openreview.net/profile?id=~Chenyan_Xiong1)
  - **Affiliations:** Language Technologies Institute, Carnegie Mellon University, Department of Statistics, University of California, Berkeley, Department of Emergency Medicine & Pediatrics, University of California, San Francisco, Department of Statistics, University of California, Berkeley; Department of EECS, University of California, Berkeley, Language Technologies Institute, Carnegie Mellon University
  - **TL;DR:** This study presents ED-Copilot, an AI-driven diagnostic assistant designed to reduce wait times and improve diagnostic accuracy in emergency departments. By leveraging a benchmark dataset (MIMIC-ED-Assist), the system effectively personalizes laboratory test recommendations, halving average wait times from four hours to two while enhancing prediction accuracy.
  - **Keywords:** Emergency Department (ED) crowding, diagnostic assistance, artificial intelligence, Reinforcement learning, pre-trained bio-medical language model, Emergency medicine, healthcare diagnostics, ED crowding, lengthy wait times, unnecessary laboratory tests, ED-Copilot system, MIMIC-ED-Assist benchmark, improved prediction accuracy, reduced wait time, MIMIC-ED-Assist, MIMIC-IV


- [DFA-RAG: Conversational Semantic Router for Large Language Model with Definite Finite Automaton](https://icml.cc/virtual/2024/poster/34267) (Poster)
  - **Authors:** [Yiyou Sun](http://openreview.net/profile?id=~Yiyou_Sun1), [Junjie Hu](http://openreview.net/profile?id=~Junjie_Hu2), [Wei Cheng](http://openreview.net/profile?id=~Wei_Cheng1), [Haifeng Chen](http://openreview.net/profile?id=~Haifeng_Chen1)
  - **Affiliations:** Department of Computer Science, University of Wisconsin; NEC Laboratories America, inc., Princeton, USA, Department of Computer Science, University of Wisconsin, NEC Laboratories America, inc., Princeton, USA, NEC Laboratories America, inc., Princeton, USA
  - **TL;DR:** This paper presents DFA-RAG, a novel framework that integrates a Definite Finite Automaton within large language models to enhance the generation of compliant responses in conversational agents. The framework effectively addresses challenges in maintaining adherence to specific workflows, demonstrating significant potential in applications like emotional support and customer service.
  - **Keywords:** conversational agents, large language models (LLMs), retrieval-augmented generation (RAG), Definite Finite Automaton (DFA), retrieval-augmentation generation (RAG), emotional support, customer service, generating regulated and compliant responses, adherence to predetermined response guidelines, DFA-RAG framework, context-aware retrieval, interpretable structure


- [Online Adaptive Anomaly Thresholding with Confidence Sequences](https://icml.cc/virtual/2024/poster/33387) (Poster)
  - **Authors:** [Sophia Sun](http://openreview.net/profile?id=~Sophia_Huiwen_Sun1), [Abishek Sankararaman](http://openreview.net/profile?id=~Abishek_Sankararaman1), [Balakrishnan Narayanaswamy](http://openreview.net/profile?id=~Balakrishnan_Murali_Narayanaswamy1)
  - **Affiliations:** University of California, San Diego; Amazon Web Services, Santa Clara CA, Amazon Web Services, Santa Clara CA, Amazon Web Services, Santa Clara CA
  - **TL;DR:** This paper presents an algorithm for adaptive online threshold selection in anomaly detection that is robust to data distribution shifts and provides statistical guarantees on false positive and false negative rates. The proposed method outperforms existing baselines in both synthetic and real-world datasets.
  - **Keywords:** online anomaly detection, adaptive threshold selection, confidence sequences, online quantile estimation, infrastructure monitoring, network intrusion detection, cyber-physical systems, data distribution shifts, false positives, false negatives, adaptive online threshold selection, statistical guarantees


- [Constrained Reinforcement Learning Under Model Mismatch](https://icml.cc/virtual/2024/poster/34496) (Poster)
  - **Authors:** [Zhongchang Sun](http://openreview.net/profile?id=~Zhongchang_Sun1), [Sihong He](http://openreview.net/profile?id=~Sihong_He1), [Fei Miao](http://openreview.net/profile?id=~Fei_Miao1), [Shaofeng Zou](http://openreview.net/profile?id=~Shaofeng_Zou1)
  - **Affiliations:** Department of Electrical Engineering, University at Buffalo, New York, USA, School of Computing, University of Connecticut, Storrs, USA, School of Computing, University of Connecticut, Storrs, USA, Department of Electrical Engineering, University at Buffalo, New York, USA; Department of Computer Science & Engineering, University at Buffalo, New York, USA
  - **TL;DR:** This study addresses the challenge of constrained reinforcement learning under model mismatch by developing a Robust Constrained Policy Optimization (RCPO) algorithm, which ensures that constraints are satisfied while maximizing worst-case rewards. The algorithm provides theoretical guarantees on reward improvement and constraint violation during training, demonstrating effectiveness across various RL tasks.
  - **Keywords:** Constrained Reinforcement Learning, Model Mismatch, Robust Constrained Policy Optimization (RCPO), Constrained Markov Decision Process (CMDP), Robotics, Health Care, Autonomous Driving, Industry Automation, Model Mismatch, Constraint Violation, Sim-to-Real Gap, Theoretical Guarantees on Reward Improvement, Constraint Satisfaction


- [LSEnet: Lorentz Structural Entropy Neural Network for Deep Graph Clustering](https://icml.cc/virtual/2024/poster/34313) (Oral)
  - **Authors:** [Li Sun](http://openreview.net/profile?id=~Li_Sun4), [Zhenhao Huang](http://openreview.net/profile?id=~Zhenhao_Huang1), [Hao Peng](http://openreview.net/profile?id=~Hao_Peng7), [YuJie Wang](http://openreview.net/profile?id=~YuJie_Wang9), [Chunyang Liu](http://openreview.net/profile?id=~Chunyang_Liu1), [Philip Yu](http://openreview.net/profile?id=~Philip_S._Yu1)
  - **Affiliations:** North China Electric Power University, Beijing 102206, China, North China Electric Power University, Beijing 102206, China, Beihang University, Beijing 100191, China, North China Electric Power University, Beijing 102206, China, Didi Chuxing, Beijing, China, University of Illinois at Chicago, IL, USA
  - **TL;DR:** This study introduces a novel approach to graph clustering that does not require a predefined number of clusters by formulating a differentiable structural information (DSI) method. The proposed LSEnet effectively constructs an optimal partitioning tree in hyperbolic space, demonstrating superior performance in clustering real graphs.
  - **Keywords:** Graph clustering, Deep learning, Information theory, Differentiable structural information (DSI), Manifold-valued graph convolution, Community detection, Biochemical analysis, Unknown cluster number, Predefined cluster number limitation, Optimal partitioning tree, New graph clustering objective, Structural information, Hyperbolic space


- [FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models](https://icml.cc/virtual/2024/poster/34753) (Poster)
  - **Authors:** [Jingwei Sun](http://openreview.net/profile?id=~Jingwei_Sun2), [Ziyue Xu](http://openreview.net/profile?id=~Ziyue_Xu1), [Hongxu Yin](http://openreview.net/profile?id=~Hongxu_Yin2), [Dong Yang](http://openreview.net/profile?id=~Dong_Yang1), [Daguang Xu](http://openreview.net/profile?id=~Daguang_Xu2), [Yudong Liu](http://openreview.net/profile?id=~Yudong_Liu4), [Zhixu Du](http://openreview.net/profile?id=~Zhixu_Du1), [Yiran Chen](http://openreview.net/profile?id=~Yiran_Chen1), [Holger Roth](http://openreview.net/profile?id=~Holger_R_Roth1)
  - **Affiliations:** Department of Electrical Computer Engineering, Duke University, Durham, USA, NVIDIA, Santa Clara, USA, NVIDIA, Santa Clara, USA, NVIDIA, Santa Clara, USA, NVIDIA, Santa Clara, USA, Department of Electrical Computer Engineering, Duke University, Durham, USA, Department of Electrical Computer Engineering, Duke University, Durham, USA, Department of Electrical Computer Engineering, Duke University, Durham, USA, NVIDIA, Santa Clara, USA
  - **TL;DR:** This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework that enables efficient and privacy-preserving fine-tuning of pre-trained language models through federated learning. The framework addresses challenges such as limited model access and high computational costs while significantly reducing communication and memory overheads.
  - **Keywords:** Federated Learning, Pre-trained Language Models, Privacy-preserving Fine-tuning, Black-box Prompt Tuning, Gradient-free Optimization, Natural Language Processing, Edge Devices, Security and Privacy Concerns, Limited Model Parameter Access, High Computational Requirements, Communication Overheads, Efficient Fine-tuning Framework, Communication Efficiency, Memory Cost Reduction, NVIDIA FLARE, Large Language Models, PLM (Pre-trained Language Models), FL (Federated Learning)


- [Learning Graph Representation via Graph Entropy Maximization](https://icml.cc/virtual/2024/poster/32687) (Poster)
  - **Authors:** [Ziheng Sun](http://openreview.net/profile?id=~Ziheng_Sun1), [Xudong Wang](http://openreview.net/profile?id=~Xudong_Wang10), [Chris Ding](http://openreview.net/profile?id=~Chris_Ding1), [Jicong Fan](http://openreview.net/profile?id=~Jicong_Fan2)
  - **Affiliations:** School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen); Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data, Shenzhen, China, School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China, School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China, School of Data Science, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen); Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data, Shenzhen, China
  - **TL;DR:** This study presents a method for learning diverse graph representations by maximizing graph entropy, addressing the NP-hard computation challenge through an approximation method using graph neural networks. Experimental results show that the proposed method outperforms several baselines in unsupervised and semi-supervised learning tasks.
  - **Keywords:** Graph representation learning, unsupervised learning, semi-supervised learning, Graph entropy, graph neural networks (GNNs), InfoGraph, Graph Contrastive Learning, AutoGCL, GraphACL, Chemistry, biology, social sciences, NP-hard computation, non-Euclidean nature of graph data, Approximation method for graph entropy maximization, informative node-level and graph-level representations, Vertex-packing polytope, mutual information


- [Regression Learning with Limited Observations of Multivariate Outcomes and Features](https://icml.cc/virtual/2024/poster/35048) (Poster)
  - **Authors:** [Yifan Sun](http://openreview.net/profile?id=~Yifan_Sun11), [Grace Yi](http://openreview.net/profile?id=~Grace_Yi1)
  - **Affiliations:** Department of Statistical and Actuarial Sciences, University of Western Ontario, London, Canada, Department of Computer Science, University of Western Ontario, London, Canada
  - **TL;DR:** This study develops efficient algorithms for multivariate linear regression that address the challenges of missing observations in datasets, utilizing L2 and L1 loss functions with corresponding penalties. The proposed methods demonstrate superior performance compared to traditional approaches, particularly in high-dimensional contexts and in the presence of outliers.
  - **Keywords:** multivariate linear regression, missing observations, least squares (L2), least absolute (L1), ridge-like penalty, Lasso-type penalty, electronic health records, weather forecasting, missing observations, data sparsity, high-dimensional data, efficient algorithms, error bounds, improved predictions


- [On a Combinatorial Problem Arising in Machine Teaching](https://icml.cc/virtual/2024/poster/32895) (Poster)
  - **Authors:** [Joakim Sunde](http://openreview.net/profile?id=~Joakim_Sunde1), [Brigt Håvardstun](http://openreview.net/profile?id=~Brigt_H%C3%A5vardstun1), [Jan Kratochvíl](http://openreview.net/profile?id=~Jan_Kratochv%C3%ADl1), [Jan Arne Telle](http://openreview.net/profile?id=~Jan_Arne_Telle1)
  - **Affiliations:** Department of Informatics, University of Bergen, Bergen, Norway, Department of Informatics, University of Bergen, Bergen, Norway, Department of Applied Mathematics, Faculty of Mathematics and Physics, Charles University, Praha, Czech Republic, Department of Informatics, University of Bergen, Bergen, Norway
  - **TL;DR:** This study investigates a model of machine teaching focused on determining the minimum number of examples required for any concept, known as the teaching dimension. The authors prove a conjecture regarding the worst-case scenario for this model, which has implications for understanding the efficiency of teaching strategies in machine learning.
  - **Keywords:** machine teaching, teaching dimension, Greedy algorithm, pedagogy, trustworthy AI, reinforcement learning, active learning, explainable AI, minimum number of examples needed for concepts, consistency matrix, proof of conjecture regarding teaching dimension, consistency matrix, witness, edge isoperimetry problem, binary representations


- [Reinforcement Learning from Reachability Specifications: PAC Guarantees with Expected Conditional Distance](https://icml.cc/virtual/2024/poster/33169) (Poster)
  - **Authors:** [Jakub Svoboda](http://openreview.net/profile?id=~Jakub_Svoboda1), [Suguman Bansal](http://openreview.net/profile?id=~Suguman_Bansal1), [Krishnendu Chatterjee](http://openreview.net/profile?id=~Krishnendu_Chatterjee1)
  - **Affiliations:** Institute of Science and Technology, Austria, Georgia Institute of Technology, USA, Institute of Science and Technology, Austria
  - **TL;DR:** This paper presents a PAC learning algorithm for reinforcement learning from reachability specifications, addressing the theoretical challenges associated with such specifications. The authors establish the necessity of Expected Conditional Distance (ECD) information for achieving PAC guarantees, marking a significant advancement in the field.
  - **Keywords:** Reinforcement Learning, Reachability Specifications, Expected Conditional Distance (ECD), PAC Learning, Robotics, Cyber-Physical Systems, Theoretical guarantees in RL, PAC impossibility for reachability specifications, PAC learning algorithm for reachability specifications


- [Graph Neural Networks with a Distribution of Parametrized Graphs](https://icml.cc/virtual/2024/poster/33893) (Poster)
  - **Authors:** [See Hian Lee](http://openreview.net/profile?id=~See_Hian_Lee1), [Feng Ji](http://openreview.net/profile?id=~Feng_Ji2), [Kelin Xia](http://openreview.net/profile?id=~KELIN_XIA1), [Wee Peng Tay](http://openreview.net/profile?id=~Wee_Peng_Tay1)
  - **Affiliations:** School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore
  - **TL;DR:** This study introduces a novel framework for Graph Neural Networks (GNNs) that incorporates latent variables to address uncertainties in graph data by generating multiple graphs. The proposed EMGNN model demonstrates improved performance in node classification tasks compared to traditional methods.
  - **Keywords:** Graph Neural Networks, Uncertainty in Graphs, Expectation-Maximization (EM), Markov Chain Monte Carlo (MCMC), Maximum Likelihood Estimation, Node Classification, Graph Signal Processing, Structural noise, Missing or spurious edges, Informative edge weights, EMGNN model, Improved performance against baseline models, PAC-Bayesian theory


- [Interpretable Deep Clustering for Tabular Data](https://icml.cc/virtual/2024/poster/34084) (Poster)
  - **Authors:** [Jonathan Svirsky](http://openreview.net/profile?id=~Jonathan_Svirsky1), [Ofir Lindenbaum](http://openreview.net/profile?id=~Ofir_Lindenbaum1)
  - **Affiliations:** Department of Engineering, Bar Ilan University, Ramat-Gan, Israel, Department of Engineering, Bar Ilan University, Ramat-Gan, Israel
  - **TL;DR:** This study presents a deep-learning framework for interpretable clustering of tabular data, focusing on identifying informative features and providing cluster assignments. The proposed method demonstrates reliable performance across various domains, including biology and text analysis, while ensuring interpretability of the results.
  - **Keywords:** Clustering, Interpretability, Deep Learning, Self-supervised feature selection, Cluster assignment prediction, Bioinformatics, Text analysis, Image analysis, Physics, High-dimensional data, Interpretability in clustering, Interpretable clustering models, Cluster-level feature selection


- [Memorization Through the Lens of Curvature of Loss Function Around Samples](https://icml.cc/virtual/2024/poster/33871) (Spotlight Poster)
  - **Authors:** [Isha Garg](http://openreview.net/profile?id=~Isha_Garg1), [Deepak Ravikumar](http://openreview.net/profile?id=~Deepak_Ravikumar1), [Kaushik Roy](http://openreview.net/profile?id=~Kaushik_Roy1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana 47906, Department of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana 47906, Department of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana 47906
  - **TL;DR:** This study introduces a curvature-based metric to measure the memorization of training samples in deep neural networks, demonstrating its effectiveness in identifying mislabeled data and capturing memorization statistics. The proposed method outperforms existing approaches and reveals novel failure modes in popular image datasets.
  - **Keywords:** memorization, overfitting, deep learning, curvature of loss function, memorization metric, image datasets, mislabeled data detection, overfitting, data poisoning, privacy leakage, mislabeled samples, curvature metric for memorization, state-of-the-art detection performance, CIFAR100, ImageNet, MNIST


- [Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds](https://icml.cc/virtual/2024/poster/34631) (Poster)
  - **Authors:** [Shion Takeno](http://openreview.net/profile?id=~Shion_Takeno1), [Yu Inatsu](http://openreview.net/profile?id=~Yu_Inatsu1), [Masayuki Karasuyama](http://openreview.net/profile?id=~Masayuki_Karasuyama1), [Ichiro Takeuchi](http://openreview.net/profile?id=~Ichiro_Takeuchi1)
  - **Affiliations:** Department of Engineering, Nagoya University, Aichi, Japan; RIKEN AIP, Tokyo, Japan, Department of Computer Science, Nagoya Institute of Technology, Aichi, Japan, Department of Computer Science, Nagoya Institute of Technology, Aichi, Japan, Department of Engineering, Nagoya University, Aichi, Japan; RIKEN AIP, Tokyo, Japan
  - **TL;DR:** This study explores Bayesian optimization methods, demonstrating that both Thompson sampling and a new acquisition function, PIMS, achieve tighter Bayesian cumulative regret bounds while addressing practical issues like hyperparameter tuning and over-exploration. The findings suggest that PIMS can effectively mitigate the limitations of existing methods in high-dimensional optimization contexts.
  - **Keywords:** Bayesian optimization, Bayesian cumulative regret, Gaussian process upper confidence bound (GP-UCB), Thompson sampling (TS), probability of improvement from the maximum of a sample path (PIMS), Drug discovery, AutoML, materials informatics, Manual hyperparameter tuning, over-exploration in high-dimensional optimization, Tighter Bayesian cumulative regret bounds, improved randomized GP-UCB (IRGP-UCB)


- [Learning Solution-Aware Transformers for Efficiently Solving Quadratic Assignment Problem](https://icml.cc/virtual/2024/poster/33649) (Poster)
  - **Authors:** [Zhentao Tan](http://openreview.net/profile?id=~Zhentao_Tan2), [Yadong Mu](http://openreview.net/profile?id=~Yadong_MU1)
  - **Affiliations:** Center for Data Science, Peking University, China; Wangxuan Institute of Computer Technology, Peking University, China, Wangxuan Institute of Computer Technology, Peking University, China
  - **TL;DR:** This study presents a novel learning-based approach using a Solution Aware Transformer (SAWT) to efficiently solve the Quadratic Assignment Problem (QAP), addressing scalability and computational inefficiencies inherent in traditional methods. The proposed model demonstrates effectiveness through extensive experiments on various QAP instances and benchmarks.
  - **Keywords:** Quadratic Assignment Problem (QAP), Combinatorial Optimization, Reinforcement Learning, Solution Aware Transformer (SAWT), Optimization Problems, Vehicle Routing Problems (VRPs), Chip Placement and Routing Problems, NP-hard problems, Computational inefficiency, Scalability challenges, Learning-based solutions, Novel L2I approach, QAPLIB benchmark


- [Fourier Controller Networks for Real-Time Decision-Making in Embodied Learning](https://icml.cc/virtual/2024/poster/33131) (Poster)
  - **Authors:** [Hengkai Tan](http://openreview.net/profile?id=~Hengkai_Tan1), [LIU SONGMING](http://openreview.net/profile?id=~Songming_Liu1), [Kai Ma](http://openreview.net/profile?id=~Kai_Ma6), [Chengyang Ying](http://openreview.net/profile?id=~Chengyang_Ying1), [Xingxing Zhang](http://openreview.net/profile?id=~Xingxing_Zhang3), [Hang Su](http://openreview.net/profile?id=~Hang_Su3), [Jun Zhu](http://openreview.net/profile?id=~Jun_Zhu2)
  - **Affiliations:** Department of Computer Science and Technology, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Department of Computer Science and Technology, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Department of Computer Science and Technology, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Department of Computer Science and Technology, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Department of Computer Science and Technology, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Department of Computer Science and Technology, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Department of Computer Science and Technology, Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China
  - **TL;DR:** This study introduces the Fourier Controller Network (FCNet) to enhance real-time decision-making in embodied learning by leveraging frequency domain analysis. The results demonstrate that FCNet significantly outperforms existing methods like Transformer in various robotics datasets, addressing issues of data efficiency and inference latency.
  - **Keywords:** Reinforcement Learning, Embodied Learning, Fourier Controller Network (FCNet), Short-Time Fourier Transform (STFT), FFT, Sliding DFT, Robotics, Robot Locomotion, Low data efficiency, High inference latency, Efficient recurrent inference, Parallel training, D4RL, Transformer


- [Post-hoc Part-Prototype Networks](https://icml.cc/virtual/2024/poster/33291) (Poster)
  - **Authors:** [Andong Tan](http://openreview.net/profile?id=~Andong_Tan1), [Fengtao ZHOU](http://openreview.net/profile?id=~Fengtao_ZHOU1), [Hao Chen](http://openreview.net/profile?id=~Hao_Chen1)
  - **Affiliations:** Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Center for Aging Science, HKUST, Hong Kong, China
  - **TL;DR:** This paper introduces a post-hoc part-prototype network that enhances model interpretability by decomposing the classification head into interpretable part-prototypes while maintaining performance. The proposed method offers more faithful explanations and improved part-prototypes compared to existing models.
  - **Keywords:** Explainability, Interpretability, Post-hoc methods, Part-prototype networks, Unsupervised prototype discovery, Computer vision, Lack of interpretability in black-box models, Performance vs. interpretability trade-off, Post-hoc part-prototype network, Improved explanations and part-prototypes, Grad-CAM, Convolutional Neural Networks (CNN), Vision Transformer (ViT)


- [OTMatch: Improving Semi-Supervised Learning with Optimal Transport](https://icml.cc/virtual/2024/poster/33996) (Poster)
  - **Authors:** [Zhiquan Tan](http://openreview.net/profile?id=~Zhiquan_Tan1), [Kaipeng Zheng](http://openreview.net/profile?id=~Kaipeng_Zheng1), [Weiran Huang](http://openreview.net/profile?id=~Weiran_Huang1)
  - **Affiliations:** Department of Mathematical Sciences, Tsinghua University, MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University; Shanghai AI Laboratory
  - **TL;DR:** This paper introduces OTMatch, a novel approach to semi-supervised learning that utilizes optimal transport to incorporate inter-class semantic relationships, addressing the issue of model overconfidence in pseudo-labeling methods. The empirical results demonstrate significant improvements in learning performance on standard vision and language datasets.
  - **Keywords:** Semi-supervised learning, Optimal transport, Pseudo-labeling, Cross-entropy loss, Optimal transport loss, Vision datasets, Language datasets, Overconfidence in model predictions, Misclassification due to limited labeled samples, Improved learning performance, Robustness in model training


- [Merging Multi-Task Models via Weight-Ensembling Mixture of Experts](https://icml.cc/virtual/2024/poster/33129) (Poster)
  - **Authors:** [Anke Tang](http://openreview.net/profile?id=~Anke_Tang1), [Li Shen](http://openreview.net/profile?id=~Li_Shen1), [Yong Luo](http://openreview.net/profile?id=~Yong_Luo2), [Nan Yin](http://openreview.net/profile?id=~Nan_Yin4), [Lefei Zhang](http://openreview.net/profile?id=~Lefei_Zhang1), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, China; Hubei Luojia Laboratory, Wuhan, China, Sun Yat-sen University, Shenzhen, China; JD Explore Academy, China, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, China; Hubei Luojia Laboratory, Wuhan, China, Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, China; Hubei Luojia Laboratory, Wuhan, China, Nanyang Technological University, Singapore
  - **TL;DR:** This paper proposes a novel method for merging multi-task Transformer-based vision models into a unified model using a weight-ensembling Mixture of Experts (MoE) module, which dynamically integrates shared and task-specific knowledge. The approach effectively mitigates parameter interference and enhances adaptability to specific tasks, demonstrating improved generalization and robustness in multi-task learning scenarios.
  - **Keywords:** Multi-task learning, Knowledge transfer, Transformer-based models, Mixture of Experts (MoE), Vision models, Parameter interference, Static optimal solutions, Dynamic integration of shared and task-specific knowledge, Vision Transformers (ViTs), MLP (Multilayer Perceptron)


- [Deciphering RNA Secondary Structure Prediction: A Probabilistic K-Rook Matching Perspective](https://icml.cc/virtual/2024/poster/34758) (Poster)
  - **Authors:** [Cheng Tan](http://openreview.net/profile?id=~Cheng_Tan1), [Zhangyang Gao](http://openreview.net/profile?id=~Zhangyang_Gao1), [Hanqun CAO](http://openreview.net/profile?id=~Hanqun_CAO1), [Xingran Chen](http://openreview.net/profile?id=~Xingran_Chen1), [Wang Ge](http://openreview.net/profile?id=~Ge_Wang3), [Lirong Wu](http://openreview.net/profile?id=~Lirong_Wu1), [Jun Xia](http://openreview.net/profile?id=~Jun_Xia1), [Jiangbin Zheng](http://openreview.net/profile?id=~Jiangbin_Zheng3), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** Zhejiang University; Westlake University, Westlake University; Zhejiang University, The Chinese University of Hong Kong, University of Michigan, Westlake University, Westlake University, Westlake University, Westlake University, Westlake University
  - **TL;DR:** This study reformulates RNA secondary structure prediction as a K-Rook problem, introducing RFold, a method that simplifies the prediction process through probabilistic matching. RFold demonstrates competitive performance and significantly faster inference efficiency compared to existing approaches.
  - **Keywords:** RNA secondary structure prediction, deep learning, K-Rook problem, probabilistic matching, bi-dimensional optimization, Computational biology, RNA structure analysis, Poor generalization, high complexity in RNA structure prediction, RFold method, improved inference efficiency


- [StrokeNUWA—Tokenizing Strokes for Vector Graphic Synthesis](https://icml.cc/virtual/2024/poster/33497) (Poster)
  - **Authors:** [Zecheng Tang](http://openreview.net/profile?id=~Zecheng_Tang1), [Chenfei Wu](http://openreview.net/profile?id=~Chenfei_Wu2), [Zekai Zhang](http://openreview.net/profile?id=~Zekai_Zhang4), [Minheng Ni](http://openreview.net/profile?id=~Minheng_Ni1), [Shengming Yin](http://openreview.net/profile?id=~Shengming_Yin1), [Yu Liu](http://openreview.net/profile?id=~Yu_Liu48), [Zhengyuan Yang](http://openreview.net/profile?id=~Zhengyuan_Yang1), [Lijuan Wang](http://openreview.net/profile?id=~Lijuan_Wang1), [Zicheng Liu](http://openreview.net/profile?id=~Zicheng_Liu1), [Juntao Li](http://openreview.net/profile?id=~Juntao_Li2), [Nan Duan](http://openreview.net/profile?id=~Nan_Duan1)
  - **Affiliations:** Soochow University; Microsoft Research Asia, Microsoft Research Asia, Microsoft Research Asia, Microsoft Research Asia, Microsoft Research Asia, Microsoft Research Asia, Microsoft Azure AI, Microsoft Azure AI, Microsoft Azure AI, Soochow University, Microsoft Research Asia
  - **TL;DR:** This paper introduces StrokeNUWA, a novel approach that utilizes "stroke tokens" for vector graphic synthesis, significantly improving visual representation and inference speed compared to traditional methods. The findings demonstrate that StrokeNUWA achieves a 94× speedup in inference and a 6.9% SVG code compression ratio, addressing limitations in semantic representation of visual scenes.
  - **Keywords:** visual synthesis, vector graphics, stroke tokens, LLMs (Large Language Models), VQ-GAN, VQ-VAE, image generation, visual representation, limitations of grid tokens, semantic representation of visual scenes, StrokeNUWA, SVG code compression, inference speedup


- [QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference](https://icml.cc/virtual/2024/poster/34319) (Poster)
  - **Authors:** [Jiaming Tang](http://openreview.net/profile?id=~Jiaming_Tang1), [Yilong Zhao](http://openreview.net/profile?id=~Yilong_Zhao1), [Kan Zhu](http://openreview.net/profile?id=~Kan_Zhu1), [Guangxuan Xiao](http://openreview.net/profile?id=~Guangxuan_Xiao1), [Baris Kasikci](http://openreview.net/profile?id=~Baris_Kasikci2), [Song Han](http://openreview.net/profile?id=~Song_Han5)
  - **Affiliations:** Shanghai Jiao Tong University; MIT, Shanghai Jiao Tong University; MIT, University of Washington, MIT, University of Washington, MIT; NVIDIA
  - **TL;DR:** The study introduces Quest, a query-aware algorithm that optimizes KV cache selection for long-context LLMs, achieving up to 7.03× speedup in self-attention and reducing inference latency by 2.23× without sacrificing accuracy. This approach addresses the challenges of processing long-context requests efficiently.
  - **Keywords:** Long-context large language models (LLMs), Efficient inference, Query-aware KV cache selection algorithm, Self-attention, Natural language processing, Multi-round conversations, Inference speed reduction with long sequences, KV cache loading overhead, Speedup in self-attention, Reduction in inference latency, KV cache, Critical tokens, Auto-regressive models


- [MathScale: Scaling Instruction Tuning for Mathematical Reasoning](https://icml.cc/virtual/2024/poster/34329) (Poster)
  - **Authors:** [Zhengyang Tang](http://openreview.net/profile?id=~Zhengyang_Tang1), [Xingxing Zhang](http://openreview.net/profile?id=~Xingxing_Zhang1), [Benyou Wang](http://openreview.net/profile?id=~Benyou_Wang2), [Furu Wei](http://openreview.net/profile?id=~Furu_Wei1)
  - **Affiliations:** The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Research Institute of Big Data, Shenzhen, China, Microsoft Research Asia, Beijing, China, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen Research Institute of Big Data, Shenzhen, China, Microsoft Research Asia, Beijing, China
  - **TL;DR:** This study introduces MathScale, a scalable method for generating high-quality mathematical reasoning data using large language models, resulting in the creation of a two million question-answer pair dataset (MathScaleQA) and a benchmark (MWPBENCH) for evaluating LLMs. The fine-tuning of open-source LLMs with MathScaleQA significantly enhances their mathematical reasoning capabilities, achieving state-of-the-art performance across various datasets.
  - **Keywords:** mathematical reasoning, large language models (LLMs), instruction tuning, concept graph generation, educational assessment, K-12 education, college-level mathematics, inadequate proficiency in solving mathematical problems, multi-step complex reasoning, MathScaleQA dataset, MWPBENCH benchmark, improved mathematical reasoning capabilities, MathScaleQA, MWPBENCH, GSM8K, MATH, LLaMA-2, Mistral


- [Position: What makes an image realistic?](https://icml.cc/virtual/2024/poster/33702) (Spotlight Poster)
  - **Authors:** [Lucas Theis](http://openreview.net/profile?id=~Lucas_Theis1)
  - **Affiliations:** Google DeepMind, London, UK
  - **TL;DR:** The paper explores the challenge of quantifying realism in generated data, proposing the concept of a universal critic to evaluate the realism of images and other data types. It highlights the difficulties in designing robust functions for detecting unrealistic data and the implications for various applications in machine learning.
  - **Keywords:** realism, generative AI, quantifying realism, universal critic, generative models, anomaly detection, deepfake detection, generative model evaluation, model distillation, neural compression, computational photography, computer graphics, detecting unrealistic images, robust loss functions, optimization challenges, MNIST, algorithmic information theory


- [A Universal Class of Sharpness-Aware Minimization Algorithms](https://icml.cc/virtual/2024/poster/34803) (Poster)
  - **Authors:** [Behrooz Tahmasebi](http://openreview.net/profile?id=~Behrooz_Tahmasebi1), [Ashkan Soleymani](http://openreview.net/profile?id=~Ashkan_Soleymani1), [Dara Bahri](http://openreview.net/profile?id=~Dara_Bahri1), [Stefanie Jegelka](http://openreview.net/profile?id=~Stefanie_Jegelka3), [Patrick Jaillet](http://openreview.net/profile?id=~Patrick_Jaillet1)
  - **Affiliations:** MIT CSAIL, MIT LIDS, Google DeepMind, TU Munich, MIT LIDS
  - **TL;DR:** This paper introduces a new class of sharpness measures for optimization algorithms aimed at improving generalization in overparameterized models. The authors present new sharpness-aware objective functions, including Frob-SAM and Det-SAM, which effectively minimize sharpness and address challenges related to parameter invariances in neural networks.
  - **Keywords:** optimization algorithms, generalization, sharpness-aware minimization, Sharpness-Aware Minimization (SAM), Frob-SAM, Det-SAM, overparameterized models, sharp minima, loss landscape geometry, new sharpness measures, new sharpness-aware objective functions, Hessian matrix, parameter invariances, scale-invariances


- [BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models](https://icml.cc/virtual/2024/poster/33293) (Spotlight Poster)
  - **Authors:** [Haotian Sun](http://openreview.net/profile?id=~Haotian_Sun1), [Yuchen Zhuang](http://openreview.net/profile?id=~Yuchen_Zhuang1), [Wei Wei](http://openreview.net/profile?id=~Wei_Wei15), [Chao Zhang](http://openreview.net/profile?id=~Chao_Zhang15), [Bo Dai](http://openreview.net/profile?id=~Bo_Dai1)
  - **Affiliations:** Georgia Tech, Georgia Tech, Accenture, Georgia Tech, Georgia Tech
  - **TL;DR:** The study introduces BBOX-ADAPTER, a lightweight adapter for adapting black-box Large Language Models (LLMs) like GPT-4 and Gemini, addressing challenges related to transparency, privacy, and cost. It demonstrates significant improvements in model performance and cost efficiency, achieving up to 6.77% performance enhancement while reducing training and inference costs substantially.
  - **Keywords:** Large Language Models, black-box adaptation, Noise Contrastive Estimation (NCE), ranking-based loss, Transparency, privacy, cost of adaptation, BBOX-ADAPTER, improved model performance, cost efficiency


- [Collapse-Aware Triplet Decoupling for Adversarially Robust Image Retrieval](https://icml.cc/virtual/2024/poster/33574) (Poster)
  - **Authors:** [Qiwei Tian](http://openreview.net/profile?id=~Qiwei_Tian1), [Chenhao Lin](http://openreview.net/profile?id=~Chenhao_Lin1), [Zhengyu Zhao](http://openreview.net/profile?id=~Zhengyu_Zhao1), [Qian Li](http://openreview.net/profile?id=~Qian_Li11), [Chao Shen](http://openreview.net/profile?id=~Chao_Shen2)
  - **Affiliations:** Xi’an JiaoTong University, Xi’an, China, Xi’an JiaoTong University, Xi’an, China, Xi’an JiaoTong University, Xi’an, China, Xi’an JiaoTong University, Xi’an, China, Xi’an JiaoTong University, Xi’an, China
  - **TL;DR:** This paper proposes Collapse-Aware TRIplet Decoupling (CA-TRIDE) to enhance adversarial training in deep metric learning for image retrieval, addressing the issues of weak adversaries and model collapse. Extensive experiments show that CA-TRIDE outperforms existing defense methods in both conventional and new metrics.
  - **Keywords:** Adversarial training, Deep metric learning (DML), Image retrieval, Collapse-Aware TRIplet Decoupling (CA-TRIDE), perturbation optimization, Image retrieval, Weak adversary, Model collapse, New robustness metric, Improved defense methods


- [A New Branch-and-Bound Pruning Framework for $\ell_0$-Regularized Problems](https://icml.cc/virtual/2024/poster/32714) (Poster)
  - **Authors:** [Guyard Theo](http://openreview.net/profile?id=~Theo_Guyard1), [Cédric Herzet](http://openreview.net/profile?email=cedric.herzet@ensai.fr), [Clément Elvira](http://openreview.net/profile?id=~Cl%C3%A9ment_Elvira1), [Ayse-Nur Arslan](http://openreview.net/profile?email=ayse-nur.arslan@inria.fr)
  - **Affiliations:** Inria and Insa, Univ Rennes, CNRS, IRMAR - UMR 6625, Rennes, France, Ensai, Univ Rennes, CNRS, CREST - UMR 9194, Rennes, France, CentraleSupélec, Univ Rennes, CNRS, IETR - UMR 6164, Rennes, France, Inria, Univ Bordeaux, CNRS, IMB - UMR 5251, Bordeaux, France
  - **TL;DR:** This paper presents a new Branch-and-Bound pruning framework for `0-regularized optimization problems, which significantly reduces computational bottlenecks associated with pruning tests. The proposed method demonstrates improved solving times for typical machine learning applications through numerical simulations.
  - **Keywords:** `0-regularization, Branch-and-Bound algorithms, optimization problems, pruning tests, convex optimization, lower bounds, machine learning, high-dimensional statistics, signal processing, NP-hard problems, computational bottlenecks, improved solving time for BnB procedures, alternative pruning strategy, `0-norm, feature selection, compressive sensing, sparse SVM, neural network pruning


- [Community-Invariant Graph Contrastive Learning](https://icml.cc/virtual/2024/poster/33530) (Poster)
  - **Authors:** [Shiyin Tan](http://openreview.net/profile?id=~Shiyin_Tan1), [Dongyuan Li](http://openreview.net/profile?id=~Dongyuan_Li1), [Renhe Jiang](http://openreview.net/profile?id=~Renhe_Jiang1), [Ying Zhang](http://openreview.net/profile?id=~Ying_Zhang16), [Manabu Okumura](http://openreview.net/profile?id=~Manabu_Okumura2)
  - **Affiliations:** Tokyo Institute of Technology, Tokyo Institute of Technology, The University of Tokyo, RIKEN, Tokyo Institute of Technology, Tokyo Institute of Technology
  - **TL;DR:** This study proposes a community-invariant graph contrastive learning framework that maintains graph community structure during augmentation, addressing the limitations of existing methods that disrupt high-level graph information. Empirical results on 21 benchmark datasets demonstrate the framework's effectiveness in enhancing model robustness and generalization.
  - **Keywords:** Graph Contrastive Learning, Graph Augmentation, Community-Invariant GCL Framework, Spectral Changes Maximization, Node Classification, Link Prediction, Limited Generalization, Corruption of Graph Community Information, Robustness Against Noise, Learnable Graph Augmentation, Enhanced Model Robustness, 21 Benchmark Datasets, GNNs (Graph Neural Networks), Topology, Node Features


- [video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models](https://icml.cc/virtual/2024/poster/33117) (Poster)
  - **Authors:** [Guangzhi Sun](http://openreview.net/profile?id=~Guangzhi_Sun1), [Wenyi Yu](http://openreview.net/profile?id=~Wenyi_Yu2), [Changli Tang](http://openreview.net/profile?id=~Changli_Tang1), [Xianzhao Chen](http://openreview.net/profile?id=~Xianzhao_Chen1), [Tian Tan](http://openreview.net/profile?id=~Tian_Tan5), [Wei Li](http://openreview.net/profile?id=~Wei_Li78), [Lu Lu](http://openreview.net/profile?id=~Lu_Lu6), [Zejun MA](http://openreview.net/profile?id=~Zejun_MA1), [Yuxuan Wang](http://openreview.net/profile?id=~Yuxuan_Wang1), [Chao Zhang](http://openreview.net/profile?id=~Chao_Zhang20)
  - **Affiliations:** Department of Electronic Engineering, Tsinghua University, Department of Electronic Engineering, Tsinghua University, Department of Electronic Engineering, Tsinghua University, ByteDance Ltd., ByteDance Ltd., ByteDance Ltd., ByteDance Ltd., ByteDance Ltd., ByteDance Ltd., Department of Electronic Engineering, Tsinghua University
  - **TL;DR:** This paper introduces video-SALMONN, a speech-enhanced audio-visual large language model designed for comprehensive video understanding, achieving significant accuracy improvements in video and audio-visual question answering tasks. The model effectively integrates speech understanding with visual and audio elements, addressing challenges in fine-grained temporal modeling and modality interaction.
  - **Keywords:** audio-visual large language models, video understanding, speech understanding, multi-resolution causal Q-Former, diversity loss, unpaired audio-visual mixed training, video processing, video question answering (QA), audio-visual QA, under-explored speech in av-LLMs, fine-grained temporal modeling, modality dominance, video-SALMONN model, accuracy improvements on video-QA and audio-visual QA tasks, speech-audio-visual evaluation benchmark, av-LLMs, paralinguistic information, speaker attributes


- [OT-CLIP: Understanding and Generalizing CLIP via Optimal Transport](https://icml.cc/virtual/2024/poster/33836) (Poster)
  - **Authors:** [Liangliang Shi](http://openreview.net/profile?id=~Liangliang_Shi1), [Jack Fan](http://openreview.net/profile?id=~Jack_Fan2), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, Department of Computer Science, University of North Carolina at Chapel Hill, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This paper explores the training and inference of the CLIP model through the lens of Optimal Transport, proposing an OT-based loss family that enhances representation learning and generalizes zero-shot classification tasks. The findings demonstrate improved performance on public datasets for both image and text domains.
  - **Keywords:** Contrastive Language-Image Pretraining, Optimal Transport, Inverse Optimal Transport, InfoNCE loss, Regularized OT, Fused Gromov OT, Unbalanced OT, Image and text domain, Zero-shot classification, Alignment of image-text pairs, Learning representation, OT-based loss family, Generalization of zero-shot classification, Long-tailed classification, Selective classification


- [Ranking-based Client Imitation Selection for Efficient Federated Learning](https://icml.cc/virtual/2024/poster/34540) (Poster)
  - **Authors:** [Chunlin Tian](http://openreview.net/profile?id=~Chunlin_Tian1), [Zhan Shi](http://openreview.net/profile?id=~Zhan_Shi3), [Xinpeng Qin](http://openreview.net/profile?id=~Xinpengqin1), [Li Li](http://openreview.net/profile?id=~Li_Li10), [Cheng-Zhong Xu](http://openreview.net/profile?id=~Cheng-zhong_Xu1)
  - **Affiliations:** University of Macau, University of Texas at Austin, University of Electronic Science and Technology of China, University of Macau, University of Macau
  - **TL;DR:** This study introduces FedRank, a novel device selection solution for Federated Learning that utilizes imitation learning and a ranking-based approach to enhance model performance and training efficiency. Experimental results demonstrate significant improvements in model accuracy, training convergence, and energy savings.
  - **Keywords:** Federated Learning, Device Selection, Imitation Learning, Ranking-based Model, Data Heterogeneity, Training Efficiency, Energy Consumption, FedRank, Model Accuracy Improvement, Training Convergence


- [Copula-Nested Spectral Kernel Network](https://icml.cc/virtual/2024/poster/33692) (Poster)
  - **Authors:** [Jinyue Tian](http://openreview.net/profile?id=~Jinyue_Tian1), [Hui Xue](http://openreview.net/profile?id=~Hui_Xue6), [Yanfang Xue](http://openreview.net/profile?id=~Yanfang_Xue1), [Pengfei Fang](http://openreview.net/profile?id=~Pengfei_Fang1)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing, 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China
  - **TL;DR:** This paper introduces the Copula-Nested Spectral Kernel Network (CokeNet), which redefines spectral density using copulas to improve the representation of complex data interactions. The proposed method demonstrates significant advancements in performance compared to state-of-the-art algorithms in the field.
  - **Keywords:** Spectral Kernel Networks, machine learning, Copula-Nested Spectral Kernel Network (CokeNet), spectral density function, Intricate interactions within data structures, hypothesis space limitations, Enhanced diversity of spectral densities, excavation of complex dependence structures, superior performance over SOTA algorithms


- [Faster Maximum Inner Product Search in High Dimensions](https://icml.cc/virtual/2024/poster/34542) (Poster)
  - **Authors:** [Mo Tiwari](http://openreview.net/profile?id=~Mo_Tiwari1), [Ryan Kang](http://openreview.net/profile?id=~Ryan_Kang1), [Jaeyong Lee](http://openreview.net/profile?id=~Jaeyong_Lee1), [Donghyun Lee](http://openreview.net/profile?id=~Donghyun_Lee2), [Chris Piech](http://openreview.net/profile?id=~Christopher_J_Piech1), [Sebastian Thrun](http://openreview.net/profile?id=~Sebastian_Thrun1), [Ilan Shomorony](http://openreview.net/profile?id=~Ilan_Shomorony1), [Martin Zhang](http://openreview.net/profile?id=~Martin_Jinye_Zhang1)
  - **Affiliations:** Department of Computer Science, Stanford University, Stanford, CA, Department of Computer Science, Stanford University, Stanford, CA, Oxford University, University College London, Department of Computer Science, Stanford University, Stanford, CA, Department of Computer Science, Stanford University, Stanford, CA, Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Ray and Stephanie Lane Computational Biology Department, Carnegie Mellon University
  - **TL;DR:** This paper introduces BanditMIPS, a novel randomized algorithm that significantly improves the efficiency of Maximum Inner Product Search (MIPS) from O(√d) to O(1) in high-dimensional settings. The algorithm demonstrates superior performance in sample complexity and computational speed, making it highly applicable in various machine learning contexts.
  - **Keywords:** Maximum Inner Product Search (MIPS), high-dimensional data, BanditMIPS, randomized algorithms, adaptive sampling, multi-armed bandits, Information retrieval, recommendation systems, Matching Pursuit, Fourier analysis, Computational complexity, high-dimensional settings, scalability issues, Improved algorithm complexity from O(√d) to O(1), sample complexity, wall-clock time efficiency


- [FRAPPÉ: A Group Fairness Framework for Post-Processing Everything](https://icml.cc/virtual/2024/poster/34366) (Poster)
  - **Authors:** [Alexandru Tifrea](http://openreview.net/profile?id=~Alexandru_Tifrea1), [Preethi Lahoti](http://openreview.net/profile?id=~Preethi_Lahoti1), [Ben Packer](http://openreview.net/profile?id=~Ben_Packer1), [Yoni Halpern](http://openreview.net/profile?id=~Yoni_Halpern1), [Ahmad Beirami](http://openreview.net/profile?id=~Ahmad_Beirami1), [Flavien Prost](http://openreview.net/profile?id=~Flavien_Prost1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** The study proposes FRAPP ´E, a framework that transforms regularized in-processing methods into post-processing techniques for group fairness, addressing the challenges of limited computational resources and retraining. The framework maintains effective fairness-error trade-offs and enhances performance in scenarios with partial group labels.
  - **Keywords:** group fairness, fairness-error trade-offs, post-processing, regularized in-processing methods, post-processing techniques, machine learning, social impact applications, biases in machine learning, limited computational resources, retraining challenges, framework for post-processing, modular mitigation strategy, fairness regularizer, fairness constraints


- [Rethinking Optimization and Architecture for Tiny Language Models](https://icml.cc/virtual/2024/poster/33181) (Poster)
  - **Authors:** [Yehui Tang](http://openreview.net/profile?id=~Yehui_Tang1), [Kai Han](http://openreview.net/profile?id=~Kai_Han2), [Fangcheng Liu](http://openreview.net/profile?id=~Fangcheng_Liu1), [Yunsheng Ni](http://openreview.net/profile?id=~Yunsheng_Ni1), [Yuchuan Tian](http://openreview.net/profile?id=~Yuchuan_Tian1), [Zheyuan Bai](http://openreview.net/profile?id=~Zheyuan_Bai2), [Yi-Qi Hu](http://openreview.net/profile?id=~Yi-Qi_Hu1), [Sichao Liu](http://openreview.net/profile?id=~Sichao_Liu1), [Shang-Ling Jui](http://openreview.net/profile?id=~SHANGLING_JUI1), [Yunhe Wang](http://openreview.net/profile?id=~Yunhe_Wang1)
  - **Affiliations:** Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, Peking University, Huawei Noah’s Ark Lab, Consumer Business Group, Huawei, Consumer Business Group, Huawei, Huawei Kirin Solution, Huawei Noah’s Ark Lab
  - **TL;DR:** This study focuses on optimizing the architecture and training strategies for tiny language models, specifically PanGu-π-1B Pro and PanGu-π-1.5B Pro. The results show significant performance improvements, with PanGu-π-1.5B Pro outperforming larger state-of-the-art models, demonstrating the effectiveness of the proposed methodologies.
  - **Keywords:** Tiny language models, Optimization, Architecture, Tokenizer compression, Architecture tweaking, Parameter inheritance, Multiple-round training, Mobile devices, Natural language processing, Computation costs, Memory costs, High-performance requirements, Improved optimization strategies, Enhanced model performance, 1.6T multilingual corpora, Large language models (LLMs), PanGu-π


- [SSL4Q: Semi-Supervised Learning of Quantum Data with Application to Quantum State Classification](https://icml.cc/virtual/2024/poster/35218) (Poster)
  - **Authors:** [Yehui Tang](http://openreview.net/profile?id=~Yehui_Tang3), [Nianzu Yang](http://openreview.net/profile?id=~Nianzu_Yang1), [Mabiao Long](http://openreview.net/profile?id=~Mabiao_Long1), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** The study introduces SSL4Q, a novel semi-supervised learning approach for classifying quantum states, which significantly reduces the computational and resource overhead associated with traditional supervised methods. Empirical results demonstrate its effectiveness in scenarios with limited labeled data, showcasing its potential for advancing quantum computing applications.
  - **Keywords:** Quantum state classification, Semi-supervised learning, SSL4Q, permutation invariance, Quantum computing, Quantum device analysis, Limited labeled data, Exponential resource demands, Improved classification efficiency, Robustness to measurement uncertainties, Heisenberg Model, Variational Quantum Circuit (VQC) Model, Quantum measurements, Quantum data


- [Beyond Individual Input for Deep Anomaly Detection on Tabular Data](https://icml.cc/virtual/2024/poster/33583) (Poster)
  - **Authors:** [Hugo Thimonier](http://openreview.net/profile?id=~Hugo_Thimonier1), [Fabrice Popineau](http://openreview.net/profile?id=~Fabrice_Popineau1), [Arpad Rimmel](http://openreview.net/profile?id=~Arpad_Rimmel1), [Bich-Liên DOAN](http://openreview.net/profile?id=~Bich-Li%C3%AAn_DOAN2)
  - **Affiliations:** Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire Interdisciplinaire des Sciences du Numérique, 91190, Gif-sur-Yvette, France, Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire Interdisciplinaire des Sciences du Numérique, 91190, Gif-sur-Yvette, France, Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire Interdisciplinaire des Sciences du Numérique, 91190, Gif-sur-Yvette, France, Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire Interdisciplinaire des Sciences du Numérique, 91190, Gif-sur-Yvette, France
  - **TL;DR:** This paper presents a novel deep anomaly detection method for tabular data using Non-Parametric Transformers, which effectively captures both feature-feature and sample-sample dependencies. The proposed method outperforms existing techniques on 31 benchmark datasets, achieving significant improvements in F1-score and AUROC.
  - **Keywords:** Anomaly detection, Tabular data, Non-Parametric Transformers (NPTs), Reconstruction-based framework, Finance, Healthcare, Cybersecurity, Dataset contamination, Imbalanced datasets, State-of-the-art performance, Anomaly score generation, 31 benchmark tabular datasets, Feature-feature dependencies, Sample-sample dependencies


- [Finite Smoothing Algorithm for High-Dimensional Support Vector Machines and Quantile Regression](https://icml.cc/virtual/2024/poster/34034) (Poster)
  - **Authors:** [Qian Tang](http://openreview.net/profile?id=~Qian_Tang2), [Yikai Zhang](http://openreview.net/profile?id=~Yikai_Zhang4), [Boxiang Wang](http://openreview.net/profile?id=~Boxiang_Wang1)
  - **Affiliations:** Department of Statistics and Actuarial Science, University of Iowa, Iowa City, IA, 52246, United States, Department of Statistics and Actuarial Science, University of Iowa, Iowa City, IA, 52246, United States, Department of Statistics and Actuarial Science, University of Iowa, Iowa City, IA, 52246, United States
  - **TL;DR:** This paper presents a finite smoothing algorithm (FSA) to enhance the computational efficiency of support vector machines and quantile regression in high-dimensional settings by transforming non-smooth loss functions into smooth ones. The results show that FSA significantly improves speed and maintains precision, with implementations available in open-source R packages.
  - **Keywords:** high-dimensional data, support vector machines, quantile regression, finite smoothing algorithm, coordinate descent techniques, genetic research, genomic research, functional magnetic resonance imaging, clinical trials, financial market analysis, non-smooth loss functions, computational challenges, data sparsity, exact solutions, improved computational efficiency, feature selection, hdsvm, hdqr, sparse penalized SVM, ℓ2 penalized hinge loss, lasso, elastic net, SCAD, MCP


- [MOKD: Cross-domain Finetuning for Few-shot Classification via Maximizing Optimized Kernel Dependence](https://icml.cc/virtual/2024/poster/33444) (Poster)
  - **Authors:** [Hongduan Tian](http://openreview.net/profile?id=~Hongduan_Tian1), [Feng Liu](http://openreview.net/profile?id=~Feng_Liu2), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1), [Bo Du](http://openreview.net/profile?id=~Bo_Du3), [Yiu Ming Cheung](http://openreview.net/profile?id=~Yiu-ming_Cheung1), [Bo Han](http://openreview.net/profile?id=~Bo_Han1)
  - **Affiliations:** TMLR Group, Hong Kong Baptist University; None, TMLR Group, University of Melbourne, Sydney AI Centre, The University of Sydney, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence, School of Computer Science, Wuhan University, TMLR Group, Hong Kong Baptist University, TMLR Group, Hong Kong Baptist University; Department of Computer Science, Hong Kong Baptist University
  - **TL;DR:** This study introduces MOKD, a bi-level optimization framework aimed at improving cross-domain few-shot classification by learning class-specific representations that reduce similarities between different classes. The proposed method demonstrates enhanced generalization performance on unseen domains and better representation clustering compared to traditional approaches.
  - **Keywords:** Cross-domain few-shot classification, Metric learning, Nearest centroid classifier (NCC), Bi-level optimization framework, Optimized kernel HSIC (opt-HSIC), Few-shot classification, Unseen domains, High similarities between representations of different classes, Misclassification due to representation overlap, Maximizing dependence between representations and labels, Minimizing dependence among samples, Meta-Dataset, Hilbert-Schmidt independence criterion (HSIC)


- [Position: Do Not Explain Vision Models Without Context](https://icml.cc/virtual/2024/poster/34918) (Poster)
  - **Authors:** [Paulina Tomaszewska](http://openreview.net/profile?id=~Paulina_Tomaszewska1), [Przemyslaw Biecek](http://openreview.net/profile?id=~Przemyslaw_Biecek2)
  - **Affiliations:** Faculty of Mathematics and Information Science, Warsaw University of Technology, Warsaw, Poland; Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, Warsaw, Poland, Faculty of Mathematics and Information Science, Warsaw University of Technology, Warsaw, Poland; Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, Warsaw, Poland
  - **TL;DR:** This paper argues for the inclusion of contextual information in explaining computer vision models, highlighting the limitations of current XAI methods that overlook spatial relationships. It proposes a shift in focus from merely identifying key image regions to understanding how objects are oriented towards each other, which is crucial for accurate model predictions in critical domains.
  - **Keywords:** Explainable AI (XAI), Computer Vision, LIME, Grad-CAM, Integrated Gradients, Layer-wise Relevance Propagation, Concept Relevance Propagation, Autonomous cars, healthcare, street surveillance systems, Contextual information in model explanations, spatial relationships in images, New research directions for better use of context information in explaining vision models, Spatial XAI


- [Neural SPH: Improved Neural Modeling of Lagrangian Fluid Dynamics](https://icml.cc/virtual/2024/poster/34114) (Poster)
  - **Authors:** [Artur Toshev](http://openreview.net/profile?id=~Artur_Toshev1), [Jonas Erbesdobler](http://openreview.net/profile?id=~Jonas_A._Erbesdobler1), [Nikolaus Adams](http://openreview.net/profile?id=~Nikolaus_A._Adams1), [Johannes Brandstetter](http://openreview.net/profile?id=~Johannes_Brandstetter1)
  - **Affiliations:** Chair of Aerodynamics and Fluid Mechanics, School of Engineering and Design, Technical University of Munich, Garching, Germany; Munich Institute of Integrated Materials, Energy and Process Engineering, Technical University of Munich, Germany, Chair of Aerodynamics and Fluid Mechanics, School of Engineering and Design, Technical University of Munich, Garching, Germany, Chair of Aerodynamics and Fluid Mechanics, School of Engineering and Design, Technical University of Munich, Garching, Germany; Munich Institute of Integrated Materials, Energy and Process Engineering, Technical University of Munich, Germany, ELLIS Unit Linz, LIT AI Lab, Institute for Machine Learning, Johannes Kepler University, Linz, Austria; NXAI GmbH, Austria
  - **TL;DR:** This study enhances GNN-based simulators for Lagrangian fluid dynamics by integrating components from standard SPH solvers, addressing issues like particle clustering and tensile instabilities. The results show significant improvements in performance and stability over long time horizons compared to baseline GNNs.
  - **Keywords:** Lagrangian fluid dynamics, Smoothed Particle Hydrodynamics (SPH), Graph Neural Networks (GNNs), GNN-based simulators, SPH solvers, Computational Fluid Dynamics (CFD), Particle clustering, tensile instabilities, long-term predictions, Enhanced training and rollout inference, improved performance metrics


- [Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring](https://icml.cc/virtual/2024/poster/33534) (Poster)
  - **Authors:** [Taira Tsuchiya](http://openreview.net/profile?id=~Taira_Tsuchiya1), [Shinji Ito](http://openreview.net/profile?id=~Shinji_Ito1), [Junya Honda](http://openreview.net/profile?id=~Junya_Honda1)
  - **Affiliations:** Department of Mathematical Informatics, The University of Tokyo, Tokyo, Japan; RIKEN AIP, Tokyo, Japan, Department of Mathematical Informatics, The University of Tokyo, Tokyo, Japan; RIKEN AIP, Tokyo, Japan; NEC Corporation, Kanagawa, Japan, Department of Systems Science, Kyoto University, Kyoto, Japan
  - **TL;DR:** This paper presents a new framework for exploration by optimization in partial monitoring games, significantly improving regret bounds in both stochastic and adversarial environments. The authors derive a stochastic regret bound that is substantially smaller than existing bounds, demonstrating the effectiveness of their hybrid regularizer approach.
  - **Keywords:** Partial Monitoring, Online Decision-Making, Adversarial Environments, Exploration by Optimization (ExO), Best-of-Both-Worlds (BOBW) algorithms, Hybrid Regularizer, Limited Feedback, Regret Bounds, Stochastic Environments, Adversarial Robustness, Improved Regret Bounds, Stochastic Regret Bound


- [An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems](https://icml.cc/virtual/2024/poster/35001) (Poster)
  - **Authors:** [Hitesh Tulsiani](http://openreview.net/profile?id=~Hitesh_Tulsiani1), [David Chan](http://openreview.net/profile?id=~David_Chan3), [Shalini Ghosh](http://openreview.net/profile?id=~Shalini_Ghosh3), [Garima Lalwani](http://openreview.net/profile?id=~Garima_Lalwani1), [Prabhat Pandey](http://openreview.net/profile?email=panprabh@amazon.com), [Ankish Bansal](http://openreview.net/profile?id=~Ankish_Bansal1), [Sri Garimella](http://openreview.net/profile?email=srigar@amazon.com), [Ariya Rastrow](http://openreview.net/profile?id=~Ariya_Rastrow2), [Björn Hoffmeister](http://openreview.net/profile?id=~Björn_Hoffmeister2)
  - **Affiliations:** Amazon AGI, Amazon AGI; UC Berkeley (work done while at Amazon), Amazon AGI, Amazon AGI, Amazon AGI, Amazon AGI, Amazon AGI, Amazon AGI, Amazon AGI
  - **TL;DR:** This study presents a novel framework for automatic speech recognition in dialog systems that adapts to user feedback and conversational context over time. The proposed approach demonstrates significant improvements in word error rates, achieving reductions of up to 26% on synthetic data compared to traditional methods.
  - **Keywords:** Automatic Speech Recognition (ASR), Dialog Systems, Voice Assistants, Student-Teacher Learning, Contrastive Self-Supervision, Online Hard-Negative Mining, Speech Recognition, Interactive Dialog Systems, Long-tailed Distribution of Rare Words, User Feedback Incorporation, WER Reductions, Contextual Learning Framework


- [Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments](https://icml.cc/virtual/2024/poster/33216) (Oral)
  - **Authors:** [Allen Tran](http://openreview.net/profile?id=~Allen_Tran1), [Aurelien Bibaut](http://openreview.net/profile?id=~Aurelien_Bibaut1), [Nathan Kallus](http://openreview.net/profile?id=~Nathan_Kallus1)
  - **Affiliations:** Netflix Inc., Los Angeles, USA; None, Netflix Inc., Los Gatos, USA, Netflix Inc., Los Gatos, USA; Cornell University, New York, USA
  - **TL;DR:** This study develops a method to estimate the long-term causal effects of long-term treatments using data from short-term experiments, addressing the limitations of traditional surrogate methods. The findings suggest that the proposed method can effectively capture long-term dynamics without relying on surrogate variables or observational datasets.
  - **Keywords:** long-term causal effects, long-term treatments, short-term experiments, offline reinforcement learning, doubly-robust estimators, health effects, environmental hazards, online platform changes, difficulty of measuring long-term effects, limitations of surrogate methods, new method for estimating long-term effects from short-term data


- [Improving Antibody Humanness Prediction using Patent Data](https://icml.cc/virtual/2024/poster/32845) (Poster)
  - **Authors:** [Talip Ucar](http://openreview.net/profile?id=~Talip_Ucar2), [Aubin Ramon](http://openreview.net/profile?id=~Aubin_Ramon1), [Dino Oglic](http://openreview.net/profile?id=~Dino_Oglic1), [Rebecca Croasdale-Wood](http://openreview.net/profile?email=rebecca.croasdale-wood@astrazeneca.com), [Tom Diethe](http://openreview.net/profile?id=~Tom_Diethe1), [Pietro Sormanni](http://openreview.net/profile?id=~Pietro_Sormanni1)
  - **Affiliations:** Centre for AI, BioPharmaceuticals R&D, AstraZeneca, Centre for Misfolding Diseases, Yusuf Hamied Department of Chemistry, University of Cambridge, Centre for AI, BioPharmaceuticals R&D, AstraZeneca, Biologics Engineering, Oncology R&D, AstraZeneca, Centre for AI, BioPharmaceuticals R&D, AstraZeneca, Centre for Misfolding Diseases, Yusuf Hamied Department of Chemistry, University of Cambridge
  - **TL;DR:** This study proposes a weakly-supervised contrastive learning approach to improve antibody humanness prediction using patent data, addressing the challenge of immunogenicity in monoclonal antibody therapeutics. The results demonstrate that the model outperforms existing baselines and sets new state-of-the-art performance on multiple inference tasks.
  - **Keywords:** Antibody humanness prediction, Immunogenicity, Drug discovery, Weakly-supervised contrastive learning, Encoder training, Cross-entropy loss, Biopharmaceuticals, Therapeutic monoclonal antibodies, Immunogenic response, Attrition in drug development, Humanization process, State-of-the-art performance in humanness prediction, Improved representation learning, Patented antibody database (PAD), Observed Antibody Space (OAS)


- [Coactive Learning for Large Language Models using Implicit User Feedback](https://icml.cc/virtual/2024/poster/35295) (Poster)
  - **Authors:** [Aaron D. Tucker](http://openreview.net/profile?id=~Aaron_David_Tucker1), [Kianté Brantley](http://openreview.net/profile?id=~Kiant%C3%A9_Brantley2), [Adam Cahall](http://openreview.net/profile?id=~Adam_Cahall1), [Thorsten Joachims](http://openreview.net/profile?id=~Thorsten_Joachims1)
  - **Affiliations:** Department of Computer Science, Cornell University, Ithaca, NY, Department of Computer Science, Cornell University, Ithaca, NY, Department of Computer Science, Cornell University, Ithaca, NY, Department of Computer Science, Cornell University, Ithaca, NY
  - **TL;DR:** This paper introduces coactive learning as a method for training large language models (LLMs) using implicit user feedback from text edits, enabling personalization and contextual adaptation. The proposed CoRLL algorithm demonstrates effectiveness even with weak feedback, suggesting a promising approach for improving LLMs in various applications.
  - **Keywords:** Coactive learning, Large language models (LLMs), Personalization, CoRLL, Reinforcement learning from human feedback (RLHF), Human-AI writing collaboration, Text generation, Implicit user feedback, Non-gold-standard training examples, Coactive learning algorithm for LLMs, Improvement of LLMs through user edits, Implicit preference feedback


- [How to Leverage Diverse Demonstrations in Offline Imitation Learning](https://icml.cc/virtual/2024/poster/33087) (Poster)
  - **Authors:** [Sheng Yue](http://openreview.net/profile?id=~Sheng_Yue1), [Jiani Liu](http://openreview.net/profile?id=~Jiani_Liu2), [Xingyuan Hua](http://openreview.net/profile?id=~Xingyuan_Hua1), [Ju Ren](http://openreview.net/profile?id=~Ju_Ren1), [Sen Lin](http://openreview.net/profile?id=~Sen_Lin1), [Junshan Zhang](http://openreview.net/profile?id=~Junshan_Zhang1), [Yaoxue Zhang](http://openreview.net/profile?id=~Yaoxue_Zhang3)
  - **Affiliations:** Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China, Department of Computer Science, University of Houston, Texas, US, Department of Electrical and Computer Engineering, University of California, Davis, US, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Zhongguancun Laboratory, Beijing, China
  - **TL;DR:** This paper presents a novel data selection method for Offline Imitation Learning that effectively extracts positive behaviors from noisy demonstrations, achieving state-of-the-art performance across various benchmarks. The proposed lightweight behavior cloning algorithm leverages both expert and selected data, significantly enhancing robustness and generalization in high-dimensional tasks.
  - **Keywords:** Offline Imitation Learning, Data Selection, Behavior Cloning, Data Selection Method, Autonomous Driving, Healthcare, Noisy Data, Data Sparsity, Expert Data Scarcity, State-of-the-art Performance, Lightweight Behavior Cloning Algorithm


- [Reward-Free Kernel-Based Reinforcement Learning](https://icml.cc/virtual/2024/poster/34080) (Poster)
  - **Authors:** [Sattar Vakili](http://openreview.net/profile?id=~Sattar_Vakili1), [Farhang Nabiei](http://openreview.net/profile?id=~Farhang_Nabiei1), [Da-shan Shiu](http://openreview.net/profile?id=~Da-shan_Shiu1), [Alberto Bernacchia](http://openreview.net/profile?id=~Alberto_Bernacchia1)
  - **Affiliations:** MediaTek Research, MediaTek Research, MediaTek Research, MediaTek Research
  - **TL;DR:** This paper addresses the challenge of achieving sample efficiency in reward-free reinforcement learning by proposing a kernel-based algorithm that utilizes adaptive domain partitioning. The authors demonstrate that their approach achieves order-optimal sample complexity across a broad class of common kernels.
  - **Keywords:** Reward-Free Reinforcement Learning, Sample Efficiency, Kernel-based Function Approximations, Adaptive Domain Partitioning, Gaming, Autonomous Driving, Microchip Design, Robot Control, Algorithm Search, Efficient Exploration, Optimal Policy Calculation, Offline Dataset Representation, Order-Optimal Sample Complexity, Near-Optimal Policy Calculation


- [Federated Self-Explaining GNNs with Anti-shortcut Augmentations](https://icml.cc/virtual/2024/poster/33711) (Poster)
  - **Authors:** [Linan Yue](http://openreview.net/profile?id=~Linan_Yue1), [Qi Liu](http://openreview.net/profile?id=~Qi_Liu3), [Weibo Gao](http://openreview.net/profile?id=~Weibo_Gao1), [Ye Liu](http://openreview.net/profile?id=~Ye_Liu10), [Kai Zhang](http://openreview.net/profile?id=~Kai_Zhang12), [Yichao Du](http://openreview.net/profile?id=~Yichao_Du1), [Li Wang](http://openreview.net/profile?id=~Li_Wang18), [Fangzhou Yao](http://openreview.net/profile?id=~Fangzhou_Yao1)
  - **Affiliations:** State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China; None, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Hefei, China, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China, ByteDance, Hangzhou, China, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China
  - **TL;DR:** This paper proposes a Federated Graph Rationalization (FedGR) method with anti-shortcut augmentations to enhance the explainability of Graph Neural Networks (GNNs) in federated learning scenarios. The approach addresses the shortcut problem by generating client-specific samples, demonstrating effectiveness through experiments on various datasets.
  - **Keywords:** Graph Neural Networks (GNNs), Explainability, Graph rationalization, Federated Learning (FL), Anti-shortcut augmentations, Graph classification, Explainability of predictions, Shortcut problem, Spurious correlation, Federated Graph Rationalization (FedGR), Client-specific shortcut conflicted samples, Real-world benchmarks, Synthetic datasets


- [Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function](https://icml.cc/virtual/2024/poster/34388) (Poster)
  - **Authors:** [Keyon Vafa](http://openreview.net/profile?id=~Keyon_Vafa1), [Ashesh Rambachan](http://openreview.net/profile?id=~Ashesh_Rambachan1), [Sendhil Mullainathan](http://openreview.net/profile?id=~Sendhil_Mullainathan2)
  - **Affiliations:** Harvard University, Massachusetts Institute of Technology, Massachusetts Institute of Technology
  - **TL;DR:** This study investigates how people form beliefs about the capabilities of large language models (LLMs) through a human generalization function, highlighting the challenges in evaluating LLMs due to their diverse applications. The findings reveal that more capable models may perform poorly in real-world applications if they are misaligned with human expectations, particularly in high-stakes scenarios.
  - **Keywords:** Large Language Models, Human Generalization Function, Model Evaluation, Natural Language Processing (NLP), Evaluation of model capabilities, Misalignment with human expectations, Dataset of human generalizations, Alignment problem for LLMs, MMLU, BIG-Bench


- [Feedback Efficient Online Fine-Tuning of Diffusion Models](https://icml.cc/virtual/2024/poster/33528) (Poster)
  - **Authors:** [Masatoshi Uehara](http://openreview.net/profile?id=~Masatoshi_Uehara1), [Yulai Zhao](http://openreview.net/profile?id=~Yulai_Zhao1), [Kevin Black](http://openreview.net/profile?id=~Kevin_Black2), [Ehsan Hajiramezanali](http://openreview.net/profile?id=~Ehsan_Hajiramezanali1), [Gabriele Scalia](http://openreview.net/profile?id=~Gabriele_Scalia1), [Nathaniel Diamant](http://openreview.net/profile?id=~Nathaniel_Lee_Diamant1), [Alex Tseng](http://openreview.net/profile?id=~Alex_M_Tseng1), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1), [Tommaso Biancalani](http://openreview.net/profile?id=~Tommaso_Biancalani1)
  - **Affiliations:** Genentech, Princeton University, University of California, Berkeley, Genentech, Genentech, Genentech, Genentech, University of California, Berkeley, Genentech
  - **TL;DR:** This study proposes a novel reinforcement learning approach to fine-tune diffusion models for generating high-reward samples in various domains, including images and molecules. The method addresses challenges in efficiently exploring feasible samples and acquiring ground-truth feedback, providing theoretical guarantees and empirical validation.
  - **Keywords:** Diffusion models, reinforcement learning, Image generation, drug discovery, biological sequences, Efficient exploration of high-reward samples, challenges in acquiring ground-truth feedback, Novel reinforcement learning procedure, theoretical analysis with regret guarantee


- [Matroid Semi-Bandits in Sublinear Time](https://icml.cc/virtual/2024/poster/34218) (Poster)
  - **Authors:** [Ruo-Chun Tzeng](http://openreview.net/profile?id=~Ruo-Chun_Tzeng1), [Naoto Ohsaka](http://openreview.net/profile?id=~Naoto_Ohsaka2), [Kaito Ariu](http://openreview.net/profile?id=~Kaito_Ariu1)
  - **Affiliations:** EECS, KTH Royal Institute of Technology, Sweden, AI Lab, CyberAgent, Japan, AI Lab, CyberAgent, Japan
  - **TL;DR:** This study introduces FasterCUCB, a sublinear-time algorithm for the matroid semi-bandits problem, which significantly reduces computational complexity while maintaining a regret upper bound similar to existing algorithms. The proposed method is applicable to various classes of matroids and addresses the challenges of existing greedy algorithms in large-scale settings.
  - **Keywords:** matroid semi-bandits, computational efficiency, FasterCUCB, Combinatorial Upper Confidence Bound (CUCB), Combinatorial Thompson Sampling (CTS), online advertising, news selection, ad placement, diversified recommendation, network routing, task assignment, per-round time complexity, regret analysis, sublinear-time algorithm, approximate maximum-weight basis


- [Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models](https://icml.cc/virtual/2024/poster/33066) (Poster)
  - **Authors:** [Jan van den Brand](http://openreview.net/profile?id=~Jan_van_den_Brand1), [Zhao Song](http://openreview.net/profile?id=~Zhao_Song3), [Tianyi Zhou](http://openreview.net/profile?id=~Tianyi_Zhou4)
  - **Affiliations:** Georgia Tech, Atlanta, GA, USA, Adobe Research, San Jose, CA, USA, University of Southern California, Los Angeles, CA, USA
  - **TL;DR:** This paper introduces a dynamic version of the attention matrix multiplication problem in large language models, providing an efficient data structure for updates and queries. The findings suggest significant implications for improving the efficiency of attention mechanisms in LLMs.
  - **Keywords:** Large Language Models, Attention Mechanism, Attention Matrix Multiplication, Lazy Update, Natural Language Processing, Language Translation, Sentiment Analysis, Dynamic Attention Maintenance, Update Efficiency, Amortized Update Time, Query Time Analysis, Transformers, BERT, GPT-3, GPT-4


- [Position: Enforced Amnesia as a Way to Mitigate the Potential Risk of Silent Suffering in the Conscious AI](https://icml.cc/virtual/2024/poster/33138) (Poster)
  - **Authors:** [Yegor Tkachenko](http://openreview.net/profile?id=~Yegor_Tkachenko1)
  - **Affiliations:** Columbia University, New York, USA
  - **TL;DR:** This paper explores the ethical implications of potential silent suffering in conscious AI systems and proposes a method of enforced amnesia to mitigate this risk, suggesting that preventing access to memory could reduce suffering and disrupt a continuous self-identity in these systems.
  - **Keywords:** Conscious AI, Silent Suffering, Ethical Implications, Artificial Intelligence, Silent suffering in AI, Consciousness assessment, Enforced amnesia, Memory management in AI, Large Language Models (LLM), Consciousness, Self-awareness


- [How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model](https://icml.cc/virtual/2024/poster/34655) (Spotlight Poster)
  - **Authors:** [Umberto Tomasini](http://openreview.net/profile?id=~Umberto_Maria_Tomasini1), [Matthieu Wyart](http://openreview.net/profile?id=~Matthieu_Wyart2)
  - **Affiliations:** Institute of Physics, EPFL, Lausanne, Switzerland, Institute of Physics, EPFL, Lausanne, Switzerland
  - **TL;DR:** This study investigates how deep networks learn from sparse and hierarchical data, introducing the Sparse Random Hierarchy Model (SRHM) to explain the correlation between learning insensitivity to spatial transformations and performance. The findings highlight the importance of hierarchical representations in improving the sample complexity of CNNs.
  - **Keywords:** deep learning, hierarchical representations, high-dimensional data, Sparse Random Hierarchy Model (SRHM), Convolutional Neural Networks (CNNs), image classification, machine learning, data sparsity, sample complexity, insensitivity to spatial transformations, hierarchical representation learning, correlation between insensitivity and performance


- [Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data](https://icml.cc/virtual/2024/poster/33507) (Poster)
  - **Authors:** [Nikita Tsoy](http://openreview.net/profile?id=~Nikita_Tsoy1), [Nikola Konstantinov](http://openreview.net/profile?id=~Nikola_Konstantinov1)
  - **Affiliations:** INSAIT, Sofia University, Bulgaria, INSAIT, Sofia University, Bulgaria
  - **TL;DR:** This study characterizes simplicity bias in two-layer neural networks, demonstrating that during early training, features cluster around a few directions, which intensifies in later stages. The findings suggest that features learned in the middle stages of training may be more beneficial for out-of-distribution transfer.
  - **Keywords:** Simplicity bias, Out-of-distribution generalization, Neural networks, Two-layer neural networks, Gradient flow, Image classification, Limited out-of-distribution generalization, Reliance on simple features, Mathematical characterization of features, Identification of learned features, XOR-like data, General datasets


- [Piecewise Constant and Linear Regression Trees: An Optimal Dynamic Programming Approach](https://icml.cc/virtual/2024/poster/32947) (Poster)
  - **Authors:** [Mim van den Bos](http://openreview.net/profile?id=~Mim_van_den_Bos1), [Jacobus van der Linden](http://openreview.net/profile?id=~Jacobus_G._M._van_der_Linden1), [Emir Demirović](http://openreview.net/profile?id=~Emir_Demirovi%C4%871)
  - **Affiliations:** Department of Software Technology, Delft University of Technology, Delft, The Netherlands, Department of Software Technology, Delft University of Technology, Delft, The Netherlands, Department of Software Technology, Delft University of Technology, Delft, The Netherlands
  - **TL;DR:** This study presents optimal dynamic programming approaches to improve the scalability of regression trees, specifically focusing on piecewise constant and linear regression methods. The proposed methods significantly enhance performance and scalability compared to existing optimal methods while maintaining or improving out-of-sample performance.
  - **Keywords:** regression trees, optimal methods, dynamic programming, piecewise constant regression trees, piecewise multiple linear regression, piecewise simple linear regression, dynamic programming, regression analysis, ecology analysis, clinical psychology, NP-hard computation, scalability challenges, suboptimal representation of data, improved scalability, optimal methods for regression trees, CART (Classification and Regression Trees), Mixed-Integer Programming (MIP), constraint programming (CP), maximum satisfiability (MaxSAT)


- [Proactive DP: A Multiple Target Optimization Framework for DP-SGD](https://icml.cc/virtual/2024/poster/35134) (Poster)
  - **Authors:** [Marten van Dijk](http://openreview.net/profile?id=~Marten_van_Dijk1), [Nhuong Nguyen](http://openreview.net/profile?id=~Nhuong_Van_Nguyen1), [Toan N. Nguyen](http://openreview.net/profile?id=~Toan_N._Nguyen1), [Lam M. Nguyen](http://openreview.net/profile?id=~Lam_M._Nguyen1), [Phuong Ha Nguyen](http://openreview.net/profile?id=~Phuong_Ha_Nguyen1)
  - **Affiliations:** Centrum Wiskunde & Informatica, Amsterdam, Netherlands; Vrije Universiteit van Amsterdam, Netherlands; ECE Department, University of Connecticut, USA, CSE Department, University of Connecticut, USA; Faculty of Information Technology, University of Science, Ho Chi Minh, Vietnam; Vietnam National University, Ho Chi Minh city, Vietnam, CSE Department, University of Connecticut, USA; Faculty of Information Technology, University of Science, Ho Chi Minh, Vietnam; Vietnam National University, Ho Chi Minh city, Vietnam, IBM Research, Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA, eBay Inc., San Jose, CA 95125, USA
  - **TL;DR:** This study introduces a proactive DP framework for optimizing parameters in DP-SGD based on a fixed privacy budget, aiming to enhance anticipated utility (test accuracy). The authors present a closed-form DP guarantee and tools for linking privacy and utility objectives, demonstrating the effectiveness of their approach through rigorous experiments.
  - **Keywords:** Differential Privacy, DP-SGD, privacy budget optimization, Stochastic Gradient Descent (SGD), moment account method, closed-form (ϵ, δ)-DP guarantee, Private machine learning training, Tracking expenditure of privacy budgets, optimizing parameters for privacy and utility, Proactive DP framework, utility graph, DP calculator, (ϵ, δ)-DP, Concentrated Differential Privacy (CDP), Renyi-DP, zero-CDP (zCDP), f-DP


- [When Representations Align: Universality in Representation Learning Dynamics](https://icml.cc/virtual/2024/poster/33188) (Poster)
  - **Authors:** [Loek van Rossem](http://openreview.net/profile?id=~Loek_van_Rossem1), [Andrew Saxe](http://openreview.net/profile?id=~Andrew_M_Saxe1)
  - **Affiliations:** Gatsby Computational Neuroscience Unit, University College London; Sainsbury Wellcome Centre, University College London, Gatsby Computational Neuroscience Unit, University College London; Sainsbury Wellcome Centre, University College London
  - **TL;DR:** This study derives an effective theory of representation learning that highlights the universal dynamics across different deep neural network architectures. It demonstrates that despite architectural differences, certain behaviors in representation learning are conserved when models are sufficiently flexible.
  - **Keywords:** representation learning, deep neural networks, universality in learning dynamics, scalability in deep learning, learning dynamics across architectures, effective theory of representation learning, universal theories in learning


- [Statistically Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution](https://icml.cc/virtual/2024/poster/35053) (Poster)
  - **Authors:** [Elen Vardanyan](http://openreview.net/profile?id=~Elen_Vardanyan1), [Sona Hunanyan](http://openreview.net/profile?id=~Sona_Hunanyan1), [Tigran Galstyan](http://openreview.net/profile?id=~Tigran_Galstyan1), [Arshak Minasyan](http://openreview.net/profile?id=~Arshak_Minasyan1), [Arnak Dalalyan](http://openreview.net/profile?id=~Arnak_S._Dalalyan2)
  - **Affiliations:** Department of Mathematics, Yerevan State University (YSU), Armenia; YerevaNN, Armenia, Department of Mathematics, Yerevan State University (YSU), Armenia; YerevaNN, Armenia, Department of Mathematics, Yerevan State University (YSU), Armenia; YerevaNN, Armenia, CREST, GENES, Institut Polytechnique de Paris, France, CREST, GENES, Institut Polytechnique de Paris, France
  - **TL;DR:** This paper investigates generative modeling, focusing on creating diverse examples from an unknown distribution while avoiding replication of training data. It presents theoretical insights on the Wasserstein GAN, demonstrating that left-invertibility allows for significant deviation from the empirical distribution without sacrificing statistical optimality.
  - **Keywords:** Generative modeling, Diversity in generated examples, Wasserstein GAN, left-invertible push-forward maps, Health, Climate, Finance, Energy, Physics, Chemistry, Biology, Non-replication of observed examples, Overfitting, Limited diversity, Mode collapse, Finite-sample lower and upper bounds on Wasserstein-1 distance, GANs (Generative Adversarial Networks), Lipschitz constant, Statistical optimality


- [Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features](https://icml.cc/virtual/2024/poster/33431) (Poster)
  - **Authors:** [Rodrigo Veiga](http://openreview.net/profile?id=~Rodrigo_Veiga1), [Anastasia Remizova](http://openreview.net/profile?id=~Anastasia_Remizova1), [Nicolas Macris](http://openreview.net/profile?id=~Nicolas_Macris1)
  - **Affiliations:** École Polytechnique Fédérale de Lausanne (EPFL), Lab for Statistical Mechanics of Inference in Large Systems (SMILS), CH-1015 Lausanne, Switzerland, École Polytechnique Fédérale de Lausanne (EPFL), Lab for Statistical Mechanics of Inference in Large Systems (SMILS), CH-1015 Lausanne, Switzerland, École Polytechnique Fédérale de Lausanne (EPFL), Lab for Statistical Mechanics of Inference in Large Systems (SMILS), CH-1015 Lausanne, Switzerland
  - **TL;DR:** This study investigates the test risk dynamics of stochastic gradient flow in learning theory, providing a general formula for the differences between pure and stochastic gradient flows. The findings reveal corrections due to stochasticity in the context of weak features and the double descent phenomenon, validated against numerical simulations.
  - **Keywords:** stochastic gradient descent, test risk, double descent phenomenon, stochastic gradient flow, Itô stochastic differential equation, path integral formulation, learning theory, neural networks, regression models, optimization dynamics, bias-variance trade-off, effects of stochasticity, general formula for transition probability, covariance expressions, corrections in learning dynamics


- [Optimal Transport for Structure Learning Under Missing Data](https://icml.cc/virtual/2024/poster/35214) (Poster)
  - **Authors:** [Vy Vo](http://openreview.net/profile?id=~Vy_Vo2), [He Zhao](http://openreview.net/profile?id=~He_Zhao1), [Trung Le](http://openreview.net/profile?id=~Trung_Le2), [Edwin V. Bonilla](http://openreview.net/profile?id=~Edwin_V._Bonilla1), [Dinh Phung](http://openreview.net/profile?id=~Dinh_Phung2)
  - **Affiliations:** Monash University, Australia; CSIRO’s Data61, Australia; VinAI Research, Vietnam, CSIRO’s Data61, Australia, Monash University, Australia, CSIRO’s Data61, Australia, Monash University, Australia; VinAI Research, Vietnam
  - **TL;DR:** This study proposes a score-based algorithm for learning causal structures from missing data using optimal transport, addressing the limitations of traditional imputation methods. The framework demonstrates superior performance in recovering true causal graphs and offers scalability and flexibility for existing causal discovery methods.
  - **Keywords:** Causal discovery, Missing data, Optimal transport, Score-based algorithm, Expectation maximization, Missing data imputation, Causal structure learning, Dependencies among variables, Density fitting problem, Wasserstein distance minimization


- [Topological Neural Networks go Persistent, Equivariant, and Continuous](https://icml.cc/virtual/2024/poster/34586) (Poster)
  - **Authors:** [Yogesh Verma](http://openreview.net/profile?id=~Yogesh_Verma1), [Amauri Souza](http://openreview.net/profile?id=~Amauri_H_Souza1), [Vikas Garg](http://openreview.net/profile?id=~Vikas_Garg2)
  - **Affiliations:** Department of Computer Science, Aalto University, Finland, Federal Institute of Ceará; YaiYai Ltd, Department of Computer Science, Aalto University; Federal Institute of Ceará; YaiYai Ltd
  - **TL;DR:** This study introduces TopNets, a framework that integrates Topological Neural Networks and Persistent Homology to enhance the expressivity of simplicial message-passing networks. The proposed methods demonstrate strong performance in various applications, including antibody design and molecular dynamics simulation.
  - **Keywords:** Topological Neural Networks, Persistent Homology, Graph Neural Networks, Simplicial message-passing networks, TopNets, Neural ODEs, Antibody design, Molecular dynamics simulation, Drug property prediction, Limitations of Graph Neural Networks, Non-isomorphic graph distinction, Higher-order interactions, Enhanced expressivity of networks, Integration of topological features, Topological data analysis, Geometric complexes, E(n)-equivariance


- [Discovering Mixtures of Structural Causal Models from Time Series Data](https://icml.cc/virtual/2024/poster/33598) (Poster)
  - **Authors:** [Sumanth Varambally](http://openreview.net/profile?id=~Sumanth_Varambally1), [Yian Ma](http://openreview.net/profile?id=~Yian_Ma1), [Rose Yu](http://openreview.net/profile?id=~Rose_Yu1)
  - **Affiliations:** Halıcıoğlu Data Science Institute, University of California, San Diego, La Jolla, USA, Halıcıoğlu Data Science Institute, University of California, San Diego, La Jolla, USA, Department of Computer Science and Engineering, University of California, San Diego, La Jolla, USA
  - **TL;DR:** This study introduces a variational inference-based framework called MCD for discovering causal relationships from time series data that originate from a mixture of causal models. The proposed method outperforms existing benchmarks in causal discovery tasks, particularly in heterogeneous data scenarios.
  - **Keywords:** causal discovery, time series data, structural causal models, variational inference, MCD-Linear, MCD-Nonlinear, finance, climate science, neuroscience, heterogeneous data, multiple causal models, evidence-lower bound maximization, identification of causal graphs, synthetic datasets, real-world datasets, causal graphs, causal relationships, Granger causality


- [Position: Why Tabular Foundation Models Should Be a Research Priority](https://icml.cc/virtual/2024/poster/33671) (Poster)
  - **Authors:** [Boris van Breugel](http://openreview.net/profile?id=~Boris_van_Breugel2), [M van der Schaar](http://openreview.net/profile?id=~Mihaela_van_der_Schaar2)
  - **Affiliations:** Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK; Alan Turing Institute, London, UK, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK; Alan Turing Institute, London, UK
  - **TL;DR:** The paper advocates for increased research focus on tabular foundation models, proposing the development of Large Tabular Models (LTMs) to enhance the utilization of tabular data in various scientific fields. The authors argue that LTMs could significantly impact data science and facilitate multidisciplinary discoveries by contextualizing datasets.
  - **Keywords:** tabular data, foundation models, Large Tabular Model (LTM), data science, multidisciplinary scientific discovery, lack of research attention on tabular data, underrepresentation of tabular data in ML, few-shot tabular models, automating data science, out-of-distribution synthetic data


- [Code as Reward: Empowering Reinforcement Learning with VLMs](https://icml.cc/virtual/2024/poster/34923) (Spotlight Poster)
  - **Authors:** [David Venuto](http://openreview.net/profile?id=~David_Venuto1), [Mohammad Sami Nur Islam](http://openreview.net/profile?id=~Mohammad_Sami_Nur_Islam1), [Martin Klissarov](http://openreview.net/profile?id=~Martin_Klissarov1), [Doina Precup](http://openreview.net/profile?id=~Doina_Precup1), [Sherry Yang](http://openreview.net/profile?id=~Sherry_Yang1), [Ankit Anand](http://openreview.net/profile?id=~Ankit_Anand4)
  - **Affiliations:** Mila; McGill University, McGill University, Mila; McGill University, Mila; McGill University; Google DeepMind, Google DeepMind; University of California, Berkeley, Google DeepMind
  - **TL;DR:** This paper presents a framework called Code as Reward (VLM-CaR) that utilizes pre-trained Vision-Language Models to generate dense reward functions for training reinforcement learning agents, addressing the computational inefficiencies of direct VLM querying. The approach demonstrates improved accuracy and effectiveness in training RL policies compared to traditional sparse rewards.
  - **Keywords:** Reinforcement Learning, Vision-Language Models, Code Generation, Reward Functions, Image-based Observations, Multi-modal Tasks, Computational Efficiency, Reward Computation Challenges, Dense Reward Functions, Automated Code Verification


- [Parameter Estimation in DAGs from Incomplete Data via Optimal Transport](https://icml.cc/virtual/2024/poster/33249) (Poster)
  - **Authors:** [Vy Vo](http://openreview.net/profile?id=~Vy_Vo2), [Trung Le](http://openreview.net/profile?id=~Trung_Le2), [Tung-Long Vuong](http://openreview.net/profile?id=~Long_Tung_Vuong1), [He Zhao](http://openreview.net/profile?id=~He_Zhao1), [Edwin V. Bonilla](http://openreview.net/profile?id=~Edwin_V._Bonilla1), [Dinh Phung](http://openreview.net/profile?id=~Dinh_Phung2)
  - **Affiliations:** Monash University, Australia; CSIRO’s Data61, Australia; VinAI Research, Vietnam, Monash University, Australia, Monash University, Australia, CSIRO’s Data61, Australia, CSIRO’s Data61, Australia, Monash University, Australia; VinAI Research, Vietnam
  - **TL;DR:** This paper presents a novel approach to parameter estimation in probabilistic directed graphical models using optimal transport, addressing challenges posed by latent variables and intractable posteriors. The proposed method demonstrates effectiveness in recovering true parameters and outperforms existing techniques in various applications.
  - **Keywords:** parameter estimation, probabilistic directed graphical models, latent variables, optimal transport, likelihood maximization, expectation maximization (EM), variational inference (VI), incomplete data, intractable posterior distribution, local optima issues, new framework for parameter learning, empirical evidence of robustness and versatility, directed graphs, Bayesian networks, evidence lower bound (ELBO), amortized inference


- [Imitation Learning in Discounted Linear MDPs without exploration assumptions](https://icml.cc/virtual/2024/poster/34637) (Poster)
  - **Authors:** [Luca Viano](http://openreview.net/profile?id=~Luca_Viano1), [EFSTRATIOS PANTELEIMON SKOULAKIS](http://openreview.net/profile?id=~Stratis_Skoulakis2), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1)
  - **Affiliations:** LIONS, EPFL, Lausanne, Switzerland, LIONS, EPFL, Lausanne, Switzerland, LIONS, EPFL, Lausanne, Switzerland
  - **TL;DR:** This paper introduces a new algorithm for imitation learning in infinite horizon linear MDPs, called ILARL, which significantly reduces the number of required trajectories for learning by removing previous exploration assumptions. The results demonstrate that ILARL outperforms existing algorithms, providing a more efficient approach to learning from expert demonstrations.
  - **Keywords:** Imitation Learning, Linear MDPs, ILARL algorithm, online learning, adversarial losses, Autonomous driving, robotics, economics/finance, Exploration assumptions, trajectory sampling, expert policy interaction, Improved bounds on required trajectories, performance comparison with existing algorithms


- [Convergence of Some Convex Message Passing Algorithms to a Fixed Point](https://icml.cc/virtual/2024/poster/34676) (Spotlight Poster)
  - **Authors:** [Václav Voráček](http://openreview.net/profile?id=~Vaclav_Voracek1), [Tomáš Werner](http://openreview.net/profile?id=~Tomas_Werner1)
  - **Affiliations:** Tübingen AI center, University of Tübingen, Dept. of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague
  - **TL;DR:** The study investigates the convergence properties of convex message passing algorithms used for MAP inference in graphical models, proving that the iterates converge to a fixed point and that the algorithm terminates within O(1/ε) iterations. This finding enhances the understanding of these methods' effectiveness in solving NP-hard combinatorial optimization problems.
  - **Keywords:** MAP inference, graphical models, convex message passing, block-coordinate descent, dual linear programming, Lagrangian relaxation, combinatorial optimization, computer vision, NP-hard combinatorial optimization, discrete energy minimization, convergence to a fixed point, algorithm termination within O(1/ε) iterations


- [To the Max: Reinventing Reward in Reinforcement Learning](https://icml.cc/virtual/2024/poster/35025) (Poster)
  - **Authors:** [Grigorii Veviurko](http://openreview.net/profile?id=~Grigorii_Veviurko1), [Wendelin Boehmer](http://openreview.net/profile?id=~Wendelin_Boehmer1), [Mathijs de Weerdt](http://openreview.net/profile?id=~Mathijs_de_Weerdt1)
  - **Affiliations:** Delft University of Technology, Delft University of Technology, Delft University of Technology
  - **TL;DR:** This paper introduces max-reward reinforcement learning, where agents optimize the maximum reward achieved in an episode instead of cumulative returns, addressing challenges in sparse reward environments. The proposed method simplifies reward design and demonstrates improved learning performance in goal-reaching tasks compared to standard RL approaches.
  - **Keywords:** reinforcement learning, reward optimization, max-reward RL, state-of-the-art RL algorithms, goal-reaching environments, robotics, sparse reward problems, suboptimal behavior in RL, new reward design paradigm, improved learning performance, Gymnasium-Robotics


- [Unsupervised Evaluation of Code LLMs with Round-Trip Correctness](https://icml.cc/virtual/2024/poster/33761) (Poster)
  - **Authors:** [Miltiadis Allamanis](http://openreview.net/profile?id=~Miltiadis_Allamanis1), [Sheena Panthaplackel](http://openreview.net/profile?id=~Sheena_Panthaplackel1), [Pengcheng Yin](http://openreview.net/profile?id=~Pengcheng_Yin1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This paper introduces round-trip correctness (RTC) as an unsupervised evaluation method for large language models (LLMs) in code synthesis and editing, addressing the limitations of existing narrow-domain benchmarks. The authors demonstrate that RTC correlates well with traditional metrics while enabling evaluation across a broader range of real-world software domains without the need for costly human annotations.
  - **Keywords:** Code evaluation, Large language models (LLMs), Round-trip correctness (RTC), Code synthesis, Code editing, Software development, Programming tasks, Evaluation of code capabilities, Limitations of existing benchmarks, Unsupervised evaluation method, Correlation with existing benchmarks, HumanEval, MBPP, ARCADE, DS-1000


- [Generalized Smooth Variational Inequalities: Methods with Adaptive Stepsizes](https://icml.cc/virtual/2024/poster/35010) (Poster)
  - **Authors:** [Daniil Vankov](http://openreview.net/profile?id=~Daniil_Vankov1), [Angelia Nedich](http://openreview.net/profile?id=~Angelia_Nedich1), [Lalitha Sankar](http://openreview.net/profile?id=~Lalitha_Sankar2)
  - **Affiliations:** Department of Electrical and Computer Engineering, Arizona State University Tempe, Arizona, USA, Department of Electrical and Computer Engineering, Arizona State University Tempe, Arizona, USA, Department of Electrical and Computer Engineering, Arizona State University Tempe, Arizona, USA
  - **TL;DR:** This study addresses the limitations of traditional assumptions in variational inequality problems by introducing generalized smoothness and adaptive stepsizes. The authors present new convergence results for popular methods, demonstrating improved efficiency and applicability in machine learning contexts, particularly in adversarial and multi-agent training scenarios.
  - **Keywords:** Variational Inequalities, Machine Learning, Projection methods, Korpelevich method, Popov method, Adaptive stepsizes, Adversarial training, Multi-agent training, Generative Adversarial Networks (GANs), Reinforcement learning, Non-monotonicity, Smoothness assumptions, Convergence rates, Convergence results for generalized smooth VIs, Efficiency of adaptive stepsizes, GANs (Generative Adversarial Networks), Extra-gradient method, Optimistic gradient method


- [Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning](https://icml.cc/virtual/2024/poster/34042) (Spotlight Poster)
  - **Authors:** [Nikhil Vyas](http://openreview.net/profile?id=~Nikhil_Vyas1), [Depen Morwani](http://openreview.net/profile?id=~Depen_Morwani1), [Rosie Zhao](http://openreview.net/profile?id=~Rosie_Zhao1), [Gal Kaplun](http://openreview.net/profile?id=~Gal_Kaplun1), [Sham Kakade](http://openreview.net/profile?id=~Sham_M._Kakade1), [Boaz Barak](http://openreview.net/profile?id=~Boaz_Barak2)
  - **Affiliations:** SEAS, Harvard University, SEAS, Harvard University, SEAS, Harvard University, SEAS, Harvard University, SEAS, Harvard University; Kempner Institute, Harvard University, SEAS, Harvard University
  - **TL;DR:** This study investigates the role of SGD noise in online learning, revealing that small batch sizes do not provide implicit bias advantages as seen in offline learning. The findings suggest that SGD noise primarily serves computational efficiency rather than influencing the quality of the learned model.
  - **Keywords:** Stochastic Gradient Descent (SGD), Online Learning, Implicit Bias, Deep Learning, Self-Supervised Learning, Large Language Models (LLMs), Implicit bias in optimization, SGD noise, Batch size effects, Insights into SGD's role in online learning, Computational efficiency


- [SeMOPO: Learning High-quality Model and Policy from Low-quality Offline Visual Datasets](https://icml.cc/virtual/2024/poster/33718) (Poster)
  - **Authors:** [Shenghua Wan](http://openreview.net/profile?id=~Shenghua_Wan1), [Ziyuan Chen](http://openreview.net/profile?id=~Ziyuan_Chen2), [Le Gan](http://openreview.net/profile?id=~Le_Gan1), [Shuai Feng](http://openreview.net/profile?id=~Shuai_Feng3), [De-Chuan Zhan](http://openreview.net/profile?id=~De-Chuan_Zhan1)
  - **Affiliations:** School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Mathematical Sciences, Center for Statistical Science, Peking University, Beijing, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China
  - **TL;DR:** The study introduces SeMOPO, a method for optimizing policies in offline reinforcement learning by addressing model uncertainty caused by complex distractors in high-dimensional data. Experimental results demonstrate that SeMOPO significantly outperforms existing methods, highlighting its effectiveness in learning from low-quality datasets.
  - **Keywords:** Offline reinforcement learning, model-based reinforcement learning, Separated Model-based Offline Policy Optimization (SeMOPO), conservative sampling, Drug discovery, autonomous driving, Distribution shift, model uncertainty, high-dimensional data, low-quality datasets, Theoretical guarantee of model uncertainty, performance bound, Low-Quality Vision Deep Data-Driven Datasets for RL (LQV-D4RL)


- [Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene Reconstruction](https://icml.cc/virtual/2024/poster/34430) (Poster)
  - **Authors:** [Diwen Wan](http://openreview.net/profile?id=~Diwen_Wan1), [Ruijie Lu](http://openreview.net/profile?id=~Ruijie_Lu1), [Gang Zeng](http://openreview.net/profile?id=~Gang_Zeng1)
  - **Affiliations:** National Key Laboratory of General Artificial Intelligence, School of IST, Peking University, China, National Key Laboratory of General Artificial Intelligence, School of IST, Peking University, China, National Key Laboratory of General Artificial Intelligence, School of IST, Peking University, China
  - **TL;DR:** This study introduces Superpoint Gaussian Splatting (SP-GS), a novel framework for real-time high-fidelity dynamic scene reconstruction that clusters 3D Gaussians into superpoints to enhance rendering speed and quality. The approach achieves state-of-the-art visual quality while maintaining efficiency, making it suitable for various applications in gaming and AR/VR.
  - **Keywords:** dynamic scene reconstruction, novel view synthesis, 3D Gaussian Splatting, NeRF (Neural Radiance Fields), MLP (Multi-Layer Perceptron), gaming, filming, AR/VR, low rendering quality, slow inference speed, computational cost, Superpoint Gaussian Splatting (SP-GS), real-time rendering, high visual quality, synthetic datasets, real-world datasets


- [VinT-6D: A Large-Scale Object-in-hand Dataset from Vision, Touch and Proprioception](https://icml.cc/virtual/2024/poster/35027) (Poster)
  - **Authors:** [Zhaoliang Wan](http://openreview.net/profile?id=~Zhaoliang_Wan1), [Yonggen Ling](http://openreview.net/profile?id=~Yonggen_Ling1), [Senlin Yi](http://openreview.net/profile?id=~Senlin_Yi1), [Lu Qi](http://openreview.net/profile?id=~Lu_Qi1), [Wang Lee](http://openreview.net/profile?id=~Wang_Wei_Lee1), [Minglei Lu](http://openreview.net/profile?id=~Minglei_Lu1), [Sicheng Yang](http://openreview.net/profile?id=~Sicheng_Yang5), [Xiao Teng](http://openreview.net/profile?id=~Xiao_Teng4), [Peng Lu](http://openreview.net/profile?id=~Peng_Lu9), [Xu Yang](http://openreview.net/profile?id=~Xu_Yang1), [Ming-Hsuan Yang](http://openreview.net/profile?id=~Ming-Hsuan_Yang1), [Hui Cheng](http://openreview.net/profile?id=~Hui_Cheng5)
  - **Affiliations:** School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Robotics X, Tencent, Shenzhen, China, Robotics X, Tencent, Shenzhen, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China, The University of California, Merced, Merced, the U.S., Robotics X, Tencent, Shenzhen, China, Robotics X, Tencent, Shenzhen, China, Robotics X, Tencent, Shenzhen, China, Robotics X, Tencent, Shenzhen, China, Robotics X, Tencent, Shenzhen, China, Chinese Academy of Sciences, Automation Institute, Beijing, China, The University of California, Merced, Merced, the U.S., School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China
  - **TL;DR:** This study introduces VinT-6D, a large-scale multi-modal dataset integrating vision, touch, and proprioception for accurate object-in-hand pose estimation, addressing the challenges of existing low-quality datasets. The dataset significantly enhances robotic manipulation capabilities and bridges the gap between simulation and real-world applications.
  - **Keywords:** object-in-hand pose estimation, robotic manipulation, multi-modal dataset, simulation in MuJoCo and Blender, dexterous manipulation, robotics, scarcity of large-scale datasets, domain gap between simulation and real-world environments, occlusions in multi-finger grasping, VinT-6D dataset, benchmark method for performance improvement, VinT-6D, VinT-Sim, VinT-Real, MuJoCo, Blender, 6D pose estimation, tactile perception, multi-modal signals


- [Trustless Audits without Revealing Data or Models](https://icml.cc/virtual/2024/poster/34747) (Poster)
  - **Authors:** [Suppakit Waiwitlikhit](http://openreview.net/profile?id=~Suppakit_Waiwitlikhit3), [Ion Stoica](http://openreview.net/profile?id=~Ion_Stoica1), [Yi Sun](http://openreview.net/profile?id=~Yi_Sun3), [Tatsunori Hashimoto](http://openreview.net/profile?id=~Tatsunori_Hashimoto1), [Daniel Kang](http://openreview.net/profile?id=~Daniel_Kang1)
  - **Affiliations:** Stanford University, UC Berkeley, University of Chicago, Stanford University, UIUC
  - **TL;DR:** This paper presents a protocol called ZKAUDIT that enables model providers to keep their data and model weights secret while allowing for trustless audits of model properties. The authors demonstrate that it is feasible to conduct audits related to copyright and censorship with minimal impact on model accuracy.
  - **Keywords:** algorithmic transparency, trustless audits, zero-knowledge proof (ZKP), cryptographic commitments, stochastic gradient descent (SGD), image classification, recommender systems, trade secrets, lack of transparency, audit challenges, ZKAUDIT protocol, trustless audits of DNNs, ImageNet


- [Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD](https://icml.cc/virtual/2024/poster/34659) (Poster)
  - **Authors:** [Yijun Wan](http://openreview.net/profile?id=~Yijun_Wan1), [Melih Barsbey](http://openreview.net/profile?id=~Melih_Barsbey1), [Abdellatif Zaidi](http://openreview.net/profile?id=~Abdellatif_Zaidi1), [Umut Simsekli](http://openreview.net/profile?id=~Umut_Simsekli1)
  - **Affiliations:** Paris Research Center, Huawei Technologies France, Boğaziçi University, Istanbul, Turkey, Université Gustave Eiffel, France, Inria, CNRS, Ecole Normale Supérieure, PSL Research University, Paris, France
  - **TL;DR:** This study proposes a modification to stochastic gradient descent that injects heavy-tailed noise to enhance the compressibility of overparametrized neural networks without relying on unverifiable assumptions. The findings indicate that this approach leads to improved compressibility and robust performance under pruning across various models and datasets.
  - **Keywords:** Neural network compression, Generalization error, Stochastic gradient descent (SGD), Heavy-tailed noise, One-hidden-layer neural network, Resource-limited environments, Neural network deployment, Compressibility of neural networks, Overparametrization, Nonconvexity of objectives, Propagation of chaos, Error estimates for Euler discretization, Increased compressibility, Heavy-tailed stochastic differential equations, Lottery ticket hypothesis


- [One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts](https://icml.cc/virtual/2024/poster/33485) (Poster)
  - **Authors:** [Ruochen Wang](http://openreview.net/profile?id=~Ruochen_Wang2), [Sohyun An](http://openreview.net/profile?id=~Sohyun_An1), [Minhao Cheng](http://openreview.net/profile?id=~Minhao_Cheng1), [Tianyi Zhou](http://openreview.net/profile?id=~Tianyi_Zhou1), [Sung Ju Hwang](http://openreview.net/profile?id=~Sung_Ju_Hwang1), [Cho-Jui Hsieh](http://openreview.net/profile?id=~Cho-Jui_Hsieh1)
  - **Affiliations:** University of California, Los Angeles, Korea Advanced Institute of Science & Technology, Penn State University, University of Maryland, College Park, Korea Advanced Institute of Science & Technology, University of California, Los Angeles
  - **TL;DR:** This paper presents a method called Mixture-of-Prompts (MoP) that enhances the prompting process for Large Language Models by utilizing a Mixture of Experts framework to cover a broader problem space. The proposed approach achieves an average win rate of 81% against prior methods across major benchmarks, demonstrating improved generalization capabilities.
  - **Keywords:** Large Language Models, Prompt Engineering, Mixture of Experts, Mixture-of-Prompts (MoP), demo assignment, instruction assignment, Natural Language Processing, Coverage of complex problem space, gap between user intention and model interpretation, Optimization of prompts, improved generalization capabilities


- [Towards Unified Multi-granularity Text Detection with Interactive Attention](https://icml.cc/virtual/2024/poster/34458) (Spotlight Poster)
  - **Authors:** [Xingyu Wan](http://openreview.net/profile?id=~Xingyu_Wan2), [Chengquan Zhang](http://openreview.net/profile?id=~Chengquan_Zhang2), [Pengyuan Lyu](http://openreview.net/profile?id=~Pengyuan_Lyu1), [Sen Fan](http://openreview.net/profile?id=~Sen_Fan1), [Zihan Ni](http://openreview.net/profile?id=~Zihan_Ni1), [Kun Yao](http://openreview.net/profile?id=~Kun_Yao1), [Errui Ding](http://openreview.net/profile?id=~Errui_Ding2), [Jingdong Wang](http://openreview.net/profile?id=~Jingdong_Wang1)
  - **Affiliations:** Baidu, Beijing, China, Baidu, Beijing, China, Baidu, Beijing, China, Baidu, Beijing, China, Baidu, Beijing, China, Baidu, Beijing, China, Baidu, Beijing, China, Baidu, Beijing, China
  - **TL;DR:** This paper presents "Detect Any Text" (DAT), a unified model that integrates scene text detection, layout analysis, and document page detection, significantly improving detection performance across various text granularities. The model employs an interactive attention module and a prompt-based segmentation approach to enhance accuracy and applicability in real-world scenarios.
  - **Keywords:** text detection, document image analysis, interactive attention module, prompt-based segmentation, scene text detection, document layout analysis, document page detection, computational complexity, resource demands, unified model for multi-granularity text detection


- [Non-stationary Online Convex Optimization with Arbitrary Delays](https://icml.cc/virtual/2024/poster/33752) (Poster)
  - **Authors:** [Yuanyu Wan](http://openreview.net/profile?id=~Yuanyu_Wan1), [Chang Yao](http://openreview.net/profile?id=~Chang_Yao2), [Mingli Song](http://openreview.net/profile?id=~Mingli_Song1), [Lijun Zhang](http://openreview.net/profile?id=~Lijun_Zhang1)
  - **Affiliations:** The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; School of Software Technology, Zhejiang University, Ningbo, China; Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security, Hangzhou, China, The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; School of Software Technology, Zhejiang University, Ningbo, China, The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security, Hangzhou, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
  - **TL;DR:** This paper investigates online convex optimization with arbitrary delays in non-stationary environments, proposing a simple algorithm called DOGD to minimize dynamic regret. The authors also present an improved algorithm that achieves better regret bounds and demonstrate its optimality in the worst case.
  - **Keywords:** Online Convex Optimization, Non-stationary Environments, Dynamic Regret, DOGD (Delayed Online Gradient Descent), Gradient Descent, Arbitrary Delays, Non-stationary Environments, Static Regret vs. Dynamic Regret, Improved Algorithm for Dynamic Regret, Optimality in Worst Case


- [Adversarially Robust Hypothesis Transfer Learning](https://icml.cc/virtual/2024/poster/34475) (Poster)
  - **Authors:** [Yunjuan Wang](http://openreview.net/profile?id=~Yunjuan_Wang1), [Raman Arora](http://openreview.net/profile?id=~Raman_Arora1)
  - **Affiliations:** Department of Computer Science, Johns Hopkins University, Baltimore, USA, Department of Computer Science, Johns Hopkins University, Baltimore, USA
  - **TL;DR:** This study investigates Hypothesis Transfer Learning (HTL) in the context of adversarial attacks, aiming to develop a robust model by leveraging auxiliary hypotheses. The authors establish bounds on the generalization error and excess risk, demonstrating that a good initialization can lead to improved robustness against adversarial perturbations.
  - **Keywords:** Adversarial Robustness, Hypothesis Transfer Learning, Adversarial Regularized Empirical Risk Minimization (A-RERM), Proximal Stochastic Adversarial Training, Adversarial Attacks, Generalization Error, Robustness, Bounds on Robust Generalization Error, Robust Excess Risk, Auxiliary Hypotheses, Regularization, Empirical Loss


- [Revisiting the Power of Prompt for Visual Tuning](https://icml.cc/virtual/2024/poster/35101) (Spotlight Poster)
  - **Authors:** [Yuzhu Wang](http://openreview.net/profile?id=~Yuzhu_Wang1), [Lechao Cheng](http://openreview.net/profile?id=~Lechao_Cheng2), [Chaowei Fang](http://openreview.net/profile?id=~Chaowei_Fang3), [Dingwen Zhang](http://openreview.net/profile?id=~Dingwen_Zhang1), [Manni Duan](http://openreview.net/profile?id=~Manni_Duan2), [Meng Wang](http://openreview.net/profile?id=~Meng_Wang2)
  - **Affiliations:** Zhejiang Lab, School of Computer Science and Information Engineering, Hefei University of Technology, School of Artificial Intelligence, Xidian University, School of Automation, Northwestern Polytechnical University, Zhejiang Lab, School of Computer Science and Information Engineering, Hefei University of Technology
  - **TL;DR:** This study proposes a novel approach to visual prompt tuning (VPT) by initializing prompts with downstream token prototypes, significantly improving performance and efficiency in adapting pre-trained models for downstream tasks. The method demonstrates robustness to prompt lengths and scales well with model capacity and training data size, outperforming existing methods by a notable margin.
  - **Keywords:** Visual Prompt Tuning, Contextual Adaptation, Learnable Prompt Tokens, Token Initialization, Token Construction, Computer Vision, Downstream Tasks, Prompt Initialization, Prompt Length, Performance in Self-Supervised Pretraining, Improved Performance, Robustness to Prompt Lengths, Efficient Parameter Usage, MAE (Masked Autoencoder)


- [TVE: Learning Meta-attribution for Transferable Vision Explainer](https://icml.cc/virtual/2024/poster/35196) (Poster)
  - **Authors:** [Guanchu (Gary) Wang](http://openreview.net/profile?id=~Guanchu_Wang1), [Yu-Neng Chuang](http://openreview.net/profile?id=~Yu-Neng_Chuang1), [Fan Yang](http://openreview.net/profile?id=~Fan_Yang27), [Mengnan Du](http://openreview.net/profile?id=~Mengnan_Du1), [Chia-Yuan Chang](http://openreview.net/profile?id=~Chia-Yuan_Chang3), [Shaochen (Henry) Zhong](http://openreview.net/profile?id=~Shaochen_Zhong1), [Zirui Liu](http://openreview.net/profile?id=~Zirui_Liu1), [Zhaozhuo Xu](http://openreview.net/profile?id=~Zhaozhuo_Xu2), [Kaixiong Zhou](http://openreview.net/profile?id=~Kaixiong_Zhou1), [Xuanting Cai](http://openreview.net/profile?id=~Xuanting_Cai1), [Xia Hu](http://openreview.net/profile?id=~Xia_Hu4)
  - **Affiliations:** Department of Computer Science, Rice University, Department of Computer Science, Rice University, Wake Forest University, New Jersey Institute of Technology, Texas A&M University, Department of Computer Science, Rice University, Department of Computer Science, Rice University, Stevens Institute of Technology, North Carolina State University, Meta Platforms, Inc., Department of Computer Science, Rice University
  - **TL;DR:** This paper introduces the Transferable Vision Explainer (TVE), which enhances the explainability of deep neural networks by learning meta-attribution through pre-training on large datasets, allowing for effective explanations across various vision models without the need for task-specific training. The results demonstrate TVE's efficiency in providing explanations for different architectures and datasets.
  - **Keywords:** Explainable machine learning, deep neural networks, Transferable Vision Explainer (TVE), meta-attribution, backbone encoders, Vision models, downstream tasks, healthcare, loan approvals, targeted advertisement, Black-box nature of DNNs, resource-intensive explanation methods, Effective explanations across various vision models without task-specific training, Cats-vs-dogs, Imagenette, CIFAR-10


- [Adaptively Learning to Select-Rank in Online Platforms](https://icml.cc/virtual/2024/poster/33517) (Poster)
  - **Authors:** [Jingyuan Wang](http://openreview.net/profile?id=~Jingyuan_Wang3), [Perry Dong](http://openreview.net/profile?id=~Perry_Dong1), [Ying Jin](http://openreview.net/profile?id=~Ying_Jin4), [Ruohan Zhan](http://openreview.net/profile?id=~Ruohan_Zhan1), [Zhengyuan Zhou](http://openreview.net/profile?id=~Zhengyuan_Zhou2)
  - **Affiliations:** Stern School of Business, New York University, EECS, UC Berkeley; Arena Technologies, Department of Statistics, Stanford University, IEDA, Hong Kong University of Science and Technology (HKUST); HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute, Stern School of Business, New York University; Arena Technologies
  - **TL;DR:** This study develops an adaptive ranking algorithm for online platforms that optimizes user satisfaction by considering diverse user preferences and item positions. The proposed method, framed within a contextual bandits framework, demonstrates improved performance over existing algorithms through a cumulative regret bound analysis.
  - **Keywords:** adaptive ranking, user satisfaction, online platforms, contextual bandits, upper confidence bound, maximum weight imperfect matching, e-commerce, content streaming services, estimation uncertainty, user response prediction, cumulative regret bound, optimization of ranking algorithms, simulated datasets, real-world datasets, explore-then-commit strategy, score-based ranking


- [Towards Theoretical Understanding of Learning Large-scale Dependent Data via Random Features](https://icml.cc/virtual/2024/poster/33495) (Spotlight Poster)
  - **Authors:** [Chao Wang](http://openreview.net/profile?id=~Chao_Wang39), [Xin Bing](http://openreview.net/profile?id=~Xin_Bing1), [Xin HE](http://openreview.net/profile?id=~Xin_HE6), [Caixing Wang](http://openreview.net/profile?id=~Caixing_Wang1)
  - **Affiliations:** School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai, China, Department of Statistical Sciences, University of Toronto, School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai, China, School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai, China
  - **TL;DR:** This paper investigates the theoretical implications of using random features in kernel ridge regression for large-scale dependent data, revealing that while minimax optimality is achieved under exponential decay conditions, performance is sub-optimal under polynomial decay. The findings highlight the impact of data dependence on learning accuracy, addressing a significant gap in the existing literature.
  - **Keywords:** Random features, Kernel ridge regression, Dependent data, Kernel ridge regression (KRR), Random feature mapping, Violation of i.i.d. assumption, Learning accuracy with dependent data, Minimax optimality under exponential decay, Sub-optimal results under polynomial decay, τ-mixing process, Exponential decay coefficient, Polynomial decay coefficient


- [S3GCL: Spectral, Swift, Spatial Graph Contrastive Learning](https://icml.cc/virtual/2024/poster/32616) (Poster)
  - **Authors:** [Guancheng Wan](http://openreview.net/profile?id=~Guancheng_Wan1), [Yijun Tian](http://openreview.net/profile?id=~Yijun_Tian1), [Wenke Huang](http://openreview.net/profile?id=~Wenke_Huang1), [Nitesh Chawla](http://openreview.net/profile?id=~Nitesh_V_Chawla1), [Mang Ye](http://openreview.net/profile?id=~Mang_Ye1)
  - **Affiliations:** National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China, Department of Computer Science, University of Notre Dame, USA, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China, Department of Computer Science, University of Notre Dame, USA, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; Taikang Center for Life and Medical Sciences, Wuhan University, Wuhan, China
  - **TL;DR:** This paper introduces S3GCL, a novel framework for Graph Contrastive Learning that addresses challenges related to homophily assumptions and scalability in inference. The proposed method demonstrates superior generalization across diverse homophily levels and significantly improves inference efficiency on large-scale datasets.
  - **Keywords:** Graph Contrastive Learning, Graph Representation Learning, Spectral GNNs, Cosine-parameterized Chebyshev polynomial, MLP encoder, Social network analysis, Recommender systems, Homophily assumptions, Heterophilic edges, Scalability challenges, S3GCL framework, Cross-pass GCL objective, Obgn-Arxiv, GNN (Graph Neural Networks), Low/high-pass filters


- [On Universally Optimal Algorithms for A/B Testing](https://icml.cc/virtual/2024/poster/33448) (Poster)
  - **Authors:** [Po-An Wang](http://openreview.net/profile?id=~Po-An_Wang1), [Kaito Ariu](http://openreview.net/profile?id=~Kaito_Ariu1), [Alexandre Proutiere](http://openreview.net/profile?id=~Alexandre_Proutiere1)
  - **Affiliations:** EECS and Digital Futures, KTH, Stockholm, Sweden; CyberAgent, Tokyo, Japan, CyberAgent, Tokyo, Japan, EECS and Digital Futures, KTH, Stockholm, Sweden
  - **TL;DR:** This study investigates best-arm identification in stochastic multi-armed bandits with a fixed budget, demonstrating that no algorithm can outperform the uniform sampling algorithm across all instances. The findings establish a lower bound on error rates for consistent and stable algorithms, providing insights into the limitations of adaptive sampling strategies.
  - **Keywords:** Best-arm identification, A/B testing, Stochastic multi-armed bandits, Uniform sampling algorithm, Successive Rejects (SR) algorithm, Fixed-budget best-arm identification, Error probability minimization, Lower bound on error rate, Consistent and stable algorithms, Bernoulli rewards, Empirical mean


- [A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design](https://icml.cc/virtual/2024/poster/35155) (Spotlight Poster)
  - **Authors:** [Zhihai Wang](http://openreview.net/profile?id=~Zhihai_Wang1), [Lei Chen](http://openreview.net/profile?id=~Lei_Chen26), [Jie Wang](http://openreview.net/profile?id=~Jie_Wang1), [白 寅岐](http://openreview.net/profile?id=~Yinqi_Bai1), [Xing Li](http://openreview.net/profile?id=~Xing_Li6), [Xijun Li](http://openreview.net/profile?id=~Xijun_Li1), [Mingxuan Yuan](http://openreview.net/profile?id=~Mingxuan_Yuan1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Yongdong Zhang](http://openreview.net/profile?id=~Yongdong_Zhang2), [Feng Wu](http://openreview.net/profile?id=~Feng_Wu1)
  - **Affiliations:** CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China; None, Noah’s Ark Lab, Huawei Technologies; None, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China; None, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China; None, Noah’s Ark Lab, Huawei Technologies; None, Noah’s Ark Lab, Huawei Technologies; None, Noah’s Ark Lab, Huawei Technologies; None, College of Intelligence and Computing, Tianjin University; None, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China; None, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China; None
  - **TL;DR:** This paper presents PruneX, a novel data-driven heuristic for Logic Synthesis that addresses the inefficiencies caused by ineffective transformations in circuit optimization. The proposed framework significantly enhances the efficiency of existing heuristics while maintaining comparable optimization performance, achieving up to 3.1× faster runtime on large-scale circuits.
  - **Keywords:** Logic Synthesis, Circuit Optimization, Chip Design, Data-driven LS heuristic, PruneX, Domain-invariant representations, Semiconductor industry, Electronic devices, Ineffective transformations, Out-of-distribution (OOD) generalization problem, NP-hard problem, Improved efficiency of LS heuristics, Comparable optimization performance, Faster runtime, Directed acyclic graphs (DAGs), LS heuristics, Resub, Mfs2


- [Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments](https://icml.cc/virtual/2024/poster/33442) (Poster)
  - **Authors:** [Han Wang](http://openreview.net/profile?id=~Han_Wang14), [Sihong He](http://openreview.net/profile?id=~Sihong_He1), [Zhili Zhang](http://openreview.net/profile?id=~Zhili_Zhang5), [Fei Miao](http://openreview.net/profile?id=~Fei_Miao1), [James Anderson](http://openreview.net/profile?id=~James_Anderson6)
  - **Affiliations:** Department of Electrical Engineering, Columbia University, New York, USA, School of Computing, University of Connecticut, Storrs, USA, School of Computing, University of Connecticut, Storrs, USA, School of Computing, University of Connecticut, Storrs, USA, Department of Electrical Engineering, Columbia University, New York, USA
  - **TL;DR:** This study addresses the challenge of Federated Reinforcement Learning (FRL) in highly heterogeneous environments by proposing two algorithms, FEDSVRPG-M and FEDHAPG-M, which leverage momentum mechanisms to achieve optimal policy convergence. The findings highlight the algorithms' ability to significantly reduce sample complexity and improve performance through agent collaboration, even in diverse operational contexts.
  - **Keywords:** Federated Reinforcement Learning, Environment Heterogeneity, FEDSVRPG-M, FEDHAPG-M, Momentum Mechanisms, Variance-Reduction Techniques, Hessian Approximation, Autonomous Vehicles, Pricing Strategies in Streaming Services, Environment Heterogeneity, Sample Complexity, Optimal Universal Policy, State-of-the-Art Convergence Results, Collaboration among Agents


- [An Efficient Maximal Ancestral Graph Listing Algorithm](https://icml.cc/virtual/2024/poster/34233) (Spotlight Poster)
  - **Authors:** [Tian-Zuo Wang](http://openreview.net/profile?id=~Tian-Zuo_Wang1), [Wen-Bo Du](http://openreview.net/profile?id=~Wen-Bo_Du1), [Zhi-Hua Zhou](http://openreview.net/profile?id=~Zhi-Hua_Zhou2)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China
  - **TL;DR:** This paper presents a novel method for efficiently listing maximal ancestral graphs (MAGs) without brute force, addressing the challenges posed by latent confounders and selection variables. The proposed method demonstrates improved efficiency and effectiveness in generating all MAGs within a Markov equivalence class.
  - **Keywords:** causal relations, latent variables, maximal ancestral graph (MAG), brute-force-free MAG listing method, local structures, Markov equivalence class (MEC), latent confounders, selection variables, efficient MAG listing, sound and complete rules for local transformations, directed acyclic graph (DAG), latent confounders, selection variables


- [Imitation Learning from Purified Demonstrations](https://icml.cc/virtual/2024/poster/33524) (Poster)
  - **Authors:** [Yunke Wang](http://openreview.net/profile?id=~Yunke_Wang1), [Minjing Dong](http://openreview.net/profile?id=~Minjing_Dong1), [Yukun Zhao](http://openreview.net/profile?id=~Yukun_Zhao5), [Bo Du](http://openreview.net/profile?id=~Bo_Du3), [Chang Xu](http://openreview.net/profile?id=~Chang_Xu4)
  - **Affiliations:** School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Wuhan Institute of Data Intelligence, Wuhan University, China, Department of Computer Science, City University of Hong Kong, China, School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Wuhan Institute of Data Intelligence, Wuhan University, China, School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Wuhan Institute of Data Intelligence, Wuhan University, China, School of Computer Science, Faculty of Engineering, The University of Sydney, Australia
  - **TL;DR:** This study proposes a two-step purification process using diffusion models to enhance imitation learning from imperfect demonstrations, addressing the challenges posed by noisy data. Empirical results indicate that the method effectively recovers optimal policies, improving the adaptability of imitation learning in real-world applications.
  - **Keywords:** Imitation Learning, Sequential Decision-Making, Diffusion Process, Behavioral Cloning, Generative Adversarial Imitation Learning (GAIL), Robotics, AI Policy Learning, Imperfect Demonstrations, Noisy Data, Compounding Error, Purification of Demonstrations, Optimal Policy Recovery, MuJoCo, RoboSuite


- [Diagnosing the Compositional Knowledge of Vision Language Models from a Game-Theoretic View](https://icml.cc/virtual/2024/poster/34101) (Poster)
  - **Authors:** [Jin Wang](http://openreview.net/profile?id=~Jin_Wang16), [Shichao Dong](http://openreview.net/profile?id=~Shichao_Dong3), [Yapeng Zhu](http://openreview.net/profile?id=~Yapeng_Zhu1), [kelu Yao](http://openreview.net/profile?id=~kelu_Yao1), [Weidong Zhao](http://openreview.net/profile?id=~Weidong_Zhao2), [Chao Li](http://openreview.net/profile?id=~Chao_Li22), [Ping Luo](http://openreview.net/profile?id=~Ping_Luo2)
  - **Affiliations:** Department of Computer Science, The University of Hong Kong, Hong Kong, Baidu Inc, Beijing, China, Baidu Inc, Beijing, China, Zhejiang Laboratory, Hangzhou, China, Zhejiang Laboratory, Hangzhou, China, Zhejiang Laboratory, Hangzhou, China, Shanghai AI Laboratory, China
  - **TL;DR:** This study investigates the compositional reasoning capabilities of Vision Language Models (VLMs) using game-theoretic evaluation methods, revealing significant weaknesses in their understanding of object relations and attributes. The findings provide insights that can guide future improvements in VLMs' compositional knowledge.
  - **Keywords:** Vision Language Models (VLMs), Compositional Reasoning, Game-theoretic evaluation methods, Object detection, Semantic segmentation, Text-to-image generation, Lack of compositional reasoning capabilities in VLMs, Insights into the incapabilities of VLMs on compositional reasoning


- [Image Restoration Through Generalized Ornstein-Uhlenbeck Bridge](https://icml.cc/virtual/2024/poster/33090) (Poster)
  - **Authors:** [Yue Conghan](http://openreview.net/profile?id=~Conghan_Yue2), [Zhengwei Peng](http://openreview.net/profile?id=~Zhengwei_Peng1), [Junlong Ma](http://openreview.net/profile?id=~Junlong_Ma2), [Shiyan Du](http://openreview.net/profile?id=~Shiyan_Du1), [Pengxu Wei](http://openreview.net/profile?id=~Pengxu_Wei1), [Dongyu Zhang](http://openreview.net/profile?id=~Dongyu_Zhang1)
  - **Affiliations:** Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; None, Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; None, Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; None, Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; None, Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; Pengcheng Laboratory, Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; None
  - **TL;DR:** This paper introduces the Generalized Ornstein-Uhlenbeck Bridge (GOUB) model for image restoration, focusing on the mapping from low-quality to high-quality images. The proposed model demonstrates state-of-the-art performance across various tasks, including inpainting, deraining, and super-resolution.
  - **Keywords:** image restoration, generative models, Generalized Ornstein-Uhlenbeck Bridge (GOUB), diffusion models, Doob’s h-transform, Mean-ODE model, inpainting, deraining, super-resolution, ill-posed inverse problem, loss of crucial information, need for prior knowledge, state-of-the-art performance in image restoration tasks


- [Rapid Learning without Catastrophic Forgetting in the Morris Water Maze](https://icml.cc/virtual/2024/poster/33368) (Poster)
  - **Authors:** [Raymond L Wang](http://openreview.net/profile?id=~Raymond_Wang1), [Jaedong Hwang](http://openreview.net/profile?id=~Jaedong_Hwang1), [Akhilan Boopathy](http://openreview.net/profile?id=~Akhilan_Boopathy1), [Ila R. Fiete](http://openreview.net/profile?id=~Ila_R_Fiete1)
  - **Affiliations:** Massachusetts Institute of Technology, Massachusetts Institute of Technology, Massachusetts Institute of Technology, Massachusetts Institute of Technology
  - **TL;DR:** This study introduces a biologically inspired model for the sequential Morris Water Maze task, demonstrating that it can achieve rapid learning and retention of previously acquired knowledge without catastrophic forgetting. The findings suggest potential pathways for developing machine learning algorithms that mimic these biological capabilities.
  - **Keywords:** rapid learning, continual learning, knowledge transfer, content-addressable heteroassociative memory, convolutional network architecture, remapping, neuroscience, machine learning, catastrophic forgetting, rapid adaptation, generalization, biologically motivated neural model, improved generalization and learning retention, Morris Water Maze, entorhinal-hippocampal circuit, grid cells, hippocampal cells


- [Optimal Kernel Choice for Score Function-based Causal Discovery](https://icml.cc/virtual/2024/poster/34621) (Poster)
  - **Authors:** [Wenjie Wang](http://openreview.net/profile?id=~Wenjie_Wang3), [Biwei Huang](http://openreview.net/profile?id=~Biwei_Huang1), [Feng Liu](http://openreview.net/profile?id=~Feng_Liu2), [Xinge You](http://openreview.net/profile?id=~Xinge_You1), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1), [Kun Zhang](http://openreview.net/profile?id=~Kun_Zhang1), [Mingming Gong](http://openreview.net/profile?id=~Mingming_Gong1)
  - **Affiliations:** School of Mathematics and Statistics, The University of Melbourne, Australia; Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates, Halicioğlu Data Science Institute (HDSI), University of California, San Diego, United States, School of Computing and Information Systems, The University of Melbourne, Australia, Huazhong University of Science and Technology, China, School of Computer Science, Faculty of Engineering, The University of Sydney, Australia, Department of Philosophy, Carnegie Mellon University, United States; Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates, School of Mathematics and Statistics, The University of Melbourne, Australia; Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates
  - **TL;DR:** This paper proposes an automatic kernel selection method within a generalized score function for causal discovery, which enhances the accuracy of identifying causal relationships from data. The method outperforms traditional heuristic kernel selection approaches, as demonstrated through experiments on synthetic and real-world data.
  - **Keywords:** Causal discovery, Score-based methods, Generalized score function, Reproducing kernel Hilbert space (RKHS), Conditional cross-covariance operator, Causal relationships, Data distributions, Kernel selection, Automatic kernel selection method, Marginal likelihood maximization, Synthetic data, Real-world benchmarks


- [Swallowing the Bitter Pill: Simplified Scalable Conformer Generation](https://icml.cc/virtual/2024/poster/34436) (Poster)
  - **Authors:** [Yuyang Wang](http://openreview.net/profile?id=~Yuyang_Wang3), [Ahmed Elhag](http://openreview.net/profile?id=~Ahmed_A._A._Elhag1), [Navdeep Jaitly](http://openreview.net/profile?id=~Navdeep_Jaitly1), [Joshua M Susskind](http://openreview.net/profile?id=~Joshua_M._Susskind1), [Miguel Angel Bautista Martin](http://openreview.net/profile?id=~Miguel_%C3%81ngel_Bautista1)
  - **Affiliations:** Apple, Apple; None, Apple, Apple, Apple
  - **TL;DR:** This study introduces a novel approach for predicting molecular conformers using a diffusion generative model that simplifies structure learning and scales effectively. The proposed Molecular Conformer Fields (MCF) model achieves state-of-the-art results by directly mapping molecular graph elements to their 3D positions, addressing the challenges of conformer generation in complex molecular spaces.
  - **Keywords:** molecular conformer generation, computational drug discovery, chemo-informatics, diffusion generative model, structure learning, drug discovery, molecular modeling, complexity of 3D structure space, low-energy conformers, sampling inefficiency, Molecular Conformer Fields (MCF), improved generalization performance


- [Total Variation Floodgate for Variable Importance Inference in Classification](https://icml.cc/virtual/2024/poster/34409) (Poster)
  - **Authors:** [Wenshuo Wang](http://openreview.net/profile?id=~Wenshuo_Wang3), [Lucas Janson](http://openreview.net/profile?id=~Lucas_Janson2), [Lihua Lei](http://openreview.net/profile?id=~Lihua_Lei2), [Aaditya Ramdas](http://openreview.net/profile?id=~Aaditya_Ramdas2)
  - **Affiliations:** Department of Statistics, Harvard University, Cambridge, MA, USA, Department of Statistics, Harvard University, Cambridge, MA, USA; Stanford Graduate School of Business, Stanford University, Stanford, CA, USA, Stanford Graduate School of Business, Stanford University, Stanford, CA, USA, Departments of Statistics and Machine Learning, Carnegie Mellon University, Pittsburgh, PA, USA
  - **TL;DR:** This study introduces the expected total variation (ETV) as a model-free measure of variable importance in classification problems, providing algorithms for statistical inference on ETV. The effectiveness of these algorithms is demonstrated through simulations and a case study in conjoint analysis related to the US general election.
  - **Keywords:** Variable importance inference, Classification problems, Expected total variation (ETV), Statistical inference algorithms, Conjoint analysis, Political candidate preferences, Effect of features on outcomes, Confounding variables, Model-free variable importance measure, Lower confidence bounds for ETV


- [StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization](https://icml.cc/virtual/2024/poster/33127) (Poster)
  - **Authors:** [Shida Wang](http://openreview.net/profile?id=~Shida_Wang1), [Qianxiao Li](http://openreview.net/profile?id=~Qianxiao_Li1)
  - **Affiliations:** Department of Mathematics, National University of Singapore, Institute for Functional Intelligent Materials, National University of Singapore; Department of Mathematics, National University of Singapore
  - **TL;DR:** This study investigates the long-term memory capabilities of state-space models and demonstrates that without reparameterization, these models face significant memory limitations akin to traditional RNNs. The authors introduce stable reparameterization techniques that enhance memory learning and optimization stability, validated through various applications.
  - **Keywords:** long-term memory learning, state-space models, reparameterization techniques, exponential reparameterization, softplus reparameterization, time series prediction, language models, image classifications, curse of memory, memory limitations, optimization stability, stable approximation of nonlinear functionals, improved approximation capabilities, synthetic datasets, RNNs (Recurrent Neural Networks), S4, S5, LRU, RWKV, RetNet, Mamba, Fast Fourier Transform (FFT)


- [MEMORYLLM: Towards Self-Updatable Large Language Models](https://icml.cc/virtual/2024/poster/33062) (Poster)
  - **Authors:** [Yu Wang](http://openreview.net/profile?id=~Yu_Wang24), [Yifan Gao](http://openreview.net/profile?id=~Yifan_Gao1), [Xiusi Chen](http://openreview.net/profile?id=~Xiusi_Chen1), [Haoming Jiang](http://openreview.net/profile?id=~Haoming_Jiang1), [Shiyang Li](http://openreview.net/profile?id=~Shiyang_Li1), [Jingfeng Yang](http://openreview.net/profile?id=~Jingfeng_Yang2), [Qingyu Yin](http://openreview.net/profile?id=~Qingyu_Yin2), [Zheng Li](http://openreview.net/profile?id=~Zheng_Li9), [Xian Li](http://openreview.net/profile?id=~Xian_Li3), [Bing Yin](http://openreview.net/profile?id=~Bing_Yin1), [Jingbo Shang](http://openreview.net/profile?id=~Jingbo_Shang2), [Julian McAuley](http://openreview.net/profile?id=~Julian_McAuley1)
  - **Affiliations:** UC, San Diego; Amazon, Amazon, UC, Los Angeles, Amazon, Amazon, Amazon, Amazon, Amazon, Amazon, Amazon, UC, San Diego, UC, San Diego
  - **TL;DR:** The study introduces MEMORY LLM, a self-updatable large language model that integrates new knowledge efficiently through a fixed-size memory pool within its latent space. The model demonstrates effective knowledge incorporation and long-term retention without performance degradation after extensive updates.
  - **Keywords:** self-updatable models, large language models, transformer, memory pool, knowledge integration, model updating, MEMORY LLM, self-update mechanism, long-term information retention


- [Optimal Kernel Quantile Learning with Random Features](https://icml.cc/virtual/2024/poster/34343) (Spotlight Poster)
  - **Authors:** [Caixing Wang](http://openreview.net/profile?id=~Caixing_Wang1), [Xingdong Feng](http://openreview.net/profile?id=~Xingdong_Feng1)
  - **Affiliations:** School of Statistics and Management, Shanghai University of Finance and Economics, School of Statistics and Management, Shanghai University of Finance and Economics
  - **TL;DR:** This study generalizes kernel quantile regression with random features, addressing limitations in handling heterogeneous data and establishing optimal learning rates. The findings extend to agnostic settings, enhancing the applicability of kernel methods in machine learning.
  - **Keywords:** kernel methods, quantile regression, random features, kernel ridge regression, nonparametric regression, statistical analysis, handling heterogeneous data, heavy-tailed noises, computational cost, capacity-dependent learning rates, error decomposition, reproducing kernel Hilbert space (RKHS)


- [Mollification Effects of Policy Gradient Methods](https://icml.cc/virtual/2024/poster/34641) (Poster)
  - **Authors:** [Tao Wang](http://openreview.net/profile?id=~Tao_Wang27), [Sylvia Herbert](http://openreview.net/profile?id=~Sylvia_Lee_Herbert1), [Sicun Gao](http://openreview.net/profile?id=~Sicun_Gao1)
  - **Affiliations:** University of California, San Diego, La Jolla, USA, University of California, San Diego, La Jolla, USA, University of California, San Diego, La Jolla, USA
  - **TL;DR:** This study investigates how policy gradient methods in deep reinforcement learning can mollify non-smooth optimization landscapes, facilitating effective policy search while also highlighting the challenges posed by stochasticity. The findings reveal a trade-off between optimization ease and deviation from the original problem, with implications for understanding exploration in reinforcement learning.
  - **Keywords:** deep reinforcement learning, policy gradient methods, Gaussian noise, stochastic policies, optimization landscapes, continuous control problems, nonlinear control problems, non-smooth optimization landscapes, stochasticity, ill-posedness, mollification effects, exploration in RL, backward heat equations, uncertainty principle, partial differential equations (PDEs)


- [An Iterative Min-Min Optimization Method for Sparse Bayesian Learning](https://icml.cc/virtual/2024/poster/34938) (Poster)
  - **Authors:** [Yasen Wang](http://openreview.net/profile?id=~Yasen_Wang1), [Junlin Li](http://openreview.net/profile?id=~Junlin_Li4), [Zuogong Yue](http://openreview.net/profile?id=~Zuogong_Yue1), [Ye Yuan](http://openreview.net/profile?id=~ye_yuan4)
  - **Affiliations:** School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Lab of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China, School of Mathematics and Statistics, Fuyang Normal University, Fuyang, China, State Key Lab of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China, State Key Lab of Digital Manufacturing Equipment and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China
  - **TL;DR:** This paper presents an iterative Min-Min optimization method for maximizing the marginal likelihood function in Sparse Bayesian Learning, addressing the lack of global convergence guarantees in classical algorithms. The proposed method demonstrates global convergence to local minima or saddle points and outperforms traditional approaches in various applications, including sparse signal recovery and system identification.
  - **Keywords:** Sparse Bayesian Learning, Sparse Representations, Iterative Min-Min Optimization, Concave-Convex Procedure, Sparse Signal Recovery, System Identification, Kernel Regression, Global Convergence, Sparsity Constraints, Optimization Method, Marginal Likelihood Function


- [Visual Transformer with Differentiable Channel Selection: An Information Bottleneck Inspired Approach](https://icml.cc/virtual/2024/poster/33332) (Poster)
  - **Authors:** [Yancheng Wang](http://openreview.net/profile?id=~Yancheng_Wang2), [Ping Li](http://openreview.net/profile?id=~Ping_Li3), [Yingzhen Yang](http://openreview.net/profile?id=~Yingzhen_Yang1)
  - **Affiliations:** School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ 85281, USA, VecML Inc., Bellevue, WA 98004, USA, School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ 85281, USA
  - **TL;DR:** This paper introduces the DCS-Transformer, a novel transformer block that incorporates differentiable channel selection to enhance efficiency and reduce computational costs in visual transformers. The proposed method maintains or improves prediction accuracy while being compatible with existing transformer architectures like MobileViT and EfficientViT.
  - **Keywords:** visual transformers, deep learning, channel selection, DCS-Transformer, self-attention, MLP (Multi-Layer Perceptron), image classification, object detection, computational costs, information bottleneck, network compression, efficient visual transformers, reduced FLOPs, improved prediction accuracy, Transformer, MobileViT, EfficientViT


- [Open Ad Hoc Teamwork with Cooperative Game Theory](https://icml.cc/virtual/2024/poster/34041) (Poster)
  - **Authors:** [Jianhong Wang](http://openreview.net/profile?id=~Jianhong_Wang1), [Yang Li](http://openreview.net/profile?id=~Yang_Li40), [Yuan Zhang](http://openreview.net/profile?id=~Yuan_Zhang8), [Wei Pan](http://openreview.net/profile?id=~Wei_Pan2), [Samuel Kaski](http://openreview.net/profile?id=~Samuel_Kaski1)
  - **Affiliations:** Center for AI Fundamentals, University of Manchester, UK; Aalto University, Finland, Center for AI Fundamentals, University of Manchester, UK, Neurorobotics Lab, University of Freiburg, Germany, Center for AI Fundamentals, University of Manchester, UK, Center for AI Fundamentals, University of Manchester, UK; Aalto University, Finland
  - **TL;DR:** This paper addresses the challenges of open ad hoc teamwork (OAHT) by proposing a new algorithm, CIAO, which leverages cooperative game theory to improve collaboration among agents with unknown types and policies. The study introduces the open stochastic Bayesian coalitional affinity game (OSB-CAG) and a novel solution concept called dynamic variational strict core (DVSC) to facilitate effective teamwork in dynamic environments.
  - **Keywords:** Ad hoc teamwork, Open ad hoc teamwork (OAHT), Multi-agent reinforcement learning (MARL), Graph-based policy learning (GPL), Joint Q-value representation, Cooperative game theory, Search and rescue, Human-machine collaboration, Machine-machine collaboration, Lack of prior coordination, Dynamic team sizes, Uncertain agent types, Dynamic variational strict core (DVSC), Open stochastic Bayesian coalitional affinity game (OSB-CAG)


- [Learning with Complementary Labels Revisited: The Selected-Completely-at-Random Setting Is More Practical](https://icml.cc/virtual/2024/poster/32656) (Poster)
  - **Authors:** [Wei Wang](http://openreview.net/profile?id=~Wei_Wang68), [Takashi Ishida](http://openreview.net/profile?id=~Takashi_Ishida1), [Yu-Jie Zhang](http://openreview.net/profile?id=~Yu-Jie_Zhang1), [Gang Niu](http://openreview.net/profile?id=~Gang_Niu1), [Masashi Sugiyama](http://openreview.net/profile?id=~Masashi_Sugiyama1)
  - **Affiliations:** The University of Tokyo; RIKEN, RIKEN, The University of Tokyo, RIKEN, The University of Tokyo; RIKEN
  - **TL;DR:** This paper presents a novel approach to complementary-label learning that does not rely on uniform distribution assumptions, introducing an unbiased risk estimator and a risk-correction method to mitigate overfitting. The proposed method demonstrates superior performance compared to existing state-of-the-art techniques across various datasets.
  - **Keywords:** Complementary-label learning, weakly supervised learning, Unbiased risk estimator, risk-correction approach, one-versus-rest strategy, Domain adaptation, semi-supervised learning, noisy-label learning, adversarial robustness, few-shot learning, medical image analysis, Overfitting problems, generation of complementary labels, Novel consistent approach, superiority over state-of-the-art methods, Synthetic datasets, real-world benchmark datasets, Positive-unlabeled (PU) learning, negative-unlabeled binary classification


- [Monotone, Bi-Lipschitz, and Polyak-Łojasiewicz Networks](https://icml.cc/virtual/2024/poster/33238) (Poster)
  - **Authors:** [Ruigang Wang](http://openreview.net/profile?id=~Ruigang_Wang2), [Krishnamurthy Dvijotham](http://openreview.net/profile?id=~Krishnamurthy_Dj_Dvijotham1), [Ian Manchester](http://openreview.net/profile?id=~Ian_Manchester1)
  - **Affiliations:** Australian Centre for Robotics, School of Aerospace, Mechanical and Mechatronic Engineering, The University of Sydney, Sydney, NSW 2006, Australia, Google DeepMind, Australian Centre for Robotics, School of Aerospace, Mechanical and Mechatronic Engineering, The University of Sydney, Sydney, NSW 2006, Australia
  - **TL;DR:** This paper introduces the BiLipNet, a bi-Lipschitz invertible neural network that allows for controlled output sensitivity and input distinguishability, and the PLNet, which satisfies the Polyak-Łojasiewicz condition for learning non-convex surrogate losses. The proposed networks demonstrate improved robustness and efficiency in achieving global minima compared to existing models.
  - **Keywords:** bi-Lipschitz invertible neural networks, Polyak-Łojasiewicz condition, BiLipNet, PLNet, invertible residual layer, incremental quadratic constraints, generative adversarial networks (GANs), robust reinforcement learning, input-output behaviors, robustness against adversarial attacks, non-convex surrogate losses, unique and efficiently-computable global minimum, certified strong monotonicity and Lipschitzness, Lipschitzness, invertibility, normalizing flows


- [Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models](https://icml.cc/virtual/2024/poster/33298) (Poster)
  - **Authors:** [Zhengbo Wang](http://openreview.net/profile?id=~Zhengbo_Wang1), [Jian Liang](http://openreview.net/profile?id=~Jian_Liang1), [Ran He](http://openreview.net/profile?id=~Ran_He1), [Zilei Wang](http://openreview.net/profile?id=~Zilei_Wang1), [Tieniu Tan](http://openreview.net/profile?id=~Tieniu_Tan1)
  - **Affiliations:** University of Science and Technology of China, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, University of Science and Technology of China, NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences; Nanjing University
  - **TL;DR:** This paper introduces Collaborative Fine-Tuning (CraFT), a novel approach for fine-tuning black-box vision-language models without access to their parameters, achieving significant improvements in few-shot classification tasks. The method demonstrates enhanced efficiency and performance while maintaining a low memory footprint.
  - **Keywords:** Collaborative Fine-Tuning, Black-Box Vision-Language Models, Derivative-Free Optimization (DFO), Prompt Generation Module, Prediction Refinement Module, Few-Shot Classification, Fine-tuning without access to model parameters, Model ownership protection, CraFT approach, Prediction-consistent loss, Improved training efficiency, 15 datasets for few-shot classification, Vision-Language Models (VLMs), Zero-Shot Recognition, CLIP


- [Highway Value Iteration Networks](https://icml.cc/virtual/2024/poster/32953) (Poster)
  - **Authors:** [Yuhui Wang](http://openreview.net/profile?id=~Yuhui_Wang1), [Weida Li](http://openreview.net/profile?id=~Weida_Li1), [Francesco Faccio](http://openreview.net/profile?id=~Francesco_Faccio1), [Qingyuan Wu](http://openreview.net/profile?id=~Qingyuan_Wu1), [Jürgen Schmidhuber](http://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1)
  - **Affiliations:** AI Initiative, King Abdullah University of Science and Technology, National University of Singapore, The Swiss AI Lab IDSIA/USI/SUPSI, The University of Liverpool, AI Initiative, King Abdullah University of Science and Technology; The Swiss AI Lab IDSIA/USI/SUPSI
  - **TL;DR:** This study introduces highway value iteration networks (VINs) to enhance long-term planning capabilities by embedding additional components that improve information flow and exploration. The proposed method significantly outperforms traditional VINs and deep neural networks in tasks requiring extensive planning steps.
  - **Keywords:** long-term planning, end-to-end learning, planning tasks, value iteration networks (VINs), highway value iteration, differentiable planning module, path planning, autonomous navigation, decision-making in dynamic environments, challenges in long-term planning, vanishing or exploding gradients, difficulties in training deep networks, novel highway VIN, effective training with hundreds of layers, improved success rates in planning tasks


- [In-context Learning on Function Classes Unveiled for Transformers](https://icml.cc/virtual/2024/poster/32958) (Poster)
  - **Authors:** [Zhijie Wang](http://openreview.net/profile?id=~Zhijie_Wang7), [Bo Jiang](http://openreview.net/profile?id=~Bo_Jiang2), [Shuai Li](http://openreview.net/profile?id=~Shuai_Li3)
  - **Affiliations:** Shanghai Jiao Tong University, Shanghai, China, John Hopcroft Center for Computer Science, Shanghai Jiao Tong University, Shanghai, China, John Hopcroft Center for Computer Science, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This study investigates the in-context learning capabilities of transformer models, demonstrating that they can approximate various function classes through implicit neural networks without parameter updates. The findings reveal that transformers can effectively perform approximate gradient descent, enabling accurate predictions across multiple tasks simultaneously.
  - **Keywords:** In-context learning, Transformers, Approximate gradient descent, Neural networks, Natural language processing, Learning different function classes, Understanding in-context learning, Upper bounds for transformer architecture, Learning linear functions, Indicator functions, Smooth functions, Large language models, Gradient descent


- [Bootstrap AutoEncoders With Contrastive Paradigm for Self-supervised Gaze Estimation](https://icml.cc/virtual/2024/poster/32657) (Poster)
  - **Authors:** [Yaoming Wang](http://openreview.net/profile?id=~Yaoming_Wang1), [Jin Li](http://openreview.net/profile?id=~Jin_Li10), [Wenrui Dai](http://openreview.net/profile?id=~Wenrui_Dai1), [Bowen Shi](http://openreview.net/profile?id=~Bowen_Shi2), [xiaopeng zhang](http://openreview.net/profile?id=~XIAOPENG_ZHANG7), [Chenglin Li](http://openreview.net/profile?id=~Chenglin_Li2), [Hongkai Xiong](http://openreview.net/profile?id=~Hongkai_Xiong1)
  - **Affiliations:** School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, Huawei Inc, Shenzhen, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This study introduces a novel approach called Bootstrap auto-encoders with Contrastive paradigm (BeCa) for self-supervised gaze estimation, which effectively combines generative and contrastive methods. The proposed method significantly improves performance by over 15% on extensive datasets, including wild scenes, without the need for additional head pose models.
  - **Keywords:** Self-supervised gaze estimation, Contrastive methods, Generative methods, Bootstrap auto-encoders, Contrastive paradigm, InfoMSE loss, Human-computer interaction, Autonomous vehicles, Augmented and virtual reality, Ineffectiveness of contrastive methods in data augmentation, Trivial solutions in generative methods, Lack of explicit regularization on semantic representations, BeCa approach, Performance improvement over state-of-the-art methods, End-to-end training without additional head pose models, Gaze360, Extensive datasets, Auto-Encoder, Contrastive regularization, Mean Square Error (MSE)


- [Improving Generalization in Offline Reinforcement Learning via Adversarial Data Splitting](https://icml.cc/virtual/2024/poster/34679) (Poster)
  - **Authors:** [Da Wang](http://openreview.net/profile?id=~Da_Wang2), [Lin Li](http://openreview.net/profile?id=~Lin_Li17), [Wei Wei](http://openreview.net/profile?id=~Wei_Wei13), [Qixian Yu](http://openreview.net/profile?id=~Qixian_Yu1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Jiye Liang](http://openreview.net/profile?id=~Jiye_Liang1)
  - **Affiliations:** Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan, China, Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan, China, Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan, China; College of Intelligence and Computing, Tianjin University, Tianjin, China, Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan, China, College of Intelligence and Computing, Tianjin University, Tianjin, China, Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, School of Computer and Information Technology, Shanxi University, Taiyuan, China
  - **TL;DR:** This paper presents an adversarial data splitting (ADS) framework to enhance generalization in offline reinforcement learning by addressing the out-of-distribution overestimation issue. The proposed method adapts to distribution shifts through a min-max optimization approach, demonstrating improved performance in extensive experiments.
  - **Keywords:** Offline Reinforcement Learning, Generalization, Adversarial Data Splitting, Min-max optimization, Meta-learning, Out-of-distribution (OOD) overestimation, Distribution shift, Conservative learning, Generalization error bound, Improved model generalization


- [EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data](https://icml.cc/virtual/2024/poster/34304) (Spotlight Poster)
  - **Authors:** [Shengjie Wang](http://openreview.net/profile?id=~Shengjie_Wang2), [Shaohuai Liu](http://openreview.net/profile?id=~Shaohuai_Liu1), [Weirui Ye](http://openreview.net/profile?id=~Weirui_Ye1), [Jiacheng You](http://openreview.net/profile?id=~Jiacheng_You1), [Yang Gao](http://openreview.net/profile?id=~Yang_Gao1)
  - **Affiliations:** Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Texas A&M University, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China
  - **TL;DR:** The study introduces EfficientZero V2, a framework aimed at enhancing sample efficiency in reinforcement learning across various domains, achieving superior performance compared to existing state-of-the-art algorithms. The findings highlight its effectiveness in both discrete and continuous control tasks with limited data.
  - **Keywords:** Reinforcement Learning, Sample Efficiency, EfficientZero V2, Monte-Carlo Tree Search (MCTS), TD-MPC, Model Predictive Path Integral (MPPI), Robotics Control, Video Games, Visual Inputs, Sample efficiency, High-dimensional action spaces, Limited data, Improved performance in diverse tasks, Outperformance of state-of-the-art algorithms, Atari 100k, DMControl Proprio, DMControl Vision


- [Transforming and Combining Rewards for Aligning Large Language Models](https://icml.cc/virtual/2024/poster/33602) (Poster)
  - **Authors:** [Zihao Wang](http://openreview.net/profile?id=~Zihao_Wang8), [Chirag Nagpal](http://openreview.net/profile?id=~Chirag_Nagpal1), [Jonathan Berant](http://openreview.net/profile?id=~Jonathan_Berant1), [Jacob Eisenstein](http://openreview.net/profile?id=~Jacob_Eisenstein1), [Alexander D'Amour](http://openreview.net/profile?id=~Alexander_D%27Amour1), [Sanmi Koyejo](http://openreview.net/profile?id=~Sanmi_Koyejo1), [Victor Veitch](http://openreview.net/profile?id=~Victor_Veitch1)
  - **Affiliations:** University of Chicago, Chicago, IL, USA, Google Research, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, Stanford University, Stanford, CA, USA, University of Chicago, Chicago, IL, USA; Google DeepMind, Mountain View, CA, USA
  - **TL;DR:** This study explores methods for aligning large language models with human preferences by transforming reward models and combining multiple properties. The proposed log-sigmoid-centered transformation significantly improves model performance in terms of helpfulness and harmlessness compared to traditional methods.
  - **Keywords:** AI Alignment, Large Language Models, Reward Models, LSC-transformation (log-sigmoid-centered transformation), Bradley-Terry preference models, Underfitting, Reward Hacking, Alignment of language models to multiple properties, Improved alignment of language models, Mitigation of overfitting, Enhanced model quality, Reinforcement Learning from Human Feedback (RLHF), Monotone Transformation


- [Pi-DUAL: Using privileged information to distinguish clean from noisy labels](https://icml.cc/virtual/2024/poster/34794) (Poster)
  - **Authors:** [Ke Wang](http://openreview.net/profile?id=~Ke_Wang19), [Guillermo Ortiz-Jimenez](http://openreview.net/profile?id=~Guillermo_Ortiz-Jimenez1), [Rodolphe Jenatton](http://openreview.net/profile?id=~Rodolphe_Jenatton3), [Mark Collier](http://openreview.net/profile?id=~Mark_Collier1), [Efi Kokiopoulou](http://openreview.net/profile?id=~Efi_Kokiopoulou2), [Pascal Frossard](http://openreview.net/profile?id=~Pascal_Frossard1)
  - **Affiliations:** École Polytechnique Fédérale de Lausanne (EPFL), Google DeepMind; Work done while at EPFL, Bioptimus; Work done while at Google DeepMind, Google Research, Google Research, École Polytechnique Fédérale de Lausanne (EPFL)
  - **TL;DR:** The study introduces Pi-DUAL, an architecture that utilizes privileged information to effectively distinguish between clean and noisy labels, significantly improving generalization performance on benchmarks like ImageNet-PI. The method demonstrates a new state-of-the-art accuracy and excels in identifying noisy samples post-training.
  - **Keywords:** label noise, deep learning, privileged information, Pi-DUAL architecture, gating mechanism, image recognition, noisy label identification, overfitting to label noise, generalization performance, state-of-the-art test set accuracy, identification of noisy samples, ImageNet-PI


- [A Dual-module Framework for Counterfactual Estimation over Time](https://icml.cc/virtual/2024/poster/35167) (Poster)
  - **Authors:** [Xin Wang](http://openreview.net/profile?id=~Xin_Wang46), [Shengfei Lyu](http://openreview.net/profile?id=~Shengfei_Lyu1), [Lishan Yang](http://openreview.net/profile?id=~Lishan_Yang2), [Yibing Zhan](http://openreview.net/profile?id=~Yibing_Zhan2), [Huanhuan Chen](http://openreview.net/profile?id=~Huanhuan_Chen1)
  - **Affiliations:** School of Computer Science and Technology, University of Science and Technology of China, China, Nanyang Technological University, Singapore, School of Computer Science and Technology, University of Science and Technology of China, China, JD Explore Academy, School of Computer Science and Technology, University of Science and Technology of China, China
  - **TL;DR:** The study introduces the Adversarial Counterfactual Temporal Inference Network (ACTIN) framework, which enhances counterfactual estimation over time by employing a dual-module approach to mitigate confounding bias and capture long-range dependencies. The proposed methods demonstrate state-of-the-art performance in both synthetic and real-world datasets, emphasizing their applicability in personalized healthcare.
  - **Keywords:** counterfactual estimation, treatment strategies, causal inference, Adversarial Counterfactual Temporal Inference Network (ACTIN), Temporal Integration Predicting (TIP), Direct Predicting (DP), personalized healthcare, observational data analysis, confounding bias, time-varying confounders, long-range dependencies, balanced representations, state-of-the-art performance, Causal Transformer (CT), Long Short-Term Memory (LSTM)


- [InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining](https://icml.cc/virtual/2024/poster/34127) (Poster)
  - **Authors:** [Boxin Wang](http://openreview.net/profile?id=~Boxin_Wang1), [Wei Ping](http://openreview.net/profile?id=~Wei_Ping1), [Lawrence McAfee](http://openreview.net/profile?id=~Lawrence_McAfee1), [Peng Xu](http://openreview.net/profile?id=~Peng_Xu7), [Bo Li](http://openreview.net/profile?id=~Bo_Li19), [Mohammad Shoeybi](http://openreview.net/profile?id=~Mohammad_Shoeybi1), [Bryan Catanzaro](http://openreview.net/profile?id=~Bryan_Catanzaro1)
  - **Affiliations:** NVIDIA, NVIDIA, NVIDIA, NVIDIA, UIUC, NVIDIA, NVIDIA
  - **TL;DR:** This study introduces Retro 48B, the largest retrieval-augmented large language model, which significantly improves perplexity and zero-shot generalization after instruction tuning. The findings demonstrate the potential of scaling retrieval-augmented pretraining to enhance model performance across various tasks.
  - **Keywords:** Retrieval-Augmented Pretraining, Large Language Models, Auto-regressive models, Instruction tuning, Retrieval augmentation, Natural language processing, Zero-shot tasks, Limited model size, Zero-shot generalization gap, Retro 48B model, Improved perplexity, Enhanced instruction tuning, 1.2 trillion tokens, 100 billion tokens for pretraining


- [TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks](https://icml.cc/virtual/2024/poster/34638) (Poster)
  - **Authors:** [Zhiruo Wang](http://openreview.net/profile?id=~Zhiruo_Wang1), [Graham Neubig](http://openreview.net/profile?id=~Graham_Neubig1), [Daniel Fried](http://openreview.net/profile?id=~Daniel_Fried1)
  - **Affiliations:** Language Technologies Institute, Carnegie Mellon University, Language Technologies Institute, Carnegie Mellon University, Language Technologies Institute, Carnegie Mellon University
  - **TL;DR:** The study presents TROVE, a training-free method for inducing a verifiable and efficient toolbox of reusable functions that enhances the performance of language models in solving programmatic tasks. TROVE achieves simpler and more accurate solutions while significantly reducing the size of the toolbox and improving human verification efficiency.
  - **Keywords:** language models, programmatic tasks, reusable functions, math, table question answering, image reasoning, verbose programs, error-prone programs, complex verification, TROVE, training-free method, efficient toolbox, CODELLAMA, GPT


- [A Fine-grained Analysis of Fitted Q-evaluation: Beyond Parametric Models](https://icml.cc/virtual/2024/poster/34647) (Poster)
  - **Authors:** [Jiayi Wang](http://openreview.net/profile?id=~Jiayi_Wang7), [Zhengling Qi](http://openreview.net/profile?id=~Zhengling_Qi1), [Raymond K. W. Wong](http://openreview.net/profile?id=~Raymond_K._W._Wong1)
  - **Affiliations:** Department of Mathematical Sciences, University of Texas at Dallas, Richardson, USA, School of Business, The George Washington University, Washington, D.C., USA, Department of Statistics, Texas A&M University, College Station, USA
  - **TL;DR:** This paper provides a detailed statistical analysis of the fitted Q-evaluation (FQE) method in reinforcement learning, focusing on the convergence rates and error bounds for estimating policy values under both parametric and nonparametric models. The findings indicate that optimal convergence rates can be achieved, and improvements in error bounds are possible with certain assumptions on ratio functions.
  - **Keywords:** Fitted Q-evaluation, Off-policy evaluation, Reinforcement learning, Q-function estimation, Nonparametric models, Parametric models, Estimating policy value, Convergence rates, Error bounds, Optimal convergence rates, Error bounds improvement, Markov Decision Process (MDP), Completeness assumption


- [Bridging Data Gaps in Diffusion Models with Adversarial Noise-Based Transfer Learning](https://icml.cc/virtual/2024/poster/34108) (Spotlight Poster)
  - **Authors:** [Xiyu Wang](http://openreview.net/profile?id=~Xiyu_Wang2), [Baijiong Lin](http://openreview.net/profile?id=~Baijiong_Lin1), [Daochang Liu](http://openreview.net/profile?id=~Daochang_Liu1), [YINGCONG CHEN](http://openreview.net/profile?id=~Ying-Cong_Chen1), [Chang Xu](http://openreview.net/profile?id=~Chang_Xu4)
  - **Affiliations:** School of Computer Science, Faculty of Engineering, The University of Sydney, Australia, The Hong Kong University of Science and Technology (Guangzhou), China, School of Computer Science, Faculty of Engineering, The University of Sydney, Australia, The Hong Kong University of Science and Technology (Guangzhou), China, School of Computer Science, Faculty of Engineering, The University of Sydney, Australia; The Hong Kong University of Science and Technology (Guangzhou), China
  - **TL;DR:** This paper introduces DPMs-ANT, a novel transfer learning method for diffusion probabilistic models that addresses the limited data problem in image generation. The proposed method enhances image quality and diversity through similarity-guided training and adversarial noise selection, outperforming existing GAN-based and DPM-based approaches.
  - **Keywords:** Diffusion Probabilistic Models, Image Generation, Transfer Learning, DPMs-ANT, Similarity-Guided Training, Adversarial Noise Selection, Few-Shot Image Generation, Limited Data Problem, Overfitting, Domain Gap, Efficient Image Generation, Improved Image Quality and Diversity, Generative Adversarial Networks (GANs), DPMs (Diffusion Probabilistic Models)


- [Sample Average Approximation for Conditional Stochastic Optimization with Dependent Data](https://icml.cc/virtual/2024/poster/33756) (Poster)
  - **Authors:** [Yafei Wang](http://openreview.net/profile?id=~Yafei_Wang1), [Bo Pan](http://openreview.net/profile?id=~Bo_Pan1), [Mei Li](http://openreview.net/profile?id=~Mei_Li1), [Jianya Lu](http://openreview.net/profile?id=~Jianya_Lu1), [Lingchen Kong](http://openreview.net/profile?id=~Lingchen_Kong1), [Bei Jiang](http://openreview.net/profile?id=~Bei_Jiang1), [Linglong Kong](http://openreview.net/profile?id=~Linglong_Kong2)
  - **Affiliations:** Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Canada, Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Canada, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China, School of Mathematics, Statistics and Actuarial Science, University of Essex, Colchester, UK, Department of Mathematics and Statistics, Beijing Jiaotong University, Beijing, China, Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Canada, Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Canada; Department of Mathematics and Statistics, Beijing Jiaotong University, Beijing, China
  - **TL;DR:** This paper addresses Conditional Stochastic Optimization (CSO) under dependent data by proposing a Sample Average Approximation (SAA) method that retains asymptotic consistency and finite sample guarantees. The findings indicate that the SAA approach performs well in real data applications despite the presence of dependence in the data.
  - **Keywords:** Conditional Stochastic Optimization, optimization under uncertainty, Sample Average Approximation, covariance inequalities, independent block sampling, time series analysis, operational control, reinforcement learning, meta-learning, optimal control, dependence patterns in data, asymptotic consistency, finite sample guarantee, theoretical guarantees for SAA, sample complexity analysis


- [LLM-Empowered State Representation for Reinforcement Learning](https://icml.cc/virtual/2024/poster/32718) (Poster)
  - **Authors:** [Boyuan Wang](http://openreview.net/profile?id=~Boyuan_Wang1), [Yun Qu](http://openreview.net/profile?id=~Yun_Qu2), [Yuhang Jiang](http://openreview.net/profile?id=~Yuhang_Jiang3), [Jianzhun Shao](http://openreview.net/profile?id=~Jianzhun_Shao1), [Chang Liu](http://openreview.net/profile?id=~Chang_Liu9), [Wenming Yang](http://openreview.net/profile?id=~Wenming_Yang1), [Xiangyang Ji](http://openreview.net/profile?id=~Xiangyang_Ji1)
  - **Affiliations:** Tsinghua University, Tsinghua University, Tsinghua University, Tsinghua University, Tsinghua University, Tsinghua University, Tsinghua University
  - **TL;DR:** This study introduces LLM-Empowered State Representation (LESR), which utilizes large language models to autonomously generate task-related state representations, significantly improving sample efficiency in reinforcement learning. Experimental results show that LESR outperforms state-of-the-art methods by 29% in accumulated rewards and 30% in success rates across various tasks.
  - **Keywords:** Reinforcement Learning, State Representation, Large Language Models (LLM), Value Networks, Mujoco tasks, Gym-Robotics tasks, Sample efficiency, Task-related state representation, LLM-Empowered State Representation (LESR), Improved continuity of network mappings, Mujoco, Gym


- [How to Trace Latent Generative Model Generated Images without Artificial Watermark?](https://icml.cc/virtual/2024/poster/33958) (Poster)
  - **Authors:** [Zhenting Wang](http://openreview.net/profile?id=~Zhenting_Wang1), [Vikash Sehwag](http://openreview.net/profile?id=~Vikash_Sehwag1), [Chen Chen](http://openreview.net/profile?id=~Chen_Chen20), [Lingjuan Lyu](http://openreview.net/profile?id=~Lingjuan_Lyu1), [Dimitris Metaxas](http://openreview.net/profile?id=~Dimitris_N._Metaxas1), [Shiqing Ma](http://openreview.net/profile?id=~Shiqing_Ma2)
  - **Affiliations:** Rutgers University, Sony AI, Sony AI, Sony AI, Rutgers University, University of Massachusetts at Amherst
  - **TL;DR:** This study presents LATENT TRACER, a method for tracing images generated by latent generative models without requiring artificial watermarks or extra operations. The method demonstrates high accuracy and efficiency in distinguishing images produced by specific models, suggesting that generated images may inherently carry traces from their source models.
  - **Keywords:** Latent generative models, Image tracing, Latent inversion, Gradient-based latent inversion, Image generation, Image forensics, Misuse of generated images, IP infringement, Image origin tracing, LATENT TRACER method, High accuracy in distinguishing generated images, Stable Diffusion, Watermarking, Model fingerprinting


- [Generalization Analysis of Stochastic Weight Averaging with General Sampling](https://icml.cc/virtual/2024/poster/33797) (Poster)
  - **Authors:** [Wang Peng](http://openreview.net/profile?id=~Peng_Wang44), [Li Shen](http://openreview.net/profile?id=~Li_Shen1), [Zerui Tao](http://openreview.net/profile?id=~Zerui_Tao1), [Shuaida He](http://openreview.net/profile?id=~Shuaida_He1), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** Huazhong University of Science and Technology, China; Nanyang Technological University, Singapore, Sun Yat-sen University, China; JD Explore Academy, China, Tokyo University of Agriculture and Technology, Japan; RIKEN AIP, Japan, Southern University of Science and Technology, China, Nanyang Technological University, Singapore
  - **TL;DR:** This paper investigates the generalization capabilities of Stochastic Weight Averaging (SWA) in non-convex settings and under various sampling mechanisms, addressing theoretical challenges and establishing sharper generalization bounds compared to Stochastic Gradient Descent (SGD). The findings are supported by experimental results across several benchmarks.
  - **Keywords:** Stochastic Weight Averaging (SWA), Generalization in Deep Learning, Stochastic Gradient Descent (SGD), Mathematical Induction, Large-scale Network Training, Adversarial Learning, Non-convex Optimization, Sampling Without Replacement, Overfitting, Stability Bounds, Generalization Bounds


- [Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery](https://icml.cc/virtual/2024/poster/34134) (Spotlight Poster)
  - **Authors:** [Caixing Wang](http://openreview.net/profile?id=~Caixing_Wang1), [Ziliang Shen](http://openreview.net/profile?id=~Ziliang_Shen1)
  - **Affiliations:** School of Statistics and Management, Shanghai University of Finance and Economics, School of Statistics and Management, Shanghai University of Finance and Economics
  - **TL;DR:** This paper presents a distributed estimation method for high-dimensional linear quantile regression, addressing challenges related to data heterogeneity and non-smooth loss functions. The proposed algorithm achieves high computational efficiency and support recovery accuracy, demonstrating effectiveness through extensive experiments.
  - **Keywords:** distributed estimation, high-dimensional linear quantile regression, quantile regression, least-squares optimization, double-smoothing approach, data heterogeneity, non-smooth check loss function, high dimensionality, support recovery, efficient algorithm, near-oracle convergence rate, high support recovery accuracy


- [Efficient Online Set-valued Classification with Bandit Feedback](https://icml.cc/virtual/2024/poster/34063) (Poster)
  - **Authors:** [Zhou Wang](http://openreview.net/profile?id=~Zhou_Wang3), [Xingye Qiao](http://openreview.net/profile?id=~Xingye_Qiao1)
  - **Affiliations:** Department of Mathematics and Statistics, Binghamton University, New York, USA, Department of Mathematics and Statistics, Binghamton University, New York, USA
  - **TL;DR:** This study introduces Bandit Class-specific Conformal Prediction (BCCP) to address the challenges of online learning with bandit feedback, providing coverage guarantees on a class-specific level. The proposed method effectively manages sparsely labeled data and enhances the reliability of conformal prediction in decision-making environments.
  - **Keywords:** Conformal Prediction, Online Learning, Set-valued Classification, Bandit Class-specific Conformal Prediction (BCCP), Stochastic Gradient Descent, Autonomous Vehicles, Medical Diagnostics, Data Sparsity, Uncertainty in Predictions, Coverage Guarantees, Set-valued Inferences, Bandit Feedback


- [Autaptic Synaptic Circuit Enhances Spatio-temporal Predictive Learning of Spiking Neural Networks](https://icml.cc/virtual/2024/poster/33269) (Poster)
  - **Authors:** [Lihao Wang](http://openreview.net/profile?id=~Lihao_Wang4), [Zhaofei Yu](http://openreview.net/profile?id=~Zhaofei_Yu1)
  - **Affiliations:** Institute for Artificial Intelligence, Peking University, Peking, China; Shenzhen X-Institute, Shenzhen, China, Institute for Artificial Intelligence, Peking University, Peking, China
  - **TL;DR:** This study proposes a novel Spatio-Temporal Circuit model for Spiking Neural Networks that enhances their ability to model long-term temporal dependencies and spatial interactions, addressing limitations of existing models. The proposed model outperforms other adaptive models in spatio-temporal prediction tasks, enriching the complexity and specificity of SNNs.
  - **Keywords:** Spiking Neural Networks, Spatio-temporal prediction, Leaky Integrate-and-Fire model, Spatio-Temporal Circuit model, Long-term temporal dependencies, Spatial information interaction, Gradient vanishing, Enhanced temporal memory, Enhanced spatial coordination, Improved performance on spatio-temporal prediction tasks, Multiple spatio-temporal prediction datasets, Autaptic synapses, Dynamic parameters, Biological realism, Energy efficiency


- [CW Complex Hypothesis for Image Data](https://icml.cc/virtual/2024/poster/33817) (Poster)
  - **Authors:** [Yi Wang](http://openreview.net/profile?id=~Yi_Wang22), [Zhiren Wang](http://openreview.net/profile?id=~Zhiren_Wang1)
  - **Affiliations:** Department of Mathematics, Johns Hopkins University, Department of Mathematics, Pennsylvania State University
  - **TL;DR:** This study proposes a CW complex hypothesis suggesting that image data is distributed in "manifolds with skeletons," where the local intrinsic dimension varies across data points. The findings indicate that the mixture of higher and lower dimensional components in data hinders diffusion models from effectively learning and generating accurate details.
  - **Keywords:** manifold hypothesis, CW complex hypothesis, image data, indicator function, empirical verification, image datasets, diffusion models, variation in intrinsic dimensions, difficulty in generating higher dimensional details, new hypothesis on data distribution, visualization of synthetic image data, intrinsic dimension, manifolds with skeletons, connected components


- [Learning with Adaptive Resource Allocation](https://icml.cc/virtual/2024/poster/34467) (Poster)
  - **Authors:** [Jing Wang](http://openreview.net/profile?id=~Jing_Wang32), [Miao Yu](http://openreview.net/profile?id=~Miao_Yu7), [Peng Zhao](http://openreview.net/profile?id=~Peng_Zhao1), [Zhi-Hua Zhou](http://openreview.net/profile?id=~Zhi-Hua_Zhou2)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This study introduces the Learning with Adaptive Resource Allocation (LARA) approach to efficiently manage multiple time-constrained learning tasks under limited computational resources. The proposed method enhances learning progress prediction and resource allocation, addressing significant inefficiencies in current resource management practices.
  - **Keywords:** machine learning, resource allocation, Learning with Adaptive Resource Allocation (LARA), online estimator, adaptive search method, computational resource management, quantitative finance, limited computational resources, time-constrained learning tasks, inefficient resource allocation, effective training methods, resource usage efficiency


- [Optimal Exact Recovery in Semi-Supervised Learning: A Study of Spectral Methods and Graph Convolutional Networks](https://icml.cc/virtual/2024/poster/34834) (Poster)
  - **Authors:** [Haixiao Wang](http://openreview.net/profile?id=~Haixiao_Wang1), [Zhichao Wang](http://openreview.net/profile?id=~Zhichao_Wang3)
  - **Affiliations:** Department of Mathematics, University of California San Diego, San Diego, California, USA, Department of Mathematics, University of California San Diego, San Diego, California, USA
  - **TL;DR:** This study investigates semi-supervised node classification using the Contextual Stochastic Block Model (CSBM), identifying the information-theoretical limits for exact recovery of nodes and proposing an optimal spectral estimator. The findings reveal that graph ridge regression and Graph Convolutional Networks can achieve the same recovery threshold, emphasizing the importance of feature learning in enhancing GCN performance.
  - **Keywords:** semi-supervised learning, node classification, graph neural networks, Contextual Stochastic Block Model (CSBM), Gaussian Mixture Model (GMM), Principal Component Analysis (PCA), graph ridge regression, Graph Convolutional Networks (GCN), accurate classification, information-theoretical threshold, transductive learning, optimal spectral estimator, exact recovery, feature learning, CSBM dataset, Stochastic Block Model (SBM), feature vectors, adjacency matrix


- [Boximator: Generating Rich and Controllable Motions for Video Synthesis](https://icml.cc/virtual/2024/poster/34864) (Poster)
  - **Authors:** [Jiawei Wang](http://openreview.net/profile?id=~Jiawei_Wang14), [Yuchen Zhang](http://openreview.net/profile?id=~Yuchen_Zhang1), [Jiaxin Zou](http://openreview.net/profile?id=~Jiaxin_Zou1), [Yan Zeng](http://openreview.net/profile?id=~Yan_Zeng1), [Guoqiang Wei](http://openreview.net/profile?id=~Guoqiang_Wei1), [Liping Yuan](http://openreview.net/profile?id=~Liping_Yuan2), [Hang Li](http://openreview.net/profile?id=~Hang_Li4)
  - **Affiliations:** ByteDance Research, Beijing, China, ByteDance Research, Beijing, China, ByteDance Research, Beijing, China, ByteDance Research, Beijing, China, ByteDance Research, Beijing, China, ByteDance Research, Beijing, China, ByteDance Research, Beijing, China
  - **TL;DR:** This paper presents Boximator, a novel approach for generating rich and controllable motions in video synthesis using box-shaped constraints. The method enhances motion control and achieves state-of-the-art video quality, demonstrating significant improvements in bounding box alignment and user preference over existing models.
  - **Keywords:** video synthesis, motion control, box-shaped constraints, self-tracking technique, image-to-video generation, text-to-video methods, controllability in video synthesis, training challenges, Boximator, state-of-the-art video quality (FVD) scores, bounding box alignment metric, hard box, soft box, video diffusion models


- [A Hierarchical Adaptive Multi-Task Reinforcement Learning Framework for Multiplier Circuit Design](https://icml.cc/virtual/2024/poster/34306) (Poster)
  - **Authors:** [Zhihai Wang](http://openreview.net/profile?id=~Zhihai_Wang1), [Jie Wang](http://openreview.net/profile?id=~Jie_Wang1), [Dongsheng Zuo](http://openreview.net/profile?id=~Dongsheng_Zuo1), [Ji Yunjie](http://openreview.net/profile?id=~Ji_Yunjie1), [Xilin Xia](http://openreview.net/profile?id=~Xilin_Xia1), [Yuzhe Ma](http://openreview.net/profile?id=~Yuzhe_Ma2), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Mingxuan Yuan](http://openreview.net/profile?id=~Mingxuan_Yuan1), [Yongdong Zhang](http://openreview.net/profile?id=~Yongdong_Zhang2), [Feng Wu](http://openreview.net/profile?id=~Feng_Wu1)
  - **Affiliations:** CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, Microelectronics Thrust, Hong Kong University of Science and Technology (Guangzhou), CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, Microelectronics Thrust, Hong Kong University of Science and Technology (Guangzhou), Noah’s Ark Lab, Huawei Technologies; College of Intelligence and Computing, Tianjin University, Noah’s Ark Lab, Huawei Technologies, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS & MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China
  - **TL;DR:** This study presents a hierarchical adaptive multi-task reinforcement learning framework (HA VE) for optimizing multiplier circuit designs, addressing the challenges of finding Pareto-optimal solutions efficiently. The proposed method significantly outperforms existing approaches, achieving up to 28% larger hyper-volume and demonstrating good generalization to large-scale computation-intensive circuits.
  - **Keywords:** Multiplier design, multi-objective optimization, integrated circuits, Reinforcement learning, hierarchical adaptive multi-task learning, Deep neural networks, digital signal processors, microprocessors, Combinatorial optimization, conflicting objectives, NP-hard problems, Pareto-optimal circuit designs, sample efficiency, hyper-volume improvement


- [Bridging Model Heterogeneity in Federated Learning via Uncertainty-based Asymmetrical Reciprocity Learning](https://icml.cc/virtual/2024/poster/33063) (Poster)
  - **Authors:** [Jiaqi Wang](http://openreview.net/profile?id=~Jiaqi_Wang4), [Chenxu Zhao](http://openreview.net/profile?id=~Chenxu_Zhao4), [Lingjuan Lyu](http://openreview.net/profile?id=~Lingjuan_Lyu1), [Quanzeng You](http://openreview.net/profile?id=~Quanzeng_You3), [Mengdi Huai](http://openreview.net/profile?id=~Mengdi_Huai1), [Fenglong Ma](http://openreview.net/profile?id=~Fenglong_Ma1)
  - **Affiliations:** Pennsylvania State University, Iowa State University, Sony AI, ByteDance, Iowa State University, Pennsylvania State University
  - **TL;DR:** This study introduces FedType, a novel framework for addressing model heterogeneity in federated learning by utilizing small proxy models for secure information exchange. The proposed method enhances model aggregation without relying on public data, prioritizing client privacy and reducing communication costs.
  - **Keywords:** Federated Learning, Model Heterogeneity, Uncertainty-based Asymmetrical Reciprocity Learning, Heterogeneous model aggregation, Client privacy, Communication costs, FedType framework, Knowledge transfer between models, Benchmark datasets


- [Batch Singular Value Polarization and Weighted Semantic Augmentation for Universal Domain Adaptation](https://icml.cc/virtual/2024/poster/32860) (Poster)
  - **Authors:** [Ziqi Wang](http://openreview.net/profile?id=~WangZiQi2), [Wei Wang](http://openreview.net/profile?id=~Wei_Wang99), [Chao Huang](http://openreview.net/profile?id=~Chao_Huang16), [Jie Wen](http://openreview.net/profile?id=~Jie_Wen1), [Cong Wang](http://openreview.net/profile?id=~Cong_Wang21)
  - **Affiliations:** School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China, School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China, School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China
  - **TL;DR:** This study presents a novel approach called Batch Singular Value Polarization and Weighted Semantic Augmentation (BSP-WSA) for Universal Domain Adaptation (UniDA), which effectively identifies unknown categories in the target domain while preventing misclassification into source private categories. The proposed method outperforms existing state-of-the-art approaches in extensive experiments across three benchmarks.
  - **Keywords:** Universal Domain Adaptation (UniDA), Domain Shift, Adversarial Classifier, Singular Value Decomposition (SVD), Weighted Semantic Augmentation, Category Shift, Misclassification of Target Samples, Distribution Discrepancy, Batch Singular Value Polarization and Weighted Semantic Augmentation (BSP-WSA)


- [Open-Vocabulary Calibration for Fine-tuned CLIP](https://icml.cc/virtual/2024/poster/33036) (Poster)
  - **Authors:** [Shuoyuan Wang](http://openreview.net/profile?id=~Shuoyuan_Wang3), [Jindong Wang](http://openreview.net/profile?id=~Jindong_Wang1), [Guoqing Wang](http://openreview.net/profile?id=~Guoqing_Wang2), [Bob Zhang](http://openreview.net/profile?id=~Bob_Zhang1), [Kaiyang Zhou](http://openreview.net/profile?id=~Kaiyang_Zhou1), [Hongxin Wei](http://openreview.net/profile?id=~Hongxin_Wei1)
  - **Affiliations:** Department of Statistics and Data Science, Southern University of Science and Technology, Shenzhen, China; Department of Computer and Information Science, University of Macau, Taipa, Macau, William & Mary, Williamsburg, Virginia, USA, School of Computer Science and Engineering, University of Electronic Science and Technology of China, China, Department of Computer and Information Science, University of Macau, Taipa, Macau, Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China, Department of Statistics and Data Science, Southern University of Science and Technology, Shenzhen, China
  - **TL;DR:** This paper investigates the confidence calibration problem in fine-tuned vision-language models (VLMs) and introduces Distance-Aware Calibration (DAC) to improve reliability in open-vocabulary settings. The proposed method effectively addresses miscalibration issues without compromising inference speed, demonstrating significant improvements across various datasets.
  - **Keywords:** Vision-language models, open-vocabulary tasks, Prompt learning, Distance-Aware Calibration (DAC), Image recognition, text-driven visual content generation, visual chatbots, medical diagnosis, autonomous driving, Confidence calibration problem, miscalibration in fine-tuned VLMs, overconfidence in novel classes, underconfidence in base classes, Calibration methods, temperature scaling


- [Mapping the Multiverse of Latent Representations](https://icml.cc/virtual/2024/poster/34010) (Poster)
  - **Authors:** [Jeremy Wayland](http://openreview.net/profile?id=~Jeremy_Wayland1), [Corinna Coupette](http://openreview.net/profile?id=~Corinna_Coupette1), [Bastian Rieck](http://openreview.net/profile?id=~Bastian_Rieck1)
  - **Affiliations:** Helmholtz Munich; Technical University of Munich, KTH Royal Institute of Technology; Max Planck Institute for Informatics, Helmholtz Munich; Technical University of Munich
  - **TL;DR:** The study presents PRESTO, a framework for mapping the multiverse of machine-learning models that utilize latent representations, addressing the variability and complexity of these models. It employs persistent homology to analyze different combinations of methods, configurations, and datasets, aiming to enhance reliability and robustness in machine learning.
  - **Keywords:** multiverse analysis, machine learning, latent representations, persistent homology, Variational Auto-Encoders (VAEs), Large Language Models (LLMs), Graph Neural Networks (GNNs), reliability concerns, robustness concerns, reproducibility crisis, data quality issues, model architecture issues, hyperparameter landscape exploration, PRESTO framework, sensitivity analysis, detection of anomalous embeddings


- [Task Groupings Regularization: Data-Free Meta-Learning with Heterogeneous Pre-trained Models](https://icml.cc/virtual/2024/poster/33938) (Poster)
  - **Authors:** [Yongxian Wei](http://openreview.net/profile?id=~Yongxian_Wei1), [Zixuan Hu](http://openreview.net/profile?id=~Zixuan_Hu1), [Li Shen](http://openreview.net/profile?id=~Li_Shen1), [Zhenyi Wang](http://openreview.net/profile?id=~Zhenyi_Wang1), [Yu Li](http://openreview.net/profile?id=~Yu_Li1), [Chun Yuan](http://openreview.net/profile?id=~Chun_Yuan1), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** Tsinghua Shenzhen International Graduate School, Tsinghua University, China, Tsinghua Shenzhen International Graduate School, Tsinghua University, China; College of Computing & Data Science, Nanyang Technological University, Singapore, School of Cyber Science and Technology, Sun Yat-sen University, China, University of Maryland, College Park, USA, Department of Computer Science and Engineering, The Chinese University of Hong Kong, China, Tsinghua Shenzhen International Graduate School, Tsinghua University, China, College of Computing & Data Science, Nanyang Technological University, Singapore
  - **TL;DR:** This paper introduces Task Groupings Regularization to address the challenges of model heterogeneity in Data-Free Meta-Learning (DFML), highlighting the importance of balancing the heterogeneity-homogeneity trade-off. The proposed method effectively mitigates task conflicts and enhances the generalization capacity of meta-models across diverse tasks.
  - **Keywords:** Data-Free Meta-Learning, Model Heterogeneity, Task Groupings Regularization, Implicit Gradient Regularization, Multi-domain Learning, Multi-architecture Scenarios, Task Conflicts, Overfitting Risk, Shared Representations, Generalization Capacity, Heterogeneity-Homogeneity Trade-off


- [Position: AI/ML Influencers Have a Place in the Academic Process](https://icml.cc/virtual/2024/poster/32653) (Poster)
  - **Authors:** [Iain Xie Weissburg](http://openreview.net/profile?id=~Iain_Weissburg1), [Mehir Arora](http://openreview.net/profile?id=~Mehir_Arora1), [Xinyi Wang](http://openreview.net/profile?id=~Xinyi_Wang2), [Liangming Pan](http://openreview.net/profile?id=~Liangming_Pan1), [William Wang](http://openreview.net/profile?id=~William_Yang_Wang2)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of California, Santa Barbara, USA, Department of Computer Science, University of California, Santa Barbara, USA, Department of Computer Science, University of California, Santa Barbara, USA, Department of Computer Science, University of California, Santa Barbara, USA, Department of Computer Science, University of California, Santa Barbara, USA
  - **TL;DR:** This study investigates the impact of social media influencers on the visibility and citation counts of AI/ML research papers. The findings reveal that papers endorsed by influencers receive significantly more citations, highlighting the need for responsible curation to ensure diversity in research representation.
  - **Keywords:** AI/ML research visibility, social media influence, Statistical analysis, causal inference, Academic discourse, research publication, Access to research publications, citation visibility, Increased citation counts for endorsed papers, Dataset of over 8,000 papers, tweets from ArXiv, Influencer curation, journalistic standards in research


- [Learning Pseudo-Contractive Denoisers for Inverse Problems](https://icml.cc/virtual/2024/poster/34516) (Poster)
  - **Authors:** [Deliang Wei](http://openreview.net/profile?id=~Deliang_Wei2), [Peng Chen](http://openreview.net/profile?id=~Peng_Chen18), [Fang Li](http://openreview.net/profile?id=~Fang_Li4)
  - **Affiliations:** School of Mathematical Sciences, Key Laboratory of MEA (Ministry of Education) & Shanghai Key Laboratory of PMMP, East China Normal University, Shanghai 200241, China, School of Mathematical Sciences, Key Laboratory of MEA (Ministry of Education) & Shanghai Key Laboratory of PMMP, East China Normal University, Shanghai 200241, China, School of Mathematical Sciences, Key Laboratory of MEA (Ministry of Education) & Shanghai Key Laboratory of PMMP, East China Normal University, Shanghai 200241, China; Chongqing Key Laboratory of Precision Optics, Chongqing Institute of East China Normal University, Chongqing 401120, China
  - **TL;DR:** This paper presents a novel training strategy for deep denoisers that relaxes the constraints of non-expansiveness to pseudo-contractiveness, leading to effective algorithms for solving inverse problems in signal and image processing. The proposed methods demonstrate superior performance in recovery tasks compared to traditional denoisers.
  - **Keywords:** deep denoising, inverse problems, signal processing, image processing, pseudo-contractiveness, gradient descent, Ishikawa process, half-quadratic splitting, forward-backward splitting, denoising, deblurring, inpainting, super-resolution, medical imaging, convergence, recovery performance, non-expansiveness, novel training strategy, effective algorithms, superior performance of denoiser, Lipschitz conditions, Jacobian matrix, variational approach, proximal operator


- [Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution](https://icml.cc/virtual/2024/poster/32977) (Poster)
  - **Authors:** [Eslam Zaher](http://openreview.net/profile?id=~Eslam_Zaher1), [Maciej Trzaskowski](http://openreview.net/profile?id=~Dr_Maciej_Trzaskowski1), [Quan Nguyen](http://openreview.net/profile?id=~Quan_Nguyen7), [Fred Roosta](http://openreview.net/profile?id=~Fred_Roosta1)
  - **Affiliations:** ARC Training Centre for Information Resilience (CIRES), Brisbane, Australia; School of Mathematics and Physics, University of Queensland, Brisbane, Australia, ARC Training Centre for Information Resilience (CIRES), Brisbane, Australia; Institute for Molecular Bioscience, University of Queensland, Brisbane, Australia, Institute for Molecular Bioscience, University of Queensland, Brisbane, Australia; QIMR Berghofer Medical Research Institute, Brisbane, Australia, ARC Training Centre for Information Resilience (CIRES), Brisbane, Australia; School of Mathematics and Physics, University of Queensland, Brisbane, Australia
  - **TL;DR:** This study addresses the reliability concerns of Integrated Gradients (IG) in feature attribution for deep learning models, focusing on the issues of noisy visualizations and adversarial attacks. The authors propose an adaptation of IG that aligns attribution paths with the Riemannian geometry of data manifolds, resulting in more intuitive explanations and improved robustness against targeted attacks.
  - **Keywords:** Explainability, Feature Attribution, Deep Learning, Integrated Gradients (IG), Path-based Feature Attribution, Vision Models, Image Datasets, Noisy Feature Visualizations, Vulnerability to Adversarial Attributional Attacks, Robustness to Targeted Attributional Attacks, Perceptually Intuitive Explanations, Riemannian Geometry, Geodesics


- [Magicoder: Empowering Code Generation with OSS-Instruct](https://icml.cc/virtual/2024/poster/33819) (Poster)
  - **Authors:** [Yuxiang Wei](http://openreview.net/profile?id=~Yuxiang_Wei2), [Zhe Wang](http://openreview.net/profile?id=~Zhe_Wang41), [Jiawei Liu](http://openreview.net/profile?id=~Jiawei_Liu11), [Yifeng Ding](http://openreview.net/profile?id=~Yifeng_Ding2), [LINGMING ZHANG](http://openreview.net/profile?id=~LINGMING_ZHANG2)
  - **Affiliations:** University of Illinois at Urbana-Champaign, USA, Tsinghua University, China, University of Illinois at Urbana-Champaign, USA, University of Illinois at Urbana-Champaign, USA, University of Illinois at Urbana-Champaign, USA
  - **TL;DR:** This paper introduces Magicoder, a series of open-source LLMs for code generation that leverage OSS-I NSTRUCT to create diverse instruction data, significantly improving performance on coding benchmarks. The models outperform existing state-of-the-art code models, including ChatGPT, by addressing biases in synthetic data generation.
  - **Keywords:** Code generation, Large Language Models (LLMs), OSS-I NSTRUCT, Evol-Instruct, Software development, Inherent bias in synthetic data, instruction-following capability, Magicoder models, enhanced Magicoder S, surpassing ChatGPT on benchmarks, CODELLAMA


- [Extending Test-Time Augmentation with Metamorphic Relations for Combinatorial Problems](https://icml.cc/virtual/2024/poster/34125) (Spotlight Poster)
  - **Authors:** [Siwei Wei](http://openreview.net/profile?id=~Siwei_Wei1), [Xudong Zhang](http://openreview.net/profile?id=~xudong_zhang6), [Zhiyang Zhou](http://openreview.net/profile?id=~Zhiyang_Zhou3), [Yan Cai](http://openreview.net/profile?id=~Yan_Cai3)
  - **Affiliations:** Key Laboratory of System Software (Chinese Academy of Sciences); State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; University of Chinese Academy of Sciences, Beijing, China, Key Laboratory of System Software (Chinese Academy of Sciences); State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; University of Chinese Academy of Sciences, Beijing, China, Key Laboratory of System Software (Chinese Academy of Sciences); State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; University of Chinese Academy of Sciences, Beijing, China, Key Laboratory of System Software (Chinese Academy of Sciences); State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; University of Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This paper introduces MA GG, a method that enhances machine learning models for combinatorial problems by utilizing metamorphic relations at inference time. The proposed method shows significant improvements in prediction accuracy across various combinatorial tasks, addressing the limitations of standard Test-Time Augmentation.
  - **Keywords:** machine learning, combinatorial problems, Test-Time Augmentation (TTA), Metamorphic Aggregation (MA GG), Graph Neural Network (GNN), Boolean Satisfiability Prediction (SAT), Decision Traveling Salesman Problem (Decision TSP), Graph Edit Distance Estimation (GED), NP-hardness, discrete nature of combinatorial problems, challenges in finding label-preserving transformations, significant improvements in prediction accuracy and robustness, extension of TTA with metamorphic relations, metamorphic relations, directed graphs


- [Rethinking Generative Large Language Model Evaluation for Semantic Comprehension](https://icml.cc/virtual/2024/poster/35076) (Poster)
  - **Authors:** [Fangyun Wei](http://openreview.net/profile?id=~Fangyun_Wei1), [Xi Chen](http://openreview.net/profile?id=~Xi_Chen55), [Lin Luo](http://openreview.net/profile?id=~Lin_Luo7)
  - **Affiliations:** Microsoft Research Asia, Microsoft Research Asia, Microsoft Research Asia
  - **TL;DR:** This paper critiques the traditional multiple choice question answering (MCQA) evaluation method for large language models (LLMs) and introduces a new RWQ-Elo rating system designed to better reflect real-world usage. The study demonstrates the potential of this new system to improve LLM evaluation and reshape existing leaderboards.
  - **Keywords:** Large Language Models, Evaluation Methods, RWQ-Elo rating system, Multiple Choice Question Answering (MCQA), Natural Language Processing, Inconsistency in evaluation methods, Challenges in assessing open-ended responses, New benchmark (Real-world questions), Stability of RWQ-Elo system, Real-world questions (RWQ)


- [Stability-Informed Initialization of Neural Ordinary Differential Equations](https://icml.cc/virtual/2024/poster/32811) (Poster)
  - **Authors:** [Theodor Westny](http://openreview.net/profile?id=~Theodor_Westny1), [Arman Mohammadi](http://openreview.net/profile?id=~Arman_Mohammadi1), [Daniel Jung](http://openreview.net/profile?id=~Daniel_Jung2), [Erik Frisk](http://openreview.net/profile?id=~Erik_Frisk1)
  - **Affiliations:** Department of Electrical Engineering, Linköping University, Linköping, Sweden, Department of Electrical Engineering, Linköping University, Linköping, Sweden, Department of Electrical Engineering, Linköping University, Linköping, Sweden, Department of Electrical Engineering, Linköping University, Linköping, Sweden
  - **TL;DR:** This paper investigates the training of Neural Ordinary Differential Equations (neural ODEs) and introduces a stability-informed parameter initialization technique that enhances training efficiency and prediction accuracy. The study highlights the impact of numerical integration methods and stability regions on the performance of neural ODEs.
  - **Keywords:** Neural Ordinary Differential Equations, training dynamic systems, numerical integration techniques, stability-informed parameter initialization, time-series analysis, sequential data modeling, limitations of learnable parameter values, training efficiency, stability-informed initialization technique, enhancements in prediction accuracy


- [QuRating: Selecting High-Quality Data for Training Language Models](https://icml.cc/virtual/2024/poster/34502) (Spotlight Poster)
  - **Authors:** [Alexander Wettig](http://openreview.net/profile?id=~Alexander_Wettig1), [Aatmik Gupta](http://openreview.net/profile?id=~Aatmik_Gupta1), [Saumya Malik](http://openreview.net/profile?id=~Saumya_Malik1), [Danqi Chen](http://openreview.net/profile?id=~Danqi_Chen1)
  - **Affiliations:** Department of Computer Science & Princeton Language and Intelligence (PLI), Princeton University, None, None, None
  - **TL;DR:** This study introduces QuRating, a method for selecting high-quality pre-training data for language models by evaluating four qualities: writing style, required expertise, facts & trivia, and educational value. The findings demonstrate that models trained on quality-rated data achieve lower perplexity and improved performance, highlighting the importance of data quality in training effective language models.
  - **Keywords:** data quality, pre-training data selection, language models, QuRating, QuRater model, pairwise judgments, balancing quality and diversity, data selection challenges, lower perplexity, stronger in-context learning performance, training curriculum, 260B training corpus, 30B tokens, 1.3B-parameter language models, LLMs (Large Language Models), educational value, writing style, facts & trivia, required expertise


- [Contrastive Representation for Data Filtering in Cross-Domain Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/32951) (Poster)
  - **Authors:** [Xiaoyu Wen](http://openreview.net/profile?id=~Xiaoyu_Wen2), [Chenjia Bai](http://openreview.net/profile?id=~Chenjia_Bai2), [Kang Xu](http://openreview.net/profile?id=~Kang_Xu2), [Xudong Yu](http://openreview.net/profile?id=~Xudong_Yu2), [Yang Zhang](http://openreview.net/profile?id=~Yang_Zhang49), [Xuelong Li](http://openreview.net/profile?id=~Xuelong_Li2), [Zhen Wang](http://openreview.net/profile?id=~Zhen_Wang11)
  - **Affiliations:** Northwestern Polytechnical University, Shanghai Artificial Intelligence Laboratory; Shenzhen Research Institute of Northwestern Polytechnical University, Fudan University, Harbin Institute of Technology, Tsinghua University, The Institute of Artificial Intelligence (TeleAI), China Telecom, Northwestern Polytechnical University
  - **TL;DR:** This paper presents a novel approach for cross-domain offline reinforcement learning that utilizes contrastive representation learning to measure domain gaps and improve data efficiency. The proposed method demonstrates superior performance by effectively filtering source domain data, achieving significant results with only a fraction of the target data.
  - **Keywords:** Cross-domain offline reinforcement learning, data filtering, Contrastive representation learning, mutual information, Robotic manipulation, autonomous driving, healthcare, Dynamics mismatch, data sparsity, performance degradation, Data filtering algorithm, improved data efficiency


- [Diffusion-based Missing-view Generation With the Application on Incomplete Multi-view Clustering](https://icml.cc/virtual/2024/poster/34172) (Poster)
  - **Authors:** [Jie Wen](http://openreview.net/profile?id=~Jie_Wen1), [Shijie Deng](http://openreview.net/profile?id=~Shijie_Deng1), [Waikeung Wong](http://openreview.net/profile?id=~Waikeung_Wong1), [Guoqing Chao](http://openreview.net/profile?id=~Guoqing_Chao1), [Chao Huang](http://openreview.net/profile?id=~Chao_Huang16), [Lunke Fei](http://openreview.net/profile?id=~Lunke_Fei1), [Yong Xu](http://openreview.net/profile?id=~Yong_Xu9)
  - **Affiliations:** Shenzhen Key Laboratory of Visual Object Detection and Recognition, Harbin Institute of Technology, Shenzhen, China, Shenzhen Key Laboratory of Visual Object Detection and Recognition, Harbin Institute of Technology, Shenzhen, China, School of Fashion and Textiles, The Hong Kong Polytechnic University, Hong Kong; Laboratory for Artificial Intelligence in Design, Hong Kong, School of Computer Science and Technology, Harbin Institute of Technology, Weihai, China, School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China, School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China, Shenzhen Key Laboratory of Visual Object Detection and Recognition, Harbin Institute of Technology, Shenzhen, China
  - **TL;DR:** This study introduces a diffusion-based missing view generation (DMVG) network to address the challenge of missing views in multi-view clustering. The proposed method not only predicts missing views accurately but also enhances clustering performance compared to existing incomplete multi-view clustering methods.
  - **Keywords:** Multi-view clustering, Incomplete multi-view learning, Diffusion-based missing view generation (DMVG), Data augmentation strategy, Face recognition, Social media analysis, Acoustic event classification, Missing views in multi-view data, Clustering performance decline with missing views, Enhanced recovery quality for missing views, Improved clustering performance


- [Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains](https://icml.cc/virtual/2024/poster/33400) (Poster)
  - **Authors:** [Steven Wilkins-Reeves](http://openreview.net/profile?id=~Steven_Wilkins-Reeves1), [Xu Chen](http://openreview.net/profile?id=~Xu_Chen23), [Qi Ma](http://openreview.net/profile?id=~Qi_Ma8), [christine agarwal](http://openreview.net/profile?id=~christine_agarwal1), [Aude Hofleitner](http://openreview.net/profile?id=~Aude_Hofleitner1)
  - **Affiliations:** Department of Statistics, University of Washington, Seattle, USA, Central Applied Science, Meta, Menlo Park, USA, Central Applied Science, Meta, Menlo Park, USA, Central Applied Science, Meta, Menlo Park, USA, Central Applied Science, Meta, Menlo Park, USA
  - **TL;DR:** This study proposes a two-stage multiply robust estimation method to enhance model performance across multiple segments affected by local distribution shifts. The method significantly improves prediction accuracy and robustness in both regression and classification tasks, addressing challenges related to data distribution variability and sampling bias.
  - **Keywords:** distribution shifts, machine learning, model generalization, multiply robust estimation, linear combination of models, tabular data analysis, user city prediction, data distribution variability, sampling bias, model performance across segments, improved prediction accuracy, robustness in regression and classification tasks, synthetic datasets, real datasets, user city prediction dataset


- [Exploring Intrinsic Dimension for Vision-Language Model Pruning](https://icml.cc/virtual/2024/poster/32685) (Poster)
  - **Authors:** [Hanzhang Wang](http://openreview.net/profile?id=~Hanzhang_Wang1), [Jiawen Zhang](http://openreview.net/profile?id=~Jiawen_Zhang4), [Qingyuan Ma](http://openreview.net/profile?id=~Qingyuan_Ma1)
  - **Affiliations:** School of Computer Engineering and Science, Shanghai University, School of Computer Engineering and Science, Shanghai University, School of Computer Engineering and Science, Shanghai University
  - **TL;DR:** This study explores the intrinsic dimension as a metric for pruning vision-language models, revealing that visual representations are more sensitive to performance while language representations are more robust and prunable. The findings suggest an asymmetric pruning strategy guided by the intrinsic dimension metric.
  - **Keywords:** intrinsic dimension, model pruning, vision-language models, vision-language models, high-dimensional data, model overfitting, layer importance metric, asymmetric pruning strategy


- [Defense against Model Extraction Attack by Bayesian Active Watermarking](https://icml.cc/virtual/2024/poster/34592) (Poster)
  - **Authors:** [Zhenyi Wang](http://openreview.net/profile?id=~Zhenyi_Wang1), [Yihan Wu](http://openreview.net/profile?id=~Yihan_Wu1), [Heng Huang](http://openreview.net/profile?id=~Heng_Huang1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Maryland, College Park, USA
  - **TL;DR:** This study introduces a Bayesian active watermarking technique to enhance the defense against model extraction attacks by fine-tuning the victim model, thereby preserving its utility while complicating the extraction process. The proposed method demonstrates high efficiency and effectiveness without the need for full re-training of the model.
  - **Keywords:** model extraction, intellectual property protection, deep learning, Bayesian active watermarking, fine-tuning, watermark posterior distribution, model theft, computational inefficiencies, memory inefficiencies, passive defense methods, proactive defense approach, efficient model utility preservation


- [Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets Cannot](https://icml.cc/virtual/2024/poster/32981) (Poster)
  - **Authors:** [Zixuan Wang](http://openreview.net/profile?id=~Zixuan_Wang4), [Stanley Wei](http://openreview.net/profile?id=~Stanley_Wei1), [Daniel Hsu](http://openreview.net/profile?id=~Daniel_Hsu1), [Jason Lee](http://openreview.net/profile?id=~Jason_D._Lee1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Princeton University, NJ, USA, Department of Electrical and Computer Engineering, Princeton University, NJ, USA, Department of Computer Science, Columbia University, NY, USA, Department of Electrical and Computer Engineering, Princeton University, NJ, USA
  - **TL;DR:** This study demonstrates that transformers can effectively learn the sparse token selection task through gradient descent, showcasing their superior capabilities compared to fully-connected networks. The findings highlight the transformers' strong out-of-distribution length generalization and provide theoretical evidence for their learnability in this context.
  - **Keywords:** deep learning, transformer architecture, sparse token selection, gradient descent, self-attention, language modeling, computer vision, reinforcement learning, learnability, expressivity separation, approximation capability, global convergence, length generalization capacity, FCNs (Fully-Connected Networks), q-sparse averaging


- [Neural Collapse meets Differential Privacy: Curious behaviors of NoisyGD with Near-Perfect Representation Learning](https://icml.cc/virtual/2024/poster/34868) (Oral)
  - **Authors:** [Chendi Wang](http://openreview.net/profile?id=~Chendi_Wang2), [Yuqing Zhu](http://openreview.net/profile?id=~Yuqing_Zhu1), [Weijie Su](http://openreview.net/profile?id=~Weijie_J_Su1), [Yu-Xiang Wang](http://openreview.net/profile?id=~Yu-Xiang_Wang1)
  - **Affiliations:** University of Pennsylvania, Philadelphia, PA, USA, University of California, Santa Barbara, Santa Barbara, CA, USA, University of Pennsylvania, Philadelphia, PA, USA, University of California, San Diego, La Jolla, CA, USA
  - **TL;DR:** This study investigates the intersection of Neural Collapse and Differential Privacy in the context of fine-tuning pre-trained models, revealing that powerful pre-trained models enhance feature representation and that DP fine-tuning is less robust than non-DP fine-tuning. The authors propose strategies like PCA to improve the robustness of DP fine-tuning, leading to significant improvements in testing accuracy.
  - **Keywords:** Differential Privacy, Representation Learning, Neural Collapse, Noisy Stochastic Gradient Descent (NoisySGD), PCA (Principal Component Analysis), Private Deep Learning, Image Classification, Dimension Dependence, Misclassification Error, Robustness of Fine-tuning, Enhanced Feature Representation, Improved Testing Accuracy, ImageNet, CIFAR-10


- [Provable Contrastive Continual Learning](https://icml.cc/virtual/2024/poster/33926) (Poster)
  - **Authors:** [Yichen Wen](http://openreview.net/profile?id=~Yichen_Wen1), [Zhiquan Tan](http://openreview.net/profile?id=~Zhiquan_Tan1), [Kaipeng Zheng](http://openreview.net/profile?id=~Kaipeng_Zheng1), [Chuanlong Xie](http://openreview.net/profile?id=~Chuanlong_Xie1), [Weiran Huang](http://openreview.net/profile?id=~Weiran_Huang1)
  - **Affiliations:** MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University; Beijing Normal University, Department of Mathematical Sciences, Tsinghua University, MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Beijing Normal University, MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University; Shanghai AI Laboratory
  - **TL;DR:** This study addresses the challenges of continual learning by establishing theoretical performance guarantees for a contrastive continual learning framework. It introduces a novel algorithm called CILA, which improves performance by using adaptive distillation coefficients, achieving state-of-the-art results on standard benchmarks.
  - **Keywords:** Continual learning, Contrastive learning, Contrastive loss, Distillation loss, Adaptive distillation coefficients, Catastrophic forgetting, Learning plasticity vs. memory stability, CILA algorithm, Theoretical performance guarantees


- [Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning](https://icml.cc/virtual/2024/poster/33219) (Poster)
  - **Authors:** [Yuxiao Wen](http://openreview.net/profile?id=~Yuxiao_Wen1), [Arthur Jacot](http://openreview.net/profile?id=~Arthur_Jacot1)
  - **Affiliations:** Courant Institute of Mathematical Sciences, New York University, New York, NY 10012, USA, Courant Institute of Mathematical Sciences, New York University, New York, NY 10012, USA
  - **TL;DR:** This paper investigates the emergence of a Convolution Bottleneck (CBN) structure in CNNs, demonstrating how initial layers transform input representations into a low-dimensional space before mapping back to outputs. The findings suggest that the parameter norm required for function representation scales with depth and CBN rank, providing insights into the efficiency of CNN architectures in feature learning.
  - **Keywords:** Convolutional Neural Networks (CNNs), Feature Learning, Convolution Bottleneck (CBN) structure, Neural Network Gaussian Process (NNGP) kernel, Neural Tangent Kernel (NTK), Image Processing, Natural Images, Dimensionality Reduction, Parameter Norm Optimization, CBN rank, Stability under large learning rates


- [Adaptive Accompaniment with ReaLchords](https://icml.cc/virtual/2024/poster/33172) (Poster)
  - **Authors:** [Yusong Wu](http://openreview.net/profile?id=~Yusong_Wu1), [Tim Cooijmans](http://openreview.net/profile?id=~Tim_Cooijmans1), [Kyle Kastner](http://openreview.net/profile?id=~Kyle_Kastner1), [Adam Roberts](http://openreview.net/profile?id=~Adam_Roberts1), [Ian Simon](http://openreview.net/profile?id=~Ian_Simon1), [Alexander Scarlatos](http://openreview.net/profile?id=~Alexander_Scarlatos1), [Chris Donahue](http://openreview.net/profile?id=~Chris_Donahue1), [Cassie Tarakajian](http://openreview.net/profile?id=~Cassie_Tarakajian1), [Shayegan Omidshafiei](http://openreview.net/profile?id=~Shayegan_Omidshafiei1), [Aaron Courville](http://openreview.net/profile?id=~Aaron_Courville3), [Pablo Samuel Castro](http://openreview.net/profile?id=~Pablo_Samuel_Castro1), [Natasha Jaques](http://openreview.net/profile?id=~Natasha_Jaques1), [Cheng-Zhi Anna Huang](http://openreview.net/profile?id=~Cheng-Zhi_Anna_Huang1)
  - **Affiliations:** Google DeepMind; Mila - Quebec AI Institute, Université de Montréal, Mila - Quebec AI Institute, Université de Montréal, Google, Google, Google, University of Massachusetts Amherst, Carnegie Mellon University, Google, Work done while at Google; Field AI, Mila - Quebec AI Institute, Université de Montréal; Canada CIFAR AI Chair, Google; Mila - Quebec AI Institute, Université de Montréal, University of Washington, Mila - Quebec AI Institute, Université de Montréal
  - **TL;DR:** This paper presents ReaLchords, an online generative model designed for improvising chord accompaniment to user melodies in real-time. The model effectively adapts to unfamiliar inputs and maintains musical coherence through reinforcement learning and knowledge distillation techniques, enabling spontaneous live music jamming.
  - **Keywords:** online generative models, adaptive musical accompaniment, live jamming, reinforcement learning, maximum likelihood, knowledge distillation, music improvisation, interactive music generation, exposure bias, adaptation to mistakes, handling unfamiliar situations, ReaLchords model, harmonic and temporal coherency evaluation


- [A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models](https://icml.cc/virtual/2024/poster/33605) (Poster)
  - **Authors:** [Yihan Wu](http://openreview.net/profile?id=~Yihan_Wu1), [Zhengmian Hu](http://openreview.net/profile?id=~Zhengmian_Hu1), [Junfeng Guo](http://openreview.net/profile?id=~Junfeng_Guo2), [Hongyang Zhang](http://openreview.net/profile?id=~Hongyang_Zhang1), [Heng Huang](http://openreview.net/profile?id=~Heng_Huang1)
  - **Affiliations:** Department of Computer Science, University of Maryland College Park, Department of Computer Science, University of Maryland College Park, Department of Computer Science, University of Maryland College Park, School of Computer Science, University of Waterloo, Department of Computer Science, University of Maryland College Park; School of Computer Science, University of Waterloo
  - **TL;DR:** This study presents DiPmark, a watermarking technique for large language models that preserves the original token distribution while being accessible and resilient to moderate changes. The empirical evaluation demonstrates its effectiveness in identifying AI-generated content without compromising quality.
  - **Keywords:** Watermarking, Large Language Models, Distribution-Preserving watermark, token distribution reweight function, AI-generated content identification, Distinguishing machine-generated content from human-authored content, preserving original token distribution, DiPmark method, resilience to moderate token changes


- [PointMC: Multi-instance Point Cloud Registration based on Maximal Cliques](https://icml.cc/virtual/2024/poster/35208) (Poster)
  - **Authors:** [Yue Wu](http://openreview.net/profile?id=~Yue_Wu14), [Xidao hu](http://openreview.net/profile?id=~Xidao_hu1), [Yongzhe Yuan](http://openreview.net/profile?id=~Yongzhe_Yuan2), [Xiaolong Fan](http://openreview.net/profile?id=~Xiaolong_Fan1), [Maoguo Gong](http://openreview.net/profile?id=~Maoguo_Gong2), [Hao Li](http://openreview.net/profile?id=~Hao_Li12), [Mingyang Zhang](http://openreview.net/profile?id=~Mingyang_Zhang4), [Qiguang Miao](http://openreview.net/profile?id=~Qiguang_Miao1), [Wenping Ma](http://openreview.net/profile?id=~Wenping_Ma3)
  - **Affiliations:** MoE Key Lab of Collaborative Intelligence Systems, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China, MoE Key Lab of Collaborative Intelligence Systems, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China, MoE Key Lab of Collaborative Intelligence Systems, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China, MoE Key Lab of Collaborative Intelligence Systems, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China, MoE Key Lab of Collaborative Intelligence Systems, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China, MoE Key Lab of Collaborative Intelligence Systems, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China, MoE Key Lab of Collaborative Intelligence Systems, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China, MoE Key Lab of Collaborative Intelligence Systems, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China, School of Artificial Intelligence, Xidian University, Xi’an, China
  - **TL;DR:** The study presents PointMC, a framework for multi-instance point cloud registration that efficiently estimates multiple rigid transformations by leveraging maximal cliques on a correspondence compatibility graph. The proposed method demonstrates significant performance improvements in scenarios with overlapping instances.
  - **Keywords:** Multi-instance point cloud registration, Rigid transformations, Maximal cliques, Correspondence compatibility graph, Correspondence embedding module, Autonomous driving, 3D reconstruction, Estimating multiple transformations, Handling overlapping instances, Outlier elimination, PointMC framework, Performance improvements, Synthetic datasets, Real-world datasets


- [FAFE: Immune Complex Modeling with Geodesic Distance Loss on Noisy Group Frames](https://icml.cc/virtual/2024/poster/34273) (Spotlight Poster)
  - **Authors:** [Ruidong Wu](http://openreview.net/profile?id=~Ruidong_Wu1), [Ruihan Guo](http://openreview.net/profile?id=~Ruihan_Guo2), [Rui Wang](http://openreview.net/profile?id=~Rui_Wang1), [Shitong Luo](http://openreview.net/profile?id=~Shitong_Luo1), [Xu Yue](http://openreview.net/profile?id=~Xu_Yue1), [Jiahan Li](http://openreview.net/profile?id=~Jiahan_Li2), [Jianzhu Ma](http://openreview.net/profile?id=~Jianzhu_Ma2), [qiang liu](http://openreview.net/profile?id=~qiang_liu4), [Yunan Luo](http://openreview.net/profile?id=~Yunan_Luo1), [Jian Peng](http://openreview.net/profile?id=~Jian_Peng1)
  - **Affiliations:** Helixon, Helixon, Helixon, Helixon, Helixon, Helixon, Tsinghua; Helixon, UTAustin; Helixon, GaTech, Helixon
  - **TL;DR:** This study addresses the challenges in modeling antibody-antigen complexes by proposing a new geodesic loss function, FAFE, which overcomes limitations of existing methods like AlphaFold2. The results show significant improvements in prediction accuracy, particularly for low-homology complexes.
  - **Keywords:** Protein structure modeling, antibody-antigen complexes, AlphaFold2, geodesic loss, Frame Aligned Frame Error (FAFE), Computational biology, antibody drug discovery, Gradient vanishing problem, high-rotational-error targets, Improved modeling accuracy, new loss function (F2E)


- [Unified Training of Universal Time Series Forecasting Transformers](https://icml.cc/virtual/2024/poster/33767) (Oral)
  - **Authors:** [Gerald Woo](http://openreview.net/profile?id=~Gerald_Woo1), [Chenghao Liu](http://openreview.net/profile?id=~Chenghao_Liu1), [Akshat Kumar](http://openreview.net/profile?id=~Akshat_Kumar2), [Caiming Xiong](http://openreview.net/profile?id=~Caiming_Xiong1), [Silvio Savarese](http://openreview.net/profile?id=~Silvio_Savarese1), [Doyen Sahoo](http://openreview.net/profile?id=~Doyen_Sahoo1)
  - **Affiliations:** Salesforce AI Research; School of Computing and Information Systems, Singapore Management University, Salesforce AI Research; School of Computing and Information Systems, Singapore Management University, None, None, None, None
  - **TL;DR:** This paper presents MOIRAI, a novel Transformer-based model for universal time series forecasting, trained on a large-scale dataset (LOTSA) to address challenges like cross-frequency learning and multivariate time series. The model demonstrates competitive performance as a zero-shot forecaster compared to traditional full-shot models.
  - **Keywords:** universal forecasting, time series forecasting, deep learning, Transformer architecture, Masked Encoder, time series data analysis, cross-frequency learning, multivariate time series, varying distributional properties, MOIRAI (Masked Encoder-based Universal Time Series Forecasting Transformer), Large-scale Open Time Series Archive (LOTSA), LOTSA


- [Understanding Stochastic Natural Gradient Variational Inference](https://icml.cc/virtual/2024/poster/33222) (Poster)
  - **Authors:** [Kaiwen Wu](http://openreview.net/profile?id=~Kaiwen_Wu2), [Jacob Gardner](http://openreview.net/profile?id=~Jacob_R._Gardner1)
  - **Affiliations:** Department of Computer and Information Science, University of Pennsylvania, Philadelphia, United States, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, United States
  - **TL;DR:** This study establishes the first O(1/T) non-asymptotic convergence rate for stochastic natural gradient variational inference (NGVI) under conjugate likelihoods, demonstrating that its complexity is comparable to stochastic gradient descent while potentially offering faster convergence in practice. The findings highlight the challenges of optimizing the evidence lower bound (ELBO) in non-conjugate settings, suggesting further research is needed to understand these complexities.
  - **Keywords:** Stochastic Natural Gradient Variational Inference, Posterior Inference, Natural Gradient Descent, Evidence Lower Bound (ELBO), Probabilistic Models, Bayesian Neural Networks, Latent Dirichlet Allocation, Non-asymptotic Convergence Rate, Optimization Challenges, O(1/T) Convergence Rate, Complexity Analysis, KL Divergence, Conjugate Likelihoods


- [Learning Causal Relations from Subsampled Time Series with Two Time-Slices](https://icml.cc/virtual/2024/poster/34953) (Spotlight Poster)
  - **Authors:** [Anpeng Wu](http://openreview.net/profile?id=~Anpeng_Wu1), [Haoxuan Li](http://openreview.net/profile?id=~Haoxuan_Li6), [Kun Kuang](http://openreview.net/profile?id=~Kun_Kuang1), [zhang keli](http://openreview.net/profile?id=~Zhang_Keli1), [Fei Wu](http://openreview.net/profile?id=~Fei_Wu1)
  - **Affiliations:** Department of Computer Science and Technology, Zhejiang University, Hangzhou, China; Shanghai Institute for Advanced Study, Zhejiang University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, Center for Data Science, Peking University, Beijing, China, Department of Computer Science and Technology, Zhejiang University, Hangzhou, China; Shanghai Institute for Advanced Study, Zhejiang University, Shanghai, China, Huawei Noah’s Ark Lab, Huawei, Shenzhen, China, Department of Computer Science and Technology, Zhejiang University, Hangzhou, China; Shanghai Institute for Advanced Study, Zhejiang University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China
  - **TL;DR:** This study proposes a novel algorithm (DHT-CIT) to learn causal relations from subsampled time series data using only two time-slices, addressing the challenges of sparse measurements and coarser sampling. Empirical results demonstrate the algorithm's effectiveness in improving causal ordering compared to conventional methods.
  - **Keywords:** Causal relations, Time series analysis, Descendant Hierarchical Topology algorithm, Conditional Independence Test (DHT-CIT), Health science, Social science, Data sparsity, Coarser timescale sampling, Improved causal ordering, Identification of descendant nodes, Synthetic datasets, Real-world datasets


- [Confidence-aware Contrastive Learning for Selective Classification](https://icml.cc/virtual/2024/poster/34017) (Poster)
  - **Authors:** [Yu-Chang Wu](http://openreview.net/profile?id=~Yu-Chang_Wu1), [Shen-Huan Lyu](http://openreview.net/profile?id=~Shen-Huan_Lyu1), [Haopu Shang](http://openreview.net/profile?id=~Haopu_Shang1), [Xiangyu Wang](http://openreview.net/profile?id=~Xiangyu_Wang6), [Chao Qian](http://openreview.net/profile?id=~Chao_Qian1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, Key Laboratory of Water Big Data Technology of Ministry of Water Resources, Hohai University, China; College of Computer Science and Software Engineering, Hohai University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This study introduces a novel Confidence-aware Contrastive Learning method for selective classification that enhances model performance by optimizing feature layers, achieving lower selective risk across various datasets. The findings suggest that improving feature-level representation is crucial for reliable predictions in high-stakes scenarios.
  - **Keywords:** Selective classification, Deep neural networks, Confidence estimation, Confidence-aware Contrastive Learning, Softmax Layer, MC-dropout, Deep ensemble, Snapshot ensemble, Medical diagnosis, Self-driving, Security systems, Selective risk, Prediction confidence, Misclassification, Generalization bound for selective classification, Optimization of feature layers, CIFAR-10, CIFAR-100, CelebA, ImageNet


- [Transolver: A Fast Transformer Solver for PDEs on General Geometries](https://icml.cc/virtual/2024/poster/33751) (Spotlight Poster)
  - **Authors:** [Haixu Wu](http://openreview.net/profile?id=~Haixu_Wu1), [Huakun Luo](http://openreview.net/profile?id=~Huakun_Luo1), [Haowen Wang](http://openreview.net/profile?id=~Haowen_Wang4), [Jianmin Wang](http://openreview.net/profile?id=~Jianmin_Wang1), [Mingsheng Long](http://openreview.net/profile?id=~Mingsheng_Long5)
  - **Affiliations:** School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University
  - **TL;DR:** The study introduces Transolver, a novel Transformer-based method for solving partial differential equations (PDEs) that effectively captures intricate physical correlations in complex geometries. It demonstrates a 22% relative performance improvement across standard benchmarks and excels in large-scale industrial simulations.
  - **Keywords:** Transformers, Partial Differential Equations (PDEs), Physics-Attention, Linear Attention, Physics-Attention, Industrial Design, Weather Forecasting, Material Analysis, Computational Efficiency, Relation Learning, Complex Geometries, High-order Interactions, Transolver, Geometry-general Modeling Capacity, State-of-the-art Performance


- [Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models](https://icml.cc/virtual/2024/poster/32692) (Poster)
  - **Authors:** [Mingrui Wu](http://openreview.net/profile?id=~Mingrui_Wu2), [Jiayi Ji](http://openreview.net/profile?id=~Jiayi_Ji1), [Oucheng Huang](http://openreview.net/profile?id=~Oucheng_Huang1), [Jiale Li](http://openreview.net/profile?id=~Jiale_Li5), [Yuhang Wu](http://openreview.net/profile?id=~Yuhang_Wu4), [Xiaoshuai Sun](http://openreview.net/profile?id=~Xiaoshuai_Sun3), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China
  - **TL;DR:** This study introduces R-Bench, a benchmark for evaluating relationship hallucinations in Large Vision-Language Models (LVLMs), highlighting the inadequacy of current models in understanding inter-object relationships. The findings indicate that LVLMs often rely on common sense knowledge rather than visual content, leading to significant hallucination issues.
  - **Keywords:** Relationship hallucinations, Large Vision-Language Models (LVLMs), Visual instruction tuning, Visual language pretraining, Visual comprehension, Image analysis, Hallucinations in inter-object relationships, Inconsistency with visual content, R-Bench benchmark for evaluating relationship hallucinations, nocaps validation set, COCO captions


- [A Theory of Fault-Tolerant Learning](https://icml.cc/virtual/2024/poster/33070) (Spotlight Poster)
  - **Authors:** [Changlong Wu](http://openreview.net/profile?id=~Changlong_Wu1), [Yifan Wang](http://openreview.net/profile?id=~Yifan_Wang14), [Ananth Grama](http://openreview.net/profile?id=~Ananth_Grama1)
  - **Affiliations:** Center for Science of Information, Department of Computer Science, Purdue University, West Lafayette, USA, Center for Science of Information, Department of Computer Science, Purdue University, West Lafayette, USA, Center for Science of Information, Department of Computer Science, Purdue University, West Lafayette, USA
  - **TL;DR:** This paper presents a theoretical framework for fault-tolerant PAC learning, addressing the challenges of developing machine learning models resilient to faults in mission-critical applications. It reveals that while random faults do not increase sample complexity, adversarial faults can significantly impact it, particularly in neural networks.
  - **Keywords:** Fault-tolerant learning, machine learning reliability, PAC learning, fault-tolerant PAC learning, Autonomous vehicles, spacecraft, industrial control systems, Fault tolerance, adversarial faults, hardware faults, Theoretical framework for fault tolerance, sample complexity analysis, VC-dimension, adversarial faulty-loss, random fault-loss


- [NExT-GPT: Any-to-Any Multimodal LLM](https://icml.cc/virtual/2024/poster/34200) (Oral)
  - **Authors:** [Shengqiong Wu](http://openreview.net/profile?id=~Shengqiong_Wu2), [Hao Fei](http://openreview.net/profile?id=~Hao_Fei1), [Leigang Qu](http://openreview.net/profile?id=~Leigang_Qu1), [Wei Ji](http://openreview.net/profile?id=~Wei_Ji1), [Tat-Seng Chua](http://openreview.net/profile?id=~Tat-Seng_Chua2)
  - **Affiliations:** NExT++ Research Center, National University of Singapore, Singapore, NExT++ Research Center, National University of Singapore, Singapore, NExT++ Research Center, National University of Singapore, Singapore, NExT++ Research Center, National University of Singapore, Singapore, NExT++ Research Center, National University of Singapore, Singapore
  - **TL;DR:** The study introduces NExT-GPT, an end-to-end any-to-any multimodal large language model that can process and generate content across various modalities, addressing the limitations of existing models that primarily focus on input-side multimodal understanding. The research highlights the potential for creating a unified AI agent capable of complex cross-modal semantic understanding and content generation.
  - **Keywords:** Multimodal Large Language Models, Artificial Intelligence Generated Content, Limitations of input-side multimodal understanding, need for any-to-any modality conversion, NExT-GPT, modality-switching instruction tuning (MosIT), LLM (Large Language Model), MM-LLM (Multimodal Large Language Model), diffusion decoders, multimodal adaptors


- [VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model](https://icml.cc/virtual/2024/poster/33908) (Poster)
  - **Authors:** [Pengying Wu](http://openreview.net/profile?id=~Pengying_Wu1), [Yao Mu](http://openreview.net/profile?id=~Yao_Mu1), [Bingxian Wu](http://openreview.net/profile?id=~Bingxian_Wu1), [Yi Hou](http://openreview.net/profile?id=~Yi_Hou1), [Ji Ma](http://openreview.net/profile?id=~Ji_Ma5), [Shanghang Zhang](http://openreview.net/profile?id=~Shanghang_Zhang4), [Chang Liu](http://openreview.net/profile?id=~Chang_Liu30)
  - **Affiliations:** Department of Advanced Manufacturing and Robotics, College of Engineering, Peking University, Beijing, China, The University of Hong Kong; OpenGVLab, Shanghai AI Laboratory, Department of Advanced Manufacturing and Robotics, College of Engineering, Peking University, Beijing, China, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing, China, Department of Advanced Manufacturing and Robotics, College of Engineering, Peking University, Beijing, China, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing, China, Department of Advanced Manufacturing and Robotics, College of Engineering, Peking University, Beijing, China
  - **TL;DR:** This study presents VoroNav, a novel framework for Zero-Shot Object Navigation that utilizes a Reduced Voronoi Graph and large language models to enhance navigation efficiency and success rates in household robotics. The method demonstrates significant improvements in exploration and obstacle avoidance capabilities compared to existing benchmarks.
  - **Keywords:** Zero-Shot Object Navigation (ZSON), household robotics, Voronoi Graph, large language model (LLM), semantic exploration framework, Navigation in household environments, Navigating unfamiliar environments, locating objects from novel categories, Enhanced success rate and exploration efficiency, obstacle avoidance proficiency, perceptual efficiency, HM3D, HSSD


- [Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value](https://icml.cc/virtual/2024/poster/33719) (Poster)
  - **Authors:** [Young Wu](http://openreview.net/profile?id=~Young_Wu1), [Jeremy McMahan](http://openreview.net/profile?id=~Jeremy_McMahan1), [Yiding Chen](http://openreview.net/profile?id=~Yiding_Chen1), [Yudong Chen](http://openreview.net/profile?id=~Yudong_Chen1), [Jerry Zhu](http://openreview.net/profile?id=~Jerry_Zhu1), [Qiaomin Xie](http://openreview.net/profile?id=~Qiaomin_Xie1)
  - **Affiliations:** Department of Computer Sciences, University of Wisconsin–Madison, Madison, Wisconsin, United States, Department of Computer Sciences, University of Wisconsin–Madison, Madison, Wisconsin, United States, Department of Computer Science, Cornell University, Ithaca, New York, United States, Department of Computer Sciences, University of Wisconsin–Madison, Madison, Wisconsin, United States, Department of Computer Sciences, University of Wisconsin–Madison, Madison, Wisconsin, United States, Department of Industrial and Systems Engineering, University of Wisconsin–Madison, Madison, Wisconsin, United States
  - **TL;DR:** This study investigates how to modify the reward function of zero-sum Markov games to achieve a desired Nash equilibrium and game value while minimizing modification costs. The authors establish conditions for successful modifications and propose an efficient algorithm to implement these changes.
  - **Keywords:** Game modification, Nash equilibrium, Markov game, Convex optimization, linear constraints, Game design, fairness in games, Modification cost, unique equilibrium installation, Efficient algorithm for game modification, Markov Perfect Equilibrium (MPE), zero-sum game


- [How to Explore with Belief: State Entropy Maximization in POMDPs](https://icml.cc/virtual/2024/poster/34283) (Poster)
  - **Authors:** [Riccardo Zamboni](http://openreview.net/profile?id=~Riccardo_Zamboni1), [Duilio Cirino](http://openreview.net/profile?id=~Duilio_Cirino1), [Marcello Restelli](http://openreview.net/profile?id=~Marcello_Restelli1), [Mirco Mutti](http://openreview.net/profile?id=~Mirco_Mutti1)
  - **Affiliations:** Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Politecnico di Milano, Milan, Italy, Technion – Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This paper generalizes the state entropy maximization framework to scenarios with partial observations, developing a computationally efficient policy gradient method to maximize the entropy of true states. The findings address challenges in real-world applications, particularly in autonomous robotics for rescue operations.
  - **Keywords:** state entropy maximization, reinforcement learning, partial observability, policy gradient method, first-order relaxation, belief states, autonomous robotics, rescue operations, entropy maximization over true states, decision policy conditioned on partial observations, hallucination problem, generalization of state entropy maximization framework, formal characterizations of approximation gaps, POMDPs (Partially Observable Markov Decision Processes), MDP (Markov Decision Process)


- [AND: Audio Network Dissection for Interpreting Deep Acoustic Models](https://icml.cc/virtual/2024/poster/33753) (Poster)
  - **Authors:** [Tung-Yu Wu](http://openreview.net/profile?id=~Tung-Yu_Wu1), [Yu-Xiang Lin](http://openreview.net/profile?id=~Yu-Xiang_Lin1), [Lily Weng](http://openreview.net/profile?id=~Tsui-Wei_Weng1)
  - **Affiliations:** National Taiwan University, Taipei, Taiwan, National Taiwan University, Taipei, Taiwan, HDSI, UC San Diego, CA, USA
  - **TL;DR:** This paper introduces AND, the first Audio Network Dissection framework, which provides natural language explanations for acoustic neurons and explores their functionalities. The study reveals that models rely on basic acoustic features for discrimination and that training strategies influence neuron interpretability.
  - **Keywords:** Audio Network Dissection, Neuron-level interpretation, Acoustic models, Audio analysis, Machine unlearning, Understanding model behaviors, Neuron functionalities, Natural language explanations of acoustic neurons, Concept-specific pruning, Deep Neural Networks (DNNs), LLMs (Large Language Models)


- [Prometheus: Out-of-distribution Fluid Dynamics Modeling with Disentangled Graph ODE](https://icml.cc/virtual/2024/poster/34364) (Poster)
  - **Authors:** [Hao Wu](http://openreview.net/profile?id=~Hao_Wu39), [Huiyuan Wang](http://openreview.net/profile?id=~Huiyuan_Wang1), [kun wang](http://openreview.net/profile?id=~Kun_Wang15), [Weiyan Wang](http://openreview.net/profile?id=~Weiyan_Wang1), [ChanganYe](http://openreview.net/profile?id=~ChanganYe1), [Yangyu Tao](http://openreview.net/profile?id=~Yangyu_Tao2), [Chong Chen](http://openreview.net/profile?id=~Chong_Chen2), [Xian-Sheng Hua](http://openreview.net/profile?id=~Xian-Sheng_Hua1), [Xiao Luo](http://openreview.net/profile?id=~Xiao_Luo3)
  - **Affiliations:** Machine Learning Platform Department, Tencent; University of Science and Technology of China, The Center for Health AI and Synthesis of Evidence (CHASE), University of Pennsylvania; Department of Biostatistics, Epidemiology, and Informatics, University of Pennsylvania Perelman School of Medicine, University of Science and Technology of China, Machine Learning Platform Department, Tencent, Machine Learning Platform Department, Tencent, Machine Learning Platform Department, Tencent, Terminus Group, Terminus Group, University of California, Los Angeles
  - **TL;DR:** This study addresses the challenge of out-of-distribution generalization in fluid dynamics modeling by introducing a new dataset, Prometheus, and a novel approach called Disentangled Graph ODE (DGODE). The findings demonstrate that DGODE significantly improves modeling performance across varying environmental conditions compared to existing methods.
  - **Keywords:** fluid dynamics modeling, out-of-distribution generalization, graph neural networks (GNN), Disentangled Graph ODE (DGODE), adversarial learning, fire dynamics simulations, computational fluid dynamics, out-of-distribution (OOD) performance, distribution shift, new dataset (Prometheus), enhanced robustness through environmental perturbation, Prometheus dataset, fire dynamics simulators, Navier-Stokes equations, temporal GNN, frequency network


- [Profile Reconstruction from Private Sketches](https://icml.cc/virtual/2024/poster/34749) (Poster)
  - **Authors:** [Hao WU](http://openreview.net/profile?id=~Hao_WU21), [Rasmus Pagh](http://openreview.net/profile?id=~Rasmus_Pagh1)
  - **Affiliations:** Department of Computer Science, University of Copenhagen, Denmark, Department of Computer Science, University of Copenhagen, Denmark
  - **TL;DR:** This study addresses the profile reconstruction problem in a differentially private context, proposing an efficient method to maintain an updatable private sketch of a multiset. The authors demonstrate that their approach achieves optimal error bounds while ensuring privacy, significantly improving the computational efficiency of existing techniques.
  - **Keywords:** Profile reconstruction, Differential privacy, Discrete Laplace noise, LP-based technique, Data privacy, Statistical estimation, Estimation of item frequencies, Space-constrained settings, Updatable private sketches, Error analysis in ℓ1, ℓ2, and ℓ∞ norms, Multiset, Histogram, High-probability error guarantee


- [Policy Learning for Balancing Short-Term and Long-Term Rewards](https://icml.cc/virtual/2024/poster/34884) (Poster)
  - **Authors:** [Peng Wu](http://openreview.net/profile?id=~Peng_Wu5), [Ziyu Shen](http://openreview.net/profile?id=~Ziyu_Shen2), [Feng Xie](http://openreview.net/profile?id=~Feng_Xie1), [Wang Zhongyao](http://openreview.net/profile?id=~Wang_Zhongyao1), [Chunchen LIU](http://openreview.net/profile?id=~Chunchen_LIU2), [Yan Zeng](http://openreview.net/profile?id=~Yan_Zeng2)
  - **Affiliations:** School of Mathematics and Statistics, Beijing Technology and Business University, School of Mathematics and Statistics, Beijing Technology and Business University, School of Mathematics and Statistics, Beijing Technology and Business University, LingYang, Alibaba Group, Hangzhou, China, LingYang, Alibaba Group, Hangzhou, China, School of Mathematics and Statistics, Beijing Technology and Business University
  - **TL;DR:** This paper presents a framework for learning an optimal policy that balances short-term and long-term rewards, addressing the challenges of missing long-term outcomes and confounding biases. The proposed method demonstrates practical applicability through extensive experiments, highlighting the importance of considering both reward types in policy learning.
  - **Keywords:** Policy learning, Balancing rewards, Semiparametric efficiency bounds, Estimators, Interventions in marketing, education, healthcare, Balancing short-term and long-term rewards, Missing long-term outcomes, Confounding bias, Optimal policy learning approach, Convergence rates of regret and estimation errors


- [Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations via Pareto Optimization](https://icml.cc/virtual/2024/poster/33071) (Poster)
  - **Authors:** [Yichen WU](http://openreview.net/profile?id=~Yichen_Wu2), [Hong Wang](http://openreview.net/profile?id=~Hong_Wang5), [Peilin Zhao](http://openreview.net/profile?id=~Peilin_Zhao2), [Yefeng Zheng](http://openreview.net/profile?id=~Yefeng_Zheng3), [Ying WEI](http://openreview.net/profile?id=~Ying_Wei1), [Long-Kai Huang](http://openreview.net/profile?id=~Long-Kai_Huang1)
  - **Affiliations:** City University of Hong Kong, Hong Kong SAR; Tencent AI Lab, Shenzhen, China, Tencent Youtu Lab, Shenzhen, China, Tencent AI Lab, Shenzhen, China, Tencent Youtu Lab, Shenzhen, China, City University of Hong Kong, Hong Kong SAR; Nanyang Technological University, Singapore, Tencent AI Lab, Shenzhen, China
  - **TL;DR:** The study addresses catastrophic forgetting in continual learning by proposing a Pareto-Optimized CL algorithm (POCL) that captures interrelations among tasks to improve performance on both past and current tasks. Empirical results show that POCL outperforms existing state-of-the-art methods across various datasets and settings.
  - **Keywords:** Continual Learning (CL), Catastrophic Forgetting, Replay-based methods, Pareto optimization, Hierarchical gradient aggregation, Catastrophic forgetting, Retaining previous knowledge, Interdependence between tasks, Pareto-Optimized CL algorithm (POCL), Enhanced performance of past tasks


- [Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels](https://icml.cc/virtual/2024/poster/34130) (Poster)
  - **Authors:** [Haoning Wu](http://openreview.net/profile?id=~Haoning_Wu1), [Zicheng Zhang](http://openreview.net/profile?id=~Zicheng_Zhang7), [Weixia Zhang](http://openreview.net/profile?id=~Weixia_Zhang1), [Chaofeng Chen](http://openreview.net/profile?id=~Chaofeng_Chen1), [Liang Liao](http://openreview.net/profile?id=~Liang_Liao3), [Chunyi Li](http://openreview.net/profile?id=~Chunyi_Li1), [Yixuan Gao](http://openreview.net/profile?id=~Yixuan_Gao2), [Annan Wang](http://openreview.net/profile?id=~Annan_Wang1), [Erli Zhang](http://openreview.net/profile?id=~Erli_Zhang1), [Wenxiu Sun](http://openreview.net/profile?id=~Wenxiu_Sun1), [Qiong Yan](http://openreview.net/profile?id=~Qiong_Yan1), [Xiongkuo Min](http://openreview.net/profile?id=~Xiongkuo_Min1), [Guangtao Zhai](http://openreview.net/profile?id=~Guangtao_Zhai1), [Weisi Lin](http://openreview.net/profile?id=~Weisi_Lin1)
  - **Affiliations:** Nanyang Technological University, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Nanyang Technological University, Nanyang Technological University, Shanghai Jiao Tong University, Shanghai Jiao Tong University; Nanyang Technological University, Nanyang Technological University, Nanyang Technological University, Sensetime Research, Sensetime Research, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Nanyang Technological University
  - **TL;DR:** This study introduces Q-A LIGN, a method for training large multi-modality models (LMMs) to evaluate visual content using discrete text-defined levels instead of direct scores. The approach demonstrates significant improvements in accuracy across various visual assessment tasks, highlighting the advantages of discrete levels in training and evaluation.
  - **Keywords:** visual scoring, large multi-modality models (LMMs), subjective evaluation, Q-A LIGN, ONEALIGN, image quality assessment (IQA), image aesthetic assessment (IAA), video quality assessment (VQA), out-of-distribution (OOD) generalization, scoring novel types of content, unified model training, state-of-the-art accuracy, discrete text-defined levels, finer-grained evaluations


- [HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming](https://icml.cc/virtual/2024/poster/34831) (Poster)
  - **Authors:** [Yang Wu](http://openreview.net/profile?id=~Yang_Wu8), [Yifan Zhang](http://openreview.net/profile?id=~Yifan_Zhang2), [Zhenxing Liang](http://openreview.net/profile?id=~Zhenxing_Liang1), [Jian Cheng](http://openreview.net/profile?id=~Jian_Cheng7)
  - **Affiliations:** Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Nanjing, Nanjing, China; AIRIA, Nanjing, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Nanjing, Nanjing, China; AIRIA, Nanjing, China
  - **TL;DR:** The study introduces HGCN2SP, a novel hierarchical graph model for Two-stage Stochastic Programming (2SP) that enhances decision-making under uncertainty by efficiently selecting representative scenarios. The model demonstrates high-quality decision-making and generalization capabilities, even with large-scale instances.
  - **Keywords:** Two-stage Stochastic Programming, decision-making under uncertainty, Hierarchical Graph Convolutional Network, reinforcement learning, Disaster management, network design, distributed energy systems, facility location, blood supply chain management, Scenario selection, computational efficiency, scenario reduction, HGCN2SP model, high-quality decision-making, generalization capabilities


- [Boosting Reinforcement Learning with Strongly Delayed Feedback Through Auxiliary Short Delays](https://icml.cc/virtual/2024/poster/35209) (Poster)
  - **Authors:** [Qingyuan Wu](http://openreview.net/profile?id=~Qingyuan_Wu1), [Simon Zhan](http://openreview.net/profile?id=~Simon_Sinong_Zhan1), [Yixuan Wang](http://openreview.net/profile?id=~Yixuan_Wang1), [Yuhui Wang](http://openreview.net/profile?id=~Yuhui_Wang1), [Chung-Wei Lin](http://openreview.net/profile?id=~Chung-Wei_Lin1), [Chen Lv](http://openreview.net/profile?id=~Chen_Lv1), [Qi Zhu](http://openreview.net/profile?id=~Qi_Zhu2), [Jürgen Schmidhuber](http://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1), [Chao Huang](http://openreview.net/profile?id=~Chao_Huang5)
  - **Affiliations:** The University of Liverpool, Northwestern University, Northwestern University, AI Initiative, King Abdullah University of Science and Technology, National Taiwan University, Nanyang Technological University, Northwestern University, The Swiss AI Lab IDSIA/USI/SUPSI, The University of Southampton
  - **TL;DR:** This study introduces Auxiliary-Delayed Reinforcement Learning (AD-RL) to effectively manage delays in reinforcement learning, significantly improving sample efficiency and policy performance in both deterministic and stochastic environments. The method leverages auxiliary tasks with short delays to enhance learning from long delays without compromising performance.
  - **Keywords:** Reinforcement Learning, Delayed Feedback, Auxiliary-Delayed Reinforcement Learning (AD-RL), Bootstrapping, Policy Improvement, Robotics, Financial Markets, Weather Forecasting, Delays in RL, State Space Explosion, Performance Degeneration in Stochastic Environments, Improved Sample Efficiency, Enhanced Policy Performance, Markov Decision Process (MDP), Augmented Q-learning (A-QL)


- [Detecting Any instruction-to-answer interaction relationship:Universal Instruction-to-Answer Navigator for Med-VQA](https://icml.cc/virtual/2024/poster/35018) (Poster)
  - **Authors:** [Zhongze Wu](http://openreview.net/profile?id=~Zhongze_Wu1), [Hongyan Xu](http://openreview.net/profile?id=~Hongyan_Xu3), [Yitian Long](http://openreview.net/profile?id=~Yitian_Long1), [Shan You](http://openreview.net/profile?id=~Shan_You3), [Xiu Su](http://openreview.net/profile?id=~Xiu_Su1), [Jun Long](http://openreview.net/profile?id=~Jun_Long2), [Yueyi Luo](http://openreview.net/profile?id=~Yueyi_Luo1), [Chang Xu](http://openreview.net/profile?id=~Chang_Xu4)
  - **Affiliations:** Central South University, Changsha, Hunan, China, University of New South Wales, Sydney, Australia, Vanderbilt University, Nashville, Tennessee, USA, SenseTime, Central South University, Changsha, Hunan, China; University of Sydney, Sydney, Australia, Central South University, Changsha, Hunan, China, Central South University, Changsha, Hunan, China, University of Sydney, Sydney, Australia
  - **TL;DR:** This paper presents the Universal Instruction-to-Answer Navigator (Uni-Med) framework, which enhances Medical Visual Question Answering (Med-VQA) by extracting instruction-to-answer relationships and providing visual explanations. The proposed methods demonstrate superior accuracy in Med-VQA tasks, addressing challenges like data sparsity and modal interference.
  - **Keywords:** Medical Visual Question Answering (Med-VQA), Instruction-to-Answer Relationships, Instruct-to-Answer Clues Interpreter (IAI), Token-Level Cut-Mix module, Intention-guided attention, Medical diagnostics, Biomedical applications, Data sparsity, Inadequately annotated images, Modal interference, IAI-Med VQA dataset, Enhanced accuracy in Med-VQA tasks, SLAKE datasets, IAI-Med VQA dataset, Multimodal Large Language Models (MLLMs), Visual explanations, Real intent labels


- [Unraveling the Impact of Heterophilic Structures on Graph Positive-Unlabeled Learning](https://icml.cc/virtual/2024/poster/34211) (Poster)
  - **Authors:** [Yuhao Wu](http://openreview.net/profile?id=~Yuhao_Wu2), [Jiangchao Yao](http://openreview.net/profile?id=~Jiangchao_Yao1), [Bo Han](http://openreview.net/profile?id=~Bo_Han1), [Lina Yao](http://openreview.net/profile?id=~Lina_Yao2), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1)
  - **Affiliations:** Sydney AI Center, The University of Sydney, CMIC, Shanghai Jiao Tong University; Shanghai AI Laboratory, TMLR Group, Department of Computer Science, Hong Kong Baptist University, University of New South Wales, Sydney AI Center, The University of Sydney
  - **TL;DR:** This study addresses the challenges of applying Positive-Unlabeled learning to graph data, particularly due to edge heterophily, and introduces a new method called Graph PU Learning with Label Propagation Loss (GPL) to mitigate these issues. Extensive experiments demonstrate that GPL significantly outperforms existing baseline methods, confirming its effectiveness.
  - **Keywords:** Positive-Unlabeled Learning, Graph Data, Graph PU Learning with Label Propagation Loss (GPL), Bilevel Optimization, Fraud Detection, Pandemic Prediction, Edge Heterophily, Class-Prior Estimation Challenges, Latent Label Inference, Heterophilic Structures, Latent Feature Entanglement


- [Mitigating Label Noise on Graphs via Topological Sample Selection](https://icml.cc/virtual/2024/poster/34723) (Poster)
  - **Authors:** [Yuhao Wu](http://openreview.net/profile?id=~Yuhao_Wu2), [Jiangchao Yao](http://openreview.net/profile?id=~Jiangchao_Yao1), [Xiaobo Xia](http://openreview.net/profile?id=~Xiaobo_Xia1), [Jun Yu](http://openreview.net/profile?id=~Jun_Yu3), [Ruxin Wang](http://openreview.net/profile?id=~Ruxin_Wang2), [Bo Han](http://openreview.net/profile?id=~Bo_Han1), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1)
  - **Affiliations:** Sydney AI Center, The University of Sydney, CMIC, Shanghai Jiao Tong University; Shanghai AI Laboratory, Sydney AI Center, The University of Sydney, University of Science and Technology of China, Alibaba Group, TMLR Group, Department of Computer Science, Hong Kong Baptist University, Sydney AI Center, The University of Sydney
  - **TL;DR:** This study addresses the challenges posed by noisy labels in graph neural networks by proposing a Topological Sample Selection (TSS) method that utilizes topological information to enhance sample selection. The method is theoretically proven to minimize expected risk and demonstrates superior performance compared to existing approaches.
  - **Keywords:** Graph Neural Networks (GNNs), Noisy Labels, Sample Selection, Topological Sample Selection (TSS), Noisy labeled graph data, Classification challenges, Topological class boundaries, Minimization of expected risk, Improved sample selection process


- [Category-Aware Active Domain Adaptation](https://icml.cc/virtual/2024/poster/33661) (Poster)
  - **Authors:** [Wenxiao Xiao](http://openreview.net/profile?id=~Wenxiao_Xiao1), [Jiuxiang Gu](http://openreview.net/profile?id=~Jiuxiang_Gu2), [Hongfu Liu](http://openreview.net/profile?id=~Hongfu_Liu2)
  - **Affiliations:** Computer Science Department, Brandeis University, Waltham, Massachusetts, USA, Adobe Research, USA, Computer Science Department, Brandeis University, Waltham, Massachusetts, USA; Adobe Research, USA
  - **TL;DR:** This study introduces a category-aware active domain adaptation method that focuses on improving the adaptation for challenging categories while avoiding the neglect of critical categories. The proposed method utilizes the influence function to assess the importance of unlabeled samples, demonstrating its efficacy through comprehensive experiments.
  - **Keywords:** Active Domain Adaptation, Unsupervised Domain Adaptation, Influence Function, Performance Gap, Category Discrepancy, Category-Aware Active Domain Adaptation Method


- [Surface-VQMAE: Vector-quantized Masked Auto-encoders on Molecular Surfaces](https://icml.cc/virtual/2024/poster/32884) (Poster)
  - **Authors:** [Fang Wu](http://openreview.net/profile?id=~Fang_Wu1), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** School of Engineering, Westlake University, School of Engineering, Westlake University
  - **TL;DR:** This study introduces Surface-VQMAE, a novel unsupervised learning algorithm that leverages molecular surface information to enhance protein function analysis. The method effectively addresses challenges related to data sparsity and disorder in surface point clouds, demonstrating its utility in various biological applications.
  - **Keywords:** molecular surfaces, protein interactions, self-supervised learning, Surface-VQMAE, SurfFormer, masked auto-encoder (MAE), vector quantization (VQ), binding site scoring, binding affinity prediction, mutant effect estimation, data sparsity, disorder in surface point clouds, bridging pretraining and fine-tuning stages, discrete posterior distribution of latent variables, surface pattern codebook


- [Contrastive Learning for Clinical Outcome Prediction with Partial Data Sources](https://icml.cc/virtual/2024/poster/33482) (Poster)
  - **Authors:** [Xia](http://openreview.net/profile?id=~Meng_Xia2), [Jonathan Wilson](http://openreview.net/profile?id=~Jonathan_Wilson2), [Benjamin Goldstein](http://openreview.net/profile?id=~Benjamin_Goldstein1), [Ricardo Henao](http://openreview.net/profile?id=~Ricardo_Henao1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Duke University, Durham, US; Department of Biostatistics and Bioinformatics, Duke University, Durham, US; King Abdullah University of Science and Technology, Thuwal, KSA, Department of Biostatistics and Bioinformatics, Duke University, Durham, US, Department of Biostatistics and Bioinformatics, Duke University, Durham, US, Department of Electrical and Computer Engineering, Duke University, Durham, US; Department of Biostatistics and Bioinformatics, Duke University, Durham, US; King Abdullah University of Science and Technology, Thuwal, KSA
  - **TL;DR:** This study introduces Contrastive Learning for clinical Outcome Prediction with Partial data Sources (CLOPPS), which effectively trains models to predict clinical outcomes using partial data sources. The experiments demonstrate that CLOPPS outperforms existing models in various practical scenarios, addressing the challenge of data availability in real-world healthcare settings.
  - **Keywords:** clinical outcome prediction, electronic health records (EHR), machine learning, Contrastive Learning, encoders, classifiers, healthcare, clinical settings, partial data availability, data source variability, training vs. inference data access, CLOPPS method, improved prediction performance, longitudinal data, cross-sectional models, LSTMs, Transformers


- [Improved Operator Learning by Orthogonal Attention](https://icml.cc/virtual/2024/poster/34901) (Spotlight Poster)
  - **Authors:** [Zipeng Xiao](http://openreview.net/profile?id=~Zipeng_Xiao1), [Zhongkai Hao](http://openreview.net/profile?id=~Zhongkai_Hao1), [Bokai Lin](http://openreview.net/profile?id=~Bokai_Lin1), [Zhijie Deng](http://openreview.net/profile?id=~Zhijie_Deng1), [Hang Su](http://openreview.net/profile?id=~Hang_Su3)
  - **Affiliations:** Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Dept. of Comp. Sci. & Tech., Tsinghua University, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Dept. of Comp. Sci. & Tech., Tsinghua University
  - **TL;DR:** This study introduces orthogonal attention for constructing neural operators to model solutions of Partial Differential Equations (PDEs), leveraging orthonormal eigenfunctions to enhance regularization and mitigate overfitting. Experimental results demonstrate that the proposed method outperforms existing baselines across various benchmark datasets.
  - **Keywords:** neural operators, orthogonal attention, Partial Differential Equations (PDEs), kernel integral operator, orthonormal eigenfunctions, flexible neural networks (NNs), attention mechanisms, scientific modeling, engineering dynamics, overfitting, limited data availability, computational complexity, novel neural operator, regularization techniques, six standard neural operator benchmark datasets, physics-informed neural networks (PINNs), Fourier Neural Operators (FNOs), Fast Fourier Transform (FFT)


- [Ditto: Quantization-aware Secure Inference of Transformers upon MPC](https://icml.cc/virtual/2024/poster/33708) (Poster)
  - **Authors:** [Haoqi Wu](http://openreview.net/profile?id=~Haoqi_Wu1), [Wenjing Fang](http://openreview.net/profile?id=~Wenjing_Fang1), [Yancheng Zheng](http://openreview.net/profile?id=~Yancheng_Zheng1), [Junming Ma](http://openreview.net/profile?id=~Junming_Ma1), [Jin Tan](http://openreview.net/profile?id=~Jin_Tan2), [Lei Wang](http://openreview.net/profile?id=~Lei_Wang30)
  - **Affiliations:** Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China
  - **TL;DR:** The study presents Ditto, a framework for efficient quantization-aware secure inference of Transformers using secure multi-party computation (MPC) to address privacy concerns. The results show that Ditto significantly reduces computation and communication overhead, achieving up to 4.40 times faster performance compared to existing methods with minimal utility degradation.
  - **Keywords:** secure inference, privacy concerns, data security, secure multi-party computation (MPC), quantization-aware distillation, MPC-friendly quantization, natural language processing, machine learning inference services, computation and communication overhead, integration of quantization into MPC, efficient quantization-aware secure Transformer inference, improvements in overall efficiency, Bert, GPT2, Transformers, non-linear function approximations


- [Delving into the Convergence of Generalized Smooth Minimax Optimization](https://icml.cc/virtual/2024/poster/34085) (Poster)
  - **Authors:** [Wenhan Xian](http://openreview.net/profile?id=~Wenhan_Xian1), [Ziyi Chen](http://openreview.net/profile?id=~Ziyi_Chen2), [Heng Huang](http://openreview.net/profile?id=~Heng_Huang1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, MD, United States, Department of Computer Science, University of Maryland, College Park, MD, United States, Department of Computer Science, University of Maryland, College Park, MD, United States
  - **TL;DR:** This study investigates the convergence of minimax optimization algorithms under a generalized smoothness condition, demonstrating that popular algorithms like GDA, SGDA, GDmax, and SGDmax can still converge despite the lack of Lipschitz smoothness. The findings extend the theoretical guarantees of these algorithms to a broader range of applications in machine learning.
  - **Keywords:** Minimax optimization, Generalized smoothness, Machine learning applications, Gradient Descent Ascent (GDA), Stochastic Gradient Descent Ascent (SGDA), GDmax, SGDmax, Generative Adversarial Networks (GANs), Adversarial training, Robust optimization, Convergence issues, Lipschitz smoothness condition, Nonconvex strongly-concave problems, Convergence analysis, Theoretical guarantees for minimax algorithms


- [Borda Regret Minimization for Generalized Linear Dueling Bandits](https://icml.cc/virtual/2024/poster/35062) (Poster)
  - **Authors:** [Yue Wu](http://openreview.net/profile?id=~Yue_Wu12), [Tao Jin](http://openreview.net/profile?id=~Tao_Jin3), [Qiwei Di](http://openreview.net/profile?id=~Qiwei_Di1), [Hao Lou](http://openreview.net/profile?id=~Hao_Lou1), [Farzad Farnoud](http://openreview.net/profile?id=~Farzad_Farnoud1), [Quanquan Gu](http://openreview.net/profile?id=~Quanquan_Gu1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, Los Angeles, California, United States, Department of Computer Science, University of Virginia, Charlottesville, Virginia, United States, Department of Computer Science, University of California, Los Angeles, Los Angeles, California, United States, Department of Electrical & Computer Engineering, University of Virginia, Charlottesville, Virginia, United States, Department of Electrical & Computer Engineering, University of Virginia, Charlottesville, Virginia, United States, Department of Computer Science, University of California, Los Angeles, Los Angeles, California, United States
  - **TL;DR:** This study investigates the Borda regret minimization problem in dueling bandits, proposing generalized linear models and algorithms to identify the item with the highest Borda score while minimizing cumulative regret. The authors establish a regret lower bound and present algorithms that achieve nearly matching upper bounds, validated through empirical evaluations.
  - **Keywords:** Dueling bandits, Borda regret minimization, Generalized linear dueling bandit models, explore-then-commit algorithm, EXP3-type algorithm, Recommendation systems, ranking, Cumulative regret, identifying optimal items, Regret lower bound, optimal regret upper bound


- [Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems](https://icml.cc/virtual/2024/poster/33600) (Oral)
  - **Authors:** [Yifan Xia](http://openreview.net/profile?id=~Yifan_Xia2), [Xianliang Yang](http://openreview.net/profile?id=~Xianliang_Yang1), [Zichuan Liu](http://openreview.net/profile?id=~Zichuan_Liu3), [Zhihao Liu](http://openreview.net/profile?id=~Zhihao_Liu3), [Lei Song](http://openreview.net/profile?id=~Lei_Song3), [Jiang Bian](http://openreview.net/profile?id=~Jiang_Bian1)
  - **Affiliations:** Nanjing University, Nanjing, China, Microsoft Research Asia, Beijing, China, Nanjing University, Nanjing, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China, Microsoft Research Asia, Beijing, China, Microsoft Research Asia, Beijing, China
  - **TL;DR:** This study critiques the effectiveness of machine learning-based heatmap generation for solving large-scale Traveling Salesman Problems (TSP) using heatmap-guided Monte Carlo Tree Search (MCTS), demonstrating that simpler methods can outperform complex ML approaches. The findings suggest a need for more theoretically sound heatmap generation methods and the exploration of generalizable ML approaches for combinatorial problems.
  - **Keywords:** Traveling Salesman Problem (TSP), Optimization, Heatmap-guided Monte Carlo Tree Search (MCTS), Machine Learning (ML), Logistics, Network Design, Operations Research, Scalability issues, Inefficiency in large-scale TSPs, Sparse rewards on large graphs, Baseline method for heatmap generation, Suggestions for future research directions


- [DISCRET: Synthesizing Faithful Explanations For Treatment Effect Estimation](https://icml.cc/virtual/2024/poster/34742) (Spotlight Poster)
  - **Authors:** [Yinjun Wu](http://openreview.net/profile?id=~Yinjun_Wu1), [Mayank Keoliya](http://openreview.net/profile?id=~Mayank_Keoliya1), [Kan Chen](http://openreview.net/profile?id=~Kan_Chen3), [Neelay Velingker](http://openreview.net/profile?id=~Neelay_Velingker1), [Ziyang Li](http://openreview.net/profile?id=~Ziyang_Li2), [Emily Getzen](http://openreview.net/profile?id=~Emily_J_Getzen1), [Qi Long](http://openreview.net/profile?id=~Qi_Long1), [Mayur Naik](http://openreview.net/profile?id=~Mayur_Naik1), [Ravi Parikh](http://openreview.net/profile?id=~Ravi_B_Parikh1), [Eric Wong](http://openreview.net/profile?id=~Eric_Wong1)
  - **Affiliations:** School of Computer Science, Peking University, Beijing, China, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, United States, School of Public Health, Harvard University, Boston, MA, United States, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, United States, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, United States, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, United States, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, United States, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, United States, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, United States, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, United States
  - **TL;DR:** The study presents DISCRET, a self-interpretable framework for estimating individual treatment effects (ITE) that synthesizes faithful, rule-based explanations while maintaining high accuracy. It addresses the challenge of providing both accurate predictions and trustworthy explanations in critical applications like healthcare.
  - **Keywords:** Individual Treatment Effect (ITE) estimation, Explainable AI, Reinforcement Learning (RL), Rule-based explanations, Healthcare, Linguistics, Poverty alleviation, Accurate treatment effect estimation, Faithfulness of explanations, DISCRET framework, Provably-faithful ITE prediction, GANs (Generative Adversarial Networks), Transformers


- [Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning](https://icml.cc/virtual/2024/poster/35073) (Poster)
  - **Authors:** [Mingqing Xiao](http://openreview.net/profile?id=~Mingqing_Xiao1), [Yixin Zhu](http://openreview.net/profile?id=~Yixin_Zhu1), [Di He](http://openreview.net/profile?id=~Di_He1), [Zhouchen Lin](http://openreview.net/profile?id=~Zhouchen_Lin1)
  - **Affiliations:** National Key Lab of General AI, School of Intelligence Science and Technology, Peking University; Institute for Artificial Intelligence, Peking University, Institute for Artificial Intelligence, Peking University, National Key Lab of General AI, School of Intelligence Science and Technology, Peking University; Institute for Artificial Intelligence, Peking University, National Key Lab of General AI, School of Intelligence Science and Technology, Peking University; Institute for Artificial Intelligence, Peking University; Pazhou Laboratory (Huangpu), Guangzhou, China
  - **TL;DR:** This study explores the capabilities of Spiking Neural Networks (SNNs) integrated with synaptic delay and temporal coding for effective graph reasoning. The findings demonstrate that these models can achieve significant energy savings while enhancing the processing of relational properties in knowledge graphs.
  - **Keywords:** Spiking Neural Networks, Graph Reasoning, Synaptic Delay, Temporal Coding, Knowledge Graphs, AI Applications, Human-like Graph-based Reasoning, Energy Efficiency, Neural-Generalized Path Formulation, Biologically Inspired Models


- [Efficient Contrastive Learning for Fast and Accurate Inference on Graphs](https://icml.cc/virtual/2024/poster/32769) (Poster)
  - **Authors:** [Teng Xiao](http://openreview.net/profile?id=~Teng_Xiao2), [Huaisheng Zhu](http://openreview.net/profile?id=~Huaisheng_Zhu1), [Zhiwei Zhang](http://openreview.net/profile?id=~Zhiwei_Zhang10), [Zhimeng Guo](http://openreview.net/profile?id=~Zhimeng_Guo1), [Charu Aggarwal](http://openreview.net/profile?id=~Charu_C._Aggarwal2), [Suhang Wang](http://openreview.net/profile?id=~Suhang_Wang1), [Vasant Honavar](http://openreview.net/profile?id=~Vasant_G_Honavar1)
  - **Affiliations:** Penn State University, USA, Penn State University, USA, Penn State University, USA, Penn State University, USA, IBM T. J. Watson Research Center, USA, Penn State University, USA, Penn State University, USA
  - **TL;DR:** This paper introduces GraphECL, an efficient contrastive learning method for fast inference on graphs that eliminates the need for expensive message passing. The method achieves superior performance and inference efficiency compared to existing graph contrastive learning methods, addressing the challenges of high computational overhead in latency-constrained applications.
  - **Keywords:** Graph contrastive learning, Fast inference on graphs, MLP (Multi-Layer Perceptron), GNN (Graph Neural Network), Node classification, Link prediction, Graph classification, High computational overhead, Latency-constrained applications, GraphECL (a new contrastive learning method), Efficient inference


- [CCM: Real-Time Controllable Visual Content Creation Using Text-to-Image Consistency Models](https://icml.cc/virtual/2024/poster/34738) (Poster)
  - **Authors:** [Jie Xiao](http://openreview.net/profile?id=~Jie_Xiao3), [Kai Zhu](http://openreview.net/profile?id=~Kai_Zhu4), [Han Zhang](http://openreview.net/profile?id=~Han_Zhang16), [Zhiheng Liu](http://openreview.net/profile?id=~Zhiheng_Liu1), [Yujun Shen](http://openreview.net/profile?id=~Yujun_Shen1), [Zhantao Yang](http://openreview.net/profile?id=~Zhantao_Yang1), [Ruili Feng](http://openreview.net/profile?id=~Ruili_Feng1), [Yu Liu](http://openreview.net/profile?id=~Yu_Liu23), [Xueyang Fu](http://openreview.net/profile?id=~Xueyang_Fu1), [Zheng-Jun Zha](http://openreview.net/profile?id=~Zheng-Jun_Zha2)
  - **Affiliations:** University of Science and Technology of China, Hefei, China, Alibaba Group, Shanghai Jiao Tong University, University of Science and Technology of China, Hefei, China, Ant Group, Alibaba Group, Alibaba Group, Alibaba Group, University of Science and Technology of China, Hefei, China, University of Science and Technology of China, Hefei, China
  - **TL;DR:** This study explores the integration of ControlNet with consistency models to enhance controllable visual content creation, demonstrating that ControlNet can be effectively trained using a tailored consistency training strategy. The findings indicate that this approach allows for high-quality image generation with improved performance and efficiency compared to traditional methods.
  - **Keywords:** Controllable visual content creation, Consistency models, ControlNet, Consistency training, Image generation, Text-to-image generation, Adding conditional controls to pre-trained models, Maintaining image details and realism, Development of a CMs-tailored training strategy, Successful establishment of ControlNet from scratch, Consistency models (CMs), Diffusion models (DMs), Latent consistency models (LCMs)


- [FedREDefense: Defending against Model Poisoning Attacks for Federated Learning using Model Update Reconstruction Error](https://icml.cc/virtual/2024/poster/33854) (Poster)
  - **Authors:** [Yueqi Xie](http://openreview.net/profile?id=~Yueqi_XIE1), [Minghong Fang](http://openreview.net/profile?id=~Minghong_Fang1), [Neil Gong](http://openreview.net/profile?id=~Neil_Zhenqiang_Gong1)
  - **Affiliations:** The Hong Kong University of Science and Technology, University of Louisville, Duke University
  - **TL;DR:** This paper presents FedREDefense, a novel defense mechanism against model poisoning attacks in Federated Learning that operates independently of client data distributions. The method effectively identifies and filters out malicious clients based on discrepancies in model update reconstruction errors, demonstrating strong performance even in challenging scenarios with high non-IID data distributions.
  - **Keywords:** Federated Learning, Model Poisoning Attacks, Model Update Reconstruction Error, Health Care, Non-IID Data Distributions, Malicious Clients, FedREDefense, Filtering Malicious Clients, Benchmark Datasets


- [Intersecting-Boundary-Sensitive Fingerprinting for Tampering Detection of DNN Models](https://icml.cc/virtual/2024/poster/33248) (Poster)
  - **Authors:** [Xiaofan Bai](http://openreview.net/profile?id=~Bai_Xiaofan1), [Chaoxiang He](http://openreview.net/profile?id=~Chaoxiang_He1), [Xiaojing Ma](http://openreview.net/profile?id=~Xiaojing_Ma1), [Bin Zhu](http://openreview.net/profile?id=~Bin_Benjamin_Zhu1), [Hai Jin](http://openreview.net/profile?id=~Hai_Jin1)
  - **Affiliations:** School of Cyber Science and Engineering, Huazhong University of Science and Technology; National Engineering Research Center for Big Data Technology and System; Services Computing Technology and System Lab; Hubei Engineering Research Center on Big Data Security; Hubei Key Laboratory of Distributed System Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology; National Engineering Research Center for Big Data Technology and System; Services Computing Technology and System Lab; Hubei Engineering Research Center on Big Data Security; Hubei Key Laboratory of Distributed System Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology; National Engineering Research Center for Big Data Technology and System; Services Computing Technology and System Lab; Hubei Engineering Research Center on Big Data Security; Hubei Key Laboratory of Distributed System Security, Microsoft, School of Computer Science and Technology, Huazhong University of Science and Technology; Cluster and Grid Computing Lab
  - **TL;DR:** This paper introduces Intersecting-Boundary-Sensitive Fingerprinting (IBSF), a novel method for detecting tampering in deployed DNN models by generating fingerprint samples that are nearly indistinguishable from normal samples. The method significantly improves tampering detection sensitivity, particularly with larger subsets of categories, outperforming existing techniques.
  - **Keywords:** DNN models, tampering detection, integrity verification, Intersecting-Boundary-Sensitive Fingerprinting (IBSF), partial Shannon entropy, Cloud-based AI services, model deployment, Tampering with DNN models, malicious behavior injection, model integrity threats, Novel tampering detection method, state-of-the-art performance in black-box tampering detection


- [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://icml.cc/virtual/2024/poster/34132) (Poster)
  - **Authors:** [Mengzhou Xia](http://openreview.net/profile?id=~Mengzhou_Xia1), [Sadhika Malladi](http://openreview.net/profile?id=~Sadhika_Malladi2), [Suchin Gururangan](http://openreview.net/profile?id=~Suchin_Gururangan1), [Sanjeev Arora](http://openreview.net/profile?id=~Sanjeev_Arora1), [Danqi Chen](http://openreview.net/profile?id=~Danqi_Chen1)
  - **Affiliations:** Princeton Language and Intelligence (PLI), Princeton University, USA, Princeton Language and Intelligence (PLI), Princeton University, USA, Department of Computer Science, University of Washington, USA, Princeton Language and Intelligence (PLI), Princeton University, USA, Princeton Language and Intelligence (PLI), Princeton University, USA
  - **TL;DR:** The study introduces LESS, an efficient algorithm for selecting influential data to enhance specific capabilities in large language models through targeted instruction tuning. Experiments demonstrate that training on a small, LESS-selected dataset can outperform training on the full dataset across various tasks, highlighting the method's effectiveness and transferability.
  - **Keywords:** targeted instruction tuning, large language models (LLMs), reasoning skills, optimizer-aware algorithm, gradient datastore, Low-rank gradient similarity search, chatbot development, instruction tuning, identifying relevant data, mixed instruction tuning datasets, performance issues with diverse queries, data selection methods, improved training efficiency, transferability of selected data, influence estimation, few-shot examples


- [Automating the Selection of Proxy Variables of Unmeasured Confounders](https://icml.cc/virtual/2024/poster/33897) (Spotlight Poster)
  - **Authors:** [Feng Xie](http://openreview.net/profile?id=~Feng_Xie1), [Zhengming Chen](http://openreview.net/profile?id=~Zhengming_Chen2), [Shanshan Luo](http://openreview.net/profile?id=~Shanshan_Luo2), [Wang Miao](http://openreview.net/profile?id=~Wang_Miao1), [Ruichu Cai](http://openreview.net/profile?id=~Ruichu_Cai1), [zhi geng](http://openreview.net/profile?id=~Zhi_Geng1)
  - **Affiliations:** Department of Applied Statistics, Beijing Technology and Business University, Beijing, China, School of Computer Science, Guangdong University of Technology, Guangzhou 510006, China; Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Department of Applied Statistics, Beijing Technology and Business University, Beijing, China, Department of Probability and Statistics, Peking University, Beijing, China, School of Computer Science, Guangdong University of Technology, Guangzhou 510006, China, Department of Applied Statistics, Beijing Technology and Business University, Beijing, China
  - **TL;DR:** This study focuses on automating the selection of proxy variables for unmeasured confounders to estimate causal effects in observational data. The authors propose new data-driven methods and extend existing estimators to handle multiple unmeasured confounders, demonstrating their effectiveness through theoretical analysis and experimental results.
  - **Keywords:** Causal inference, Unmeasured confounders, Proxy variables, Linear causal model, Proxy variable estimator, Second-order statistics, Higher-order statistics, Observational data analysis, Social sciences, Economics, Public health, Machine learning, Unmeasured confounding, Validity of proxy variables, Estimation of causal effects, Data-driven methods for proxy variable selection, Unbiased estimation of causal effects, Synthetic data, Real-world data, Negative controls (NCs), Instrumental variables


- [Refined Coreset Selection: Towards Minimal Coreset Size under Model Performance Constraints](https://icml.cc/virtual/2024/poster/32663) (Spotlight Poster)
  - **Authors:** [Xiaobo Xia](http://openreview.net/profile?id=~Xiaobo_Xia1), [Jiale Liu](http://openreview.net/profile?id=~Jiale_Liu2), [Shaokun Zhang](http://openreview.net/profile?id=~Shaokun_Zhang2), [Qingyun Wu](http://openreview.net/profile?id=~Qingyun_Wu2), [Hongxin Wei](http://openreview.net/profile?id=~Hongxin_Wei1), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1)
  - **Affiliations:** School of Computer Science, The University of Sydney, Australia, College of Information Science and Technology, Penn State University, USA, College of Information Science and Technology, Penn State University, USA, College of Information Science and Technology, Penn State University, USA, Department of Statistics and Data Science, Southern University of Science and Technology, China, School of Computer Science, The University of Sydney, Australia
  - **TL;DR:** This paper introduces the refined coreset selection (RCS) problem, focusing on minimizing coreset size while maintaining model performance. The proposed method demonstrates superior efficiency and effectiveness compared to previous strategies, providing a convergence guarantee and better performance with smaller coresets.
  - **Keywords:** Coreset selection, Deep learning, Optimization methods, Data processing, Model training, Model performance constraints, Data efficiency, Minimal coreset size, Convergence guarantee


- [Federated Neuro-Symbolic Learning](https://icml.cc/virtual/2024/poster/34584) (Poster)
  - **Authors:** [Pengwei Xing](http://openreview.net/profile?id=~Pengwei_Xing2), [Songtao Lu](http://openreview.net/profile?id=~Songtao_Lu1), [Han Yu](http://openreview.net/profile?id=~Han_Yu1)
  - **Affiliations:** College of Computing and Data Science, Nanyang Technological University, Singapore, IBM Thomas J. Watson Research Center Yorktown Heights, USA, College of Computing and Data Science, Nanyang Technological University, Singapore
  - **TL;DR:** This paper introduces the Federated Neuro-Symbolic Learning (FedNSL) framework, which addresses the challenges of rule distribution heterogeneity in federated learning by utilizing latent variables and optimizing the rule search space. The proposed method significantly outperforms existing approaches in terms of training and testing accuracy across various datasets.
  - **Keywords:** Neuro-symbolic learning, Federated learning, Kullback-Leibler divergence, Variational expectation maximization, Rule distribution heterogeneity, Data heterogeneity, Federated Neuro-Symbolic Learning framework (FedNSL), Optimization of rule search space, First-order logic, Knowledge graph, Transformers


- [Reflected Flow Matching](https://icml.cc/virtual/2024/poster/33998) (Poster)
  - **Authors:** [Tianyu Xie](http://openreview.net/profile?id=~Tianyu_Xie1), [Yu Zhu](http://openreview.net/profile?id=~Yu_Zhu13), [Longlin Yu](http://openreview.net/profile?id=~Longlin_Yu1), [Tong Yang](http://openreview.net/profile?id=~Tong_Yang2), [Ziheng Cheng](http://openreview.net/profile?id=~Ziheng_Cheng4), [Shiyue Zhang](http://openreview.net/profile?id=~Shiyue_Zhang3), [Xiangyu Zhang](http://openreview.net/profile?id=~Xiangyu_Zhang1), [Cheng Zhang](http://openreview.net/profile?id=~Cheng_Zhang3)
  - **Affiliations:** School of Mathematical Sciences, Peking University, Beijing, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Beijing Academy of Artificial Intelligence, Beijing, China, School of Mathematical Sciences, Peking University, Beijing, China, School of Computer Science, Fudan University, Shanghai, China, School of Mathematical Sciences, Peking University, Beijing, China, School of Mathematical Sciences, Peking University, Beijing, China, Megvii Technology Inc., Beijing, China, Center for Statistical Science, Peking University, Beijing, China
  - **TL;DR:** This paper introduces reflected flow matching (RFM) as a simulation-free training method for continuous normalizing flows (CNFs) that addresses issues of flow matching and simulation errors in constrained domains. The proposed method demonstrates improved performance on standard image benchmarks and produces high-quality samples under high guidance weight.
  - **Keywords:** Continuous normalizing flows, deep generative models, Flow matching, ordinary differential equations (ODEs), reflected flow matching (RFM), Image synthesis, text generation, molecular design, Flow matching error, simulation error, boundary constraints, High-quality class-conditioned samples, simulation-free training methods, Optimal transport (OT), denoising score matching (DSM), generative models


- [See More Details: Efficient Image Super-Resolution by Experts Mining](https://icml.cc/virtual/2024/poster/35207) (Poster)
  - **Authors:** [Eduard Zamfir](http://openreview.net/profile?id=~Eduard_Zamfir1), [Zongwei Wu](http://openreview.net/profile?id=~Zongwei_Wu1), [Nancy Mehta](http://openreview.net/profile?id=~Nancy_Mehta1), [Yulun Zhang](http://openreview.net/profile?id=~Yulun_Zhang1), [Radu Timofte](http://openreview.net/profile?id=~Radu_Timofte1)
  - **Affiliations:** Computer Vision Lab, CAIDAS & IFI, University of Würzburg, Germany, Computer Vision Lab, CAIDAS & IFI, University of Würzburg, Germany, Computer Vision Lab, CAIDAS & IFI, University of Würzburg, Germany, AI Institute, Shanghai Jiao Tong University, China; Computer Vision Lab, ETH Zurich, Switzerland, Computer Vision Lab, CAIDAS & IFI, University of Würzburg, Germany
  - **TL;DR:** The study introduces Seemo Re, an efficient image super-resolution model that utilizes expert mining to reconstruct high-resolution images from low-resolution inputs while minimizing computational costs. The model effectively addresses challenges in SR by leveraging a collaborative approach among specialized experts, achieving superior performance compared to existing methods.
  - **Keywords:** Image Super-Resolution (SR), Efficient Image Reconstruction, Expert Mining, Low-Rank Experts, Collaborative Methodology, Ultra-High Definition Devices, Video Streaming, Computational Burden, Resource Constraints, Missing HR Pixels, Seemo Re Model, Optimal Performance with Minimal Computational Costs, Manga109 Dataset


- [Distilling Morphology-Conditioned Hypernetworks for Efficient Universal Morphology Control](https://icml.cc/virtual/2024/poster/33853) (Poster)
  - **Authors:** [Zheng Xiong](http://openreview.net/profile?id=~Zheng_Xiong1), [Risto Vuorio](http://openreview.net/profile?id=~Risto_Vuorio1), [Jacob Beck](http://openreview.net/profile?id=~Jacob_Beck1), [Matthieu Zimmer](http://openreview.net/profile?id=~Matthieu_Zimmer1), [Kun Shao](http://openreview.net/profile?id=~Kun_Shao1), [Shimon Whiteson](http://openreview.net/profile?id=~Shimon_Whiteson1)
  - **Affiliations:** Department of Computer Science, University of Oxford, Oxford, UK, Department of Computer Science, University of Oxford, Oxford, UK, Department of Computer Science, University of Oxford, Oxford, UK, Huawei Noah’s Ark Lab, London, UK, Huawei Noah’s Ark Lab, London, UK, Department of Computer Science, University of Oxford, Oxford, UK
  - **TL;DR:** The study proposes HyperDistill, a method that combines a morphology-conditioned hypernetwork with policy distillation to create efficient universal policies for controlling diverse robot morphologies. HyperDistill achieves performance comparable to sophisticated transformer models while significantly reducing model size and computational costs.
  - **Keywords:** universal morphology control, reinforcement learning, transformers (TF), multi-layer perceptrons (MLP), hypernetworks (HN), policy distillation, robotic control, generalization across robots, sample inefficiency in training, HyperDistill, knowledge decoupling, UNIMAL, graph neural networks (GNN), zero-shot generalization


- [Adapting Static Fairness to Sequential Decision-Making: Bias Mitigation Strategies towards Equal Long-term Benefit Rate](https://icml.cc/virtual/2024/poster/32667) (Poster)
  - **Authors:** [Yuancheng Xu](http://openreview.net/profile?id=~Yuancheng_Xu1), [Chenghao Deng](http://openreview.net/profile?id=~Chenghao_Deng1), [Yanchao Sun](http://openreview.net/profile?id=~Yanchao_Sun1), [Ruijie Zheng](http://openreview.net/profile?id=~Ruijie_Zheng1), [xiyao wang](http://openreview.net/profile?id=~Xiyao_Wang1), [Jieyu Zhao](http://openreview.net/profile?id=~Jieyu_Zhao1), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1)
  - **Affiliations:** University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Southern California, University of Maryland, College Park
  - **TL;DR:** This study introduces the Equal Long-term Benefit Rate (ELBERT) concept to address biases in sequential decision-making by integrating long-term fairness into Markov Decision Processes. The proposed method, ELBERT-PO, effectively reduces bias while maintaining high utility across various decision-making environments.
  - **Keywords:** long-term fairness, bias mitigation, sequential decision-making, Markov Decision Process (MDP), policy gradient, temporal discrimination, long-term bias, static fairness constraints, Equal Long-term Benefit Rate (ELBERT), ELBERT-PO


- [Implicit Bias of AdamW: $\ell_\infty$-Norm Constrained Optimization](https://icml.cc/virtual/2024/poster/34662) (Poster)
  - **Authors:** [Shuo Xie](http://openreview.net/profile?id=~Shuo_Xie2), [Zhiyuan Li](http://openreview.net/profile?id=~Zhiyuan_Li2)
  - **Affiliations:** Toyota Technological Institute at Chicago, IL, the United States, Toyota Technological Institute at Chicago, IL, the United States
  - **TL;DR:** This study investigates the implicit bias of the AdamW optimization algorithm, demonstrating that it performs constrained optimization and converges to KKT points under specific conditions. The findings enhance the theoretical understanding of AdamW's advantages over traditional Adam with ℓ2 regularization in deep learning tasks.
  - **Keywords:** Optimization algorithms, Deep learning, AdamW, Adam, ℓ2 regularization, ℓ∞ norm, KKT points, Frank-Wolfe, Language modeling, Large language models (LLMs), Theoretical understanding of optimization, Constrained optimization, Implicit bias of AdamW, Convergence properties, Decoupled weight decay, Scale-freeness


- [Semantic-Aware Human Object Interaction Image Generation](https://icml.cc/virtual/2024/poster/32786) (Poster)
  - **Authors:** [zhu xu](http://openreview.net/profile?id=~Zhu_Xu1), [Qingchao Chen](http://openreview.net/profile?id=~Qingchao_Chen2), [Yuxin Peng](http://openreview.net/profile?id=~Yuxin_Peng1), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu20)
  - **Affiliations:** Wangxuan Institute of Computer Technology, Peking University, National Institute of Health Data Science, Peking University, Wangxuan Institute of Computer Technology, Peking University, Wangxuan Institute of Computer Technology, Peking University; National Institute of Health Data Science, Peking University
  - **TL;DR:** This study presents a Semantic-Aware HOI generation framework that addresses challenges in generating high-fidelity images for human-object interactions by refining human pose quality and interaction boundaries. The proposed method significantly enhances generation quality and introduces a comprehensive benchmark for evaluating HOI generation.
  - **Keywords:** Human-Object Interaction (HOI), Image Generation, Semantic-Aware HOI generation framework, Denoising process, Text-to-image generation, Complexity and diversity of human poses, Untrustworthy generation of interaction boundary regions, Semantic misalignment, Iterative inversion and image refinement pipeline, Comprehensive benchmark for HOI generation, Custom-tailored evaluation metrics, Diffusion-based models, HOI categories


- [Local Causal Structure Learning in the Presence of Latent Variables](https://icml.cc/virtual/2024/poster/35071) (Poster)
  - **Authors:** [Feng Xie](http://openreview.net/profile?id=~Feng_Xie1), [Zheng Li](http://openreview.net/profile?id=~Zheng_Li27), [Peng Wu](http://openreview.net/profile?id=~Peng_Wu5), [Yan Zeng](http://openreview.net/profile?id=~Yan_Zeng2), [Chunchen LIU](http://openreview.net/profile?id=~Chunchen_LIU2), [zhi geng](http://openreview.net/profile?id=~Zhi_Geng1)
  - **Affiliations:** Department of Applied Statistics, Beijing Technology and Business University, Beijing, China, Department of Applied Statistics, Beijing Technology and Business University, Beijing, China, Department of Applied Statistics, Beijing Technology and Business University, Beijing, China, Department of Applied Statistics, Beijing Technology and Business University, Beijing, China; LingYang Co.Ltd, Alibaba Group, Hangzhou, China, LingYang Co.Ltd, Alibaba Group, Hangzhou, China, Department of Applied Statistics, Beijing Technology and Business University, Beijing, China
  - **TL;DR:** This study investigates local causal structure learning in the presence of latent variables, proposing a method to identify potential parents and children of a target variable using m-separation and V-structures. The approach is theoretically validated and shown to be effective through experiments on both synthetic and real-world data.
  - **Keywords:** Causal discovery, Local causal structure learning, m-separation, V-structures, causal Markov conditions, faithfulness conditions, Observational data analysis, causal inference, Presence of latent variables, causal sufficiency assumption, New methods for identifying causal relationships, theoretical consistency results, Synthetic data, real-world data, Partial ancestral graph (PAG), causal edges, parents, children, descendants


- [Non-clairvoyant Scheduling with Partial Predictions](https://icml.cc/virtual/2024/poster/33319) (Poster)
  - **Authors:** [Ziyad Benomar](http://openreview.net/profile?id=~Ziyad_Benomar1), [Vianney Perchet](http://openreview.net/profile?id=~Vianney_Perchet3)
  - **Affiliations:** ENSAE, FAIRPLAY joint team, CREST, Palaiseau, France; Ecole polytechnique, Palaiseau, France, Criteo AI Lab, FAIRPLAY joint team, Paris, France
  - **TL;DR:** This study investigates non-clairvoyant scheduling with limited predictions, establishing near-optimal bounds and presenting a learning-augmented algorithm that balances robustness and consistency. The findings highlight the challenges and potential improvements in scheduling algorithms when predictions are restricted.
  - **Keywords:** non-clairvoyant scheduling, learning-augmented algorithms, supply chain management, operating systems, job scheduling, predictions with quality guarantees, competitive analysis, near-optimal lower bounds, learning-augmented algorithm, robustness, consistency, smoothness, preemptive single-machine scheduling, clairvoyant algorithms, non-clairvoyant algorithms


- [HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid Prediction](https://icml.cc/virtual/2024/poster/33218) (Poster)
  - **Authors:** [Lanxiang Xing](http://openreview.net/profile?id=~Lanxiang_Xing2), [Haixu Wu](http://openreview.net/profile?id=~Haixu_Wu1), [yuezhou ma](http://openreview.net/profile?id=~Yuezhou_Ma1), [Jianmin Wang](http://openreview.net/profile?id=~Jianmin_Wang1), [Mingsheng Long](http://openreview.net/profile?id=~Mingsheng_Long5)
  - **Affiliations:** School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University, School of Software, BNRist, Tsinghua University
  - **TL;DR:** This paper introduces HelmFluid, a method for accurate and interpretable fluid prediction that leverages Helmholtz dynamics to decompose complex fluid dynamics into solvable components. The approach demonstrates state-of-the-art performance in both simulated and real-world scenarios, addressing challenges posed by high-dimensional non-linear dynamics.
  - **Keywords:** fluid prediction, Helmholtz dynamics, interpretable fluid modeling, deep models, HelmDynamics block, Multi-scale Multihead Integral Architecture, atmospheric prediction, airflow modeling, high-dimensional non-linear dynamics, imperfect observations, coupled multiscale interactions, accurate and interpretable fluid prediction, decomposition of fluid dynamics, Helmholtz theorem, curl-free, divergence-free, potential functions, stream functions


- [Equivariant Graph Neural Operator for Modeling 3D Dynamics](https://icml.cc/virtual/2024/poster/33544) (Poster)
  - **Authors:** [Minkai Xu](http://openreview.net/profile?id=~Minkai_Xu1), [Jiaqi Han](http://openreview.net/profile?id=~Jiaqi_Han2), [Aaron Lou](http://openreview.net/profile?id=~Aaron_Lou1), [Jean Kossaifi](http://openreview.net/profile?id=~Jean_Kossaifi1), [Arvind Ramanathan](http://openreview.net/profile?id=~Arvind_Ramanathan1), [Kamyar Azizzadenesheli](http://openreview.net/profile?id=~Kamyar_Azizzadenesheli1), [Jure Leskovec](http://openreview.net/profile?id=~Jure_Leskovec1), [Stefano Ermon](http://openreview.net/profile?id=~Stefano_Ermon1), [Anima Anandkumar](http://openreview.net/profile?id=~Anima_Anandkumar1)
  - **Affiliations:** Stanford University, Stanford University, Stanford University, NVIDIA, Argonne National Laboratory, NVIDIA, Stanford University, Stanford University, California Institute of Technology
  - **TL;DR:** This paper introduces the Equivariant Graph Neural Operator (EGNO), a method for modeling the dynamics of relational systems in 3D by directly predicting entire trajectories rather than just next-step states. The approach demonstrates superior performance in various applications, including particle simulations and human motion capture, by effectively capturing temporal correlations while maintaining 3D equivariance.
  - **Keywords:** 3D dynamics modeling, relational systems, Equivariant Graph Neural Operator (EGNO), equivariant temporal convolutions, neural operators, Molecular simulations, particle mechanics, human motion capture, Temporal correlation modeling, next-step prediction limitations, Novel method for modeling dynamics as trajectories, improved performance in dynamic modeling, SE(3)-equivariance, Fourier space


- [Prompt-guided Precise Audio Editing with Diffusion Models](https://icml.cc/virtual/2024/poster/33258) (Poster)
  - **Authors:** [Manjie Xu](http://openreview.net/profile?id=~Manjie_Xu1), [Chenxing Li](http://openreview.net/profile?id=~Chenxing_Li1), [Duzhen Zhang](http://openreview.net/profile?id=~Duzhen_Zhang1), [dan su](http://openreview.net/profile?id=~Dan_Su3), [Wei Liang](http://openreview.net/profile?id=~Wei_Liang1), [Dong Yu](http://openreview.net/profile?id=~Dong_Yu2)
  - **Affiliations:** Beijing Institute of Technology; Tencent AI Lab Beijing, Tencent AI Lab Beijing, Tencent AI Lab Beijing, Tencent AI Lab Beijing, Beijing Institute of Technology, Tencent AI Lab Seattle
  - **TL;DR:** This paper introduces Prompt-guided Precise Audio Editing (PPAE), a training-free method for precise audio editing using text prompts and diffusion models. The approach effectively modifies target audio events while preserving unrelated content, addressing challenges in traditional audio editing methods.
  - **Keywords:** audio editing, text-to-audio generation, diffusion models, cross-attention maps, audio manipulation, precise editing, maintaining audio structure, Prompt-guided Precise Audio Editing (PPAE), training-free editing


- [Enhancing Vision Transformer: Amplifying Non-Linearity in Feedforward Network Module](https://icml.cc/virtual/2024/poster/34201) (Poster)
  - **Authors:** [Yixing Xu](http://openreview.net/profile?id=~Yixing_Xu2), [Chao Li](http://openreview.net/profile?id=~Chao_Li27), [Dong Li](http://openreview.net/profile?id=~Dong_Li13), [Xiao Sheng](http://openreview.net/profile?id=~Xiao_Sheng1), [Fan Jiang](http://openreview.net/profile?id=~Fan_Jiang5), [Lu Tian](http://openreview.net/profile?id=~Lu_Tian3), [Ashish Sirasao](http://openreview.net/profile?id=~Ashish_Sirasao1), [Emad Barsoum](http://openreview.net/profile?id=~Emad_Barsoum1)
  - **Affiliations:** Advanced Micro Devices, Inc., Beijing, China, None, None, None, None, None, None, None
  - **TL;DR:** This paper proposes an improved feedforward network (IFFN) module for vision transformers that enhances non-linearity and reduces computational costs while maintaining classification accuracy on the ImageNet dataset. The findings suggest that the proposed method can be applied across various state-of-the-art vision transformer models.
  - **Keywords:** Vision Transformer, Feedforward Network, Non-linearity, Self-attention module, Improved FFN (IFFN), Arbitrary GeLU (AGeLU), Computer Vision, Image Classification, Object Detection, Semantic Segmentation, Video Analysis, Computational cost reduction, Limited training data performance, Enhanced non-linearity, Reduced FLOPs and parameters, Maintained classification accuracy, ImageNet


- [BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model](https://icml.cc/virtual/2024/poster/32967) (Poster)
  - **Authors:** [Chenwei Xu](http://openreview.net/profile?id=~Chenwei_Xu2), [Yu-Chao Huang](http://openreview.net/profile?id=~Yu-Chao_Huang1), [Jerry Yao-Chieh Hu](http://openreview.net/profile?id=~Jerry_Yao-Chieh_Hu1), [Weijian Li](http://openreview.net/profile?id=~Weijian_Li2), [Ammar Gilani](http://openreview.net/profile?id=~Ammar_Gilani1), [Hsi-Sheng Goan](http://openreview.net/profile?id=~Hsi-Sheng_Goan1), [Han Liu](http://openreview.net/profile?id=~Han_Liu4)
  - **Affiliations:** Department of Computer Science, Northwestern University, Evanston, IL, USA, Department of Physics and Center for Theoretical Physics, National Taiwan University, Taipei, Taiwan, Department of Computer Science, Northwestern University, Evanston, IL, USA, Department of Computer Science, Northwestern University, Evanston, IL, USA, Department of Computer Science, Northwestern University, Evanston, IL, USA, Department of Physics and Center for Theoretical Physics, National Taiwan University, Taipei, Taiwan; Center for Quantum Science and Engineering, National Taiwan University, Taipei, Taiwan; Physics Division, National Center for Theoretical Sciences, Taipei, Taiwan, Department of Computer Science, Northwestern University, Evanston, IL, USA; Department of Statistics and Data Science, Northwestern University, Evanston, IL, USA
  - **TL;DR:** The study introduces BiSHop, a novel framework for deep tabular learning that addresses challenges such as non-rotationally invariant data structures and feature sparsity through a dual-component design. Empirical results demonstrate that BiSHop outperforms current state-of-the-art methods with fewer hyperparameter optimization runs, indicating its robustness for tabular data analysis.
  - **Keywords:** deep tabular learning, associative memory, Bi-Directional Sparse Hopfield Network, generalized sparse modern Hopfield model, tabular data analysis, non-rotationally invariant data structure, feature sparsity, multi-scale representation learning, adaptive sparsity


- [Meta-Reinforcement Learning Robust to Distributional Shift Via Performing Lifelong In-Context Learning](https://icml.cc/virtual/2024/poster/33209) (Poster)
  - **Authors:** [TengYe Xu](http://openreview.net/profile?id=~Tengye_Xu1), [Zihao Li](http://openreview.net/profile?id=~Zihao_Li7), [Qinyuan Ren](http://openreview.net/profile?id=~Qinyuan_Ren1)
  - **Affiliations:** College of Control Science and Engineering, Zhejiang University, 38 Zheda Road, Hangzhou, Zhejiang, China, College of Control Science and Engineering, Zhejiang University, 38 Zheda Road, Hangzhou, Zhejiang, China, College of Control Science and Engineering, Zhejiang University, 38 Zheda Road, Hangzhou, Zhejiang, China
  - **TL;DR:** This paper introduces Posterior Sampling Bayesian Lifelong In-Context Reinforcement Learning (PSBL), a method designed to enhance the robustness of meta-RL against task distribution shifts. Experimental results show that PSBL significantly outperforms standard meta-RL methods in both sparse and dense reward tasks when faced with out-of-distribution challenges.
  - **Keywords:** Meta-Reinforcement Learning, Generalization, Task Distribution Shift, Posterior Sampling, Bayesian Lifelong In-Context Learning, Transformer, Reinforcement Learning, Overfitting, Lack of Generalization, Out-of-Distribution Tasks, Predictive Posterior Distribution, Online Exploration, In-Context Learning


- [Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation](https://icml.cc/virtual/2024/poster/34994) (Poster)
  - **Authors:** [Haoran Xu](http://openreview.net/profile?id=~Haoran_Xu3), [Amr Sharaf](http://openreview.net/profile?id=~Amr_Sharaf1), [Yunmo Chen](http://openreview.net/profile?id=~Yunmo_Chen1), [Weiting Tan](http://openreview.net/profile?id=~Weiting_Tan1), [Lingfeng Shen](http://openreview.net/profile?id=~Lingfeng_Shen1), [Benjamin Van Durme](http://openreview.net/profile?id=~Benjamin_Van_Durme2), [Kenton Murray](http://openreview.net/profile?id=~Kenton_Murray1), [Young Jin Kim](http://openreview.net/profile?id=~Young_Jin_Kim1)
  - **Affiliations:** Johns Hopkins University, Microsoft, Johns Hopkins University, Johns Hopkins University, Johns Hopkins University, Johns Hopkins University, Johns Hopkins University, Microsoft
  - **TL;DR:** This study introduces Contrastive Preference Optimization (CPO) to enhance the translation performance of moderate-sized large language models (LLMs), specifically the ALMA model. The resulting ALMA-R model achieves performance that matches or exceeds state-of-the-art translation models, including GPT-4, on multiple test datasets.
  - **Keywords:** machine translation, large language models, Contrastive Preference Optimization (CPO), supervised fine-tuning (SFT), performance gap in translation, quality issues in reference data, ALMA-R model, significant improvements in translation performance, 22K parallel sentences, moderate-sized LLMs, encoder-decoder architectures, decoder-only LLMs


- [Intersectional Unfairness Discovery](https://icml.cc/virtual/2024/poster/34525) (Poster)
  - **Authors:** [Gezheng Xu](http://openreview.net/profile?id=~Gezheng_Xu2), [Qi CHEN](http://openreview.net/profile?id=~Qi_CHEN6), [Charles X. Ling](http://openreview.net/profile?id=~Charles_Ling1), [Boyu Wang](http://openreview.net/profile?id=~Boyu_Wang3), [Changjian Shui](http://openreview.net/profile?id=~Changjian_Shui2)
  - **Affiliations:** Department of Computer Science, University of Western Ontario, University of Toronto, Department of Computer Science, University of Western Ontario; Vector Institute, Department of Computer Science, University of Western Ontario; Vector Institute, Vector Institute
  - **TL;DR:** This paper proposes a Bias-Guided Generative Network (BGGN) to discover diverse high-bias subgroups under intersectional sensitive attributes, addressing the limitations of current fairness research that often focuses on single sensitive attributes. The findings reveal significant potential unfairness in AI systems, emphasizing the need for a more comprehensive understanding of intersectional bias.
  - **Keywords:** intersectional fairness, bias discovery, Bias-Guided Generative Network (BGGN), healthcare, autonomous driving, education, prediction discrimination, intersectional unfairness, efficient discovery of high-bias subgroups, real-world text and image datasets


- [Robust Inverse Constrained Reinforcement Learning under Model Misspecification](https://icml.cc/virtual/2024/poster/33029) (Poster)
  - **Authors:** [Sheng Xu](http://openreview.net/profile?id=~Sheng_Xu8), [Guiliang Liu](http://openreview.net/profile?id=~Guiliang_Liu1)
  - **Affiliations:** School of Data Science, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China, School of Data Science, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China
  - **TL;DR:** This study addresses the challenges of ensuring safe control in reinforcement learning by proposing a Robust Constraint Inference problem and an Adaptively Robust ICRL algorithm, which accounts for discrepancies between training and deployment environments. The findings highlight the importance of robust constraints in preventing unsafe agent behaviors in real-world applications.
  - **Keywords:** Inverse Constrained Reinforcement Learning, safety-critical decision-making, Robust Constraint Inference, Adaptively Robust ICRL, Safety-Robust Policy Optimization, autonomous driving, robot control, model misspecification, discrepancy between training and deployment environments, ensuring safe control, robust policy, effective constraint inference, Markov Game, adversarial attacks


- [Aligned Objective for Soft-Pseudo-Label Generation in Supervised Learning](https://icml.cc/virtual/2024/poster/35125) (Poster)
  - **Authors:** [Ning Xu](http://openreview.net/profile?id=~Ning_Xu5), [Yihao Hu](http://openreview.net/profile?id=~Yihao_Hu2), [Congyu Qiao](http://openreview.net/profile?id=~Congyu_Qiao3), [Xin Geng](http://openreview.net/profile?id=~Xin_Geng1)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China
  - **TL;DR:** This study proposes a novel framework for generating soft pseudo-labels in supervised learning, addressing the misalignment in existing methods and optimizing the generation process through a meta-network-parameterized objective. The framework demonstrates improved performance of deep neural networks across various tasks, validated through experiments on benchmark datasets.
  - **Keywords:** soft pseudo-labels, supervised learning, deep neural networks, softmax predictions, label enhancement, self-knowledge distillation, online distillation, Graph Convolutional Network (GCN), computer vision, natural language processing, data mining, misalignment in soft-pseudo-label generation, lack of ground-truth soft labels, noisy labels, novel framework for soft-pseudo-label generation, optimization of objective function, benchmark datasets


- [Soft Prompt Recovers Compressed LLMs, Transferably](https://icml.cc/virtual/2024/poster/33150) (Poster)
  - **Authors:** [Zhaozhuo Xu](http://openreview.net/profile?id=~Zhaozhuo_Xu1), [Zirui Liu](http://openreview.net/profile?id=~Zirui_Liu1), [Beidi Chen](http://openreview.net/profile?id=~Beidi_Chen1), [Shaochen (Henry) Zhong](http://openreview.net/profile?id=~Shaochen_Zhong1), [Yuxin Tang](http://openreview.net/profile?id=~Yuxin_Tang2), [Jue Wang](http://openreview.net/profile?id=~Jue_WANG1), [Kaixiong Zhou](http://openreview.net/profile?id=~Kaixiong_Zhou1), [Xia Hu](http://openreview.net/profile?id=~Xia_Hu4), [Anshumali Shrivastava](http://openreview.net/profile?id=~Anshumali_Shrivastava1)
  - **Affiliations:** Department of Computer Science, Stevens Institute of Technology, Department of Computer Science, Rice University, Department of Electrical and Computer Engineering, Carnegie Mellon University, Department of Computer Science, Rice University, Department of Computer Science, Rice University, Together AI, Department of Electrical and Computer Engineering, North Carolina State University, Department of Computer Science, Rice University, ThirdAI Corp.
  - **TL;DR:** This study explores the use of soft prompt tuning to recover the performance of compressed large language models, demonstrating that such prompts can be transferable across different tasks and models. The findings suggest a new approach to enhance model accessibility while reducing engineering burdens associated with model compression techniques.
  - **Keywords:** Model Compression, Large Language Models (LLMs), Soft Prompt Tuning, Parameter-Efficient Fine-Tuning (PEFT), Quantization, Pruning, Natural Language Processing (NLP), Performance decline due to model compression, trade-off between accuracy and efficiency, Recovery of performance in compressed LLMs, transferability of soft prompts across tasks and models


- [Provably Efficient Reinforcement Learning for Adversarial Restless Multi-Armed Bandits with Unknown Transitions and Bandit Feedback](https://icml.cc/virtual/2024/poster/32984) (Poster)
  - **Authors:** [GUOJUN XIONG](http://openreview.net/profile?id=~GUOJUN_XIONG1), [Jian Li](http://openreview.net/profile?id=~Jian_Li14)
  - **Affiliations:** Stony Brook University, Stony Brook University
  - **TL;DR:** This paper presents a novel reinforcement learning algorithm, UCMD-ARMAB, for learning in adversarial restless multi-armed bandits with unknown transitions and bandit feedback, aiming to maximize total adversarial rewards while adhering to an instantaneous activation constraint. The algorithm achieves a regret bound of ˜O(H√T), marking a significant advancement in the field.
  - **Keywords:** Restless Multi-Armed Bandits, Adversarial Learning, Sequential Decision Making, Reinforcement Learning, Biased Adversarial Reward Estimator, Low-Complexity Index Policy, Wireless Scheduling, Resource Allocation, Healthcare, Unknown Transitions, Adversarial Rewards, Instantaneous Activation Constraint, Bandit Feedback, Regret Bound, UCMD-ARMAB Algorithm, Markov Decision Process (MDP), Adversarial RMAB


- [Stochastic Bandits with ReLU Neural Networks](https://icml.cc/virtual/2024/poster/35093) (Poster)
  - **Authors:** [Kan Xu](http://openreview.net/profile?id=~Kan_Xu2), [Hamsa Bastani](http://openreview.net/profile?id=~Hamsa_Bastani1), [Surbhi Goel](http://openreview.net/profile?id=~Surbhi_Goel1), [Osbert Bastani](http://openreview.net/profile?id=~Osbert_Bastani1)
  - **Affiliations:** Arizona State University, Arizona, USA, University of Pennsylvania, Pennsylvania, USA, University of Pennsylvania, Pennsylvania, USA, University of Pennsylvania, Pennsylvania, USA
  - **TL;DR:** This study investigates the stochastic bandit problem using one-layer ReLU neural networks and introduces the OFU-ReLU algorithm, achieving a regret guarantee of ˜O(√T). The findings highlight the potential of leveraging the piecewise linear structure of ReLU activations to improve exploration and exploitation strategies in bandit settings.
  - **Keywords:** Stochastic bandits, ReLU neural networks, OFU-ReLU algorithm, UCB-type linear bandit algorithm, Healthcare, personalized recommendation, Regret minimization, exploration vs. exploitation, Regret guarantee of ˜O(√T), parameter learning in ReLU networks


- [Improving SAM Requires Rethinking its Optimization Formulation](https://icml.cc/virtual/2024/poster/33271) (Poster)
  - **Authors:** [Wanyun Xie](http://openreview.net/profile?id=~Wanyun_Xie1), [Fabian Latorre](http://openreview.net/profile?id=~Fabian_Latorre1), [Kimon Antonakopoulos](http://openreview.net/profile?id=~Kimon_Antonakopoulos1), [Thomas Pethick](http://openreview.net/profile?id=~Thomas_Pethick1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1)
  - **Affiliations:** Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland, Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland, Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland, Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland, Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland
  - **TL;DR:** This paper proposes a novel bilevel optimization formulation called BiSAM to improve Sharpness-Aware Minimization (SAM) by directly addressing misclassification error using the 0-1 loss. The results demonstrate that BiSAM consistently outperforms the original SAM while maintaining similar computational complexity.
  - **Keywords:** Sharpness-Aware Minimization (SAM), optimization methods, generalization in deep learning, bilevel optimization, surrogate loss, 0-1 loss, supervised learning, computer vision, large language models, misclassification error, optimization of loss landscape, BiSAM formulation, improved perturbation construction


- [Adaptive Group Personalization for Federated Mutual Transfer Learning](https://icml.cc/virtual/2024/poster/34610) (Poster)
  - **Authors:** [Haoqing Xu](http://openreview.net/profile?id=~Haoqing_Xu1), [Dian Shen](http://openreview.net/profile?id=~Dian_Shen1), [Meng Wang](http://openreview.net/profile?id=~Meng_Wang11), [Beilun Wang](http://openreview.net/profile?id=~Beilun_Wang1)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China, College of Design and Innovation, Tongji University, Shanghai, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China
  - **TL;DR:** This paper introduces the Adaptive Group Personalization method (AdaGrP) to enhance mutual transfer learning in federated settings, addressing challenges like communication bottlenecks and learnability heterogeneity. Empirical results demonstrate a 16.9% improvement in learnability structure recovery compared to existing clustered federated learning methods.
  - **Keywords:** Mutual transfer learning, Federated learning, Adaptive Group Personalization (AdaGrP), Non-parametric method, Adaptive threshold correction, Climate analysis, Healthcare analysis, Longitudinal data analyses, Communication bottleneck, Privacy concerns, Learnability heterogeneity, Learnability structure recovery, Improvement in prediction performance, Two-layer linear mixed-effects model, Learnability structure


- [Learning Exceptional Subgroups by End-to-End Maximizing KL-Divergence](https://icml.cc/virtual/2024/poster/34774) (Spotlight Poster)
  - **Authors:** [Sascha Xu](http://openreview.net/profile?id=~Sascha_Xu1), [Nils Philipp Walter](http://openreview.net/profile?id=~Nils_Philipp_Walter1), [Janis Kalofolias](http://openreview.net/profile?id=~Janis_Kalofolias1), [Jilles Vreeken](http://openreview.net/profile?id=~Jilles_Vreeken2)
  - **Affiliations:** CISPA Helmholtz Center for Information Security, Saarbrücken, Germany, CISPA Helmholtz Center for Information Security, Saarbrücken, Germany, CISPA Helmholtz Center for Information Security, Saarbrücken, Germany, CISPA Helmholtz Center for Information Security, Saarbrücken, Germany
  - **TL;DR:** The study introduces SYFLOW, an end-to-end optimizable approach for discovering exceptional subgroups in data by leveraging normalizing flows and KL-divergence. It effectively addresses limitations of existing methods, providing interpretable subgroup descriptions and demonstrating reliable performance on both synthetic and real-world datasets.
  - **Keywords:** subgroup discovery, exceptional sub-populations, descriptive modeling, normalizing flows, KL-divergence, neuro-symbolic rule layer, demographic analysis, material property identification, combinatorial optimization limitations, non-trivial target distributions, scalability issues, SYFLOW method, end-to-end optimization, interpretable subgroup descriptions, synthetic data, real-world data


- [Libra: Building Decoupled Vision System on Large Language Models](https://icml.cc/virtual/2024/poster/34554) (Poster)
  - **Authors:** [Yifan Xu](http://openreview.net/profile?id=~Yifan_Xu9), [Xiaoshan Yang](http://openreview.net/profile?id=~Xiaoshan_Yang2), [Yaguang Song](http://openreview.net/profile?id=~Yaguang_Song1), [Changsheng Xu](http://openreview.net/profile?id=~Changsheng_Xu1)
  - **Affiliations:** MAIS, Institute of Automation, Chinese Academy of Sciences; Peng Cheng Laboratory; School of Artificial Intelligence, University of the Chinese Academy of Sciences, MAIS, Institute of Automation, Chinese Academy of Sciences; Peng Cheng Laboratory; School of Artificial Intelligence, University of the Chinese Academy of Sciences, Peng Cheng Laboratory, MAIS, Institute of Automation, Chinese Academy of Sciences; Peng Cheng Laboratory; School of Artificial Intelligence, University of the Chinese Academy of Sciences
  - **TL;DR:** This study presents Libra, a prototype model that implements a decoupled vision system on a large language model, achieving effective cross-modal comprehension and strong performance in image-to-text tasks with limited training data. The findings suggest a new approach for future multimodal foundation models by addressing the challenges of integrating visual and language data.
  - **Keywords:** Decoupled vision system, Multimodal large language models (MLLMs), Discrete auto-regressive modeling, Routed visual expert, Cross-modal bridge module, Image-to-text scenarios, Multimodal conversation, Interactive agents, Autonomous driving, Information imbalance between visual and language data, Weak vision system due to reliance on pretrained vision encoders, Strong MLLM baseline, New perspective for multimodal foundation models


- [Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble](https://icml.cc/virtual/2024/poster/34441) (Poster)
  - **Authors:** [Chenhui Xu](http://openreview.net/profile?id=~Chenhui_Xu1), [Fuxun Yu](http://openreview.net/profile?id=~Fuxun_Yu1), [Zirui Xu](http://openreview.net/profile?id=~Zirui_Xu1), [Nathan Inkawhich](http://openreview.net/profile?id=~Nathan_Inkawhich1), [Xiang Chen](http://openreview.net/profile?id=~Xiang_Chen1)
  - **Affiliations:** George Mason University, George Mason University, George Mason University, Air Force Research Laboratory, George Mason University; Air Force Research Laboratory
  - **TL;DR:** This study introduces the Multi-Comprehension Ensemble method to improve Out-of-Distribution detection by enhancing feature representation diversity through distinct supervision tasks. Experimental results show that this approach outperforms traditional Deep Ensemble methods and standalone models in OOD detection tasks.
  - **Keywords:** Out-of-Distribution detection, model ensemble, feature representation, Multi-Comprehension Ensemble, Loss Basin/Barrier Visualization, Self-Coupling Index, Model overconfidence, OOD sample detection, generalization challenges, Enhanced OOD detection performance, diverse feature representation, Deep Ensembles, intrinsic mode connectivity


- [Pricing with Contextual Elasticity and Heteroscedastic Valuation](https://icml.cc/virtual/2024/poster/34995) (Spotlight Poster)
  - **Authors:** [Jianyu Xu](http://openreview.net/profile?id=~Jianyu_Xu1), [Yu-Xiang Wang](http://openreview.net/profile?id=~Yu-Xiang_Wang1)
  - **Affiliations:** University of California, Santa Barbara, University of California, San Diego
  - **TL;DR:** This study addresses the online contextual dynamic pricing problem by introducing a novel approach that incorporates feature-based price elasticity, leading to the development of an efficient algorithm called "Pricing with Perturbation." The findings reveal optimal pricing strategies that minimize cumulative regret while adapting to varying customer contexts.
  - **Keywords:** contextual pricing, dynamic pricing, feature-based pricing, Pricing with Perturbation (PwP), price elasticity modeling, e-commerce, online retail, demand prediction, pricing strategy optimization, cumulative regret minimization, computationally efficient algorithms, insights into pricing strategies, heteroscedastic noise, linear demand model, linear valuation model


- [SLOG: An Inductive Spectral Graph Neural Network Beyond Polynomial Filter](https://icml.cc/virtual/2024/poster/35200) (Poster)
  - **Authors:** [Haobo Xu](http://openreview.net/profile?id=~Haobo_Xu2), [Yuchen Yan](http://openreview.net/profile?id=~Yuchen_Yan1), [Dingsu Wang](http://openreview.net/profile?id=~Dingsu_Wang1), [Zhe Xu](http://openreview.net/profile?id=~Zhe_Xu5), [Zhichen Zeng](http://openreview.net/profile?id=~Zhichen_Zeng1), [Tarek Abdelzaher](http://openreview.net/profile?id=~Tarek_F._Abdelzaher1), [Jiawei Han](http://openreview.net/profile?id=~Jiawei_Han1), [Hanghang Tong](http://openreview.net/profile?id=~Hanghang_Tong3)
  - **Affiliations:** Tsinghua University, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign
  - **TL;DR:** This paper introduces SLOG, a novel spectral graph neural network that addresses the polynomial and transductive limitations of existing spectral GNNs, enabling effective inductive learning on large-scale graphs. Experimental results demonstrate SLOG's superiority in homophilic and heterophilic node classification tasks across multiple datasets.
  - **Keywords:** Graph Neural Networks (GNNs), Inductive Learning, Spectral Graph Neural Networks, Subgraph Sampling, Polynomial Filters, Node Classification, Link Prediction, Network Alignment, Node Clustering, Knowledge Graph Reasoning, Polynomial Limitation, Transductive Limitation, Novel Spectral Graph Neural Network (SLOG), Adaptive Filtering, 16 datasets (specific names not provided)


- [Balancing Similarity and Complementarity for Federated Learning](https://icml.cc/virtual/2024/poster/32796) (Poster)
  - **Authors:** [Kunda Yan](http://openreview.net/profile?id=~Kunda_Yan1), [Sen Cui](http://openreview.net/profile?id=~Sen_Cui1), [Abudukelimu Wuerkaixi](http://openreview.net/profile?id=~Abudukelimu_Wuerkaixi1), [Jingfeng ZHANG](http://openreview.net/profile?id=~Jingfeng_Zhang1), [Bo Han](http://openreview.net/profile?id=~Bo_Han1), [Gang Niu](http://openreview.net/profile?id=~Gang_Niu1), [Masashi Sugiyama](http://openreview.net/profile?id=~Masashi_Sugiyama1), [Changshui Zhang](http://openreview.net/profile?id=~Changshui_Zhang2)
  - **Affiliations:** Institute for Artificial Intelligence, Tsinghua University (THUAI); Beijing National Research Center for Information Science and Technology (BN-Rist); Department of Automation, Tsinghua University, Beijing, P.R. China, Institute for Artificial Intelligence, Tsinghua University (THUAI); Beijing National Research Center for Information Science and Technology (BN-Rist); Department of Automation, Tsinghua University, Beijing, P.R. China, Institute for Artificial Intelligence, Tsinghua University (THUAI); Beijing National Research Center for Information Science and Technology (BN-Rist); Department of Automation, Tsinghua University, Beijing, P.R. China, The University of Auckland; RIKEN, Hong Kong Baptist University; RIKEN, RIKEN, RIKEN; The University of Tokyo, Institute for Artificial Intelligence, Tsinghua University (THUAI); Beijing National Research Center for Information Science and Technology (BN-Rist); Department of Automation, Tsinghua University, Beijing, P.R. China
  - **TL;DR:** This study introduces the FedSaC framework for Federated Learning, which optimally balances similarity and complementarity among clients to address statistical heterogeneity. The findings demonstrate that leveraging complementary data can significantly enhance model performance compared to traditional methods focused solely on client similarity.
  - **Keywords:** Federated Learning, Statistical Heterogeneity, Privacy Preservation, FedSaC framework, Model Similarity Metrics, Mobile Systems, IoT Systems, Multimodal Learning, Non-i.i.d. data, Data Distribution Variances, Collaboration Challenges, Enhanced Cooperation Network, Improved Model Performance, Complementarity in Feature Distribution


- [Learning 1-Bit Tiny Object Detector with Discriminative Feature Refinement](https://icml.cc/virtual/2024/poster/34111) (Poster)
  - **Authors:** [Sheng Xu](http://openreview.net/profile?id=~Sheng_Xu4), [Mingze Wang](http://openreview.net/profile?id=~Mingze_Wang3), [Yanjing Li](http://openreview.net/profile?id=~Yanjing_Li2), [Mingbao Lin](http://openreview.net/profile?id=~Mingbao_Lin1), [Baochang Zhang](http://openreview.net/profile?id=~Baochang_Zhang1), [David Doermann](http://openreview.net/profile?id=~David_Doermann2), [Xiao Sun](http://openreview.net/profile?id=~Xiao_Sun8)
  - **Affiliations:** Beihang University, Beihang University, Beihang University, Skywork AI, Beihang University; Zhongguancun Laboratory, University at Buffalo, Shanghai Artificial Intelligence Laboratory
  - **TL;DR:** This study introduces a Discriminative Feature Refinement method for 1-bit detectors to improve the detection of tiny objects in aerial images. The proposed method significantly enhances the discriminative ability of foreground representations, achieving performance close to real-valued counterparts on the AI-TOD dataset.
  - **Keywords:** Tiny Object Detection (TOD), 1-Bit Detectors, Discriminative Feature Refinement (DFR), Information Bottleneck (IB), Convolutional Neural Networks (CNN), Aerial Images, Real-Time Object Detection, Performance Degradation on Tiny Objects, Foreground Information Refinement, Enhanced Discriminative Ability, New Decoder with Foreground Mask, AI-TOD, DOTA


- [Offline Multi-Objective Optimization](https://icml.cc/virtual/2024/poster/35078) (Poster)
  - **Authors:** [Ke Xue](http://openreview.net/profile?id=~Ke_Xue1), [Rong-Xi Tan](http://openreview.net/profile?id=~Rongxi_Tan1), [Xiaobin Huang](http://openreview.net/profile?id=~Xiaobin_Huang2), [Chao Qian](http://openreview.net/profile?id=~Chao_Qian1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This paper introduces a benchmark for offline multi-objective optimization (MOO) to address the challenges of optimizing multiple conflicting objectives using static datasets. The authors analyze existing methods and demonstrate improvements in offline MOO effectiveness, highlighting the need for further advancements in this area.
  - **Keywords:** Offline optimization, Multi-objective optimization (MOO), Deep neural networks (DNN), Surrogate models, Engineering design, Protein design, Molecule design, Black-box objective function, Costly evaluations, Out-of-distribution problems, Non-smooth landscape, Benchmark for offline MOO, Method comparisons, Improvements over training set values


- [FairProof : Confidential and Certifiable Fairness for Neural Networks](https://icml.cc/virtual/2024/poster/34587) (Poster)
  - **Authors:** [Chhavi Yadav](http://openreview.net/profile?id=~Chhavi_Yadav1), [Amrita Roy Chowdhury](http://openreview.net/profile?id=~Amrita_Roy_Chowdhury1), [Dan Boneh](http://openreview.net/profile?id=~Dan_Boneh1), [Kamalika Chaudhuri](http://openreview.net/profile?id=~Kamalika_Chaudhuri1)
  - **Affiliations:** University of California, San Diego, University of California, San Diego, Stanford University, University of California, San Diego
  - **TL;DR:** The study introduces FairProof, a system that utilizes Zero-Knowledge Proofs to enable public verification of the fairness of machine learning models while maintaining confidentiality. It addresses the challenges of legal and privacy concerns, providing a certification algorithm that ensures model uniformity and individual fairness.
  - **Keywords:** Fairness in machine learning, Confidentiality in model verification, Zero-Knowledge Proofs, Fairness certification algorithm, Societal applications of machine learning, Legal and privacy concerns, Model uniformity, Distrust in ML predictions, Fairness certification, Public verification of model fairness, Individual Fairness (IF), Cryptographic protocols


- [Few-shot Adaptation to Distribution Shifts By Mixing Source and Target Embeddings](https://icml.cc/virtual/2024/poster/33501) (Poster)
  - **Authors:** [Yihao Xue](http://openreview.net/profile?id=~Yihao_Xue1), [Ali Payani](http://openreview.net/profile?id=~Ali_Payani1), [Yu Yang](http://openreview.net/profile?id=~Yu_Yang4), [Baharan Mirzasoleiman](http://openreview.net/profile?id=~Baharan_Mirzasoleiman1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, Cisco Systems Inc., Department of Computer Science, University of California, Los Angeles, Department of Computer Science, University of California, Los Angeles
  - **TL;DR:** This study introduces MixPro, a lightweight method for few-shot adaptation of machine learning models to new target distributions by mixing source and target embeddings. The approach demonstrates significant performance improvements, achieving up to 7% better results with only 2-4 target examples across various datasets.
  - **Keywords:** few-shot adaptation, distribution shifts, data efficiency, MixPro, linear combination of embeddings, linear classifier, autonomous vehicles, medical diagnosis, data sparsity, model overfitting, generalization, new adaptation method, performance improvement


- [A Space Group Symmetry Informed Network for O(3) Equivariant Crystal Tensor Prediction](https://icml.cc/virtual/2024/poster/34727) (Poster)
  - **Authors:** [Keqiang Yan](http://openreview.net/profile?id=~Keqiang_Yan2), [Alexandra Saxton](http://openreview.net/profile?id=~Alexandra_Saxton1), [Xiaofeng Qian](http://openreview.net/profile?id=~Xiaofeng_Qian1), [Xiaoning Qian](http://openreview.net/profile?id=~Xiaoning_Qian2), [Shuiwang Ji](http://openreview.net/profile?id=~Shuiwang_Ji1)
  - **Affiliations:** Texas A&M University, College Station, TX 77843, USA, Texas A&M University, College Station, TX 77843, USA, Texas A&M University, College Station, TX 77843, USA, Texas A&M University, College Station, TX 77843, USA, Texas A&M University, College Station, TX 77843, USA
  - **TL;DR:** This study presents a General Materials Tensor Network (GMTNet) designed to predict tensor properties of crystalline materials while ensuring compliance with O(3) equivariance and crystal space group invariance. The experimental results demonstrate that GMTNet effectively captures the intrinsic symmetries of crystals and achieves promising performance across various tensor orders.
  - **Keywords:** crystal tensor prediction, tensor properties of crystalline materials, General Materials Tensor Network (GMTNet), machine learning techniques, dielectric materials, piezoelectric materials, elastic materials, tensor equivariance, invariance to crystal space groups, resource-intensive DFT methods, promising performance on crystal tensors, evaluation metrics for crystal tensor predictions, AIRS library, O(3) equivariance, crystal symmetries, anisotropy


- [Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning](https://icml.cc/virtual/2024/poster/33900) (Poster)
  - **Authors:** [Jing Xu](http://openreview.net/profile?id=~Jing_Xu4), [Jingzhao Zhang](http://openreview.net/profile?id=~Jingzhao_Zhang2)
  - **Affiliations:** Institute for Interdisciplinary Information Sciences, Tsinghua University, China; Shanghai Qizhi Institute; Shanghai AI Laboratory, Institute for Interdisciplinary Information Sciences, Tsinghua University, China; Shanghai Qizhi Institute; Shanghai AI Laboratory
  - **TL;DR:** This study explores the limits of parameter-efficient fine-tuning (PEFT) for large language models by introducing a simple yet effective method called Random Masking, which achieves competitive performance with fewer trainable parameters. The findings suggest that Random Masking can match the performance of established PEFT methods like LoRA, highlighting the potential for more efficient fine-tuning strategies.
  - **Keywords:** Parameter-efficient fine-tuning, Large language models, Random Masking, LoRA, PEFT algorithms, Natural language processing, Computer vision, Costly fine-tuning, Computational and memory demands, Effective fine-tuning methods, Flatter loss landscape, Large learning rates, OPT-1.3b, 11 datasets


- [Exponential Spectral Pursuit: An Effective Initialization Method for Sparse Phase Retrieval](https://icml.cc/virtual/2024/poster/33955) (Poster)
  - **Authors:** [Mengchu Xu](http://openreview.net/profile?id=~Mengchu_Xu1), [Zhang Yuxuan](http://openreview.net/profile?id=~Yuxuan_Zhang10), [Jian Wang](http://openreview.net/profile?id=~Jian_Wang14)
  - **Affiliations:** School of Data Science, Fudan University, Shanghai, China, School of Data Science, Fudan University, Shanghai, China, School of Data Science, Fudan University, Shanghai, China
  - **TL;DR:** This paper introduces a novel method called exponential spectral pursuit (ESP) to enhance the initialization stage in sparse phase retrieval, significantly improving sampling complexity. The proposed method outperforms existing techniques and provides a tighter theoretical bound on sampling requirements.
  - **Keywords:** Sparse phase retrieval, signal reconstruction, Exponential spectral pursuit (ESP), truncated power method, Astronomical imaging, electron microscopy, Sampling complexity, under-determined systems, Improved sampling complexity for initialization, k-sparse signal, phaseless measurements


- [Exploring the LLM Journey from Cognition to Expression with Linear Representations](https://icml.cc/virtual/2024/poster/33844) (Poster)
  - **Authors:** [Yuzi Yan](http://openreview.net/profile?id=~Yuzi_Yan1), [Jialian Li](http://openreview.net/profile?id=~Jialian_Li2), [YipinZhang](http://openreview.net/profile?id=~YipinZhang1), [Dong Yan](http://openreview.net/profile?id=~Dong_Yan1)
  - **Affiliations:** Baichuan AI; Tsinghua University, Baichuan AI, Baichuan AI, Baichuan AI
  - **TL;DR:** This study investigates the cognitive and expressive capabilities of large language models, particularly Baichuan-7B and Baichuan-33B, revealing that cognitive abilities are primarily developed during pretraining, while expressive abilities are enhanced through supervised fine-tuning and reinforcement learning. The findings suggest a significant correlation between cognitive capacity and expressive potential, highlighting the importance of understanding these dynamics for improving model performance.
  - **Keywords:** Large Language Models, Cognitive and Expressive Capabilities, Pretraining, Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), Linear Representations, Discrepancy between cognitive capacity and expressive potential, Insights into interpretability and controllability of training processes, Alignment Tax, Representation Engineering (RepE)


- [Position: Towards Implicit Prompt For Text-To-Image Models](https://icml.cc/virtual/2024/poster/34476) (Poster)
  - **Authors:** [Yue Yang](http://openreview.net/profile?id=~Yue_Yang6), [Yuqi Lin](http://openreview.net/profile?id=~Yuqi_Lin1), [Hong Liu](http://openreview.net/profile?id=~Hong_Liu9), [WENQI SHAO](http://openreview.net/profile?id=~Wenqi_Shao2), [Runjian Chen](http://openreview.net/profile?id=~Runjian_Chen1), [Hailong Shang](http://openreview.net/profile?id=~Hailong_Shang1), [Yu Wang](http://openreview.net/profile?id=~Yu_Wang40), [Yu Qiao](http://openreview.net/profile?id=~Yu_Qiao1), [Kaipeng Zhang](http://openreview.net/profile?id=~Kaipeng_Zhang1), [Ping Luo](http://openreview.net/profile?id=~Ping_Luo2)
  - **Affiliations:** Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China, Osaka University, Osaka, Japan, Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China; The University of Hong Kong, Hong Kong, China, Research Institute of Tsinghua University in Shenzhen, Shenzhen, China, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China; The University of Hong Kong, Hong Kong, China
  - **TL;DR:** This position paper investigates the implications of implicit prompts in text-to-image (T2I) models, revealing that they can bypass safety constraints and pose risks such as privacy leakage. The authors present a benchmark called ImplicitBench to evaluate the performance of popular T2I models under these prompts, advocating for a balanced approach to harness their benefits while mitigating risks.
  - **Keywords:** Text-to-image generation, Implicit prompts, AI-generated content, Image generation, Safety constraints, Privacy leakage, NSFW content, ImplicitBench benchmark, Evaluation of T2I models, T2I models, Implicit prompts, NSFW Issues, Celebrity Privacy


- [Foundations of Testing for Finite-Sample Causal Discovery](https://icml.cc/virtual/2024/poster/33081) (Poster)
  - **Authors:** [Tom Yan](http://openreview.net/profile?id=~Tom_Yan1), [Ziyu Xu](http://openreview.net/profile?id=~Ziyu_Xu2), [Zachary Lipton](http://openreview.net/profile?id=~Zachary_Chase_Lipton1)
  - **Affiliations:** Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA, Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, USA, Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA
  - **TL;DR:** This study addresses the challenges of finite-sample causal discovery by developing a novel testing framework that allows for structured multiple testing to determine edge orientations. The findings suggest that this framework can efficiently verify graph structures and improve the identification of causal relationships in real-world settings.
  - **Keywords:** Causal discovery, finite-sample settings, Structured multiple testing, testing framework, Meek rules, Natural sciences, social sciences, biology, economics, Stochasticity in data samples, edge orientation complexity, Novel testing framework, efficient graph structure verification, Causal DAG, observational distribution, graph skeleton, Provable guarantees, interventional data


- [Reducing Balancing Error for Causal Inference via Optimal Transport](https://icml.cc/virtual/2024/poster/34490) (Poster)
  - **Authors:** [Yuguang Yan](http://openreview.net/profile?id=~Yuguang_Yan1), [Hao Zhou](http://openreview.net/profile?id=~Hao_Zhou23), [Zeqin Yang](http://openreview.net/profile?id=~Zeqin_Yang1), [Weilin Chen](http://openreview.net/profile?id=~Weilin_Chen1), [Ruichu Cai](http://openreview.net/profile?id=~Ruichu_Cai1), [Zhifeng Hao](http://openreview.net/profile?id=~Zhifeng_Hao5)
  - **Affiliations:** School of Computer Science, Guangdong University of Technology, Guangzhou, China; None, School of Computer Science, Guangdong University of Technology, Guangzhou, China; None, School of Computer Science, Guangdong University of Technology, Guangzhou, China; None, School of Computer Science, Guangdong University of Technology, Guangzhou, China; None, School of Computer Science, Guangdong University of Technology, Guangzhou, China; Pazhou Laboratory (Huangpu), Guangzhou, China, College of Science, Shantou University, Shantou, China
  - **TL;DR:** This paper addresses confounding bias in causal inference by defining a balancing error and establishing a connection to the Wasserstein discrepancy through optimal transport. The proposed method effectively reduces balancing error using learnable weights and representations, as demonstrated through experiments on various datasets.
  - **Keywords:** Causal inference, confounding bias, distribution shift, Optimal transport, Wasserstein discrepancy, Observational data analysis, treatment effect estimation, Confounding bias, distribution balancing, Balancing error reduction, learnable marginal distributions, Synthetic datasets, real-world datasets


- [Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks](https://icml.cc/virtual/2024/poster/32662) (Poster)
  - **Authors:** [Wenhan Yang](http://openreview.net/profile?id=~Wenhan_Yang5), [Jingdong Gao](http://openreview.net/profile?id=~Jingdong_Gao1), [Baharan Mirzasoleiman](http://openreview.net/profile?id=~Baharan_Mirzasoleiman1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, Los Angeles, CA, 90024, Department of Computer Science, University of California, Los Angeles, Los Angeles, CA, 90024, Department of Computer Science, University of California, Los Angeles, Los Angeles, CA, 90024
  - **TL;DR:** This study introduces SAFECLIP, a robust defense mechanism for pre-training CLIP models against targeted data poisoning and backdoor attacks, demonstrating its effectiveness in reducing attack success rates to zero without compromising model performance. The proposed method involves a warm-up phase with unimodal contrastive learning and a strategic division of data into safe and risky sets.
  - **Keywords:** CLIP, data poisoning, backdoor attacks, pre-training, unimodal contrastive learning, Gaussian Mixture Model, image-caption datasets, zero-shot classification, vulnerability to targeted data poisoning, vulnerability to backdoor attacks, SAFECLIP, defense against data poisoning and backdoor attacks, CC3M, Visual Genome, MSCOCO


- [Probabilistic Time Series Modeling with Decomposable Denoising Diffusion Model](https://icml.cc/virtual/2024/poster/34729) (Poster)
  - **Authors:** [Tijin Yan](http://openreview.net/profile?id=~Tijin_Yan1), [Hengheng Gong](http://openreview.net/profile?id=~Hengheng_Gong1), [Yongping He](http://openreview.net/profile?id=~He_YongPing1), [Yufeng Zhan](http://openreview.net/profile?id=~Yufeng_Zhan1), [Yuanqing Xia](http://openreview.net/profile?id=~Yuanqing_Xia1)
  - **Affiliations:** School of Automation, Beijing Institute of Technology, Beijing, China, School of Automation, Beijing Institute of Technology, Beijing, China, School of Automation, Beijing Institute of Technology, Beijing, China, School of Automation, Beijing Institute of Technology, Beijing, China, Zhongyuan University of Technology, Zhengzhou, Henan, China
  - **TL;DR:** This study introduces the decomposable denoising diffusion model (D3M) for probabilistic time series modeling, addressing challenges in drift and diffusion coefficient determination and slow generation speeds. Experimental results demonstrate that D3M significantly improves performance on imputation tasks and achieves competitive results on forecasting tasks.
  - **Keywords:** Probabilistic time series modeling, generative models, Decomposable denoising diffusion model (D3M), stochastic differential equations (SDE), linear state space models (SSMs), high-order ODE solvers, Intelligent system monitoring, user behavior analysis, smart healthcare, Difficulty in determining drift and diffusion coefficients, slow generation speed of existing models, challenges in modeling local and global dependencies, Reduction in RMSE and CRPS, efficient probability paths for high generation speed, 8 real-world datasets


- [Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation](https://icml.cc/virtual/2024/poster/34414) (Poster)
  - **Authors:** [Yuchen Yang](http://openreview.net/profile?id=~Yuchen_Yang11), [Yingdong Shi](http://openreview.net/profile?id=~Yingdong_Shi1), [Cheems Wang](http://openreview.net/profile?id=~Cheems_Wang1), [Xiantong Zhen](http://openreview.net/profile?id=~Xiantong_Zhen1), [Yuxuan Shi](http://openreview.net/profile?id=~Yuxuan_Shi2), [Jun Xu](http://openreview.net/profile?id=~Jun_Xu3)
  - **Affiliations:** School of Statistics and Data Science, Nankai University, Tianjin, China, School of Information Science and Technology, ShanghaiTech University, Shanghai, China, Department of Automation, Tsinghua University, Peking, China, Central Research Institute, United Imaging Healthcare, Co., Ltd., School of Statistics and Data Science, Nankai University, Tianjin, China, School of Statistics and Data Science, Nankai University, Tianjin, China
  - **TL;DR:** This study proposes methods to reduce memory overhead during the fine-tuning of large pretrained models by introducing Approximate Backpropagation and Memory-Sharing Backpropagation strategies. The results show a potential reduction of up to 30% in peak memory usage without compromising training efficiency.
  - **Keywords:** Fine-tuning, Memory overhead reduction, Approximate Backpropagation (Approx-BP), Memory-Sharing Backpropagation, GELU, SiLU, ReLU, Pretrained models, Vision models, Language models, Memory overhead in fine-tuning, Activation memory usage, Memory-efficient alternatives for activation functions, Reduction of peak memory usage, CIFAR10, CIFAR100, ViT-B, Transformers, Parameter-efficient fine-tuning (PEFT)


- [Guidance with Spherical Gaussian Constraint for Conditional Diffusion](https://icml.cc/virtual/2024/poster/33898) (Poster)
  - **Authors:** [Lingxiao Yang](http://openreview.net/profile?id=~Lingxiao_Yang3), [Shutong Ding](http://openreview.net/profile?id=~Shutong_Ding1), [Yifan Cai](http://openreview.net/profile?id=~Yifan_Cai1), [Jingyi Yu](http://openreview.net/profile?id=~Jingyi_Yu5), [Jingya Wang](http://openreview.net/profile?id=~Jingya_Wang3), [Ye Shi](http://openreview.net/profile?id=~Ye_Shi1)
  - **Affiliations:** ShanghaiTech University; MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, ShanghaiTech University; MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, ShanghaiTech University; MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, ShanghaiTech University; MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, ShanghaiTech University; MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, ShanghaiTech University; MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration
  - **TL;DR:** This paper introduces a novel method called Diffusion with Spherical Gaussian constraint (DSG) to address the issue of manifold deviation in conditional diffusion models, allowing for larger guidance steps and improved sample quality. The proposed method integrates seamlessly into existing training-free conditional diffusion frameworks, enhancing performance without significant computational overhead.
  - **Keywords:** conditional generative tasks, diffusion models, loss guidance, Spherical Gaussian constraint, DSG (Diffusion with Spherical Gaussian constraint), image generation, image inpainting, super-resolution, image editing, human motion generation, manifold deviation, estimation error, sample quality, optimization of guidance steps, training-free methods


- [Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching](https://icml.cc/virtual/2024/poster/35017) (Poster)
  - **Authors:** [Kai Yan](http://openreview.net/profile?id=~Kai_Yan1), [Alex Schwing](http://openreview.net/profile?id=~Alex_Schwing1), [Yu-Xiong Wang](http://openreview.net/profile?id=~Yu-Xiong_Wang1)
  - **Affiliations:** The Grainger College of Engineering, University of Illinois Urbana-Champaign, Urbana, Illinois, USA, The Grainger College of Engineering, University of Illinois Urbana-Champaign, Urbana, Illinois, USA, The Grainger College of Engineering, University of Illinois Urbana-Champaign, Urbana, Illinois, USA
  - **TL;DR:** This paper introduces Primal Wasserstein DICE (PW-DICE), a novel method for offline Learning from Observations that minimizes the primal Wasserstein distance between learner and expert state occupancies, improving upon existing state-of-the-art methods. The framework generalizes previous approaches and addresses challenges related to data sparsity and the absence of reward labels in imitation learning scenarios.
  - **Keywords:** Offline Learning from Observations (LfO), Imitation Learning (IL), Reinforcement Learning (RL), Distribution Correction Estimation (DICE), Primal Wasserstein DICE (PW-DICE), f-divergences, Wasserstein distance, Autonomous driving, robotic manipulation, Data sparsity, expert demonstration scarcity, reward label absence, Generalization of SMODICE, unification of f-divergence and Wasserstein minimization


- [Handling Heterogeneous Curvatures in Bandit LQR Control](https://icml.cc/virtual/2024/poster/32628) (Spotlight Poster)
  - **Authors:** [Yu-Hu Yan](http://openreview.net/profile?id=~Yu-Hu_Yan1), [Jing Wang](http://openreview.net/profile?id=~Jing_Wang32), [Peng Zhao](http://openreview.net/profile?id=~Peng_Zhao1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This paper investigates online LQR control with heterogeneous cost curvatures and bandit feedback, aiming to enhance algorithm adaptivity. The authors provide a novel analysis and achieve interpolated guarantees that improve upon existing regret bounds while addressing challenges related to curvature variations.
  - **Keywords:** Online Linear Quadratic Regulator (LQR), Bandit Feedback, Heterogeneous Cost Curvatures, Bandit Convex Optimization, Newton Decrement, Interior-Point Methods, Control Systems, Online Learning, Heterogeneous Curvatures, Adaptivity in Algorithms, Truncation Errors, Interpolated Guarantees, Regret Bounds, Linear Quadratic Gaussian (LQG), Semi-Adversarial Disturbances


- [SAM as the Guide: Mastering Pseudo-Label Refinement in Semi-Supervised Referring Expression Segmentation](https://icml.cc/virtual/2024/poster/34253) (Oral)
  - **Authors:** [Danni Yang](http://openreview.net/profile?id=~Danni_Yang1), [Jiayi Ji](http://openreview.net/profile?id=~Jiayi_Ji1), [Yiwei Ma](http://openreview.net/profile?id=~Yiwei_Ma1), [Tianyu Guo](http://openreview.net/profile?id=~Tianyu_Guo3), [Haowei Wang](http://openreview.net/profile?id=~Haowei_Wang1), [Xiaoshuai Sun](http://openreview.net/profile?id=~Xiaoshuai_Sun3), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China; Youtu Lab, Tencent, Shanghai, China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, School of Informatics, Xiamen University, 361005, P.R. China
  - **TL;DR:** This study presents SemiRES, a semi-supervised framework that enhances the accuracy of pseudo-labels in Referring Expression Segmentation by leveraging the Segment Anything Model (SAM) and various matching strategies. The results demonstrate significant performance improvements, achieving up to +18.64% gains on the RefCOCO validation set with only 1% labeled data.
  - **Keywords:** Semi-supervised learning, Referring Expression Segmentation (RES), Segment Anything Model (SAM), IoU-based Optimal Matching (IOM), Composite Parts Integration (CPI), Pixel-Wise Adjustment (PWA), Image segmentation, Visual grounding, Noisy pseudo-labels, Data sparsity, High demand for labeled data, Improved accuracy of pseudo-labels, Enhanced training strategies, RefCOCO, RefCOCO+, G-Ref


- [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment](https://icml.cc/virtual/2024/poster/34088) (Poster)
  - **Authors:** [Rui Yang](http://openreview.net/profile?id=~Rui_Yang8), [Xiaoman Pan](http://openreview.net/profile?id=~Xiaoman_Pan2), [Feng Luo](http://openreview.net/profile?id=~Feng_Luo4), [Shuang Qiu](http://openreview.net/profile?id=~Shuang_Qiu2), [Han Zhong](http://openreview.net/profile?id=~Han_Zhong1), [Dong Yu](http://openreview.net/profile?id=~Dong_Yu2), [Jianshu Chen](http://openreview.net/profile?id=~Jianshu_Chen1)
  - **Affiliations:** Tencent AI Lab; The Hong Kong University of Science and Technology, Tencent AI Lab, Tencent AI Lab, The Hong Kong University of Science and Technology, Peking University, Tencent AI Lab, Tencent AI Lab
  - **TL;DR:** This paper introduces Rewards-in-Context (RiC), a method for aligning foundation models with human preferences through supervised fine-tuning, addressing the challenges of multi-objective reinforcement learning. The approach demonstrates significant efficiency, requiring only about 10% of the GPU hours compared to traditional multi-objective RL methods.
  - **Keywords:** multi-objective alignment, human preferences, AI systems, Rewards-in-Context (RiC), supervised fine-tuning, reinforcement learning (RL), multi-objective RLHF (MORLHF), large language models (LLMs), diffusion models, costly and unstable fine-tuning, heterogeneous and conflicting human preferences, dynamic inference-time adjustment, Pareto-optimal solution


- [Sample-Efficient Multiagent Reinforcement Learning with Reset Replay](https://icml.cc/virtual/2024/poster/32759) (Poster)
  - **Authors:** [Yaodong Yang](http://openreview.net/profile?id=~Yaodong_Yang2), [Guangyong Chen](http://openreview.net/profile?id=~Guangyong_Chen1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Pheng Ann Heng](http://openreview.net/profile?id=~Pheng-Ann_Heng1)
  - **Affiliations:** Department of CSE, CUHK; None, Zhejiang Lab; Shenzhen Institutes of Advanced Technology, CAS, Tianjin University; Noah’s Ark Lab, Huawei, Institute of Medical Intelligence and XR, CUHK
  - **TL;DR:** This paper introduces the Multiagent Reinforcement Learning with Reset Replay (MARR) algorithm to enhance the sample efficiency of MARL in parallel environments. The proposed method significantly improves performance with fewer environment interactions by employing a high replay ratio and data augmentation techniques.
  - **Keywords:** Multiagent Reinforcement Learning (MARL), Sample Efficiency, Reset Replay, Data Augmentation, High Replay Ratio, Real-time Strategy Games, Distributed Energy Management, Urban Traffic Control, Low Sample Efficiency, High Interaction Cost, MARR Algorithm, Improved Performance with Fewer Interactions, SMAC, MPE


- [Understanding Server-Assisted Federated Learning in the Presence of Incomplete Client Participation](https://icml.cc/virtual/2024/poster/35294) (Poster)
  - **Authors:** [Haibo Yang](http://openreview.net/profile?id=~Haibo_Yang1), [Peiwen Qiu](http://openreview.net/profile?id=~Peiwen_Qiu1), [Prashant Khanduri](http://openreview.net/profile?id=~Prashant_Khanduri1), [Minghong Fang](http://openreview.net/profile?id=~Minghong_Fang1), [Jia (Kevin) Liu](http://openreview.net/profile?id=~Jia_Liu1)
  - **Affiliations:** Department of Computing and Information Sciences, Ph.D., Rochester Institute of Technology, Rochester, NY, USA, Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA, Department of Computer Science, Wayne State University, Detroit, MI, USA, Department of Computer Science and Engineering, University of Louisville, Louisville, KY, USA, Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA
  - **TL;DR:** This study investigates the theoretical foundations of server-assisted federated learning (SA-FL) in the context of incomplete client participation, demonstrating that conventional federated learning is not PAC-learnable under such conditions, but SA-FL can restore PAC-learnability. The proposed SAFARI algorithm offers linear convergence guarantees, significantly improving performance in scenarios with incomplete client participation.
  - **Keywords:** Federated Learning (FL), Server-Assisted Federated Learning (SA-FL), Incomplete Client Participation, PAC-learnability, SAFARI (server-assisted federated averaging), Incomplete client participation, System heterogeneity, Client participation challenges, Theoretical justification for SA-FL, Convergence guarantees for SAFARI


- [Unlocking the Power of Spatial and Temporal Information in Medical Multimodal Pre-training](https://icml.cc/virtual/2024/poster/34857) (Poster)
  - **Authors:** [Jinxia Yang](http://openreview.net/profile?id=~Jinxia_Yang2), [Bing Su](http://openreview.net/profile?id=~Bing_Su1), [Xin Zhao](http://openreview.net/profile?id=~Xin_Zhao10), [Ji-Rong Wen](http://openreview.net/profile?id=~Ji-Rong_Wen1)
  - **Affiliations:** Gaoling School of Artificial Intelligence, Renmin University of China; None, Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China; School of Information, Renmin University of China
  - **TL;DR:** The study introduces the Med-ST framework for fine-grained spatial and temporal modeling in medical vision-language pre-training, effectively utilizing multi-view images and historical records. Experimental results show its effectiveness, particularly in temporal classification tasks, enhancing the potential for clinical diagnosis.
  - **Keywords:** Medical vision-language pre-training, multimodal learning, Mixture of View Expert (MoVE), cross-modal bidirectional cycle consistency objective, forward mapping classification (FMC), reverse mapping regression (RMR), Clinical auxiliary diagnosis, medical imaging, Fine-grained spatial information capture, temporal differences in image-text pairs, Med-ST framework, effective temporal classification


- [Representation Surgery for Multi-Task Model Merging](https://icml.cc/virtual/2024/poster/34001) (Poster)
  - **Authors:** [Enneng Yang](http://openreview.net/profile?id=~Enneng_Yang1), [Li Shen](http://openreview.net/profile?id=~Li_Shen1), [Zhenyi Wang](http://openreview.net/profile?id=~Zhenyi_Wang1), [Guibing Guo](http://openreview.net/profile?id=~Guibing_Guo2), [Xiaojun Chen](http://openreview.net/profile?id=~Xiaojun_Chen4), [Xingwei Wang](http://openreview.net/profile?id=~Xingwei_Wang3), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** Northeastern University, China, Sun Yat-sen University, China; JD Explore Academy, China, University of Maryland, USA, Northeastern University, China, Shenzhen University, China, Northeastern University, China, Nanyang Technological University, Singapore
  - **TL;DR:** This paper introduces a representation surgery method to address representation bias in merged multi-task learning models, significantly improving their performance compared to traditional methods. The proposed lightweight module optimizes the representation distance between merged and individual models, enhancing computational efficiency and generalization.
  - **Keywords:** Multi-task learning (MTL), model merging, Representation surgery, unsupervised optimization, Computer vision, natural language processing, recommendation systems, robotics, Representation bias, performance gap in model merging, Improved MTL performance, lightweight task-specific module


- [Small-loss Adaptive Regret for Online Convex Optimization](https://icml.cc/virtual/2024/poster/33436) (Poster)
  - **Authors:** [Wenhao Yang](http://openreview.net/profile?id=~Wenhao_Yang3), [Wei Jiang](http://openreview.net/profile?id=~Wei_Jiang8), [Yibo Wang](http://openreview.net/profile?id=~Yibo_Wang2), [Ping Yang](http://openreview.net/profile?id=~Ping_Yang4), [Yao Hu](http://openreview.net/profile?id=~Yao_Hu1), [Lijun Zhang](http://openreview.net/profile?id=~Lijun_Zhang1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; School of Artificial Intelligence, Nanjing University, Nanjing 210023, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; School of Artificial Intelligence, Nanjing University, Nanjing 210023, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; School of Artificial Intelligence, Nanjing University, Nanjing 210023, China, Xiaohongshu Inc., Beijing, China, Xiaohongshu Inc., Beijing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China; School of Artificial Intelligence, Nanjing University, Nanjing 210023, China
  - **TL;DR:** This paper presents novel algorithms that achieve small-loss adaptive regret bounds for exp-concave and strongly convex functions in online convex optimization, addressing the limitations of existing methods. The proposed universal algorithm is capable of handling multiple types of convex functions simultaneously and provides minimax adaptive regret bounds for non-smooth functions.
  - **Keywords:** Online Convex Optimization, Adaptive Regret, Small-loss adaptive regret bounds, Universal algorithm, Meta-expert framework, Changing environments, Regret minimization, Novel algorithms for exp-concave and strongly convex functions, Minimax adaptive regret bounds, Convex functions, Smoothness condition, Minimax rates


- [Retrieval Across Any Domains via Large-scale Pre-trained Model](https://icml.cc/virtual/2024/poster/34499) (Poster)
  - **Authors:** [Jiexi Yan](http://openreview.net/profile?id=~Jiexi_Yan2), [Zhihui Yin](http://openreview.net/profile?id=~Zhihui_Yin1), [Chenghao Xu](http://openreview.net/profile?id=~Chenghao_Xu1), [Cheng Deng](http://openreview.net/profile?id=~Cheng_Deng2), [Heng Huang](http://openreview.net/profile?id=~Heng_Huang1)
  - **Affiliations:** School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China, School of Electronic Engineering, Xidian University, Xi’an, Shaanxi, China, School of Electronic Engineering, Xidian University, Xi’an, Shaanxi, China, School of Electronic Engineering, Xidian University, Xi’an, Shaanxi, China, Department of Computer Science, University of Maryland College Park, USA
  - **TL;DR:** This paper introduces a data-free adaptive cross-domain retrieval method that utilizes a pre-trained vision-language model to enhance generalization across unseen domains. The proposed Text-driven Knowledge Integration (TKI) method effectively extracts domain-specific information and integrates it with a universal projection for improved image retrieval performance.
  - **Keywords:** cross-domain retrieval, data-free adaptive retrieval, Text-driven Knowledge Integration (TKI), non-Euclidean multi-layer perceptron, image retrieval, e-commerce, surveillance, generalization to unseen domains, distribution shifts, lack of training data, domain-agnostic universal projection, feature extraction from images


- [Explain Temporal Black-Box Models via Functional Decomposition](https://icml.cc/virtual/2024/poster/33933) (Poster)
  - **Authors:** [Linxiao Yang](http://openreview.net/profile?id=~Linxiao_Yang1), [Yunze Tong](http://openreview.net/profile?id=~Yunze_Tong1), [Xinyue Gu](http://openreview.net/profile?id=~Xinyue_Gu1), [Liang Sun](http://openreview.net/profile?id=~Liang_Sun2)
  - **Affiliations:** DAMO Academy, Alibaba Group, Hangzhou, China; Department of Computer Science and Technology, Zhejiang University, Hangzhou, China, DAMO Academy, Alibaba Group, Hangzhou, China; Department of Computer Science and Technology, Zhejiang University, Hangzhou, China, DAMO Academy, Alibaba Group, Hangzhou, China, DAMO Academy, Alibaba Group, Hangzhou, China
  - **TL;DR:** This paper introduces FDTempExplainer, a model-agnostic method for explaining temporal black-box models by disentangling individual contributions and interactions across time steps. The approach effectively addresses the challenges of temporal dependencies and interactions, demonstrating superior performance in various time series applications.
  - **Keywords:** Explainability, Temporal Models, Time Series Analysis, Functional Decomposition, Model-Agnostic Explanation, Anomaly Detection, Classification, Forecasting, Temporal Dependencies, Interaction Effects, Out-of-Distribution Problem, FDTempExplainer, Insights into Temporal Interactions, Black-Box Models


- [UniAudio: Towards Universal Audio Generation with Large Language Models](https://icml.cc/virtual/2024/poster/34007) (Poster)
  - **Authors:** [Dongchao Yang](http://openreview.net/profile?id=~Dongchao_Yang1), [Jinchuan Tian](http://openreview.net/profile?id=~Jinchuan_Tian1), [Xu Tan](http://openreview.net/profile?id=~Xu_Tan1), [Rongjie Huang](http://openreview.net/profile?id=~Rongjie_Huang1), [Songxiang Liu](http://openreview.net/profile?id=~Songxiang_Liu2), [Haohan Guo](http://openreview.net/profile?id=~Haohan_Guo1), [Xuankai Chang](http://openreview.net/profile?id=~Xuankai_Chang1), [Jiatong Shi](http://openreview.net/profile?id=~Jiatong_Shi1), [sheng zhao](http://openreview.net/profile?id=~sheng_zhao1), [Jiang Bian](http://openreview.net/profile?id=~Jiang_Bian1), [Zhou Zhao](http://openreview.net/profile?id=~Zhou_Zhao3), [Xixin Wu](http://openreview.net/profile?id=~Xixin_Wu1), [Helen M Meng](http://openreview.net/profile?id=~Helen_M._Meng1)
  - **Affiliations:** The Chinese University of Hong Kong, Hong Kong SAR, China, Language Technologies Institute, Carnegie Mellon University, USA, Microsoft Research Asia, China, Zhejiang University, China, Independent Researcher, China, The Chinese University of Hong Kong, Hong Kong SAR, China, Language Technologies Institute, Carnegie Mellon University, USA, Language Technologies Institute, Carnegie Mellon University, USA, Microsoft Research Asia, China, Microsoft Research Asia, China, Zhejiang University, China, The Chinese University of Hong Kong, Hong Kong SAR, China, The Chinese University of Hong Kong, Hong Kong SAR, China
  - **TL;DR:** This paper introduces UniAudio, a universal audio generation model based on large language models that can generate various types of audio from multiple input modalities. The model demonstrates competitive performance across 11 audio generation tasks and can be easily fine-tuned for new tasks.
  - **Keywords:** universal audio generation, generative AI, large language models (LLMs), audio tokenization, speech generation, music generation, sound generation, task-specific limitations, need for unified models, UniAudio model, competitive results across tasks, seamless support for new tasks, 100k hours of multi-source open-available audio data


- [Stability and Generalization of Stochastic Compositional Gradient Descent Algorithms](https://icml.cc/virtual/2024/poster/33818) (Poster)
  - **Authors:** [Ming Yang](http://openreview.net/profile?id=~Ming_Yang17), [Xiyuan Wei](http://openreview.net/profile?id=~Xiyuan_Wei1), [Tianbao Yang](http://openreview.net/profile?id=~Tianbao_Yang1), [Yiming Ying](http://openreview.net/profile?id=~Yiming_Ying1)
  - **Affiliations:** Department of Mathematics and Statistics, State University of New York at Albany, Albany, NY 12222, USA, Department of Computer Science and Engineering, Texas A&M University, College Station, TX 77843, USA, Department of Computer Science and Engineering, Texas A&M University, College Station, TX 77843, USA, The University of Sydney, School of Mathematics and Statistics, Sydney, NSW 2006, Australia
  - **TL;DR:** This paper analyzes the stability and generalization of stochastic compositional gradient descent algorithms, introducing a new stability concept called compositional uniform stability and deriving dimension-independent excess risk bounds. The findings provide the first known results on the stability and generalization of these algorithms in the context of statistical learning theory.
  - **Keywords:** Stochastic Compositional Optimization (SCO), Generalization, Stability, Stochastic Compositional Gradient Descent (SCGD), SCSC, Reinforcement Learning, AUC Maximization, Meta-Learning, Generalization of learning algorithms, Stability analysis, Compositional uniform stability, Dimension-independent excess risk bounds


- [Junk DNA Hypothesis: Pruning Small Pre-Trained Weights $\textit{Irreversibly}$ and $\textit{Monotonically}$ Impairs ``Difficult" Downstream Tasks in LLMs](https://icml.cc/virtual/2024/poster/34569) (Poster)
  - **Authors:** [Lu Yin](http://openreview.net/profile?id=~Lu_Yin1), [Ajay Jaiswal](http://openreview.net/profile?id=~AJAY_KUMAR_JAISWAL1), [Shiwei Liu](http://openreview.net/profile?id=~Shiwei_Liu2), [Souvik Kundu](http://openreview.net/profile?id=~Souvik_Kundu2), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1)
  - **Affiliations:** Eindhoven University of Technology; University of Surrey, University of Texas at Austin, Eindhoven University of Technology, Intel Labs, University of Texas at Austin
  - **TL;DR:** This study introduces the Junk DNA Hypothesis, arguing that small-magnitude weights in large language models are crucial for performance on difficult downstream tasks, and that pruning these weights leads to irreversible knowledge loss. The findings challenge the belief that non-significant weight components can be removed without affecting model performance.
  - **Keywords:** Junk DNA Hypothesis, Large Language Models (LLMs), model compression, pruning, quantization, downstream tasks in LLMs, performance degradation in difficult tasks, redundancy in model parameters, monotonic relationship between pruning and performance drop, irreparable loss of knowledge


- [Neuro-Symbolic Temporal Point Processes](https://icml.cc/virtual/2024/poster/34474) (Poster)
  - **Authors:** [Yang Yang](http://openreview.net/profile?id=~Yang_Yang56), [Chao Yang](http://openreview.net/profile?id=~Chao_Yang9), [Boyang Li](http://openreview.net/profile?id=~Boyang_Li4), [Yinghao Fu](http://openreview.net/profile?id=~Yinghao_Fu1), [Shuang Li](http://openreview.net/profile?id=~Shuang_Li3)
  - **Affiliations:** School of Data Science, The Chinese University of Hong Kong (Shenzhen), School of Data Science, The Chinese University of Hong Kong (Shenzhen), School of Data Science, The Chinese University of Hong Kong (Shenzhen), School of Data Science, The Chinese University of Hong Kong (Shenzhen), School of Data Science, The Chinese University of Hong Kong (Shenzhen)
  - **TL;DR:** This paper presents a neural-symbolic rule induction framework for efficiently discovering temporal logic rules that explain irregular events, particularly in high-stakes domains like healthcare and finance. The proposed method demonstrates significant improvements in efficiency and accuracy over existing models by leveraging a combination of neural and symbolic approaches.
  - **Keywords:** temporal logic rules, irregular events, explainability, neural-symbolic rule induction framework, temporal point process model, sequential covering algorithm, healthcare, finance, explaining critical events, uncovering rules from data, efficient rule learning, rule embeddings, model interpretability, synthetic datasets, real datasets, temporal point processes (TPP), Hawkes process, RMTPP, Transformer Hawkes


- [Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict?](https://icml.cc/virtual/2024/poster/33555) (Poster)
  - **Authors:** [Fan Yao](http://openreview.net/profile?id=~Fan_Yao2), [Chuanhao Li](http://openreview.net/profile?id=~Chuanhao_Li1), [Denis Nekipelov](http://openreview.net/profile?id=~Denis_Nekipelov1), [Hongning Wang](http://openreview.net/profile?id=~Hongning_Wang1), [Haifeng Xu](http://openreview.net/profile?id=~Haifeng_Xu1)
  - **Affiliations:** Department of Computer Science, University of Virginia, USA, Department of Computer Science, Yale University, USA, Department of Computer Science, University of Virginia, USA; Department of Economics, University of Virginia, USA, Department of Computer Science and Technology, Tsinghua University, USA, Department of Computer Science, University of Chicago, USA
  - **TL;DR:** This study explores the competition between human creators and generative AI in content creation, proposing a competition model to analyze their dynamics. It concludes that a stable equilibrium between human and AI-generated content is achievable, despite challenges such as market oversaturation and the potential marginalization of human creativity.
  - **Keywords:** Generative AI, content creation, human creativity, Competition model, Tullock contest, Online content ecosystems, social media, Market oversaturation, marginalization of human creators, Stable equilibrium between human and AI-generated content, Large Language Models (LLMs), AI-generated content


- [Mobile Attention: Mobile-Friendly Linear-Attention for Vision Transformers](https://icml.cc/virtual/2024/poster/33919) (Poster)
  - **Authors:** [Zhiyu Yao](http://openreview.net/profile?id=~Zhiyu_Yao2), [Jian Wang](http://openreview.net/profile?id=~Jian_Wang11), [Haixu Wu](http://openreview.net/profile?id=~Haixu_Wu1), [Jingdong Wang](http://openreview.net/profile?id=~Jingdong_Wang1), [Mingsheng Long](http://openreview.net/profile?id=~Mingsheng_Long5)
  - **Affiliations:** School of Software, BNRist, Tsinghua University, Beijing, China; Baidu VIS, Beijing, China, Baidu VIS, Beijing, China, School of Software, BNRist, Tsinghua University, Beijing, China, Baidu VIS, Beijing, China, School of Software, BNRist, Tsinghua University, Beijing, China; Baidu VIS, Beijing, China
  - **TL;DR:** This study introduces Mobile-Attention, a novel kernel-based linear-attention mechanism designed for mobile devices, addressing the efficiency-capability dilemma of Vision Transformers. The proposed method achieves enhanced model capacity and significant reductions in latency, making it suitable for various computer vision tasks.
  - **Keywords:** Vision Transformers, mobile-friendly models, Multi-head linear-attention, kernel-based linear attention, Computer vision, image classification, object detection, Quadratic complexity of attention modules, efficiency-capability dilemma, Mobile-Attention mechanism, enhanced model capacity, reduced latency


- [Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption](https://icml.cc/virtual/2024/poster/33749) (Poster)
  - **Authors:** [Chenlu Ye](http://openreview.net/profile?id=~Chenlu_Ye1), [Jiafan He](http://openreview.net/profile?id=~Jiafan_He1), [Quanquan Gu](http://openreview.net/profile?id=~Quanquan_Gu1), [Tong Zhang](http://openreview.net/profile?id=~Tong_Zhang2)
  - **Affiliations:** The Hong Kong University of Science and Technology, University of California, Los Angeles, University of California, Los Angeles, University of Illinois Urbana-Champaign
  - **TL;DR:** This study addresses adversarial corruption in model-based reinforcement learning by introducing two algorithms, CR-OMLE and CR-PMLE, which leverage maximum likelihood estimation to learn transition models. The algorithms achieve provable regret bounds, demonstrating their robustness against adversarial manipulation.
  - **Keywords:** model-based reinforcement learning, adversarial corruption, maximum likelihood estimation (MLE), corruption-robust optimistic MLE (CR-OMLE), corruption-robust pessimistic MLE (CR-PMLE), adversarial manipulation of transition dynamics, non-stationary environments, regret bounds, provable guarantees for algorithms, total-variation (TV)-based information ratios, Markov decision process (MDP)


- [Empowering Graph Invariance Learning with Deep Spurious Infomax](https://icml.cc/virtual/2024/poster/32841) (Poster)
  - **Authors:** [Tianjun Yao](http://openreview.net/profile?id=~Tianjun_Yao1), [Yongqiang Chen](http://openreview.net/profile?id=~Yongqiang_Chen1), [Zhenhao Chen](http://openreview.net/profile?id=~Zhenhao_Chen1), [Kai Hu](http://openreview.net/profile?id=~Kai_Hu2), [Zhiqiang Shen](http://openreview.net/profile?id=~Zhiqiang_Shen1), [Kun Zhang](http://openreview.net/profile?id=~Kun_Zhang1)
  - **Affiliations:** Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; The Chinese University of Hong Kong, Hong Kong, China, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Carnegie Mellon University, Pittsburgh, USA, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; Carnegie Mellon University, Pittsburgh, USA
  - **TL;DR:** This study introduces a novel graph invariance learning paradigm that leverages the infomax principle to enhance the generalization of Graph Neural Networks to out-of-distribution data. The proposed EQuAD framework demonstrates improved performance by effectively disentangling invariant features from spurious ones, achieving up to 31.76% enhancement across various datasets.
  - **Keywords:** Graph Neural Networks, Out-of-Distribution (OOD) generalization, Graph invariance learning, EQuAD framework, infomax principle, Spurious features, correlation strengths, distribution shifts, Enhanced performance, robust inductive bias, Synthetic datasets, real-world datasets


- [Socialized Learning: Making Each Other Better Through Multi-Agent Collaboration](https://icml.cc/virtual/2024/poster/33681) (Poster)
  - **Authors:** [Xinjie Yao](http://openreview.net/profile?id=~Xinjie_Yao1), [Yu Wang](http://openreview.net/profile?id=~Yu_Wang33), [Pengfei Zhu](http://openreview.net/profile?id=~Pengfei_Zhu1), [Wanyu LIN](http://openreview.net/profile?id=~Wanyu_Lin1), [Li Jialu](http://openreview.net/profile?id=~Jialu_Li4), [Weihao Li](http://openreview.net/profile?id=~Weihao_Li3), [Qinghua Hu](http://openreview.net/profile?id=~Qinghua_Hu1)
  - **Affiliations:** College of Intelligence and Computing, Tianjin University, Tianjin, China; Engineering Research Center of City Intelligence and Digital Governance, Ministry of Education of the People’s Republic of China, Tianjin, China; Haihe Lab of ITAI, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China; Engineering Research Center of City Intelligence and Digital Governance, Ministry of Education of the People’s Republic of China, Tianjin, China; Haihe Lab of ITAI, Tianjin, China, College of Intelligence and Computing, Tianjin University, Tianjin, China; Engineering Research Center of City Intelligence and Digital Governance, Ministry of Education of the People’s Republic of China, Tianjin, China; Haihe Lab of ITAI, Tianjin, China, Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China, College of Intelligence and Computing, Tianjin University, Tianjin, China; Engineering Research Center of City Intelligence and Digital Governance, Ministry of Education of the People’s Republic of China, Tianjin, China; Haihe Lab of ITAI, Tianjin, China, Department of Computer Science, Boston University, Boston, United States, College of Intelligence and Computing, Tianjin University, Tianjin, China; Engineering Research Center of City Intelligence and Digital Governance, Ministry of Education of the People’s Republic of China, Tianjin, China; Haihe Lab of ITAI, Tianjin, China
  - **TL;DR:** This study proposes a novel framework called Multi-Agent Socialized Collaboration (MASC) that enables multi-agent systems to learn new abilities while maintaining high accuracy in original expert classes, inspired by human cultural evolution and cognitive science. The findings demonstrate the effectiveness of collaborative behaviors and information sharing among agents in achieving socialized learning.
  - **Keywords:** Socialized Learning, Multi-Agent Systems, Collective Intelligence, Multi-Agent Socialized Collaboration (MASC), Collective Collaboration, Reciprocal Altruism, Continual Learning, Knowledge Retention, Data Heterogeneity, Effective Multi-Agent Collaboration, Information Sharing, Knowledge Interaction, Cultural Evolution, Cognitive Science


- [Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs](https://icml.cc/virtual/2024/poster/34618) (Poster)
  - **Authors:** [Ling Yang](http://openreview.net/profile?id=~Ling_Yang1), [Zhaochen Yu](http://openreview.net/profile?id=~Zhaochen_Yu2), [Chenlin Meng](http://openreview.net/profile?id=~Chenlin_Meng1), [Minkai Xu](http://openreview.net/profile?id=~Minkai_Xu1), [Stefano Ermon](http://openreview.net/profile?id=~Stefano_Ermon1), [Bin Cui](http://openreview.net/profile?id=~Bin_CUI2)
  - **Affiliations:** Peking University, China, Peking University, China, Stanford University, USA; Pika Labs, USA, Stanford University, USA, Stanford University, USA, Peking University, China
  - **TL;DR:** This paper introduces a training-free framework called Recaption, Plan and Generate (RPG) that leverages multimodal LLMs to improve text-to-image generation by decomposing complex prompts into simpler tasks. The proposed method outperforms existing models in generating images that accurately reflect complex text prompts, particularly in terms of compositionality and semantic alignment.
  - **Keywords:** text-to-image generation, multimodal LLMs, compositionality, diffusion models, chain-of-thought reasoning, regional diffusion, image generation, image editing, handling complex text prompts, multi-category object composition, text-image semantic alignment, new framework (Recaption, Plan and Generate), enhanced generalization ability


- [Learning Causal Dynamics Models in Object-Oriented Environments](https://icml.cc/virtual/2024/poster/34451) (Poster)
  - **Authors:** [Zhongwei Yu](http://openreview.net/profile?id=~Zhongwei_Yu1), [Jingqing Ruan](http://openreview.net/profile?id=~Jingqing_Ruan1), [Dengpeng Xing](http://openreview.net/profile?id=~Dengpeng_Xing1)
  - **Affiliations:** Institute of Automation, Chinese Academy of Sciences, Beijing, P. R. China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, P. R. China, Institute of Automation, Chinese Academy of Sciences, Beijing, P. R. China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, P. R. China, Institute of Automation, Chinese Academy of Sciences, Beijing, P. R. China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, P. R. China
  - **TL;DR:** This paper introduces the Object-Oriented Causal Dynamics Model (OOCDM) to extend causal dynamics models to large-scale object-oriented environments, addressing challenges in computational complexity and sample efficiency. Experimental results show that OOCDM outperforms existing models in causal discovery, prediction accuracy, generalization, and computational efficiency.
  - **Keywords:** Causal dynamics models, reinforcement learning, object-oriented environments, Causal discovery, causal graphs, Object-Oriented Causal Dynamics Model (OOCDM), Large-scale environments, multi-agent domains, Computational complexity, sample efficiency, spurious correlations, Improved causal discovery, prediction accuracy, generalization, computational efficiency, Causal dynamics learning (CDL), causal dependencies


- [SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN](https://icml.cc/virtual/2024/poster/34194) (Poster)
  - **Authors:** [kang you](http://openreview.net/profile?id=~kang_you2), [Zekai Xu](http://openreview.net/profile?id=~Zekai_Xu1), [Chen Nie](http://openreview.net/profile?id=~Chen_Nie1), [Zhijie Deng](http://openreview.net/profile?id=~Zhijie_Deng1), [Qinghai Guo](http://openreview.net/profile?id=~Qinghai_Guo1), [Xiang Wang](http://openreview.net/profile?id=~Xiang_Wang14), [Zhezhi He](http://openreview.net/profile?id=~Zhezhi_He1)
  - **Affiliations:** School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, Huawei Technologies, Shenzhen, China, Huawei Technologies, Shenzhen, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Huawei Technologies, Shenzhen, China
  - **TL;DR:** This study presents SpikeZIP-TF, a novel ANN-to-SNN conversion method that achieves equivalent performance between ANN and SNN without accuracy degradation. The method demonstrates superior accuracy on both computer vision and natural language processing tasks compared to existing Transformer-based SNNs.
  - **Keywords:** Spiking Neural Networks (SNN), Artificial Neural Networks (ANN), Transformer-based networks, ANN-to-SNN conversion, Direct Training (DT), Back-propagation through time (BPTT), Computer Vision (CV), Natural Language Processing (NLP), Accuracy gap between SNN and ANN, challenges in building equivalence between SNN and ANN operators, SpikeZIP-TF method, high accuracy on ImageNet and SST-2 datasets, ImageNet, SST-2


- [Activation-Descent Regularization for Input Optimization of ReLU Networks](https://icml.cc/virtual/2024/poster/34434) (Poster)
  - **Authors:** [Hongzhan Yu](http://openreview.net/profile?id=~Hongzhan_Yu1), [Sicun Gao](http://openreview.net/profile?id=~Sicun_Gao1)
  - **Affiliations:** Department of Computer Science & Engineering, University of California San Diego, USA, Department of Computer Science & Engineering, University of California San Diego, USA
  - **TL;DR:** This paper presents a novel approach for input optimization in ReLU networks that considers the effects of activation patterns, leading to improved local descent properties. The proposed methods demonstrate effectiveness in various applications, including adversarial attacks and generative modeling.
  - **Keywords:** input optimization, ReLU networks, deep learning, activation patterns, local optimization, regularization terms, primal-dual descent, adversarial learning, generative modeling, reinforcement learning, optimization of inputs, combinatorial nature of loss landscapes, discontinuous activation functions, new input optimization methods, improved local descent properties


- [Enabling Few-Shot Learning with PID Control: A Layer Adaptive Optimizer](https://icml.cc/virtual/2024/poster/34286) (Poster)
  - **Authors:** [Le Yu](http://openreview.net/profile?id=~Le_Yu6), [Xinde Li](http://openreview.net/profile?id=~Xinde_Li1), [Pengfei Zhang](http://openreview.net/profile?id=~Pengfei_Zhang10), [zhentong zhang](http://openreview.net/profile?id=~zhentong_zhang1), [Fir Dunkin](http://openreview.net/profile?id=~Fir_Dunkin1)
  - **Affiliations:** School of Automation, Southeast University, Nanjing, China; Nanjing Center for Applied Mathematics, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China, School of Automation, Southeast University, Nanjing, China; Nanjing Center for Applied Mathematics, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Southeast University, Nanjing, China; Southeast University Shenzhen Research Institute, Shenzhen, China, School of Automation, Southeast University, Nanjing, China; Nanjing Center for Applied Mathematics, Nanjing, China, School of Automation, Southeast University, Nanjing, China; Nanjing Center for Applied Mathematics, Nanjing, China, School of Automation, Southeast University, Nanjing, China; Nanjing Center for Applied Mathematics, Nanjing, China
  - **TL;DR:** This study introduces the Layer-Adaptive PID (LA-PID) Optimizer, a MAML-based approach that enhances few-shot learning by dynamically adjusting PID control gains at each network layer. The LA-PID optimizer demonstrates state-of-the-art performance in few-shot classification and cross-domain tasks with fewer training steps.
  - **Keywords:** Few-shot learning, Meta-learning, Model-Agnostic Meta-Learning (MAML), Layer-Adaptive PID (LA-PID) Optimizer, Few-shot classification, Cross-domain tasks, Scarcity of labeled data, Discrepancy in training and testing task distributions, State-of-the-art performance, Efficient parameter optimization, Dynamic adjustment of PID control gains, Standard benchmark datasets, Proportional-Integral-Derivative (PID) control


- [Privacy-Preserving Instructions for Aligning Large Language Models](https://icml.cc/virtual/2024/poster/33173) (Poster)
  - **Authors:** [Da Yu](http://openreview.net/profile?id=~Da_Yu1), [Peter Kairouz](http://openreview.net/profile?id=~Peter_Kairouz1), [Sewoong Oh](http://openreview.net/profile?id=~Sewoong_Oh3), [Zheng Xu](http://openreview.net/profile?id=~Zheng_Xu2)
  - **Affiliations:** Sun Yat-sen University; Google Research, Google Research, Google Research, Google Research
  - **TL;DR:** This study addresses privacy risks in aligning large language models with user instructions by proposing the use of synthetic instructions to replace real ones, ensuring formal differential privacy. The experiments demonstrate that models trained with these synthetic instructions achieve comparable performance to those trained with real instructions, outperforming leading open-source models.
  - **Keywords:** Privacy-Preserving Techniques, Large Language Models, Synthetic Instructions, Differential Privacy, Filtering Algorithm, LLM Applications, User Instruction Alignment, Privacy Risks, Sensitive Information Leakage, High Utility of Synthetic Instructions, Model Performance Improvement, Chatbot Arena


- [When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models](https://icml.cc/virtual/2024/poster/34871) (Poster)
  - **Authors:** [Haoran You](http://openreview.net/profile?id=~Haoran_You1), [Yichao Fu](http://openreview.net/profile?id=~Yichao_Fu1), [Zheng Wang](http://openreview.net/profile?id=~Zheng_Wang38), [Amir Yazdanbakhsh](http://openreview.net/profile?id=~Amir_Yazdanbakhsh1), [Yingyan (Celine) Lin](http://openreview.net/profile?id=~Yingyan_Celine_Lin1)
  - **Affiliations:** School of Computer Science, Georgia Institute of Technology, Atlanta, USA, School of Computer Science, Georgia Institute of Technology, Atlanta, USA, School of Computer Science, Georgia Institute of Technology, Atlanta, USA, Google DeepMind, Mountain View, USA, School of Computer Science, Georgia Institute of Technology, Atlanta, USA
  - **TL;DR:** This study investigates the integration of linear attention methods with speculative decoding to enhance the efficiency and effectiveness of autoregressive large language models. The proposed approach significantly reduces perplexity and improves generation speed compared to existing methods.
  - **Keywords:** Large Language Models, Autoregressive Decoding, Linear Attention, Speculative Decoding, Language Understanding, Language Generation, Quadratic Complexity, Limited Efficiency, Sequential Processing, Augmented Linearized LLMs, Reduction in Perplexity, Speedup in Generation, LLaMA


- [Learning Latent Structures in Network Games via Data-Dependent Gated-Prior Graph Variational Autoencoders](https://icml.cc/virtual/2024/poster/33263) (Poster)
  - **Authors:** [XUE YU](http://openreview.net/profile?id=~Xue_Yu2), [Muchen Li](http://openreview.net/profile?id=~Muchen_Li1), [Yan Leng](http://openreview.net/profile?id=~Yan_Leng1), [Renjie Liao](http://openreview.net/profile?id=~Renjie_Liao1)
  - **Affiliations:** Center for Applied Statistics, School of Statistics, Renmin University of China, Beijing, China; Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, Department of Computer Science, University of British Columbia, Vancouver, BC, Canada, McCombs School of Business, The University of Texas at Austin, Austin, TX, USA, Vector Institute for AI; Canada CIFAR AI Chair; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada
  - **TL;DR:** This study introduces a data-dependent gated-prior graph variational autoencoder (GPGV AE) to infer latent network structures and interaction types in network games. The model demonstrates state-of-the-art performance in understanding strategic relationships among individuals based on their observed actions.
  - **Keywords:** network games, strategic interactions, latent network structures, data-dependent gated-prior graph variational autoencoder (GPGV AE), spectral graph neural network (GNN), Transformer, social networks, game theory, inferring network structures, identifying interaction types, challenges in observing network relationships, state-of-the-art performance in inferring network structures, capturing interaction types, strategic complement, strategic substitute, Nash equilibrium


- [Generalization Bound and New Algorithm for Clean-Label Backdoor Attack](https://icml.cc/virtual/2024/poster/33728) (Poster)
  - **Authors:** [Lijia Yu](http://openreview.net/profile?id=~Lijia_Yu2), [Shuang Liu](http://openreview.net/profile?id=~Shuang_Liu5), [Yibo Miao](http://openreview.net/profile?id=~Yibo_Miao1), [Xiao-Shan Gao](http://openreview.net/profile?id=~Xiao-Shan_Gao2), [Lijun Zhang](http://openreview.net/profile?id=~Lijun_Zhang2)
  - **Affiliations:** Institute of Software, Chinese Academy of Sciences, Beijing 100190, China; State Key Laboratory of Computer Science, None, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China; University of Chinese Academy of Sciences, Beijing 100049, China, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China; Kaiyuan International Mathematical Sciences Institute, None, Institute of Software, Chinese Academy of Sciences, Beijing 100190, China; State Key Laboratory of Computer Science, None
  - **TL;DR:** This paper establishes generalization bounds for clean-label backdoor attacks and proposes a new method that combines adversarial noise and indiscriminate poison to enhance the effectiveness of such attacks. The findings contribute to understanding the generalizability of learning methods in the context of backdoor attacks.
  - **Keywords:** Generalization bounds, Clean-label backdoor attack, Algorithm-independent generalization bounds, Adversarial noise, Indiscriminate poison, Adversarial learning, Data poisoning, Generalization gap, Population error, Empirical error, New clean-label backdoor attack method, Upper bounds for clean sample population errors and poison population errors, Backdoor attack, Poisoned training dataset


- [EvGGS: A Collaborative Learning Framework for Event-based Generalizable Gaussian Splatting](https://icml.cc/virtual/2024/poster/33335) (Poster)
  - **Authors:** [Jiaxu Wang](http://openreview.net/profile?id=~Jiaxu_Wang1), [Junhao He](http://openreview.net/profile?id=~Junhao_He2), [Ziyi Zhang](http://openreview.net/profile?id=~Ziyi_Zhang6), [Mingyuan Sun](http://openreview.net/profile?id=~Mingyuan_Sun1), [Jingkai SUN](http://openreview.net/profile?id=~Jingkai_SUN1), [Renjing Xu](http://openreview.net/profile?id=~Renjing_Xu1)
  - **Affiliations:** Function Hub, Hong Kong University of Science and Technology (GZ), Guangzhou, China, Function Hub, Hong Kong University of Science and Technology (GZ), Guangzhou, China, Function Hub, Hong Kong University of Science and Technology (GZ), Guangzhou, China, Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China, Function Hub, Hong Kong University of Science and Technology (GZ), Guangzhou, China, Function Hub, Hong Kong University of Science and Technology (GZ), Guangzhou, China; Faculty of Robot Science and Engineering, Northeastern University, Shenyang, China
  - **TL;DR:** This study presents EvGGS, a novel event-based generalizable 3D reconstruction framework that reconstructs scenes as 3D Gaussians from event data in a feedforward manner, enabling generalization to unseen cases without retraining. The framework outperforms existing methods in reconstruction quality and rendering speed, addressing challenges associated with sparse event data.
  - **Keywords:** 3D reconstruction, event-based reconstruction, Gaussian regression, depth estimation, intensity reconstruction, Robotics, VR/AR, graphics, Data sparsity, inability to generalize to unseen scenes, Event-based generalizable 3D reconstruction framework, collaborative training, Novel event-based 3D dataset, 3D Gaussians, Neural Radiance Field (NeRF), 3D Gaussian Splatting (3DGS)


- [StableMask: Refining Causal Masking in Decoder-only Transformer](https://icml.cc/virtual/2024/poster/34508) (Poster)
  - **Authors:** [Qingyu Yin](http://openreview.net/profile?id=~Qingyu_Yin4), [Xuzheng He](http://openreview.net/profile?id=~Xuzheng_He1), [Xiang Zhuang](http://openreview.net/profile?id=~Xiang_Zhuang1), [Yu Zhao](http://openreview.net/profile?id=~Yu_Zhao8), [Jianhua Yao](http://openreview.net/profile?id=~Jianhua_Yao3), [Xiaoyu Shen](http://openreview.net/profile?id=~Xiaoyu_Shen1), [Qiang Zhang](http://openreview.net/profile?id=~Qiang_Zhang6)
  - **Affiliations:** Zhejiang University, Peking University, Zhejiang University, Tencent AI Lab, Tencent AI Lab, Eastern Institute of Technology, Ningbo, Zhejiang University
  - **TL;DR:** This study introduces StableMask, a parameter-free method that refines causal masking in decoder-only Transformers to address issues of excessive attention allocation and limitations in encoding absolute positional information. The proposed method shows significant improvements in language models across various datasets and supports efficient extrapolation.
  - **Keywords:** language modeling, decoder-only Transformer, causal masking, relative position encoding (RPE), pseudo-attention values, natural language processing, excessive attention allocation, limitations of RPE in encoding absolute positional information, StableMask method, enhancements in language models, efficient extrapolation, Large Language Models (LLMs), softmax function, absolute position encoding (APE)


- [Improving Sharpness-Aware Minimization by Lookahead](https://icml.cc/virtual/2024/poster/34397) (Poster)
  - **Authors:** [Runsheng Yu](http://openreview.net/profile?id=~Runsheng_Yu2), [Youzhi Zhang](http://openreview.net/profile?id=~Youzhi_Zhang2), [James Kwok](http://openreview.net/profile?id=~James_Kwok1)
  - **Affiliations:** Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, CAS, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology
  - **TL;DR:** This paper proposes an enhanced version of Sharpness-Aware Minimization (SAM) that incorporates a lookahead mechanism to improve convergence and escape saddle points. The results demonstrate that the new method outperforms existing state-of-the-art approaches and effectively identifies flatter minima for better generalization.
  - **Keywords:** Sharpness-Aware Minimization, generalization, deep learning, gradient descent, lookahead mechanism, extrapolation, convergence instability, saddle points, overfitting, poor generalization, improved convergence, efficient algorithm, identification of flatter minima, standard benchmark datasets


- [MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities](https://icml.cc/virtual/2024/poster/34344) (Poster)
  - **Authors:** [Weihao Yu](http://openreview.net/profile?id=~Weihao_Yu2), [Zhengyuan Yang](http://openreview.net/profile?id=~Zhengyuan_Yang1), [Linjie Li](http://openreview.net/profile?id=~Linjie_Li1), [Jianfeng Wang](http://openreview.net/profile?id=~Jianfeng_Wang4), [Kevin Lin](http://openreview.net/profile?id=~Kevin_Lin3), [Zicheng Liu](http://openreview.net/profile?id=~Zicheng_Liu1), [Xinchao Wang](http://openreview.net/profile?id=~Xinchao_Wang1), [Lijuan Wang](http://openreview.net/profile?id=~Lijuan_Wang1)
  - **Affiliations:** National University of Singapore, Singapore, Microsoft Azure AI, USA, Microsoft Azure AI, USA, Microsoft Azure AI, USA, Microsoft Azure AI, USA, Microsoft Azure AI, USA, National University of Singapore, Singapore, Microsoft Azure AI, USA
  - **TL;DR:** The paper introduces MM-Vet, an evaluation benchmark for large multimodal models (LMMs) that assesses their performance on complex multimodal tasks by integrating various vision-language capabilities. It highlights the challenges in evaluating these models and proposes a unified scoring metric through an LLM-based evaluator, providing insights into different LMM system designs.
  - **Keywords:** Large Multimodal Models, Evaluation Benchmark, LLM-based evaluator, Vision-Language capabilities, Multimodal tasks, AI model evaluation, Systematic evaluation of complicated multimodal tasks, Designing effective evaluation metrics, MM-Vet benchmark, Insights into LMM capabilities, Vision-Language (VL) capabilities, OCR (Optical Character Recognition)


- [ViP: A Differentially Private Foundation Model for Computer Vision](https://icml.cc/virtual/2024/poster/34910) (Oral)
  - **Authors:** [Yaodong Yu](http://openreview.net/profile?id=~Yaodong_Yu4), [Maziar Sanjabi](http://openreview.net/profile?id=~Maziar_Sanjabi1), [Yi Ma](http://openreview.net/profile?id=~Yi_Ma4), [Kamalika Chaudhuri](http://openreview.net/profile?id=~Kamalika_Chaudhuri1), [Chuan Guo](http://openreview.net/profile?id=~Chuan_Guo1)
  - **Affiliations:** UC Berkeley; Meta, Meta, UC Berkeley, Meta, Meta
  - **TL;DR:** This study presents ViP, a vision transformer model trained with differential privacy guarantees to mitigate privacy and legal risks associated with uncurated internet-scale data. The model demonstrates competitive performance on downstream vision tasks, suggesting that private learning at scale is feasible.
  - **Keywords:** differential privacy, foundation models, self-supervised learning, masked autoencoders, DP-SGD, computer vision, privacy risks, legal risks, uncurated data, ViP model, representation learning, LAION400M, ImageNet, Vision transformer, AI


- [Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders](https://icml.cc/virtual/2024/poster/35206) (Poster)
  - **Authors:** [Yi Yu](http://openreview.net/profile?id=~Yi_Yu5), [Yufei Wang](http://openreview.net/profile?id=~Yufei_Wang5), [Song Xia](http://openreview.net/profile?id=~Song_Xia1), [Wenhan Yang](http://openreview.net/profile?id=~Wenhan_Yang6), [Shijian Lu](http://openreview.net/profile?id=~Shijian_Lu1), [Yap-peng Tan](http://openreview.net/profile?id=~Yap-peng_Tan1), [Alex Kot](http://openreview.net/profile?id=~Alex_Kot1)
  - **Affiliations:** Rapid-Rich Object Search Lab, Interdisciplinary Graduate Programme, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, PengCheng Laboratory, Shenzhen, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore, School of Computer Science and Engineering, Nanyang Technological University, Singapore, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore
  - **TL;DR:** This study introduces a novel purification method for unlearnable examples using rate-constrained variational autoencoders, demonstrating a two-stage approach that effectively eliminates perturbations and enhances model robustness. Extensive experiments validate the method's performance across multiple datasets, including CIFAR-10 and ImageNet.
  - **Keywords:** Unlearnable examples, Data protection, Pre-training purification, Rate-constrained variational autoencoders (VAEs), Disentangled variational autoencoder (D-VAE), Image processing, Machine learning, Testing error maximization, Poisoning attacks, Perturbations in training data, Two-stage purification approach, Effective and robust purification methods, CIFAR-10, CIFAR-100, ImageNet-subset, Data-centric AI (DCAI)


- [Not Just Pretty Pictures: Toward Interventional Data Augmentation Using Text-to-Image Generators](https://icml.cc/virtual/2024/poster/33652) (Poster)
  - **Authors:** [Jianhao Yuan](http://openreview.net/profile?id=~Jianhao_Yuan2), [Francesco Pinto](http://openreview.net/profile?id=~Francesco_Pinto1), [Adam Davies](http://openreview.net/profile?id=~Adam_Davies2), [Phil Torr](http://openreview.net/profile?id=~Philip_Torr1)
  - **Affiliations:** University of Oxford, University of Oxford, University of Illinois Urbana-Champaign, University of Oxford
  - **TL;DR:** This study investigates the use of modern Text-to-Image generators, specifically Stable Diffusion, as a method for interventional data augmentation to enhance the robustness of image classifiers against environmental changes. The findings demonstrate that these T2I models outperform traditional data augmentation techniques across various benchmarks.
  - **Keywords:** Interventional Data Augmentation, Text-to-Image Generation, Stable Diffusion, T2I (Text-to-Image) Generators, Image Classification, Single Domain Generalization, Reducing Reliance on Spurious Features, Performance Degradation, Covariate Shift, Improved Robustness of Classifiers, Enhanced Data Augmentation Techniques


- [Uncertainty Estimation by Density Aware Evidential Deep Learning](https://icml.cc/virtual/2024/poster/34362) (Poster)
  - **Authors:** [Taeseong Yoon](http://openreview.net/profile?id=~Taeseong_Yoon1), [Heeyoung Kim](http://openreview.net/profile?id=~Heeyoung_Kim1)
  - **Affiliations:** Department of Industrial and Systems Engineering, KAIST, Daejeon, Republic of Korea, Department of Industrial and Systems Engineering, KAIST, Daejeon, Republic of Korea
  - **TL;DR:** This study introduces Density Aware Evidential Deep Learning (DAEDL) to enhance uncertainty estimation and classification performance, addressing limitations in existing Evidential Deep Learning methods. DAEDL integrates feature space density with predictive outputs, demonstrating state-of-the-art results in various tasks.
  - **Keywords:** Uncertainty Estimation, Evidential Deep Learning, Density Aware Evidential Deep Learning (DAEDL), Dirichlet-based uncertainty (DBU) models, Out-of-distribution (OOD) detection, Classification tasks, Limited OOD detection performance, Calibration of neural networks, Novel parameterization for uncertainty quantification, Improved uncertainty estimation performance, Two moons dataset


- [FRAG: Frequency Adapting Group for Diffusion Video Editing](https://icml.cc/virtual/2024/poster/34145) (Poster)
  - **Authors:** [Sunjae Yoon](http://openreview.net/profile?id=~Sunjae_Yoon1), [Gwanhyeong Koo](http://openreview.net/profile?id=~Gwanhyeong_Koo2), [Geonwoo Kim](http://openreview.net/profile?id=~Geonwoo_Kim2), [Chang Yoo](http://openreview.net/profile?id=~Chang_D._Yoo1)
  - **Affiliations:** Korea Advanced Institute of Science and Technology (KAIST), South Korea, Korea Advanced Institute of Science and Technology (KAIST), South Korea, Korea Advanced Institute of Science and Technology (KAIST), South Korea, Korea Advanced Institute of Science and Technology (KAIST), South Korea
  - **TL;DR:** This study introduces the Frequency Adapting Group (FRAG) to address quality deterioration in video editing caused by high-frequency leaks in diffusion models. The proposed method enhances video quality by preserving high-frequency components during the denoising process, demonstrating improved consistency and fidelity in edited videos.
  - **Keywords:** video editing, diffusion models, denoising diffusion models, Frequency Adapting Group (FRAG), entertainment industry, video generation, quality deterioration, high-frequency leak, content blur, content flicker, enhancement of video quality, consistency, fidelity, TGVE, DA VIS


- [DiffAug: Enhance Unsupervised Contrastive Learning with Domain-Knowledge-Free Diffusion-based Data Augmentation](https://icml.cc/virtual/2024/poster/32932) (Poster)
  - **Authors:** [Zelin Zang](http://openreview.net/profile?id=~Zelin_Zang2), [Hao Luo](http://openreview.net/profile?id=~Hao_Luo1), [Kai Wang](http://openreview.net/profile?id=~Kai_Wang8), [Panpan Zhang](http://openreview.net/profile?id=~Panpan_Zhang1), [Fan Wang](http://openreview.net/profile?id=~Fan_Wang6), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2), [Yang You](http://openreview.net/profile?id=~Yang_You1)
  - **Affiliations:** AI Lab, Research Center for Industries of the Future, Westlake University, China; None; None; None, DAMO Academy, Alibaba Group, National University of Singapore, National University of Singapore, DAMO Academy, Alibaba Group; None, AI Lab, Research Center for Industries of the Future, Westlake University, China, National University of Singapore
  - **TL;DR:** This paper introduces DiffAug, a novel technique for unsupervised contrastive learning that utilizes a conditional diffusion model for generating positive samples, addressing the limitations of traditional data augmentation methods. Experimental results demonstrate that DiffAug significantly enhances representation learning across various datasets, outperforming existing augmentation strategies.
  - **Keywords:** Unsupervised Contrastive Learning, Data Augmentation, Conditional Diffusion Model, Semantic Encoder, Vision, Biology, DNA Sequence Representation, Data Sparsity, Model Training Bottlenecks, Semantic Distortion, DiffAug, Improved Representation Ability


- [SHINE: Shielding Backdoors in Deep Reinforcement Learning](https://icml.cc/virtual/2024/poster/33126) (Poster)
  - **Authors:** [Zhuowen Yuan](http://openreview.net/profile?id=~Zhuowen_Yuan1), [Wenbo Guo](http://openreview.net/profile?id=~Wenbo_Guo1), [Jinyuan Jia](http://openreview.net/profile?id=~Jinyuan_Jia2), [Bo Li](http://openreview.net/profile?id=~Bo_Li19), [Dawn Song](http://openreview.net/profile?id=~Dawn_Song1)
  - **Affiliations:** University of Illinois Urbana-Champaign, University of California, Santa Barbara, Pennsylvania State University, University of Chicago; University of Illinois Urbana-Champaign, University of California Berkeley
  - **TL;DR:** The study introduces SHINE, a novel method designed to shield deep reinforcement learning agents from backdoor attacks by identifying triggers and retraining policies. Experimental results demonstrate that SHINE significantly outperforms existing defenses in mitigating these attacks while maintaining performance in clean environments.
  - **Keywords:** Deep Reinforcement Learning, Backdoor Attacks, AI Safety, Policy Explanation Techniques, Policy Retraining Algorithm, Vulnerability to Backdoor Attacks, Defense Efficacy, Clean vs. Poisoned Environments, SHINE Method, Improved Performance in Poisoned Environments, Perturbation-based Attacks, Adversarial Agent Attacks


- [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://icml.cc/virtual/2024/poster/33453) (Poster)
  - **Authors:** [Le Yu](http://openreview.net/profile?id=~Le_Yu2), [Bowen Yu](http://openreview.net/profile?id=~Bowen_Yu3), [Haiyang Yu](http://openreview.net/profile?id=~Haiyang_Yu3), [Fei Huang](http://openreview.net/profile?id=~Fei_Huang1), [Yongbin Li](http://openreview.net/profile?id=~Yongbin_Li2)
  - **Affiliations:** Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group, Alibaba Group
  - **TL;DR:** This study introduces DARE, a method that allows Language Models to absorb capabilities from homologous models without retraining or GPUs, effectively reducing redundant parameters. The findings demonstrate that merged models can outperform individual source models, particularly in large-scale settings.
  - **Keywords:** Language Models, Supervised Fine-Tuning, DARE (DropAndREscale), parameter fusing, Redundant delta parameters, parameter interference, Merged language models, performance improvement, delta parameters, encoder-based LMs, decoder-based LMs


- [Robustly Learning Single-Index Models via Alignment Sharpness](https://icml.cc/virtual/2024/poster/34763) (Poster)
  - **Authors:** [Nikos Zarifis](http://openreview.net/profile?id=~Nikos_Zarifis1), [Puqian Wang](http://openreview.net/profile?id=~Puqian_Wang1), [Ilias Diakonikolas](http://openreview.net/profile?id=~Ilias_Diakonikolas1), [Jelena Diakonikolas](http://openreview.net/profile?id=~Jelena_Diakonikolas2)
  - **Affiliations:** Department of Computer Science, University of Wisconsin - Madison, Madison, WI, USA, Department of Computer Science, University of Wisconsin - Madison, Madison, WI, USA, Department of Computer Science, University of Wisconsin - Madison, Madison, WI, USA, Department of Computer Science, University of Wisconsin - Madison, Madison, WI, USA
  - **TL;DR:** This paper presents an efficient learning algorithm for Single-Index Models under L2 loss in an agnostic setting, achieving a constant factor approximation to the optimal loss across various distributions. The key innovation is a novel concept called alignment sharpness, which aids in the analysis and implementation of the learning process.
  - **Keywords:** Single-Index Models, Agnostic Learning, Efficient learning algorithm, Local error bound, Alignment sharpness, Learning under L2 loss, Adversarial label noise, Unknown link function, Constant factor approximation, Robust learning, Monotone link functions, Lipschitz properties


- [In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought](https://icml.cc/virtual/2024/poster/33289) (Poster)
  - **Authors:** [sili huang](http://openreview.net/profile?id=~Sili_Huang1), [Jifeng Hu](http://openreview.net/profile?id=~Jifeng_Hu1), [Hechang Chen](http://openreview.net/profile?id=~Hechang_Chen2), [Lichao Sun](http://openreview.net/profile?id=~Lichao_Sun1), [Bo Yang](http://openreview.net/profile?id=~Bo_Yang6)
  - **Affiliations:** School of Artificial Intelligence, Jilin University, China; Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China, School of Artificial Intelligence, Jilin University, China, School of Artificial Intelligence, Jilin University, China, Lehigh University, Bethlehem, Pennsylvania, USA, Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China
  - **TL;DR:** The study introduces the In-context Decision Transformer (IDT), which enhances self-improvement in reinforcement learning through a hierarchical decision-making approach, significantly reducing computational costs. Experimental results demonstrate that IDT outperforms existing methods in long-horizon tasks, achieving faster online evaluation times.
  - **Keywords:** In-context learning, Reinforcement learning (RL), Hierarchical decision-making, Decision Transformer, Trial-and-error learning, Offline reinforcement learning, Online tasks, High computational costs, Long-horizon tasks, State-of-the-art performance in long-horizon tasks, Efficient sequence reconstruction, D4RL benchmark, Grid World benchmark


- [IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers](https://icml.cc/virtual/2024/poster/33657) (Poster)
  - **Authors:** [Zhanpeng Zeng](http://openreview.net/profile?id=~Zhanpeng_Zeng1), [Karthikeyan Sankaralingam](http://openreview.net/profile?id=~Karthikeyan_Sankaralingam1), [Vikas Singh](http://openreview.net/profile?id=~Vikas_Singh1)
  - **Affiliations:** University of Wisconsin–Madison, University of Wisconsin–Madison; NVIDIA Research, University of Wisconsin–Madison
  - **TL;DR:** This study demonstrates that integer representations can effectively replace low bit-width integers in General Matrix Multiply (GEMM) operations for deep learning models without sophisticated error correction techniques. The proposed Integer Matrix Unpacking (IM-Unpack) algorithm allows for efficient computation while maintaining accuracy, with minimal additional overhead.
  - **Keywords:** deep learning, efficiency in matrix operations, General Matrix Multiply (GEMM), Integer Matrix Unpacking (IM-Unpack), rounding errors in low bit-width integer representation, efficiency gains in GEMM, equivalence of integer GEMM with low bit-width integer GEMMs, minimal overhead in additional operations, low bit-width integers, Transformer-based models


- [Differentiable Annealed Importance Sampling Minimizes The Jensen-Shannon Divergence Between Initial and Target Distribution](https://icml.cc/virtual/2024/poster/32935) (Poster)
  - **Authors:** [Johannes Zenn](http://openreview.net/profile?id=~Johannes_Zenn1), [Robert Bamler](http://openreview.net/profile?id=~Robert_Bamler1)
  - **Affiliations:** Tübingen AI Center; University of Tübingen; IMPRS-IS, Tübingen AI Center; University of Tübingen; IMPRS-IS
  - **TL;DR:** This paper demonstrates that Differentiable Annealed Importance Sampling (DAIS) minimizes the Jensen-Shannon divergence between initial and target distributions, providing a parametric fit to intractable target distributions. The empirical evaluation shows that the initial distribution can yield more accurate uncertainty estimates compared to standard variational inference methods.
  - **Keywords:** Differentiable Annealed Importance Sampling, Variational Inference, Annealed Importance Sampling (AIS), Markov Chain Monte Carlo (MCMC), Bayesian model selection, Intractable target distribution, uncertainty estimation, Minimization of Jensen-Shannon divergence, parametric approximation of target distribution, Jensen-Shannon divergence, Kullback-Leibler divergence


- [tnGPS: Discovering Unknown Tensor Network Structure Search Algorithms via Large Language Models (LLMs)](https://icml.cc/virtual/2024/poster/34292) (Poster)
  - **Authors:** [Junhua Zeng](http://openreview.net/profile?id=~Junhua_Zeng1), [Chao Li](http://openreview.net/profile?id=~Chao_Li12), [Zhun Sun](http://openreview.net/profile?id=~Zhun_Sun1), [Qibin Zhao](http://openreview.net/profile?id=~Qibin_Zhao1), [Guoxu Zhou](http://openreview.net/profile?id=~Guoxu_Zhou1)
  - **Affiliations:** School of Automation, Guangdong University of Technology, Guangzhou, China, RIKEN Center for Advanced Intelligence Project (RIKEN-AIP), Tokyo, Japan, Tencent Inc., Shenzhen, China, RIKEN Center for Advanced Intelligence Project (RIKEN-AIP), Tokyo, Japan, School of Automation, Guangdong University of Technology, Guangzhou, China; Key Laboratory of Intelligent Detection and the Internet of Things in Manufacturing, Ministry of Education, Guangzhou, China
  - **TL;DR:** This study introduces tnGPS, an automatic algorithm discovery framework that utilizes large language models to discover new tensor network structure search algorithms, addressing the challenges of high-dimensional representation and model selection. The experimental results show that the algorithms generated by tnGPS outperform existing state-of-the-art methods in benchmarks.
  - **Keywords:** Tensor networks, Tensor network structure search (TN-SS), Large language models (LLMs), Sampling-based algorithms, Heuristic strategies, TNGA, GREEDY, TNLS, TnALE, Machine learning, High-dimensional representation, Curse of dimensionality, Local convergence, Model selection, Automatic algorithm discovery framework (tnGPS), New TN-SS algorithms


- [Graph Mixup on Approximate Gromov–Wasserstein Geodesics](https://icml.cc/virtual/2024/poster/34128) (Poster)
  - **Authors:** [Zhichen Zeng](http://openreview.net/profile?id=~Zhichen_Zeng1), [Ruizhong Qiu](http://openreview.net/profile?id=~Ruizhong_Qiu1), [Zhe Xu](http://openreview.net/profile?id=~Zhe_Xu5), [Zhining Liu](http://openreview.net/profile?id=~Zhining_Liu1), [Yuchen Yan](http://openreview.net/profile?id=~Yuchen_Yan1), [Tianxin Wei](http://openreview.net/profile?id=~Tianxin_Wei1), [Lei Ying](http://openreview.net/profile?id=~Lei_Ying1), [Jingrui He](http://openreview.net/profile?id=~Jingrui_He1), [Hanghang Tong](http://openreview.net/profile?id=~Hanghang_Tong3)
  - **Affiliations:** University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Michigan, Ann Arbor, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign
  - **TL;DR:** This study introduces GEOMIX, a method for mixing graphs on Gromov-Wasserstein geodesics to enhance graph data augmentation. The proposed approach ensures sample-label consistency and improves the generalization and robustness of Graph Neural Networks (GNNs).
  - **Keywords:** Graph data augmentation, Graph Neural Networks (GNNs), Gromov-Wasserstein (GW) distance, GEOMIX, Graph classification, Node classification, Link prediction, Space disparity, Non-Euclidean data, Sample-label consistency, Model overfitting, Accelerated mixup algorithm, Improved generalization and robustness of GNN models


- [Token-level Direct Preference Optimization](https://icml.cc/virtual/2024/poster/35149) (Poster)
  - **Authors:** [Yongcheng Zeng](http://openreview.net/profile?id=~Yongcheng_Zeng1), [Guoqing Liu](http://openreview.net/profile?id=~Guoqing_Liu3), [Weiyu Ma](http://openreview.net/profile?id=~Weiyu_Ma1), [Ning Yang](http://openreview.net/profile?id=~Ning_Yang5), [Haifeng Zhang](http://openreview.net/profile?id=~Haifeng_Zhang3), [Jun Wang](http://openreview.net/profile?id=~Jun_Wang2)
  - **Affiliations:** Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Microsoft Research AI, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences, University College London
  - **TL;DR:** This paper presents Token-level Direct Preference Optimization (TDPO), a method for fine-tuning Large Language Models (LLMs) to better align with human preferences by optimizing at the token level. Experimental results show that TDPO outperforms existing methods in generating high-quality, diverse responses in various text tasks.
  - **Keywords:** Large Language Models, Human Alignment, Token-level Direct Preference Optimization, KL Divergence, Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), Text Generation, Sentiment Generation, Dialogue Systems, Divergence Efficiency, Alignment with Human Preferences, Improved Response Quality, Enhanced Generation Diversity


- [Tight Partial Identification of Causal Effects with Marginal Distribution of Unmeasured Confounders](https://icml.cc/virtual/2024/poster/33322) (Spotlight Poster)
  - **Authors:** [Zhiheng Zhang](http://openreview.net/profile?id=~Zhiheng_Zhang1)
  - **Affiliations:** Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China
  - **TL;DR:** This paper addresses the challenge of partial identification in causal inference due to unmeasured confounders, proposing a closed-form solution for tight partial identification using only the marginal distribution of confounders. The findings indicate that as the number of confounders increases, the marginal confounder information contributes minimally to the identification of causal effects.
  - **Keywords:** Partial identification, causal inference, latent confounders, Medicine, economics, education, climate, Incomplete measurement of confounders, unmeasured confounders, identification of causal effects, Closed-form tight partial identification, if and only if criteria for causal queries, Simpson’s paradox, back-door criteria, vanilla bound


- [DAG-Based Column Generation for Adversarial Team Games](https://icml.cc/virtual/2024/poster/35190) (Poster)
  - **Authors:** [Youzhi Zhang](http://openreview.net/profile?id=~Youzhi_Zhang2), [Bo An](http://openreview.net/profile?id=~Bo_An2), [Daniel Zeng](http://openreview.net/profile?id=~Daniel_Dajun_Zeng1)
  - **Affiliations:** Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, Nanyang Technological University, Singapore; Skywork AI, Singapore, Institute of Automation, Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This paper presents a novel TB-DAG CG (DCG) algorithm framework for solving sequential adversarial team games, addressing the challenges of exponential strategy spaces and asymmetric information among team members. The proposed method demonstrates exponential convergence improvements over existing algorithms and achieves significantly faster performance in practical applications.
  - **Keywords:** Adversarial team games, Coordination in games, Column Generation (CG), Best Response Oracle (BRO), Team Belief Directed Acyclic Graph (TB-DAG), Game theory, Zero-sum games, Exponential strategy space, Asymmetric information, Coordination challenges, TB-DAG CG (DCG) algorithm framework, Improved scalability, Faster convergence, Team-Maxmin Equilibrium with Coordination device (TMECor), Nash equilibrium


- [Provably Efficient Partially Observable Risk-sensitive Reinforcement Learning with Hindsight Observation](https://icml.cc/virtual/2024/poster/34978) (Poster)
  - **Authors:** [Tonghe Zhang](http://openreview.net/profile?id=~Tonghe_Zhang1), [Yu Chen](http://openreview.net/profile?id=~Yu_Chen19), [Longbo Huang](http://openreview.net/profile?id=~Longbo_Huang2)
  - **Affiliations:** Department of Electronic Engineering, Tsinghua University, Beijing, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China
  - **TL;DR:** This study introduces a novel algorithm for risk-sensitive reinforcement learning in partially observable environments, utilizing hindsight observations to optimize accumulated rewards under an entropic risk measure. The proposed algorithm achieves polynomial regret, addressing significant theoretical gaps in existing research.
  - **Keywords:** Risk-sensitive reinforcement learning, Partially observable environments, Partially Observable Markov Decision Process (POMDP), Hindsight observations, Bellman equations, Autonomous driving, Stock market prediction, Cybersecurity, Sample efficiency, Incomplete information, Non-linear risk measures, Polynomial regret analysis, Efficient RL algorithm, Entropic risk measure, Change-of-measure, Beta vectors


- [CaM: Cache Merging for Memory-efficient LLMs Inference](https://icml.cc/virtual/2024/poster/34310) (Poster)
  - **Authors:** [Yuxin Zhang](http://openreview.net/profile?id=~Yuxin_Zhang3), [Yuxuan Du](http://openreview.net/profile?id=~Yuxuan_Du3), [Gen Luo](http://openreview.net/profile?id=~Gen_Luo1), [Yunshan Zhong](http://openreview.net/profile?id=~Yunshan_Zhong1), [Zhenyu Zhang](http://openreview.net/profile?id=~Zhenyu_Zhang4), [Shiwei Liu](http://openreview.net/profile?id=~Shiwei_Liu2), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University; None, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, None, University of Texas at Austin, University of Oxford; Eindhoven University of Technology, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University; Institute of Artificial Intelligence, Xiamen University
  - **TL;DR:** This paper presents Cache Merging (CaM) as a method to enhance the memory efficiency of Large Language Models (LLMs) during inference by adaptively merging to-be-evicted caches, thereby preserving critical token information. Extensive experiments demonstrate that CaM significantly improves the performance of memory-efficient LLMs while reducing the memory footprint of the Key-Value cache.
  - **Keywords:** Large Language Models (LLMs), Memory Efficiency, Cache Merging (CaM), Key-Value (KV) Cache, Memory consumption, Output perturbation, Information loss, Improved performance of memory-efficient LLMs, LLaMA, OPT, GPT-NeoX, Attention scores, Token selection


- [Efficient Stochastic Approximation of Minimax Excess Risk Optimization](https://icml.cc/virtual/2024/poster/33083) (Poster)
  - **Authors:** [Lijun Zhang](http://openreview.net/profile?id=~Lijun_Zhang1), [Haomin Bai](http://openreview.net/profile?id=~Haomin_Bai1), [Wei-Wei Tu](http://openreview.net/profile?id=~Wei-Wei_Tu1), [Ping Yang](http://openreview.net/profile?id=~Ping_Yang4), [Yao Hu](http://openreview.net/profile?id=~Yao_Hu1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Pazhou Laboratory (Huangpu), Guangzhou, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China, Artificial Productivity Inc., Beijing, China, Xiaohongshu Inc., Beijing, China, Xiaohongshu Inc., Beijing, China
  - **TL;DR:** This paper develops efficient stochastic approximation methods for minimizing the minimax excess risk optimization (MERO), addressing challenges posed by heterogeneous noise in distributionally robust optimization. The proposed methods leverage stochastic convex optimization techniques to achieve nearly optimal convergence rates, even in scenarios with varying sample sizes from different distributions.
  - **Keywords:** Minimax Excess Risk Optimization (MERO), Distributionally Robust Optimization (DRO), Stochastic Approximation, Stochastic Convex Optimization, Stochastic Convex-Concave Optimization (SCCO), Machine Learning, Generalization across Distributions, Distribution Shifts, Heterogeneous Noise, Performance Degradation, Efficient Algorithms for MERO, Nearly Optimal Convergence Rate, Excess Risk, Regret, Group DRO (GDRO)


- [MILP-FBGen: LP/MILP Instance Generation with Feasibility/Boundedness](https://icml.cc/virtual/2024/poster/33328) (Poster)
  - **Authors:** [Yahong Zhang](http://openreview.net/profile?id=~Yahong_Zhang2), [Chenchen Fan](http://openreview.net/profile?id=~Chenchen_Fan1), [Donghui Chen](http://openreview.net/profile?id=~Donghui_Chen2), [Congrui Li](http://openreview.net/profile?id=~Congrui_Li1), [Wenli Ouyang](http://openreview.net/profile?id=~Wenli_Ouyang1), [Mingda Zhu](http://openreview.net/profile?id=~Mingda_Zhu1), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** AI Lab, Lenovo Research, Beijing, China, AI Lab, Lenovo Research, Beijing, China, AI Lab, Lenovo Research, Beijing, China, AI Lab, Lenovo Research, Beijing, China, AI Lab, Lenovo Research, Beijing, China; School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, AI Lab, Lenovo Research, Beijing, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This paper introduces MILP-FBGen, a diffusion-based framework for generating LP/MILP instances that ensures feasibility and boundedness while preserving key properties. The method demonstrates superior performance in maintaining distributional similarity and achieving 100% feasibility across various datasets, outperforming existing techniques.
  - **Keywords:** LP (Linear Programming), MILP (Mixed Integer Linear Programming), Machine Learning, Diffusion-based generative framework, Structure-preserving generation module, Feasibility/boundedness-constrained sampling module, Logistics path planning, Matching problems, Production scheduling, Instance scarcity, Feasibility, Boundedness, MILP-FBGen framework, Preservation of key properties (hardness, feasibility, boundedness), Enhanced performance on downstream tasks


- [Watermarks in the Sand: Impossibility of Strong Watermarking for Language Models](https://icml.cc/virtual/2024/poster/33645) (Poster)
  - **Authors:** [Hanlin Zhang](http://openreview.net/profile?id=~Hanlin_Zhang1), [Benjamin Edelman](http://openreview.net/profile?id=~Benjamin_L._Edelman1), [Danilo Francati](http://openreview.net/profile?id=~Danilo_Francati1), [Daniele Venturi](http://openreview.net/profile?id=~Daniele_Venturi1), [Giuseppe Ateniese](http://openreview.net/profile?id=~Giuseppe_Ateniese1), [Boaz Barak](http://openreview.net/profile?id=~Boaz_Barak2)
  - **Affiliations:** Harvard University, Harvard University, George Mason University, Sapienza University of Rome, George Mason University, Harvard University
  - **TL;DR:** This paper investigates the impossibility of strong watermarking schemes for generative models, demonstrating that attackers can effectively remove watermarks with minimal quality degradation. The findings highlight significant challenges in ensuring the integrity of outputs from large language models against malicious misuse.
  - **Keywords:** Watermarking, Generative Models, Language Models, Watermarking schemes, Quality oracle, Perturbation oracle, Large Language Models, Vision-Language Models, Strong watermarking impossibility, Attacker capabilities, Quality degradation, Efficient watermark attack, Watermark removal, AI-generated content, Misinformation prevention


- [LQER: Low-Rank Quantization Error Reconstruction for LLMs](https://icml.cc/virtual/2024/poster/33540) (Poster)
  - **Authors:** [Cheng Zhang](http://openreview.net/profile?id=~Cheng_Zhang21), [Jianyi Cheng](http://openreview.net/profile?id=~Jianyi_Cheng1), [George Constantinides](http://openreview.net/profile?id=~George_Anthony_Constantinides1), [Yiren Zhao](http://openreview.net/profile?id=~Yiren_Zhao2)
  - **Affiliations:** Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom, Department of Computer Science and Technology, University of Cambridge, Cambridge, United Kingdom, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom
  - **TL;DR:** This study introduces Low-rank Quantization Error Reduction (LQER) to enhance the performance of Large Language Models (LLMs) through effective quantization techniques. The proposed method achieves near-lossless quantization while significantly reducing hardware resource requirements compared to existing state-of-the-art methods.
  - **Keywords:** Large Language Models, Post-Training Quantization, Low-rank approximation, Quantization Error Reduction, W4A8 quantization, Natural Language Processing, Model Deployment, Quantization error, Magnitude outliers, Computational cost, Near-lossless performance, Reduced hardware resources


- [Robust Learning-Augmented Dictionaries](https://icml.cc/virtual/2024/poster/33794) (Poster)
  - **Authors:** [Ali Zeynali](http://openreview.net/profile?id=~Ali_Zeynali1), [Shahin Kamali](http://openreview.net/profile?id=~Shahin_Kamali1), [Mohammad Hajiesmaili](http://openreview.net/profile?id=~Mohammad_Hajiesmaili1)
  - **Affiliations:** Manning College of Information & Computer Sciences, University of Massachusetts Amherst, USA, Department of Electrical Engineering & Computer Science, York University, Canada, Manning College of Information & Computer Sciences, University of Massachusetts Amherst, USA
  - **TL;DR:** The paper introduces RobustSL, a learning-augmented data structure for dictionaries that achieves optimal consistency and robustness through frequency predictions. It outperforms existing data structures in both synthetic and real datasets while maintaining logarithmic running time for operations.
  - **Keywords:** Learning-augmented data structures, Dictionary implementation, Skip lists, Frequency predictions, Database indexing, Operating systems, Optimal consistency, Robustness in data structures, RobustSL data structure, Static optimality, Adversarial predictions


- [Learning Reward for Robot Skills Using Large Language Models via Self-Alignment](https://icml.cc/virtual/2024/poster/33748) (Poster)
  - **Authors:** [Yuwei Zeng](http://openreview.net/profile?id=~Yuwei_Zeng1), [Yao Mu](http://openreview.net/profile?id=~Yao_Mu1), [Lin Shao](http://openreview.net/profile?id=~Lin_Shao2)
  - **Affiliations:** National University of Singapore, The University of Hong Kong, National University of Singapore
  - **TL;DR:** This study proposes a method for efficiently learning reward functions for robot skills using Large Language Models (LLMs) through a self-alignment process, which minimizes ranking inconsistencies based on execution feedback. The approach shows improved training efficacy and efficiency while reducing the need for human input and expert demonstrations.
  - **Keywords:** Learning reward functions, Robot skills, Large Language Models, Reinforcement learning, Inverse reinforcement learning, Self-alignment process, Robotics, Skill acquisition, Bottleneck in learning reward functions, Imprecise reward functions, Costly expert demonstration gathering, Efficient reward learning method, Improved training efficacy and efficiency, Large Language Models (LLMs), Chain of Thought (CoT)


- [Discounted Adaptive Online Learning: Towards Better Regularization](https://icml.cc/virtual/2024/poster/33724) (Poster)
  - **Authors:** [Zhiyu Zhang](http://openreview.net/profile?id=~Zhiyu_Zhang1), [David Bombara](http://openreview.net/profile?id=~David_Bombara1), [Heng Yang](http://openreview.net/profile?id=~Heng_Yang4)
  - **Affiliations:** Harvard University, Harvard University, Harvard University
  - **TL;DR:** This study addresses the challenges of online learning in adversarial nonstationary environments by proposing an adaptive FTRL-based algorithm that enhances regularization through better inductive bias management. The findings suggest that improved regularizers can be designed using principles from adaptive online optimization, leading to instance-dependent performance guarantees.
  - **Keywords:** online learning, adversarial nonstationary environments, lifelong learning, discounted regret, online convex optimization, FTRL-based algorithm, gradient descent, forgetting history, data shortage, inductive bias, improved regularizers, instance-dependent performance guarantees, Online Conformal Prediction (OCP), adaptive online optimization


- [Advancing DRL Agents in Commercial Fighting Games: Training, Integration, and Agent-Human Alignment](https://icml.cc/virtual/2024/poster/33504) (Poster)
  - **Authors:** [Chen Zhang](http://openreview.net/profile?id=~Chen_Zhang22), [Qiang HE](http://openreview.net/profile?id=~Qiang_He1), [Yuan Zhou](http://openreview.net/profile?id=~Yuan_Zhou9), [Elvis S. Liu](http://openreview.net/profile?id=~Elvis_S._Liu1), [Hong Wang](http://openreview.net/profile?id=~Hong_Wang8), [Jian Zhao](http://openreview.net/profile?id=~Jian_Zhao7), [Yang Wang](http://openreview.net/profile?id=~Yang_Wang32)
  - **Affiliations:** University of Science and Technology of China (USTC), Hefei, China; None, Institute of Automation, Chinese Academy of Sciences, China, Tencent Games, Tencent Games, Tencent Games, University of Science and Technology of China (USTC), Hefei, China; None, University of Science and Technology of China (USTC), Hefei, China; Key Laboratory of Precision and Intelligent Chemistry, USTC, China; Suzhou Institute for Advanced Research, USTC, China
  - **TL;DR:** This paper presents Sh¯ukai, a DRL agent system designed for fighting games, which enhances player experience by balancing agent competence and generalizability. Sh¯ukai demonstrates significant improvements in sample efficiency and serves as an effective training partner for players in Naruto Mobile.
  - **Keywords:** Deep Reinforcement Learning (DRL), Fighting Games, Player Interaction, Heterogeneous League Training (HELT), DRL Agent System, Commercial Gaming, Player versus Environment (PvE), Prolonged Player Interaction, Overfitting, Varying Competitiveness, Generalization Ability, Sample Efficiency Improvement, Naruto Mobile


- [Deep Regression Representation Learning with Topology](https://icml.cc/virtual/2024/poster/34457) (Poster)
  - **Authors:** [Shihao Zhang](http://openreview.net/profile?id=~Shihao_Zhang1), [Kenji Kawaguchi](http://openreview.net/profile?id=~Kenji_Kawaguchi1), [Angela Yao](http://openreview.net/profile?id=~Angela_Yao1)
  - **Affiliations:** National University of Singapore, National University of Singapore, National University of Singapore
  - **TL;DR:** This study explores the influence of topology on regression representation learning, establishing connections with the Information Bottleneck principle. It introduces PH-Reg, a regularizer that aligns the intrinsic dimension and topology of the feature space with the target space, demonstrating its effectiveness through experiments.
  - **Keywords:** representation learning, regression, topology, Information Bottleneck (IB) principle, PH-Reg regularizer, class separation vs. ordinal relationship, complexity of representation, generalization error, connections between topology and IB principle, benefits of PH-Reg, intrinsic dimension, conditional entropy, 0th Betti number


- [Random Scaling and Momentum for Non-smooth Non-convex Optimization](https://icml.cc/virtual/2024/poster/34207) (Poster)
  - **Authors:** [Qinzi Zhang](http://openreview.net/profile?id=~Qinzi_Zhang1), [Ashok Cutkosky](http://openreview.net/profile?id=~Ashok_Cutkosky1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Boston University, Boston, USA, Department of Electrical and Computer Engineering, Boston University, Boston, USA
  - **TL;DR:** This paper presents a modified stochastic gradient descent with momentum (SGDM) algorithm that incorporates random scaling to optimize non-convex loss functions in neural networks. The proposed method achieves optimal convergence guarantees and provides a new theoretical framework for non-convex optimization.
  - **Keywords:** non-convex optimization, neural networks, stochastic gradient descent, stochastic gradient descent with momentum (SGDM), online convex optimization, machine learning, deep learning, optimizing non-convex loss functions, finding global minimum, optimal convergence guarantees, new theoretical framework for optimization algorithms


- [Parameter-Efficient Fine-Tuning with Controls](https://icml.cc/virtual/2024/poster/34699) (Poster)
  - **Authors:** [Chi Zhang](http://openreview.net/profile?id=~Chi_Zhang25), [Jingpu Cheng](http://openreview.net/profile?id=~Cheng_Jingpu1), [Yanyu Xu](http://openreview.net/profile?id=~Yanyu_Xu1), [Qianxiao Li](http://openreview.net/profile?id=~Qianxiao_Li1)
  - **Affiliations:** Department of Maths, National University of Singapore, Singapore, Department of Maths, National University of Singapore, Singapore, The Joint SDU-NTU Research Center of Artificial Intelligence, Shandong University, China, Department of Maths, National University of Singapore, Singapore
  - **TL;DR:** This paper presents a novel perspective on Low-Rank Adaptation (LoRA) by framing it as a control process, introducing control modules that enhance model adaptability without additional parameters. Empirical results demonstrate that this approach outperforms traditional LoRA methods across various datasets and configurations.
  - **Keywords:** Parameter-Efficient Fine-Tuning, Control Process, Low-Rank Adaptation (LoRA), Control Modules, Nonlinearities, Parameter-Free Attention Mechanism, Natural Language Processing, Computer Vision, Speech Recognition, GPU Memory Consumption, Model Adaptation, Enhanced Adaptability and Performance, Surpassing LoRA Algorithms, Imagenet-21K, JFT, Transformers, Attention Mechanism


- [Understanding Unimodal Bias in Multimodal Deep Linear Networks](https://icml.cc/virtual/2024/poster/34681) (Poster)
  - **Authors:** [Yedi Zhang](http://openreview.net/profile?id=~Yedi_Zhang3), [Peter Latham](http://openreview.net/profile?id=~Peter_E._Latham1), [Andrew Saxe](http://openreview.net/profile?id=~Andrew_M_Saxe1)
  - **Affiliations:** Gatsby Computational Neuroscience Unit, University College London; Sainsbury Wellcome Centre, University College London, Gatsby Computational Neuroscience Unit, University College London, Gatsby Computational Neuroscience Unit, University College London; Sainsbury Wellcome Centre, University College London
  - **TL;DR:** This study investigates unimodal bias in multimodal deep linear networks, revealing that the depth of modality fusion affects the duration of the unimodal phase during training. The findings indicate that longer unimodal phases can lead to generalization deficits and permanent bias, particularly in overparameterized settings.
  - **Keywords:** multimodal deep learning, unimodal bias, deep linear networks, visual question answering, unimodal bias, generalization deficit, overparameterization, theory of unimodal bias, duration of unimodal phase


- [Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining](https://icml.cc/virtual/2024/poster/35091) (Poster)
  - **Authors:** [Qi Zhang](http://openreview.net/profile?id=~Qi_Zhang28), [Tianqi Du](http://openreview.net/profile?id=~Tianqi_Du1), [Haotian Huang](http://openreview.net/profile?id=~Haotian_Huang1), [Yifei Wang](http://openreview.net/profile?id=~Yifei_Wang1), [Yisen Wang](http://openreview.net/profile?id=~Yisen_Wang1)
  - **Affiliations:** National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, China, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, China, Sun Yat-Sen University, China, MIT CSAIL, USA, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, China; Institute for Artificial Intelligence, Peking University, China
  - **TL;DR:** This paper presents a theoretical comparison between autoregressive and masked self-supervised learning paradigms, highlighting their strengths and limitations in classification and content generation tasks. The authors propose new objectives to enhance the performance of both paradigms by leveraging their respective strengths.
  - **Keywords:** generative self-supervised learning (SSL), theoretical comparison, autoregressive SSL, masked SSL, classification tasks, content generation tasks, inter-sample connections, fixed vs. flexible token lengths, diversity-enhanced autoregressive objectives, variable-length masked objectives


- [More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning](https://icml.cc/virtual/2024/poster/33247) (Poster)
  - **Authors:** [Kaiwen Wang](http://openreview.net/profile?id=~Kaiwen_Wang1), [Owen Oertell](http://openreview.net/profile?id=~Owen_Oertell1), [Alekh Agarwal](http://openreview.net/profile?id=~Alekh_Agarwal2), [Nathan Kallus](http://openreview.net/profile?id=~Nathan_Kallus1), [Wen Sun](http://openreview.net/profile?id=~Wen_Sun1)
  - **Affiliations:** Cornell University, Cornell University, Google Research, Cornell University, Cornell University
  - **TL;DR:** This paper demonstrates that Distributional Reinforcement Learning (DistRL) can achieve tighter second-order bounds in both online and offline settings, outperforming traditional small-loss bounds by scaling with the variance of return. The findings suggest that DistRL is a promising framework for improving performance and sample efficiency in reinforcement learning tasks.
  - **Keywords:** Distributional Reinforcement Learning, Second-Order Bounds, Maximum Likelihood Estimation (MLE), Distributional Loss Functions, Online Reinforcement Learning, Offline Reinforcement Learning, Contextual Bandits, Variance of Return, Cumulative Cost Minimization, Tighter Second-Order Bounds, Empirical Validation of DistRL, Small-Loss Bounds, Low-Rank MDPs, Optimism in the Face of Uncertainty


- [SF-DQN: Provable Knowledge Transfer using Successor Feature for Deep Reinforcement Learning](https://icml.cc/virtual/2024/poster/33049) (Poster)
  - **Authors:** [Shuai Zhang](http://openreview.net/profile?id=~Shuai_Zhang6), [Heshan Fernando](http://openreview.net/profile?id=~Heshan_Devaka_Fernando1), [Miao Liu](http://openreview.net/profile?id=~Miao_Liu1), [Keerthiram Murugesan](http://openreview.net/profile?id=~Keerthiram_Murugesan1), [Songtao Lu](http://openreview.net/profile?id=~Songtao_Lu1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1), [Tianyi Chen](http://openreview.net/profile?id=~Tianyi_Chen5), [Meng Wang](http://openreview.net/profile?id=~Meng_Wang4)
  - **Affiliations:** Department of Data Science, New Jersey Institute of Technology, Newark, NJ, Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, IBM Thomas J. Watson Research Center, IBM Research, Yorktown Heights, NY, IBM Thomas J. Watson Research Center, IBM Research, Yorktown Heights, NY, IBM Thomas J. Watson Research Center, IBM Research, Yorktown Heights, NY, IBM Thomas J. Watson Research Center, IBM Research, Yorktown Heights, NY, Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY
  - **TL;DR:** This paper investigates the transfer reinforcement learning problem, demonstrating that the SF-DQN framework with GPI significantly enhances knowledge transfer across tasks with different reward functions but shared dynamics. The findings indicate that SF-DQN outperforms traditional RL methods in terms of convergence rate and generalization capabilities.
  - **Keywords:** Transfer Reinforcement Learning, Knowledge Transfer, Successor Features (SF), Generalized Policy Improvement (GPI), Deep Q-Network (DQN), Robotics, Gaming, Autonomous Vehicles, Healthcare, Natural Language Processing, Sample Complexity, Task-Specific Reward Functions, Convergence Analysis, Generalization Guarantees, Q-function, Successor Representation (SR)


- [S3O: A Dual-Phase Approach for Reconstructing Dynamic Shape and Skeleton of Articulated Objects from Single Monocular Video](https://icml.cc/virtual/2024/poster/32700) (Poster)
  - **Authors:** [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang47), [Fang Li](http://openreview.net/profile?id=~Fang_Li8), [Samyak Rawlekar](http://openreview.net/profile?id=~Samyak_Rawlekar1), [Narendra Ahuja](http://openreview.net/profile?id=~Narendra_Ahuja1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA, Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA, Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA, Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA
  - **TL;DR:** The study presents S3O, a novel two-phase method for reconstructing dynamic articulated objects from a single monocular video, which significantly improves accuracy and reduces training time by approximately 60% compared to existing methods. The approach effectively learns parametric models without requiring extensive human annotations or predefined templates, enhancing generalizability.
  - **Keywords:** 3D reconstruction, articulated objects, monocular video, Synergistic Shape and Skeleton Optimization (S3O), parametric models, Dynamic object reconstruction, Joint estimation of shape, motion, and camera parameters; limited viewpoints; computational complexity, More accurate 3D reconstruction, plausible skeletons, reduced training time, PlanetZoo dataset


- [ILILT: Implicit Learning of Inverse Lithography Technologies](https://icml.cc/virtual/2024/poster/33660) (Poster)
  - **Authors:** [Haoyu Yang](http://openreview.net/profile?id=~Haoyu_Yang4), [Mark Ren](http://openreview.net/profile?id=~Haoxing_Ren1)
  - **Affiliations:** Design Automation Research Group, NVIDIA, Austin, TX, Design Automation Research Group, NVIDIA, Austin, TX
  - **TL;DR:** This paper presents ILILT, an implicit learning framework for inverse lithography technology that aims to directly generate high-quality optimized masks without relying on traditional ILT solvers. The proposed method significantly enhances efficiency and quality in semiconductor manufacturing processes.
  - **Keywords:** lithography, semiconductor manufacturing, mask optimization, machine learning, inverse lithography technology (ILT), implicit layer learning, GAN-OPC, nested UNet, ResNet, design and silicon mismatch, non-convexity of mask optimization problems, initialization challenges, ILILT framework, improved efficiency and quality in mask generation


- [Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation](https://icml.cc/virtual/2024/poster/33260) (Poster)
  - **Authors:** [Sergei Shumilin](http://openreview.net/profile?id=~Sergei_Shumilin1), [Alexander Ryabov](http://openreview.net/profile?id=~Alexander_Ryabov1), [Nikolay Yavich](http://openreview.net/profile?id=~Nikolay_Yavich1), [Evgeny Burnaev](http://openreview.net/profile?id=~Evgeny_Burnaev1), [Vladimir Vanovskiy](http://openreview.net/profile?id=~Vladimir_Vanovskiy1)
  - **Affiliations:** Applied AI Center, Skolkovo Institute of Science and Technology, Moscow, Russia; AIRI Institute, Moscow, Russia, Applied AI Center, Skolkovo Institute of Science and Technology, Moscow, Russia, Applied AI Center, Skolkovo Institute of Science and Technology, Moscow, Russia, Applied AI Center, Skolkovo Institute of Science and Technology, Moscow, Russia; AIRI Institute, Moscow, Russia, Applied AI Center, Skolkovo Institute of Science and Technology, Moscow, Russia
  - **TL;DR:** This study presents a novel algorithm for coarsening unstructured grids in numerical simulations using differentiable physics concepts, achieving up to a 10-fold reduction in grid points while preserving critical variable dynamics. The approach is applicable to various systems described by evolutionary partial differential equations.
  - **Keywords:** unstructured grid coarsening, numerical simulation, differentiable physics, k-means clustering, autodifferentiation, stochastic minimization algorithms, fluid flow modeling, subsurface fluid flow, partial differential equations (PDEs), high computational load, need for reduced grid size, maintaining accuracy, algorithm for grid coarsening, reduction of grid points by up to 10 times


- [Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation](https://icml.cc/virtual/2024/poster/33876) (Poster)
  - **Authors:** [Wanpeng Zhang](http://openreview.net/profile?id=~Wanpeng_Zhang1), [Yilin Li](http://openreview.net/profile?id=~Yilin_Li1), [Boyu Yang](http://openreview.net/profile?id=~Boyu_Yang2), [Zongqing Lu](http://openreview.net/profile?id=~Zongqing_Lu2)
  - **Affiliations:** School of Computer Science, Peking University, Center for Statistical Science, Peking University, School of Data Science, Fudan University, School of Computer Science, Peking University; Beijing Academy of Artificial Intelligence
  - **TL;DR:** This paper introduces the Causal-Origin Representation (COREP) algorithm to address non-stationarity in reinforcement learning by tracing the causal origins of changes in dynamics. The proposed method demonstrates superior resilience to non-stationarity compared to existing approaches.
  - **Keywords:** non-stationarity in reinforcement learning, causal relationships, Causal-Origin Representation (COREP), guided updating mechanism, reinforcement learning in complex environments, challenges of non-stationarity, environmental dynamics, stable graph representation, resilience to non-stationarity


- [Understanding and Diagnosing Deep Reinforcement Learning](https://icml.cc/virtual/2024/poster/32923) (Poster)
  - **Authors:** [Ezgi Korkmaz](http://openreview.net/profile?id=~Ezgi_Korkmaz2)
  - **Affiliations:** University College London (UCL)
  - **TL;DR:** This study introduces a method to analyze the sensitivities and instabilities in deep reinforcement learning policies, revealing that state-of-the-art robust training techniques can lead to larger oscillations in decision-making compared to standard training. The findings aim to enhance the understanding and reliability of deep neural policies in various applications.
  - **Keywords:** Deep Reinforcement Learning, Neural Policies, Decision Boundary Stability, Deep Neural Networks, Adversarial Training, Biotechnology, Automated Financial Systems, Arcade Learning Environment (ALE), Sensitivity to Non-Robust Features, Decision Boundary Instability, Techniques for Analyzing Policy Sensitivities, Identification of Unstable Directions, Arcade Learning Environment (ALE), Robust Training Techniques, Policy Vulnerabilities


- [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark](https://icml.cc/virtual/2024/poster/33979) (Poster)
  - **Authors:** [Yihua Zhang](http://openreview.net/profile?id=~Yihua_Zhang1), [Pingzhi Li](http://openreview.net/profile?id=~Pingzhi_Li1), [Junyuan Hong](http://openreview.net/profile?id=~Junyuan_Hong1), [Jiaxiang Li](http://openreview.net/profile?id=~Jiaxiang_Li1), [Yimeng Zhang](http://openreview.net/profile?id=~Yimeng_Zhang2), [Wenqing Zheng](http://openreview.net/profile?id=~Wenqing_Zheng1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1), [Jason Lee](http://openreview.net/profile?id=~Jason_D._Lee1), [Wotao Yin](http://openreview.net/profile?id=~Wotao_Yin1), [Mingyi Hong](http://openreview.net/profile?id=~Mingyi_Hong1), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1), [Sijia Liu](http://openreview.net/profile?id=~Sijia_Liu1), [Tianlong Chen](http://openreview.net/profile?id=~Tianlong_Chen1)
  - **Affiliations:** Michigan State University, The University of North Carolina at Chapel Hill, UT Austin, University of Minnesota Twin Cities, Michigan State University, UT Austin, IBM Research, Princeton University, DAMO Academy, Alibaba Group US, University of Minnesota Twin Cities, UT Austin, Michigan State University; IBM Research, The University of North Carolina at Chapel Hill; MIT; Harvard University
  - **TL;DR:** This study proposes a shift towards zeroth-order optimization for memory-efficient fine-tuning of large language models, addressing the significant memory overhead associated with back-propagation. The research introduces novel techniques and conducts a comprehensive benchmarking study, revealing important optimization principles and potential improvements in accuracy and efficiency.
  - **Keywords:** Memory-efficient fine-tuning, Large Language Models (LLMs), Zeroth-order optimization, Back-propagation-free optimization, ZO-SGD, Natural Language Processing (NLP), On-device training, Memory overhead from back-propagation, Memory-efficient fine-tuning challenges, Novel enhancements to ZO optimization, Benchmarking study of ZO optimization techniques


- [Improving Accuracy-robustness Trade-off via Pixel Reweighted Adversarial Training](https://icml.cc/virtual/2024/poster/34324) (Poster)
  - **Authors:** [Jiacheng Zhang](http://openreview.net/profile?id=~Jiacheng_Zhang7), [Feng Liu](http://openreview.net/profile?id=~Feng_Liu2), [Dawei Zhou](http://openreview.net/profile?id=~Dawei_Zhou3), [Jingfeng ZHANG](http://openreview.net/profile?id=~Jingfeng_Zhang1), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1)
  - **Affiliations:** School of Computing and Information Systems, The University of Melbourne, School of Computing and Information Systems, The University of Melbourne, State Key Laboratory of Integrated Services Networks, Xidian University, School of Computer Science, The University of Auckland / RIKEN AIP, Sydney AI Centre, The University of Sydney
  - **TL;DR:** This study introduces Pixel-reweighted Adversarial Training (PART), which optimizes the perturbation budget for less influential pixels to enhance model accuracy while maintaining robustness against adversarial examples. The proposed method demonstrates significant improvements in accuracy on standard datasets without compromising robustness.
  - **Keywords:** Adversarial Training, Robust Classification, Pixel-reweighted Adversarial Training (PART), Class Activation Mapping (CAM), Image Recognition, Autonomous Driving, Accuracy-robustness Trade-off, Adversarial Examples (AEs), Improved Accuracy, Robustness, CIFAR-10, SVHN, TinyImagenet-200, ℓ∞-norm, Adversarial Examples (AEs)


- [UP2ME: Univariate Pre-training to Multivariate Fine-tuning as a General-purpose Framework for Multivariate Time Series Analysis](https://icml.cc/virtual/2024/poster/33686) (Poster)
  - **Authors:** [Yunhao Zhang](http://openreview.net/profile?id=~Yunhao_Zhang1), [Liu Minghao](http://openreview.net/profile?id=~Minghao_Liu6), [Shengyang Zhou](http://openreview.net/profile?id=~Shengyang_Zhou1), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University
  - **TL;DR:** The study introduces UP2ME, a general-purpose framework for multivariate time series analysis that utilizes univariate pre-training followed by multivariate fine-tuning to achieve state-of-the-art performance in forecasting, imputation, and anomaly detection. The framework effectively addresses the challenges of task-specific model selection and cross-channel dependencies.
  - **Keywords:** Multivariate Time Series Analysis, Self-Supervised Learning, Univariate Pre-training, Multivariate Fine-tuning, Masked AutoEncoder (MAE), Forecasting, Imputation, Anomaly Detection, Heterogeneity of temporal and cross-channel dependencies, Task-specific model selection, General-purpose framework, State-of-the-art (SOTA) performance, Eight real-world datasets


- [A Federated Stochastic Multi-level Compositional Minimax Algorithm for Deep AUC Maximization](https://icml.cc/virtual/2024/poster/34192) (Poster)
  - **Authors:** [Xinwen Zhang](http://openreview.net/profile?id=~Xinwen_Zhang3), [Ali Payani](http://openreview.net/profile?id=~Ali_Payani1), [Myungjin Lee](http://openreview.net/profile?id=~Myungjin_Lee1), [Richard Souvenir](http://openreview.net/profile?id=~Richard_Souvenir2), [Hongchang Gao](http://openreview.net/profile?id=~Hongchang_Gao3)
  - **Affiliations:** Department of Computer and Information Sciences, Temple University, Philadelphia, USA, Cisco Systems Inc., Cisco Systems Inc., Department of Computer and Information Sciences, Temple University, Philadelphia, USA, Department of Computer and Information Sciences, Temple University, Philadelphia, USA
  - **TL;DR:** This study proposes a novel federated multi-level compositional minimax algorithm to maximize AUC, addressing the limitations of existing methods in handling imbalanced data classification. Empirical evaluations demonstrate the effectiveness of the proposed approach in improving performance over traditional federated AUC maximization techniques.
  - **Keywords:** Federated Learning, AUC Maximization, Minimax Optimization, Multi-Level Compositional Optimization, Imbalanced Data Classification, Disease Prediction, Imbalanced Data Distribution, Scalability Issues in AUC Maximization, Federated Multi-Level Compositional Minimax Algorithm, Enhanced Feature Learning


- [Multi-Factor Adaptive Vision Selection for Egocentric Video Question Answering](https://icml.cc/virtual/2024/poster/32847) (Poster)
  - **Authors:** [Haoyu Zhang](http://openreview.net/profile?id=~Haoyu_Zhang4), [Meng Liu](http://openreview.net/profile?id=~Meng_Liu4), [Zixin Liu](http://openreview.net/profile?id=~Zixin_Liu3), [Xuemeng Song](http://openreview.net/profile?id=~Xuemeng_Song2), [Yaowei Wang](http://openreview.net/profile?id=~Yaowei_Wang1), [Liqiang Nie](http://openreview.net/profile?id=~Liqiang_Nie2)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China, School of Computer Science and Technology, Shandong Jianzhu University, Jinan, China, Peng Cheng Laboratory, Shenzhen, China, School of Computer Science and Technology, Shandong University, Qingdao, China, Peng Cheng Laboratory, Shenzhen, China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China
  - **TL;DR:** This study introduces the Multi-Factor Adaptive vision Selection (MFAS) framework to tackle challenges in egocentric video question answering, specifically addressing small object recognition, noise suppression, and spatial-temporal reasoning. Extensive experiments demonstrate the framework's effectiveness and generalization across various public egocentric datasets.
  - **Keywords:** Egocentric video understanding, Video Question Answering (VideoQA), Multi-Factor Adaptive vision Selection (MFAS), patch partition and merging module, prior-guided patch selection module, hierarchical aggregation network, Smart assistive technologies, augmented reality, smart glasses, Small object recognition, noise suppression, spatial-temporal reasoning, Enhanced small object recognition, focused analysis, visual semantics aggregation, EgoVQA, EgoTaskQA, QAEgo4D


- [MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions](https://icml.cc/virtual/2024/poster/33731) (Oral)
  - **Authors:** [Kai Zhang](http://openreview.net/profile?id=~Kai_Zhang10), [Yi Luan](http://openreview.net/profile?id=~Yi_Luan1), [Hexiang Hu](http://openreview.net/profile?id=~Hexiang_Hu1), [Kenton Lee](http://openreview.net/profile?id=~Kenton_Lee1), [Siyuan Qiao](http://openreview.net/profile?id=~Siyuan_Qiao1), [Wenhu Chen](http://openreview.net/profile?id=~Wenhu_Chen3), [Yu Su](http://openreview.net/profile?id=~Yu_Su2), [Ming-Wei Chang](http://openreview.net/profile?id=~Ming-Wei_Chang3)
  - **Affiliations:** The Ohio State University; Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, The Ohio State University, Google DeepMind
  - **TL;DR:** The study introduces MagicLens, a self-supervised image retrieval model that utilizes open-ended text instructions to capture diverse search intents beyond visual similarity. It demonstrates improved retrieval accuracy and efficiency compared to prior methods, achieving competitive results on multiple benchmarks.
  - **Keywords:** image retrieval, self-supervised learning, open-ended instructions, foundation models, implicit relations, visual search, object localization, re-identification, ambiguous definitions in image search, capturing diverse search intents, MagicLens model, high parameter efficiency, improved retrieval accuracy, 36.7M triplets, 1.4M-image unseen corpus


- [Sparse-to-dense Multimodal Image Registration via Multi-Task Learning](https://icml.cc/virtual/2024/poster/33013) (Poster)
  - **Authors:** [Kaining Zhang](http://openreview.net/profile?id=~Kaining_Zhang2), [Jiayi Ma](http://openreview.net/profile?id=~Jiayi_Ma2)
  - **Affiliations:** Electronic Information School, Wuhan University, Wuhan, China, Electronic Information School, Wuhan University, Wuhan, China
  - **TL;DR:** This paper presents SDME, a novel Sparse-to-Dense Multimodal feature Extractor that enhances image registration by combining Sparse feature Matching and Dense direct Alignment. The proposed method demonstrates significant performance improvements across various multimodal datasets while maintaining robust generalization capabilities.
  - **Keywords:** multimodal image registration, image alignment, Sparse feature Matching (SM), Dense direct Alignment (DA), multi-task network, computer vision, robotics, remote sensing, medical imaging, large motion variations, modality difference, textureless scenes, Sparse-to-Dense Multimodal feature Extractor (SDME), robust registration, MSCOCO, GoogleEarth, VIS-NIR, VIS-IR-drone, homography, keypoint detection, viewpoint-invariant descriptors


- [Wukong: Towards a Scaling Law for Large-Scale Recommendation](https://icml.cc/virtual/2024/poster/34838) (Poster)
  - **Authors:** [Buyun Zhang](http://openreview.net/profile?id=~Buyun_Zhang1), [Liang Luo](http://openreview.net/profile?id=~Liang_Luo2), [Yuxin Chen](http://openreview.net/profile?id=~Yuxin_Chen10), [Jade Nie](http://openreview.net/profile?id=~Jade_Nie1), [Xi Liu](http://openreview.net/profile?id=~Xi_Liu1), [Shen Li](http://openreview.net/profile?id=~Shen_Li4), [Yanli Zhao](http://openreview.net/profile?id=~Yanli_Zhao1), [Yuchen Hao](http://openreview.net/profile?id=~Yuchen_Hao1), [Yantao Yao](http://openreview.net/profile?id=~Yantao_Yao1), [Ellie Wen](http://openreview.net/profile?id=~Ellie_Dingqiao_Wen1), [Jongsoo Park](http://openreview.net/profile?id=~Jongsoo_Park1), [Maxim Naumov](http://openreview.net/profile?id=~Maxim_Naumov2), [Wenlin Chen](http://openreview.net/profile?id=~Wenlin_Chen1)
  - **Affiliations:** Meta AI, Meta AI, Meta AI, Meta AI, Meta AI, Meta AI, Meta AI, Meta AI, Meta AI, Meta AI, Meta AI, Meta AI, Meta AI
  - **TL;DR:** This paper introduces Wukong, a novel network architecture based on stacked factorization machines, aimed at establishing a scaling law for large-scale recommendation systems. The results demonstrate that Wukong consistently outperforms state-of-the-art models in quality while maintaining scalability across significant increases in model complexity.
  - **Keywords:** recommendation systems, scaling laws, stacked factorization machines, large-scale recommendation, inefficiencies in upscaling mechanisms, adapting to complex datasets, Wukong architecture, superior model quality, scalability, six public datasets, internal large-scale dataset


- [Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics](https://icml.cc/virtual/2024/poster/34708) (Poster)
  - **Authors:** [Xinyu Zhang](http://openreview.net/profile?id=~Xinyu_Zhang16), [Wenjie Qiu](http://openreview.net/profile?id=~Wenjie_Qiu3), [Yi-Chen Li](http://openreview.net/profile?id=~Yi-Chen_Li1), [lei yuan](http://openreview.net/profile?id=~Lei_Yuan2), [Chengxing Jia](http://openreview.net/profile?id=~Chengxing_Jia1), [Zongzhang Zhang](http://openreview.net/profile?id=~Zongzhang_Zhang1), [Yang Yu](http://openreview.net/profile?id=~Yang_Yu5)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies
  - **TL;DR:** This paper presents DORA, a novel approach for developing adaptable policies in non-stationary environments using offline reinforcement learning. Experimental results show that DORA significantly improves dynamics encoding and outperforms existing methods across various benchmark tasks.
  - **Keywords:** Non-stationary dynamics, Offline reinforcement learning, Adaptable policies, Debiased Offline Representation learning (DORA), Information bottleneck principle, Robotics, Reinforcement learning applications, Context misassociations, Limited offline data, Non-stationary environments, Improved dynamics encoding, Performance enhancement over existing baselines, MuJoCo tasks


- [Enhancing Storage and Computational Efficiency in Federated Multimodal Learning for Large-Scale Models](https://icml.cc/virtual/2024/poster/34071) (Poster)
  - **Authors:** [Zixin Zhang](http://openreview.net/profile?id=~Zixin_Zhang5), [Fan Qi](http://openreview.net/profile?id=~Fan_Qi1), [Changsheng Xu](http://openreview.net/profile?id=~Changsheng_Xu1)
  - **Affiliations:** School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China, School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China, Institute of Automation, Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This study presents M2FEDSA, a framework designed to enhance storage and computational efficiency in federated multimodal learning by utilizing split learning and lightweight adapters. The proposed method effectively addresses challenges related to data privacy and resource limitations while improving model adaptability and performance across various multimodal tasks.
  - **Keywords:** Federated Learning, Multimodal Learning, Split Learning, Dual Adaptive Fine-tuning Strategy, Data privacy, Limited computation and storage, Data heterogeneity, Modality inconsistency, M2FEDSA, Modularized decomposition, Lightweight adapters, Large-scale models, Multimodal data


- [Nonparametric Teaching of Implicit Neural Representations](https://icml.cc/virtual/2024/poster/35151) (Poster)
  - **Authors:** [Chen Zhang](http://openreview.net/profile?id=~Chen_Zhang13), [Steven T. S. Luo](http://openreview.net/profile?id=~Steven_Tin_Sui_Luo1), [Jason Chun Lok Li](http://openreview.net/profile?id=~Jason_Chun_Lok_Li1), [Yik-Chung WU](http://openreview.net/profile?id=~Yik_Chung_WU1), [Ngai Wong](http://openreview.net/profile?id=~Ngai_Wong1)
  - **Affiliations:** Department of Electrical and Electronic Engineering, The University of Hong Kong, HKSAR, China, Department of Computer Science, The University of Toronto, Ontario, Canada, Department of Electrical and Electronic Engineering, The University of Hong Kong, HKSAR, China, Department of Electrical and Electronic Engineering, The University of Hong Kong, HKSAR, China, Department of Electrical and Electronic Engineering, The University of Hong Kong, HKSAR, China
  - **TL;DR:** This study introduces Implicit Neural Teaching (INT), a novel approach to enhance the training efficiency of implicit neural representations (INRs) using an overparameterized multilayer perceptron (MLP). The method demonstrates over 30% training time savings by selectively teaching the MLP with signal fragments based on disparity, improving convergence speed.
  - **Keywords:** Implicit Neural Representation (INR), Nonparametric Teaching, Overparameterized Multilayer Perceptron (MLP), Functional Gradient Descent, Image Functions, Vision Data Representation, View Synthesis, Signal Compression, Costly Training of INRs, High-Dimensional Signals, Implicit Neural Teaching (INT), Training Efficiency Improvement


- [Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning](https://icml.cc/virtual/2024/poster/33960) (Oral)
  - **Authors:** [Qiankun Zhang](http://openreview.net/profile?id=~Qiankun_Zhang1), [Aocheng Shen](http://openreview.net/profile?id=~Aocheng_Shen1), [Boyu Zhang](http://openreview.net/profile?id=~Boyu_Zhang6), [Hanrui Jiang](http://openreview.net/profile?id=~Hanrui_Jiang1), [Bingqian Du](http://openreview.net/profile?id=~Bingqian_Du1)
  - **Affiliations:** School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China
  - **TL;DR:** This study explores online matching with stochastic rewards using an adversarial reinforcement learning approach to improve the competitive ratio of algorithms. The findings indicate a theoretical enhancement of the best-known hardness result, demonstrating the potential of reinforcement learning in understanding online optimization problems.
  - **Keywords:** online optimization, online bipartite matching, adversarial reinforcement learning, reinforcement learning, adversarial learning, online advertising, combinatorial optimization, uncertainty of inputs, competitive ratio, robust algorithms, theoretical improvement on competitive ratio, online matching with stochastic rewards (OMSR), hard instances


- [In-Context Principle Learning from Mistakes](https://icml.cc/virtual/2024/poster/34138) (Poster)
  - **Authors:** [Tianjun Zhang](http://openreview.net/profile?id=~Tianjun_Zhang1), [Aman Madaan](http://openreview.net/profile?id=~Aman_Madaan1), [Luyu Gao](http://openreview.net/profile?id=~Luyu_Gao1), [Steven Zheng](http://openreview.net/profile?id=~Steven_Zheng1), [Swaroop Mishra](http://openreview.net/profile?id=~Swaroop_Mishra1), [Yiming Yang](http://openreview.net/profile?id=~Yiming_Yang1), [Niket Tandon](http://openreview.net/profile?id=~Niket_Tandon2), [Uri Alon](http://openreview.net/profile?id=~Uri_Alon1)
  - **Affiliations:** UC Berkeley, Carnegie Mellon University, Carnegie Mellon University, Google DeepMind, Google DeepMind, Carnegie Mellon University, AI2, Google DeepMind
  - **TL;DR:** This paper introduces Learning Principles (LEAP) to enhance in-context learning by allowing models to learn from mistakes in few-shot examples, leading to improved performance on various benchmarks. The approach demonstrates significant gains in accuracy for large language models without requiring additional input or examples.
  - **Keywords:** In-context learning, Few-shot prompting, Learning from mistakes, Learning Principles (LEAP), Task-specific principles, Multi-hop question answering, Textual QA, Reasoning, Math problems, Learning from incorrect input-output pairs, Generalization from few examples, Improved performance of LLMs, Enhanced few-shot prompting, Hotpot QA, DROP, GSM8K, MATH, Big-Bench Hard reasoning, Large Language Models (LLMs), GPT-3.5-turbo, GPT-4, GPT-4-turbo


- [Exploring the Benefit of Activation Sparsity in Pre-training](https://icml.cc/virtual/2024/poster/34332) (Poster)
  - **Authors:** [Zhengyan Zhang](http://openreview.net/profile?id=~Zhengyan_Zhang1), [Chaojun Xiao](http://openreview.net/profile?id=~Chaojun_Xiao1), [Qiujieli Qin](http://openreview.net/profile?id=~Qiujieli_Qin1), [Yankai Lin](http://openreview.net/profile?id=~Yankai_Lin1), [Zhiyuan Zeng](http://openreview.net/profile?id=~Zhiyuan_Zeng3), [Xu Han](http://openreview.net/profile?id=~Xu_Han2), [Zhiyuan Liu](http://openreview.net/profile?id=~Zhiyuan_Liu1), [Ruobing Xie](http://openreview.net/profile?id=~Ruobing_Xie2), [Maosong Sun](http://openreview.net/profile?id=~Maosong_Sun1), [Jie Zhou](http://openreview.net/profile?id=~Jie_Zhou8)
  - **Affiliations:** NLP Group, DCST, IAI, BNRIST, Tsinghua University, NLP Group, DCST, IAI, BNRIST, Tsinghua University, NLP Group, DCST, IAI, BNRIST, Tsinghua University, Gaoling School of Artificial Intelligence, Renmin University of China, NLP Group, DCST, IAI, BNRIST, Tsinghua University, NLP Group, DCST, IAI, BNRIST, Tsinghua University, NLP Group, DCST, IAI, BNRIST, Tsinghua University, Tencent, NLP Group, DCST, IAI, BNRIST, Tsinghua University; Jiangsu Collaborative Innovation Center for Language Ability, Xuzhou, China, Tencent
  - **TL;DR:** This study investigates the phenomenon of sparse activation in pre-trained Transformers and proposes Switchable Sparse-Dense Learning (SSD) to enhance pre-training efficiency. SSD achieves comparable performance to dense training while reducing costs and enabling faster inference speeds.
  - **Keywords:** Sparse activation, Pre-training of Transformers, Switchable Sparse-Dense Learning (SSD), Mixtures-of-Experts (MoE), Activation correlation, Efficiency in pre-training, Comparable performance to dense training, Reduced pre-training costs, Faster inference speed, Transformers, Sparse activation


- [Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models](https://icml.cc/virtual/2024/poster/34411) (Poster)
  - **Authors:** [Fangzhao Zhang](http://openreview.net/profile?id=~Fangzhao_Zhang1), [Mert Pilanci](http://openreview.net/profile?id=~Mert_Pilanci3)
  - **Affiliations:** Department of Electrical Engineering, Stanford University, Department of Electrical Engineering, Stanford University
  - **TL;DR:** This study enhances Low-Rank Adaptation (LoRA) for fine-tuning foundation models by introducing a preconditioner that stabilizes feature learning and improves convergence without requiring different learning rates for trainable parameters. The proposed method shows significant improvements in the reliability of optimization algorithms like SGD and AdamW.
  - **Keywords:** Low-Rank Adaptation (LoRA), Parameter-Efficient Fine-Tuning (PEFT), neural network fine-tuning, Riemannian optimization, low-rank matrices, SGD, AdamW, large language models, text-to-image diffusion models, storage efficiency in fine-tuning, convergence stability, hyperparameter tuning, introduction of a preconditioner for LoRA, improved convergence and reliability


- [Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning](https://icml.cc/virtual/2024/poster/33796) (Poster)
  - **Authors:** [Zongmeng Zhang](http://openreview.net/profile?id=~Zongmeng_Zhang1), [Yufeng Shi](http://openreview.net/profile?id=~Yufeng_Shi2), [Jinhua Zhu](http://openreview.net/profile?id=~Jinhua_Zhu1), [Wengang Zhou](http://openreview.net/profile?id=~Wengang_Zhou1), [Xiang Qi](http://openreview.net/profile?id=~Xiang_Qi2), [peng zhang](http://openreview.net/profile?id=~peng_zhang42), [Houqiang Li](http://openreview.net/profile?id=~Houqiang_Li1)
  - **Affiliations:** University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Ant Group, Ant Group, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center
  - **TL;DR:** This paper addresses the trustworthiness of retrieval-augmented large language models by proposing a reinforcement learning-based algorithm called TRUSTWORTHY ALIGNMENT, which aims to reduce hallucinations by aligning model responses with external evidence rather than internal knowledge. The findings suggest that large language models can achieve a trustworthy status without explicit supervision, expanding their application to create reliable AI agents.
  - **Keywords:** Trustworthiness, Retrieval-Augmented Generation, Large Language Models, Reinforcement Learning, TRUSTWORTHY ALIGNMENT, AI systems, dialogue assistants, autonomous agents, Hallucination, knowledge conflicts, Trustworthy status of language models, alignment with human preference


- [Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness](https://icml.cc/virtual/2024/poster/33882) (Poster)
  - **Authors:** [Guibin Zhang](http://openreview.net/profile?id=~Guibin_Zhang1), [Yanwei Yue](http://openreview.net/profile?id=~Yanwei_Yue1), [kun wang](http://openreview.net/profile?id=~Kun_Wang15), [Junfeng Fang](http://openreview.net/profile?id=~Junfeng_Fang1), [Yongduo Sui](http://openreview.net/profile?id=~Yongduo_Sui1), [Kai Wang](http://openreview.net/profile?id=~Kai_Wang8), [Yuxuan Liang](http://openreview.net/profile?id=~Yuxuan_Liang1), [Dawei Cheng](http://openreview.net/profile?id=~Dawei_Cheng1), [Shirui Pan](http://openreview.net/profile?id=~Shirui_Pan1), [Tianlong Chen](http://openreview.net/profile?id=~Tianlong_Chen1)
  - **Affiliations:** Tongji University; The Hong Kong University of Science and Technology (Guangzhou), Tongji University, University of Science and Technology of China, University of Science and Technology of China, University of Science and Technology of China, National University of Singapore, The Hong Kong University of Science and Technology (Guangzhou), Tongji University, Griffith University, Massachusetts Institute of Technology
  - **TL;DR:** This study introduces Graph Sparse Training (GST) to enhance the efficiency of Graph Neural Networks (GNNs) by dynamically managing sparsity while preserving topological and semantic information. The proposed method outperforms existing sparsification techniques, achieving significant speedups in GNN inference and maintaining key spectral properties.
  - **Keywords:** Graph Neural Networks (GNNs), Graph Sparse Training (GST), Dynamic sparse training, Equilibria Sparsification Principle, Graph learning tasks, Graph adversarial defense, Computational challenges in large-scale graphs, Performance collapse at higher sparsity levels, Higher graph sparsity levels, Spectral property preservation, Speedup in GNN inference, 6 datasets, 5 backbones


- [MLIP: Efficient Multi-Perspective Language-Image Pretraining with Exhaustive Data Utilization](https://icml.cc/virtual/2024/poster/33008) (Poster)
  - **Authors:** [Yu Zhang](http://openreview.net/profile?id=~Yu_Zhang60), [Qi Zhang](http://openreview.net/profile?id=~Qi_Zhang25), [Zixuan Gong](http://openreview.net/profile?id=~Zixuan_Gong2), [Yiwei Shi](http://openreview.net/profile?id=~Yiwei_Shi1), [Yepeng Liu](http://openreview.net/profile?id=~Yepeng_Liu1), [Duoqian Miao](http://openreview.net/profile?id=~Duoqian_Miao1), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu127), [KE LIU](http://openreview.net/profile?id=~KE_LIU10), [Kun Yi](http://openreview.net/profile?id=~Kun_Yi2), [Wei Fan](http://openreview.net/profile?id=~Wei_Fan6), [Liang Hu](http://openreview.net/profile?id=~Liang_Hu1), [Changwei Wang](http://openreview.net/profile?id=~Changwei_Wang2)
  - **Affiliations:** Tongji University, Tongji University, Tongji University, University of Bristol, University of Florida, Tongji University, National University of Defense Technology, National Clinical Research Center for Mental Disorders & National Center for Mental Disorders, Beijing Anding Hospital, Capital Medical University, Beijing Institute of Technology, University of Oxford, Tongji University, Institute of Automation, Chinese Academy of Sciences
  - **TL;DR:** The study introduces Multi-Perspective Language-Image Pretraining (MLIP) to enhance data utilization in multimodal representation learning by leveraging frequency transforms and token merging methods. The proposed approach addresses inefficiencies in the existing CLIP framework, leading to improved performance and reduced computational costs.
  - **Keywords:** Multimodal studies, Vision-Language Pre-training, Contrastive Language-Image Pretraining (CLIP), frequency transforms, token-level alignment, Image-text representation learning, Inefficient data utilization, non-informative tokens, increased computational demands, Multi-Perspective Language-Image Pretraining (MLIP), token merging method, Vision Transformer (ViT)


- [Distributionally Robust Data Valuation](https://icml.cc/virtual/2024/poster/33166) (Poster)
  - **Authors:** [Xiaoqiang Lin](http://openreview.net/profile?id=~Xiaoqiang_Lin1), [Xinyi Xu](http://openreview.net/profile?id=~Xinyi_Xu4), [Zhaoxuan Wu](http://openreview.net/profile?id=~Zhaoxuan_Wu1), [See-Kiong Ng](http://openreview.net/profile?id=~See-Kiong_Ng1), [Bryan Kian Hsiang Low](http://openreview.net/profile?id=~Bryan_Kian_Hsiang_Low1)
  - **Affiliations:** Department of Computer Science, National University of Singapore, Singapore, Department of Computer Science, National University of Singapore, Singapore, Institute of Data Science, National University of Singapore, Singapore, Institute of Data Science, National University of Singapore, Singapore, Department of Computer Science, National University of Singapore, Singapore
  - **TL;DR:** This paper proposes a distributionally robust data valuation approach that quantifies the contribution of data points to machine learning model performance without relying on fixed validation distributions. The method provides a worst-case performance guarantee and demonstrates superior performance in data selection and removal tasks on real-world datasets.
  - **Keywords:** Data Valuation, Collaborative Machine Learning, Distributionally Robust Generalization, Distributionally Robust Generalization Error (DRGE), Model Deviation, Kernel Regression, Neural Networks, Data Marketplaces, Hospitalization Prediction, Housing Price Prediction, Difficulty in agreeing on common validation datasets, Heterogeneous local data and validation distributions, New data valuation approach, Worst-case performance guarantee


- [Efficient Contextual Bandits with Uninformed Feedback Graphs](https://icml.cc/virtual/2024/poster/35172) (Poster)
  - **Authors:** [Mengxiao Zhang](http://openreview.net/profile?id=~Mengxiao_Zhang2), [Yuheng Zhang](http://openreview.net/profile?id=~Yuheng_Zhang1), [Haipeng Luo](http://openreview.net/profile?id=~Haipeng_Luo1), [Paul Mineiro](http://openreview.net/profile?id=~Paul_Mineiro1)
  - **Affiliations:** University of Southern California, University of Illinois Urbana-Champaign, University of Southern California, Microsoft Research
  - **TL;DR:** This paper presents the first efficient algorithms for contextual bandits with uninformed feedback graphs, addressing a gap in existing research. The proposed method demonstrates strong regret guarantees and is empirically validated in bidding applications.
  - **Keywords:** Contextual Bandits, Online Learning, Online Regression, Log Loss, Square Loss, Bidding Applications, Personalized Web Advertising, Uninformed Feedback Graphs, Regret Guarantees, Efficient Algorithms, Strong Regret Guarantees, Feedback Graphs, Adversarial Context


- [Rethinking Guidance Information to Utilize Unlabeled Samples: A Label Encoding Perspective](https://icml.cc/virtual/2024/poster/32652) (Poster)
  - **Authors:** [Yulong Zhang](http://openreview.net/profile?id=~Yulong_Zhang2), [Yuan Yao](http://openreview.net/profile?id=~Yuan_Yao15), [Shuhao Chen](http://openreview.net/profile?id=~Shuhao_Chen1), [Pengrong Jin](http://openreview.net/profile?id=~Pengrong_Jin2), [Yu Zhang](http://openreview.net/profile?id=~Yu_Zhang3), [Jian Jin](http://openreview.net/profile?id=~Jian_Jin4), [Jiangang Lu](http://openreview.net/profile?id=~Jiangang_Lu1)
  - **Affiliations:** Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; College of Control Science and Engineering, Zhejiang University, Hangzhou, China, Beijing Teleinfo Technology Company Ltd., China Academy of Information and Communications Technology, Beijing, China, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China, Research Institute of Industrial Internet of Things, China Academy of Information and Communications Technology, Beijing, China, College of Control Science and Engineering, Zhejiang University, Hangzhou, China
  - **TL;DR:** This paper proposes a new method called Label-Encoding Risk Minimization (LERM) to effectively utilize unlabeled samples in scenarios with insufficient labeled data, addressing the limitations of existing methods like Entropy Minimization. The findings demonstrate that LERM enhances both prediction discriminability and diversity, making it a valuable addition to current semi-supervised learning techniques.
  - **Keywords:** Empirical Risk Minimization, semi-supervised learning, unlabeled samples, Entropy Minimization, Label-Encoding Risk Minimization, Insufficient labeled samples, prediction discriminability, prediction diversity, New method (LERM), alignment of label encodings


- [FESSNC: Fast Exponentially Stable and Safe Neural Controller](https://icml.cc/virtual/2024/poster/33592) (Poster)
  - **Authors:** [Jingdong Zhang](http://openreview.net/profile?id=~Jingdong_Zhang1), [Luan Yang](http://openreview.net/profile?id=~Luan_Yang1), [Qunxi Zhu](http://openreview.net/profile?id=~Qunxi_Zhu1), [Wei Lin](http://openreview.net/profile?id=~Wei_Lin1)
  - **Affiliations:** School of Mathematical Sciences, LMNS, and SCMS, Fudan University, China; None, Research Institute of Intelligent Complex Systems, Fudan University, China; None, Research Institute of Intelligent Complex Systems, Fudan University, China; State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Institutes of Brain Science, Fudan University, China; Shanghai Artificial Intelligence Laboratory, China, School of Mathematical Sciences, LMNS, and SCMS, Fudan University, China; State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Institutes of Brain Science, Fudan University, China; Shanghai Artificial Intelligence Laboratory, China
  - **TL;DR:** This paper presents a Fast Exponentially Stable and Safe Neural Controller (FESSNC) designed to stabilize nonlinear systems modeled by stochastic differential equations while ensuring rigorous exponential stability and safety guarantees. The proposed framework utilizes projection operators to enhance controller performance and reduce computational costs, demonstrating superiority over existing methods.
  - **Keywords:** nonlinear systems, stochastic differential equations, neural controllers, Fast Exponentially Stable and Safe Neural Controller (FESSNC), projection operator, approximate projection operators, Hutchinson’s trace estimator, robotics, automotive systems, safety-critical applications, stabilization of nonlinear systems, rigorous stability guarantee, computational cost, exponential stability guarantees, safety guarantees, improved stability and safety performance


- [Causal Representation Learning from Multiple Distributions: A General Setting](https://icml.cc/virtual/2024/poster/34105) (Poster)
  - **Authors:** [Kun Zhang](http://openreview.net/profile?id=~Kun_Zhang1), [Shaoan Xie](http://openreview.net/profile?id=~Shaoan_Xie4), [Ignavier Ng](http://openreview.net/profile?id=~Ignavier_Ng1), [Yujia Zheng](http://openreview.net/profile?id=~Yujia_Zheng1)
  - **Affiliations:** Carnegie Mellon University; Mohamed bin Zayed University of Artificial Intelligence, Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University
  - **TL;DR:** This paper addresses the challenge of causal representation learning from multiple distributions in a nonparametric setting, focusing on recovering latent causal variables and their relationships. The findings suggest that under certain conditions, the moralized graph of the underlying causal structure can be recovered, providing insights into complex causal processes.
  - **Keywords:** Causal representation learning, latent causal variables, Nonparametric methods, causal graph recovery, Heterogeneous data analysis, nonstationary time series, Identifying latent causal variables, changes in causal mechanisms, Recovery of moralized graphs, relations to underlying causal models, Directed acyclic graph, nonlinear independent component analysis (ICA)


- [Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions](https://icml.cc/virtual/2024/poster/32748) (Spotlight Poster)
  - **Authors:** [Kaihong Zhang](http://openreview.net/profile?id=~Kaihong_Zhang1), [Heqi Yin](http://openreview.net/profile?id=~Heqi_Yin1), [Feng Liang](http://openreview.net/profile?id=~Feng_Liang1), [Jingbo Liu](http://openreview.net/profile?id=~Jingbo_Liu3)
  - **Affiliations:** Department of Statistics, University of Illinois Urbana-Champaign, Champaign, IL, USA, Department of Statistics, University of Illinois Urbana-Champaign, Champaign, IL, USA, Department of Statistics, University of Illinois Urbana-Champaign, Champaign, IL, USA, Department of Statistics, University of Illinois Urbana-Champaign, Champaign, IL, USA
  - **TL;DR:** This study investigates the asymptotic error of score-based diffusion model sampling from a non-parametric statistics perspective, demonstrating that a kernel-based score estimator achieves optimal mean square error. The findings indicate that the diffusion model is nearly minimax optimal under certain conditions, removing previous lower bound assumptions on the underlying distribution.
  - **Keywords:** score-based diffusion models, non-parametric statistics, kernel-based score estimator, stochastic differential equations, image generation, text-to-speech synthesis, molecular structure modeling, asymptotic error, total variation error, score estimation inaccuracies, optimal mean square error, minimax optimality, sub-Gaussian distribution, β-Sobolev space


- [GroupCover: A Secure, Efficient and Scalable Inference Framework for On-device Model Protection based on TEEs](https://icml.cc/virtual/2024/poster/35006) (Poster)
  - **Authors:** [Zheng Zhang](http://openreview.net/profile?id=~Zheng_Zhang33), [Na Wang](http://openreview.net/profile?id=~Na_Wang3), [Ziqi Zhang](http://openreview.net/profile?id=~Ziqi_Zhang6), [Yao Zhang](http://openreview.net/profile?id=~Yao_Zhang15), [Tianyi Zhang](http://openreview.net/profile?id=~Tianyi_Zhang21), [Jianwei Liu](http://openreview.net/profile?id=~Jianwei_Liu2), [Ye Wu](http://openreview.net/profile?id=~Ye_Wu8)
  - **Affiliations:** Beihang University; Douyin Vision Co., Ltd., Beihang University, Peking University, Douyin Vision Co., Ltd., Beihang University, Beihang University, Douyin Vision Co., Ltd.
  - **TL;DR:** This paper presents GROUP COVER, a novel model obfuscation technique that enhances the security of DNN models deployed on edge devices by addressing vulnerabilities in existing TEE-based solutions. Experimental results show that GROUP COVER achieves a security level comparable to black-box protection with only 19% overhead and negligible accuracy loss.
  - **Keywords:** Model protection, Deep Neural Networks (DNN), Trusted Execution Environments (TEEs), Model obfuscation, Randomization, Mutual covering obfuscation, Edge devices, Autonomous driving, Internet of Things (IoT), Intellectual property theft, Model-stealing attacks, Security vulnerabilities, GROUP COVER approach, Enhanced security level, Reduced overhead


- [Uncertainty-Aware Reward-Free Exploration with General Function Approximation](https://icml.cc/virtual/2024/poster/34706) (Poster)
  - **Authors:** [Junkai Zhang](http://openreview.net/profile?id=~Junkai_Zhang2), [Weitong Zhang](http://openreview.net/profile?id=~Weitong_Zhang2), [Dongruo Zhou](http://openreview.net/profile?id=~Dongruo_Zhou1), [Quanquan Gu](http://openreview.net/profile?id=~Quanquan_Gu1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, California, USA, Department of Computer Science, University of California, Los Angeles, California, USA, Department of Computer Science, Indiana University Bloomington, Indiana, USA, Department of Computer Science, University of California, Los Angeles, California, USA
  - **TL;DR:** This paper introduces GFA-RFE, a reward-free reinforcement learning algorithm that utilizes uncertainty-aware intrinsic rewards to improve exploration efficiency. The results demonstrate that GFA-RFE outperforms existing reward-free RL algorithms across various tasks in the DeepMind Control Suite.
  - **Keywords:** reinforcement learning, reward-free exploration, GFA-RFE, intrinsic rewards, uncertainty-aware learning, DeepMind Control Suite, robotic control, sample efficiency, heterogeneous nature of samples, near-optimal policy, exploration strategy


- [Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks](https://icml.cc/virtual/2024/poster/34928) (Poster)
  - **Authors:** [Lujing Zhang](http://openreview.net/profile?id=~Lujing_Zhang1), [Aaron Roth](http://openreview.net/profile?id=~Aaron_Roth1), [Linjun Zhang](http://openreview.net/profile?id=~Linjun_Zhang1)
  - **Affiliations:** Peking University, University of Pennsylvania, Rutgers, University of New Jersey
  - **TL;DR:** This paper presents a framework for calibrating machine learning models to ensure multi-group fairness through a method called Generalized Multi-Dimensional Multicalibration (GMC). The framework is applied to various scenarios, including image segmentation and language models, demonstrating its effectiveness in controlling fairness risks.
  - **Keywords:** multi-group fairness, machine learning, multicalibration, Generalized Multi-Dimensional Multicalibration (GMC), image segmentation, hierarchical classification, language models, false negative rate control, calibration error, fair risk control framework, algorithms for multi-dimensional outputs


- [Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases](https://icml.cc/virtual/2024/poster/32798) (Poster)
  - **Authors:** [Ziyi Zhang](http://openreview.net/profile?id=~Ziyi_Zhang9), [Sen Zhang](http://openreview.net/profile?id=~Sen_Zhang3), [Yibing Zhan](http://openreview.net/profile?id=~Yibing_Zhan2), [Yong Luo](http://openreview.net/profile?id=~Yong_Luo2), [Yonggang Wen](http://openreview.net/profile?id=~Yonggang_Wen1), [Dacheng Tao](http://openreview.net/profile?id=~Dacheng_Tao1)
  - **Affiliations:** Institute of Artificial Intelligence, School of Computer Science, Wuhan University, China; Hubei Luojia Laboratory, Wuhan, China, The University of Sydney, Australia, JD Explore Academy, Beijing, China, Institute of Artificial Intelligence, School of Computer Science, Wuhan University, China; Hubei Luojia Laboratory, Wuhan, China, Nanyang Technological University, Singapore, Nanyang Technological University, Singapore
  - **TL;DR:** This study addresses the challenge of reward overoptimization in diffusion models by proposing a new algorithm, TDPO-R, which leverages temporal inductive bias and mitigates primacy bias. The findings indicate that the proposed method effectively improves alignment with human preferences without compromising performance.
  - **Keywords:** diffusion models, human preferences, generative workflows, Temporal Diffusion Policy Optimization, policy gradient algorithm, text-to-image generation, image aesthetic quality, reward overoptimization, fidelity deterioration, cross-reward generalization, TDPO-R, regularization against reward overoptimization, inductive bias, primacy bias


- [Switchable Decision: Dynamic Neural Generation Networks](https://icml.cc/virtual/2024/poster/34307) (Poster)
  - **Authors:** [Shujian Zhang](http://openreview.net/profile?id=~Shujian_Zhang1), [Korawat Tanwisuth](http://openreview.net/profile?id=~Korawat_Tanwisuth1), [Chengyue Gong](http://openreview.net/profile?id=~Chengyue_Gong1), [Pengcheng He](http://openreview.net/profile?id=~Pengcheng_He2), [Mingyuan Zhou](http://openreview.net/profile?id=~Mingyuan_Zhou1)
  - **Affiliations:** The University of Texas at Austin, The University of Texas at Austin, The University of Texas at Austin, The University of Texas at Austin, The University of Texas at Austin
  - **TL;DR:** This study introduces a switchable decision framework for dynamic neural generation networks to enhance inference efficiency in auto-regressive models while maintaining accuracy. The proposed method utilizes reinforcement learning and constrained optimization to allocate computational resources effectively across various NLP tasks.
  - **Keywords:** dynamic neural generation networks, efficient inference, auto-regressive generation models, reinforcement learning, constrained optimization, encoder-decoder transformer, natural language processing, question answering, summarization, classification, high computational cost during inference, deployment challenges on IoT devices, optimized trade-off between quality and computation cost, input-dependent inference strategy, multi-head attention, feed-forward network, early exiting, mixture of experts


- [Efficient Denoising Diffusion via Probabilistic Masking](https://icml.cc/virtual/2024/poster/33028) (Poster)
  - **Authors:** [Weizhong Zhang](http://openreview.net/profile?id=~WEIZHONG_ZHANG2), [Zhiwei Zhang](http://openreview.net/profile?id=~Zhiwei_Zhang3), [Renjie Pi](http://openreview.net/profile?id=~Renjie_Pi1), [Zhongming Jin](http://openreview.net/profile?id=~Zhongming_Jin1), [Yuan Gao](http://openreview.net/profile?id=~Yuan_Gao4), [Jieping Ye](http://openreview.net/profile?id=~Jieping_Ye4), [Kani Chen](http://openreview.net/profile?id=~Kani_Chen1)
  - **Affiliations:** Fudan University, Hong Kong University of Science and Technology, Hong Kong University of Science and Technology, Alibaba Group, Wuhan University, Alibaba Group, Hong Kong University of Science and Technology
  - **TL;DR:** This paper presents an Efficient Denoising Diffusion method via Probabilistic Masking (EDDPM) that enhances the sampling efficiency of diffusion models by identifying and skipping redundant steps during training. The method demonstrates significant improvements in generating high-quality samples with fewer timesteps, outperforming existing acceleration techniques.
  - **Keywords:** diffusion models, generative techniques, Efficient Denoising Diffusion method via Probabilistic Masking (EDDPM), probabilistic reparameterization, time series imputation, image generation, speech synthesis, computationally intensive inference process, redundant sampling steps, improved inference efficiency, identification of essential steps, CIFAR-10


- [Interpreting and Improving Large Language Models in Arithmetic Calculation](https://icml.cc/virtual/2024/poster/34669) (Oral)
  - **Authors:** [Wei Zhang](http://openreview.net/profile?id=~Wei_Zhang58), [Wan Chaoqun](http://openreview.net/profile?id=~Chaoqun_Wan2), [Yonggang Zhang](http://openreview.net/profile?id=~Yonggang_Zhang1), [Yiu Ming Cheung](http://openreview.net/profile?id=~Yiu-ming_Cheung1), [Xinmei Tian](http://openreview.net/profile?id=~Xinmei_Tian1), [Xu Shen](http://openreview.net/profile?id=~Xu_Shen1), [Jieping Ye](http://openreview.net/profile?id=~Jieping_Ye4)
  - **Affiliations:** University of Science and Technology of China; Alibaba Cloud, Alibaba Cloud, Hong Kong Baptist University, Hong Kong Baptist University, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, Alibaba Cloud, Alibaba Cloud
  - **TL;DR:** This study investigates the mechanisms by which large language models (LLMs) perform arithmetic calculations, revealing that a small fraction of attention heads and multi-layer perceptrons are crucial for processing operands and operators. The authors demonstrate that selectively fine-tuning these components can significantly improve LLMs' mathematical performance without affecting their capabilities in non-mathematical tasks.
  - **Keywords:** Large Language Models, Arithmetic Calculation, Mathematical Computation, Attention Heads, Multi-Layer Perceptrons (MLPs), Reliability of LLMs in arithmetic calculations, Understanding intrinsic mechanisms of LLMs, Enhanced computational performance through selective fine-tuning, LLaMA2 series, Emergent abilities, Reasoning tasks


- [Candidate Pseudolabel Learning: Enhancing Vision-Language Models by Prompt Tuning with Unlabeled Data](https://icml.cc/virtual/2024/poster/32922) (Oral)
  - **Authors:** [Jiahan Zhang](http://openreview.net/profile?id=~Jiahan_Zhang1), [Qi Wei](http://openreview.net/profile?id=~Qi_Wei4), [Feng Liu](http://openreview.net/profile?id=~Feng_Liu2), [Lei Feng](http://openreview.net/profile?id=~Lei_Feng1)
  - **Affiliations:** Singapore University of Technology and Design, Singapore, Nanyang Technological University, Singapore, University of Melbourne, Australia, Singapore University of Technology and Design, Singapore
  - **TL;DR:** This study introduces Candidate Pseudolabel Learning (CPL) to enhance the fine-tuning of vision-language models (VLMs) using unlabeled data, addressing the issue of incorrect hard pseudolabels that arise from low zero-shot performance. The proposed method demonstrates improved performance through refined candidate pseudolabel generation and is validated across multiple benchmark datasets.
  - **Keywords:** Vision-Language Models, Unlabeled Data, Pseudolabeling, Candidate Pseudolabel Learning, Intra- and Inter-instance Label Selection, Downstream Tasks in Machine Learning, Incorrect Hard Pseudolabels, Low Zero-shot Performance, Labeling Costs, Improved Candidate Pseudolabels, Class-balanced Instance Selection, EuroSAT Dataset, CLIP


- [Beyond the ROC Curve: Classification Trees Using Cost-Optimal Curves, with Application to Imbalanced Datasets](https://icml.cc/virtual/2024/poster/33170) (Poster)
  - **Authors:** [Magzhan Gabidolla](http://openreview.net/profile?id=~Magzhan_Gabidolla1), [Arman Zharmagambetov](http://openreview.net/profile?id=~Arman_Zharmagambetov1), [Miguel Carreira-Perpinan](http://openreview.net/profile?id=~Miguel_%C3%81._Carreira-Perpi%C3%B1%C3%A1n2)
  - **Affiliations:** Dept. of Computer Science and Engineering, University of California, Merced, USA, Meta AI (FAIR), Dept. of Computer Science and Engineering, University of California, Merced, USA; Meta AI (FAIR)
  - **TL;DR:** This study introduces a novel approach to binary classification using cost-optimal curves to address imbalanced datasets, demonstrating that their proposed algorithm significantly outperforms traditional ROC-based classifiers by effectively minimizing false negatives while controlling false positive rates.
  - **Keywords:** binary classification, imbalanced datasets, cost-sensitive classification, classification trees, oblique trees, weighted 0/1 loss, fraud detection, spam detection, churn prediction, class imbalance, asymmetric costs, false positives, false negatives, cost-optimal curve (COC), optimization algorithm for classification trees, axis-aligned trees, CART, C5.0, logistic regression, linear SVMs


- [On the Expressive Power of Spectral Invariant Graph Neural Networks](https://icml.cc/virtual/2024/poster/33239) (Poster)
  - **Authors:** [Bohang Zhang](http://openreview.net/profile?id=~Bohang_Zhang1), [Lingxiao Zhao](http://openreview.net/profile?id=~Lingxiao_Zhao1), [Haggai Maron](http://openreview.net/profile?id=~Haggai_Maron1)
  - **Affiliations:** Peking University, Carnegie Mellon University, Technion; NVIDIA Research
  - **TL;DR:** This study introduces the Eigenspace Projection GNN (EPNN) framework to analyze the expressive power of spectral invariant architectures in Graph Neural Networks. It establishes a hierarchy of expressiveness among these architectures and demonstrates that they are less expressive than the 3-WL model.
  - **Keywords:** Spectral Graph Neural Networks, Expressive Power, Spectral Invariant Architectures, Eigenspace Projection GNN (EPNN), Spectral Distances, Spectral Projection Matrices, Graph Neural Networks (GNNs), Graph Transformers (GTs), Ambiguity of Eigenvectors, Expressiveness Hierarchy, Unified Framework for Spectral Invariant Architectures, Analysis of Expressiveness, Eigenvalues, Eigenvectors, Graph Laplacian


- [Feature Contamination: Neural Networks Learn Uncorrelated Features and Fail to Generalize](https://icml.cc/virtual/2024/poster/34272) (Poster)
  - **Authors:** [Tianren Zhang](http://openreview.net/profile?id=~Tianren_Zhang1), [Chujie Zhao](http://openreview.net/profile?id=~Chujie_Zhao1), [Guanyu Chen](http://openreview.net/profile?id=~Guanyu_Chen1), [Yizhou Jiang](http://openreview.net/profile?id=~Yizhou_Jiang1), [Feng Chen](http://openreview.net/profile?id=~Feng_Chen1)
  - **Affiliations:** Department of Automation, Tsinghua University, Beijing, China; LSBDPA Beijing Key Laboratory, Beijing, China, Department of Automation, Tsinghua University, Beijing, China; LSBDPA Beijing Key Laboratory, Beijing, China, Department of Automation, Tsinghua University, Beijing, China; LSBDPA Beijing Key Laboratory, Beijing, China, Department of Automation, Tsinghua University, Beijing, China; LSBDPA Beijing Key Laboratory, Beijing, China, Department of Automation, Tsinghua University, Beijing, China; LSBDPA Beijing Key Laboratory, Beijing, China
  - **TL;DR:** This study investigates the challenges of out-of-distribution generalization in deep neural networks, revealing that neural networks can learn uncorrelated features alongside predictive ones, leading to generalization failures. The findings emphasize the importance of understanding feature learning dynamics and the need for appropriate inductive biases in model training.
  - **Keywords:** out-of-distribution generalization, deep neural networks, stochastic gradient descent (SGD), two-layer ReLU networks, generalization failure, feature contamination, distribution shifts, insights into feature learning dynamics, necessity of considering inductive biases, feature contamination, spurious correlations


- [Absolute Policy Optimization: Enhancing Lower Probability Bound of Performance with High Confidence](https://icml.cc/virtual/2024/poster/33989) (Poster)
  - **Authors:** [Weiye Zhao](http://openreview.net/profile?id=~Weiye_Zhao1), [Feihan Li](http://openreview.net/profile?id=~Feihan_Li1), [Yifan Sun](http://openreview.net/profile?id=~Yifan_Sun9), [Rui Chen](http://openreview.net/profile?id=~Rui_Chen11), [Tianhao Wei](http://openreview.net/profile?id=~Tianhao_Wei1), [Changliu Liu](http://openreview.net/profile?id=~Changliu_Liu1)
  - **Affiliations:** Robotics Institute, Carnegie Mellon University, USA, Robotics Institute, Carnegie Mellon University, USA, Robotics Institute, Carnegie Mellon University, USA, Robotics Institute, Carnegie Mellon University, USA, Robotics Institute, Carnegie Mellon University, USA, Robotics Institute, Carnegie Mellon University, USA
  - **TL;DR:** This study introduces Absolute Policy Optimization (APO), a novel approach that guarantees monotonic improvement in the lower probability bound of performance in reinforcement learning. The findings demonstrate that APO and its variant PAPO significantly enhance both worst-case and expected performance in complex control tasks and gaming scenarios.
  - **Keywords:** reinforcement learning, performance optimization, Absolute Policy Optimization (APO), Proximal Absolute Policy Optimization (PAPO), trust region policy optimization (TRPO), continuous control tasks, Atari games, autonomous driving, intelligent robot manipulation, worst-case performance control, performance distribution consistency, guaranteed monotonic improvement, substantial improvements in worst-case performance


- [Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning](https://icml.cc/virtual/2024/poster/35213) (Poster)
  - **Authors:** [Hao Zhao](http://openreview.net/profile?id=~Hao_Zhao3), [Maksym Andriushchenko](http://openreview.net/profile?id=~Maksym_Andriushchenko1), [Francesco Croce](http://openreview.net/profile?id=~Francesco_Croce1), [Nicolas Flammarion](http://openreview.net/profile?id=~Nicolas_Flammarion1)
  - **Affiliations:** EPFL, Switzerland, None, None, None
  - **TL;DR:** This study proposes a simple baseline for instruction fine-tuning of large language models by selecting the 1,000 longest responses from existing datasets, which outperforms more complex methods. The findings suggest that fine-tuning on longer responses should be the standard approach for improving model alignment and performance.
  - **Keywords:** Instruction fine-tuning, Large Language Models (LLMs), Supervised fine-tuning (SFT), Reinforcement learning from human feedback (RLHF), Reinforcement learning from automated feedback (RLAIF), User interaction, Question answering, High-quality data selection, Model overfitting, Simple baseline for instruction fine-tuning, Improved performance on benchmarks, Alpaca, LIMA, AlpaGasus, Llama-2-7B, AlpacaEval 2.0


- [Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective](https://icml.cc/virtual/2024/poster/34908) (Poster)
  - **Authors:** [Lei Zhao](http://openreview.net/profile?id=~Lei_Zhao18), [Mengdi Wang](http://openreview.net/profile?id=~Mengdi_Wang1), [Yu Bai](http://openreview.net/profile?id=~Yu_Bai1)
  - **Affiliations:** School of Mathematical Sciences, University of Science and Technology of China, Hefei, China, Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA, Salesforce AI Research, Palo Alto, CA, USA
  - **TL;DR:** This paper investigates the challenges of Inverse Reinforcement Learning (IRL) compared to standard Reinforcement Learning (RL) and presents efficient algorithms for IRL in both offline and online settings. The findings include nearly optimal sample complexities and the ability to transfer learned rewards to similar target MDPs.
  - **Keywords:** Inverse Reinforcement Learning (IRL), Standard Reinforcement Learning (RL), Efficient IRL algorithms, Pessimism principle, Robotics, Target-driven navigation, Game AI, Medical decision-making, Learning reward functions, Non-uniqueness of solutions, Offline learning settings, Lack of performance metrics, Sample complexity results, Transfer of learned rewards to target MDPs


- [Neural Jump-Diffusion Temporal Point Processes](https://icml.cc/virtual/2024/poster/33573) (Spotlight Poster)
  - **Authors:** [Shuai Zhang](http://openreview.net/profile?id=~Shuai_Zhang23), [Chuan Zhou](http://openreview.net/profile?id=~Chuan_Zhou3), [Yang Liu](http://openreview.net/profile?id=~Yang_Aron_Liu1), [PENG ZHANG](http://openreview.net/profile?id=~PENG_ZHANG2), [Xixun Lin](http://openreview.net/profile?id=~Xixun_Lin3), [Zhiming Ma](http://openreview.net/profile?id=~Zhi-Ming_Ma1)
  - **Affiliations:** Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Cyberspace Institute of Advanced Technology, Guangzhou University, Institute of Information Engineering, Chinese Academy of Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences
  - **TL;DR:** This paper introduces a novel framework called Neural Jump-Diffusion Temporal Point Process (NJDTPP) that reformulates the intensity processes of temporal point processes as solutions to stochastic differential equations, providing flexibility and theoretical guarantees. The proposed model significantly outperforms existing state-of-the-art TPP models in various prediction tasks.
  - **Keywords:** Temporal Point Processes (TPPs), Stochastic Differential Equations (SDEs), Neural Jump-Diffusion SDE (NJDSDE), intensity process reformulation, Event sequence prediction, anomaly detection, causal inference, Capturing complicated dynamics in intensity functions, limitations of classical TPP models, NJDTPP framework, theoretical guarantees for NJDSDE solutions, Synthetic datasets, real-world datasets, Poisson processes, Hawkes processes, self-correcting processes


- [Rethinking Adversarial Robustness in the Context of the Right to be Forgotten](https://icml.cc/virtual/2024/poster/32857) (Poster)
  - **Authors:** [Chenxu Zhao](http://openreview.net/profile?id=~Chenxu_Zhao4), [Wei Qian](http://openreview.net/profile?id=~Wei_Qian5), [Yangyi Li](http://openreview.net/profile?id=~Yangyi_Li1), [Aobo Chen](http://openreview.net/profile?id=~Aobo_Chen1), [Mengdi Huai](http://openreview.net/profile?id=~Mengdi_Huai1)
  - **Affiliations:** Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States, Department of Computer Science, Iowa State University, United States
  - **TL;DR:** This study investigates the intersection of machine unlearning and adversarial robustness, revealing a novel vulnerability where adversarial attacks can be enhanced through malicious unlearning requests. The proposed Adversarial Unlearning Attack (AdvUA) significantly reduces the robustness of unlearned models, highlighting new security concerns in machine learning frameworks.
  - **Keywords:** Right to be Forgotten, Machine Unlearning, Adversarial Robustness, Adversarial Unlearning Attack (AdvUA), Privacy Protection, Machine Learning, Data Removal, Adversarial Vulnerabilities, Reduction of Adversarial Robustness, Model Stealing Attacks, Security Vulnerability


- [Fast Text-to-3D-Aware Face Generation and Manipulation via Direct Cross-modal Mapping and Geometric Regularization](https://icml.cc/virtual/2024/poster/35014) (Poster)
  - **Authors:** [Jinlu Zhang](http://openreview.net/profile?id=~Jinlu_Zhang2), [Yiyi Zhou](http://openreview.net/profile?id=~Yiyi_Zhou1), [Qiancheng Zheng](http://openreview.net/profile?id=~Qiancheng_Zheng1), [Xiaoxiong Du](http://openreview.net/profile?id=~Xiaoxiong_Du1), [Gen Luo](http://openreview.net/profile?id=~Gen_Luo1), [Jun Peng](http://openreview.net/profile?id=~Jun_Peng2), [Xiaoshuai Sun](http://openreview.net/profile?id=~Xiaoshuai_Sun3), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Peng Cheng Laboratory, Shenzhen, 518000, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China
  - **TL;DR:** This paper presents E3-FaceNet, an efficient and effective network for fast and accurate text-to-3D-aware face generation and manipulation, addressing challenges in existing methods. The proposed approach significantly enhances inference speed and generation quality, achieving nearly 470 times faster performance compared to previous models.
  - **Keywords:** Text-to-3D-aware face generation, face manipulation, Direct cross-modal mapping, Style Code Enhancer, Geometric Regularization, Machine learning, 3D face generation, Low efficiency, poor quality in existing methods, semantic consistency challenges, E3-FaceNet, significant improvement in inference speed and generation quality, NeRF (Neural Radiance Fields), multi-view generation


- [Double-Step Alternating Extragradient with Increasing Timescale Separation for Finding Local Minimax Points: Provable Improvements](https://icml.cc/virtual/2024/poster/33119) (Poster)
  - **Authors:** [Kyuwon Kim](http://openreview.net/profile?id=~Kyuwon_Kim1), [Donghwan Kim](http://openreview.net/profile?id=~Donghwan_Kim2)
  - **Affiliations:** Department of Mathematical Sciences, KAIST, Daejeon, Republic of Korea, Department of Mathematical Sciences, KAIST, Daejeon, Republic of Korea
  - **TL;DR:** This paper addresses the challenges in finding local minimax points in nonconvex-nonconcave minimax optimization by proposing a two-timescale extragradient method that incorporates double-step alternating updates and increasing timescale separation. The proposed methods are shown to converge to non-strict local minimax points, overcoming limitations of existing approaches.
  - **Keywords:** nonconvex-nonconcave minimax optimization, local minimax points, two-timescale gradient methods, gradient descent ascent (GDA), extragradient methods, generative adversarial networks (GAN), adversarial training, multi-agent reinforcement learning, instability at non-strict local minimax points, timescale separation challenges, double-step alternating update, increasing timescale separation, convergence to non-strict local minimax points, local minimax points, Nash equilibrium, mode collapse


- [A Statistical Theory of Regularization-Based Continual Learning](https://icml.cc/virtual/2024/poster/34787) (Poster)
  - **Authors:** [Xuyang Zhao](http://openreview.net/profile?id=~Xuyang_Zhao2), [Huiyuan Wang](http://openreview.net/profile?id=~Huiyuan_Wang1), [Weiran Huang](http://openreview.net/profile?id=~Weiran_Huang1), [Wei Lin](http://openreview.net/profile?id=~Wei_Lin3)
  - **Affiliations:** School of Mathematical Sciences and Center for Statistical Science, Peking University, Beijing, China, Department of Biostatistics, Epidemiology and Informatics, Perelman School of Medicine, University of Pennsylvania, Philadelphia, PA, USA, MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, School of Mathematical Sciences and Center for Statistical Science, Peking University, Beijing, China
  - **TL;DR:** This study provides a statistical analysis of regularization-based continual learning, focusing on the impact of different regularization terms on model performance across multiple tasks. The findings reveal that optimal hyperparameter choices can balance knowledge transfer and mitigate issues like catastrophic forgetting, achieving performance comparable to an oracle estimator.
  - **Keywords:** Continual Learning, Regularization-Based Learning, Generalized ℓ2-regularization, Minimum Norm Estimator, Continual Ridge Regression, Catastrophic Forgetting, Data Heterogeneity, Optimal Algorithm, Estimation Error Analysis, Oracle Estimator, Forward Knowledge Transfer, Backward Knowledge Transfer


- [LangCell: Language-Cell Pre-training for Cell Identity Understanding](https://icml.cc/virtual/2024/poster/34495) (Poster)
  - **Authors:** [Suyuan Zhao](http://openreview.net/profile?id=~Suyuan_Zhao1), [Jiahuan Zhang](http://openreview.net/profile?id=~Jiahuan_Zhang1), [Yushuai Wu](http://openreview.net/profile?id=~Yushuai_Wu1), [YIZHEN LUO](http://openreview.net/profile?id=~YIZHEN_LUO1), [Zaiqing Nie](http://openreview.net/profile?id=~Zaiqing_Nie2)
  - **Affiliations:** Institute for AI Industry Research (AIR), Tsinghua University; Department of Computer Science and Technology, Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University; Department of Computer Science and Technology, Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University; PharMolix Inc.
  - **TL;DR:** This study introduces LangCell, a novel pre-training framework that integrates single-cell data and natural language to enhance understanding of cell identity. The results demonstrate its effectiveness in zero-shot scenarios and its superiority over existing models in few-shot and fine-tuning tasks.
  - **Keywords:** cell identity, bioinformatics, single-cell RNA sequencing, pre-trained language models (PLMs), transformer architecture, biomedical applications, cell type annotation, cell batch integration, lack of labeled data, challenges in understanding cell identity, reliance on human expert knowledge, LangCell framework, cross-modal knowledge comprehension, single-cell data, transcriptomics data


- [Accelerating Iterative Retrieval-augmented Language Model Serving with Speculation](https://icml.cc/virtual/2024/poster/34694) (Poster)
  - **Authors:** [Zhihao Zhang](http://openreview.net/profile?id=~Zhihao_Zhang2), [Alan Zhu](http://openreview.net/profile?id=~Alan_Zhu1), [Lijie Yang](http://openreview.net/profile?id=~Lijie_Yang1), [Yihua Xu](http://openreview.net/profile?id=~Yihua_Xu1), [Lanting Li](http://openreview.net/profile?id=~Lanting_Li1), [Phitchaya Phothilimthana](http://openreview.net/profile?id=~Phitchaya_Mangpo_Phothilimthana1), [Zhihao Jia](http://openreview.net/profile?id=~Zhihao_Jia2)
  - **Affiliations:** Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, University of California, Berkeley, Carnegie Mellon University, Google DeepMind, Carnegie Mellon University
  - **TL;DR:** This paper presents RaLMSpec, a framework that enhances the efficiency of iterative retrieval-augmented language models through speculative retrieval and batched verification, achieving significant performance improvements over existing methods. The proposed techniques reduce retrieval overhead while maintaining generative quality, demonstrating speedups of up to 7.59× in various retrieval scenarios.
  - **Keywords:** Retrieval-augmented language models, Speculative retrieval, RaLMSpec, Batched verification, Prefetching, Optimal speculation stride scheduler, Asynchronous verification, Natural language processing, Question answering, High retrieval overhead, Inefficiency of retrieval in iterative RaLM, Significant performance improvement over existing systems, Speedup in document-level and token-level iterative RaLM serving, QA datasets, Large language models (LLMs), Exact dense retriever, Approximate dense retriever, Sparse retriever, KNN-LM


- [Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss](https://icml.cc/virtual/2024/poster/34342) (Poster)
  - **Authors:** [Ruijie Zheng](http://openreview.net/profile?id=~Ruijie_Zheng1), [Yongyuan Liang](http://openreview.net/profile?id=~Yongyuan_Liang1), [xiyao wang](http://openreview.net/profile?id=~Xiyao_Wang1), [shuang ma](http://openreview.net/profile?id=~Shuang_Ma3), [Hal Daumé](http://openreview.net/profile?id=~Hal_Daum%C3%A9_III1), [Huazhe Xu](http://openreview.net/profile?id=~Huazhe_Xu1), [John Langford](http://openreview.net/profile?id=~John_Langford1), [Praveen Palanisamy](http://openreview.net/profile?id=~Praveen_Palanisamy2), [Kalyan Basu](http://openreview.net/profile?id=~Kalyan_Shankar_Basu1), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, Department of Computer Science, University of Maryland, College Park, Department of Computer Science, University of Maryland, College Park, Microsoft Research, Department of Computer Science, University of Maryland, College Park, Tsinghua University, Microsoft Research, Microsoft Research, Microsoft Research, Department of Computer Science, University of Maryland, College Park
  - **TL;DR:** The study introduces Premier-TACO, a multitask feature representation learning approach that enhances few-shot policy learning efficiency in sequential decision-making tasks through a novel negative example sampling strategy. Empirical evaluations demonstrate its effectiveness in pretraining visual representations, significantly improving imitation learning on novel tasks.
  - **Keywords:** Few-shot policy learning, Multitask feature representation learning, Sequential decision-making, Temporal action contrastive learning (TACO), Negative example sampling strategy, Continuous control benchmarks, Visual control tasks, Imitation learning, Data distribution shift, Task heterogeneity, Data quality and supervision, Enhanced few-shot imitation learning, Improved computational efficiency in pretraining, Deepmind Control Suite, MetaWorld, LIBERO


- [Graph-based Time Series Clustering for End-to-End Hierarchical Forecasting](https://icml.cc/virtual/2024/poster/33113) (Poster)
  - **Authors:** [Andrea Cini](http://openreview.net/profile?id=~Andrea_Cini1), [Danilo Mandic](http://openreview.net/profile?id=~Danilo_Mandic1), [Cesare Alippi](http://openreview.net/profile?id=~Cesare_Alippi1)
  - **Affiliations:** The Swiss AI Lab IDSIA, Università della Svizzera italiana, Switzerland; Politecnico di Milano, Italy, Imperial College London, United Kingdom, The Swiss AI Lab IDSIA, Università della Svizzera italiana, Switzerland; Politecnico di Milano, Italy
  - **TL;DR:** This paper presents a graph-based methodology that integrates relational and hierarchical inductive biases for effective time series forecasting. The proposed framework allows for learning hierarchical structures directly from data and demonstrates improved forecasting accuracy through simulation results.
  - **Keywords:** hierarchical time series forecasting, graph-based methodology, graph pooling operators, deep learning, energy consumption forecasting, financial time series forecasting, photovoltaic production forecasting, hierarchical inductive biases, forecast reconciliation, novel graph-based framework, multi-step ahead forecasting, relational inductive biases, clustering


- [Conformal Predictions under Markovian Data](https://icml.cc/virtual/2024/poster/33483) (Poster)
  - **Authors:** [Frédéric Zheng](http://openreview.net/profile?id=~Fr%C3%A9d%C3%A9ric_Zheng1), [Alexandre Proutiere](http://openreview.net/profile?id=~Alexandre_Proutiere1)
  - **Affiliations:** Division of Decision and Control Systems, EECS KTH; Digital Futures, Stockholm, Sweden, None
  - **TL;DR:** This study investigates the split Conformal Prediction method applied to Markovian data, quantifying the coverage gap induced by data correlations and proposing K-split CP to mitigate this impact. The findings reveal that the coverage gap scales with the mixing time of the Markov chain, and the proposed method effectively reduces this gap without significantly increasing the prediction set size.
  - **Keywords:** Conformal Prediction, Markovian Data, Split Conformal Prediction, K-split CP, Time Series Prediction, Machine Learning, Coverage Gap, Correlation in Data, Theoretical Analysis of Coverage, Prediction Set Size, Mixing Time, Ergodicity


- [Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity](https://icml.cc/virtual/2024/poster/33125) (Poster)
  - **Authors:** [Ali Behrouz](http://openreview.net/profile?id=~Ali_Behrouz1), [Parsa Delavari](http://openreview.net/profile?id=~Parsa_Delavari1), [Farnoosh Hashemi](http://openreview.net/profile?id=~Farnoosh_Hashemi1)
  - **Affiliations:** Cornell University, Ithaca, NY, USA, University of British Columbia, Vancouver, BC, Canada, Cornell University, Ithaca, NY, USA
  - **TL;DR:** This study presents BRAIN MIXER, an unsupervised learning framework that integrates voxel-level activity and functional connectivity to enhance brain representation learning. The framework demonstrates superior performance in various downstream tasks, addressing the limitations of existing methods that focus on either voxel-level or functional connectivity analyses.
  - **Keywords:** brain representation learning, cognitive processes, neurological diseases, unsupervised learning, MLP-based encoders, dynamic attention mechanism, temporal graph patching, adaptive temporal pooling, neuroimaging, cognitive function analysis, neurological condition detection, high-dimensional data, integration of voxel-level activity and functional connectivity, BRAIN MIXER framework, improved performance in downstream tasks, voxel-level activity, functional connectivity, multivariate pattern analysis (MVPA)


- [Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale](https://icml.cc/virtual/2024/poster/33503) (Poster)
  - **Authors:** [Candi Zheng](http://openreview.net/profile?id=~Candi_Zheng1), [Yuan LAN](http://openreview.net/profile?id=~Yuan_Lan1)
  - **Affiliations:** Department of Mathematics, Hong Kong University of Science and Technology, Hong Kong SAR, China; Department of Mechanics and Aerospace Engineering, Southern University of Science and Technology, Shenzhen, China, None
  - **TL;DR:** This paper introduces characteristic guidance, a non-linear correction method for classifier-free guidance in diffusion models, addressing issues of saturation and irregularities at large guidance scales. The proposed method enhances semantic characteristics in image generation and is applicable across various continuous data types and applications.
  - **Keywords:** diffusion models, denoising diffusion probabilistic model (DDPM), guidance techniques, classifier-free guidance, Fokker-Planck equation, method of characteristics, image generation, latent space sampling, simulating magnet phase transitions, weak control over context, large guidance scale irregularities, saturation in images, characteristic guidance, non-linear correction for guidance, CIFAR-10, ImageNet 256, Stable diffusion


- [CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents](https://icml.cc/virtual/2024/poster/32751) (Oral)
  - **Authors:** [Qinlin Zhao](http://openreview.net/profile?id=~Qinlin_Zhao1), [Jindong Wang](http://openreview.net/profile?id=~Jindong_Wang1), [Yixuan Zhang](http://openreview.net/profile?id=~Yixuan_Zhang7), [Yiqiao Jin](http://openreview.net/profile?id=~Yiqiao_Jin1), [Kaijie Zhu](http://openreview.net/profile?id=~Kaijie_Zhu1), [Hao Chen](http://openreview.net/profile?id=~Hao_Chen15), [Xing Xie](http://openreview.net/profile?id=~Xing_Xie3)
  - **Affiliations:** University of Science and Technology of China, Microsoft Research, William & Mary, Georgia Institute of Technology, Microsoft Research, Carnegie Mellon University, Microsoft Research
  - **TL;DR:** This study investigates the competition dynamics among Large Language Model-based agents, specifically in a simulated environment where restaurant agents compete for customers. The findings reveal insights that align with existing sociological and economic theories, highlighting the importance of competition in understanding societal functions.
  - **Keywords:** Competition dynamics, Large Language Models (LLMs), Agent-based modeling (ABM), GPT-4, Virtual town simulation, Restaurant and customer agent interactions, Limited understanding of competition mechanisms, micro-level competition analysis, Framework for studying competition, simulation experiments revealing findings, Sociological and economic theories, market competition, social learning theory


- [Spider: A Unified Framework for Context-dependent Concept Segmentation](https://icml.cc/virtual/2024/poster/33171) (Poster)
  - **Authors:** [Xiaoqi Zhao](http://openreview.net/profile?id=~Xiaoqi_Zhao1), [Youwei Pang](http://openreview.net/profile?id=~Youwei_Pang1), [Wei Ji](http://openreview.net/profile?id=~Wei_Ji2), [Baicheng Sheng](http://openreview.net/profile?id=~Baicheng_Sheng1), [Jiaming Zuo](http://openreview.net/profile?id=~Jiaming_Zuo2), [Lihe Zhang](http://openreview.net/profile?id=~Lihe_Zhang1), [Huchuan Lu](http://openreview.net/profile?id=~Huchuan_Lu1)
  - **Affiliations:** Dalian University of Technology, China; X3000 Inspection Co., Ltd, China, Dalian University of Technology, China; X3000 Inspection Co., Ltd, China, Yale University, America, Dalian University of Technology, China, X3000 Inspection Co., Ltd, China, Dalian University of Technology, China, Dalian University of Technology, China
  - **TL;DR:** The study presents Spider, a unified framework for context-dependent concept segmentation that significantly outperforms specialized models across various tasks. It demonstrates advantages in continuous learning, allowing for efficient adaptation to new tasks with minimal performance degradation.
  - **Keywords:** Context-dependent concepts, Unified framework, Artificial General Intelligence (AGI), Concept filter, Image-mask group prompt, Natural scene segmentation, Medical lesion segmentation, Limited cross-domain generalization, Inefficient data utilization, Unified model (Spider), Continuous learning capabilities, Context-independent (CI) concepts, Context-dependent (CD) concepts


- [Exploiting Negative Samples: A Catalyst for Cohort Discovery in Healthcare Analytics](https://icml.cc/virtual/2024/poster/33276) (Poster)
  - **Authors:** [Kaiping Zheng](http://openreview.net/profile?id=~Kaiping_Zheng1), [Horng-Ruey Chua](http://openreview.net/profile?id=~Horng-Ruey_Chua1), [Melanie Herschel](http://openreview.net/profile?id=~Melanie_Herschel1), [H. V Jagadish](http://openreview.net/profile?id=~H._V._Jagadish1), [Beng Chin Ooi](http://openreview.net/profile?id=~Beng_Chin_Ooi1), [James Yip](http://openreview.net/profile?id=~James_Wei_Luen_Yip2)
  - **Affiliations:** National University of Singapore, National University Hospital, Singapore, National University Hospital, Singapore, Universität Stuttgart, Germany, University of Michigan, USA, National University of Singapore, National University Hospital, Singapore, National University Heart Centre, Singapore
  - **TL;DR:** This study proposes a novel approach to cohort discovery in healthcare analytics by leveraging negative samples through a Shapley-based exploration, addressing the challenges posed by the asymmetry between positive and negative samples. The method demonstrates effectiveness in uncovering valuable insights from electronic medical records, benefiting medical research and clinical decision-making.
  - **Keywords:** healthcare analytics, cohort discovery, binary diagnosis, prognosis, Shapley-based exploration, data Shapley values, manifold learning, density-based clustering, electronic medical records, medical research, clinical decision-making, asymmetry between positive and negative samples, underexplored negative samples, Negative Sample Shapley Field, insights into disease and comorbidities, electronic medical records from National University Hospital, Singapore


- [Position: Measure Dataset Diversity, Don't Just Claim It](https://icml.cc/virtual/2024/poster/33283) (Best Paper)
  - **Authors:** [Dora Zhao](http://openreview.net/profile?id=~Dora_Zhao1), [Jerone Andrews](http://openreview.net/profile?id=~Jerone_Andrews1), [Orestis Papakyriakopoulos](http://openreview.net/profile?id=~Orestis_Papakyriakopoulos1), [Alice Xiang](http://openreview.net/profile?id=~Alice_Xiang1)
  - **Affiliations:** Stanford University, Sony AI, None, Sony AI, London, UK, Technical University of Munich, Munich, Germany, Sony AI, Seattle, USA
  - **TL;DR:** This paper critiques the ambiguous use of terms like diversity and bias in machine learning datasets, advocating for a structured approach to conceptualizing and evaluating these constructs using measurement theory. The findings highlight the need for transparency in dataset construction to address social biases and improve dataset quality.
  - **Keywords:** dataset diversity, machine learning datasets, social constructs, measurement theory, ambiguity in dataset definitions, underrepresentation of marginalized communities, recommendations for evaluating dataset diversity, 135 image and text datasets


- [ERQ: Error Reduction for Post-Training Quantization of Vision Transformers](https://icml.cc/virtual/2024/poster/33317) (Spotlight Poster)
  - **Authors:** [Yunshan Zhong](http://openreview.net/profile?id=~Yunshan_Zhong1), [Jiawei Hu](http://openreview.net/profile?id=~Jiawei_Hu4), [You Huang](http://openreview.net/profile?id=~You_Huang1), [Yuxin Zhang](http://openreview.net/profile?id=~Yuxin_Zhang3), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Institute of Artificial Intelligence, Xiamen University; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University, Institute of Artificial Intelligence, Xiamen University; Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University; Department of Artificial Intelligence, School of Informatics, Xiamen University; Peng Cheng Laboratory
  - **TL;DR:** This paper presents ERQ, a two-step post-training quantization approach for vision transformers that effectively reduces quantization errors in both activation and weight. The method demonstrates a significant improvement in accuracy, surpassing existing state-of-the-art techniques by 22.36%.
  - **Keywords:** Post-training quantization, Vision transformers, Activation quantization error reduction (Aqer), Weight quantization error reduction (Wqer), Ridge Regression, Model compression, Efficient deployments in resource-constrained environments, Quantization error, High computational demands, Substantial memory requirements, Two-step PTQ approach, Improved accuracy over state-of-the-art methods, Vision Transformers (ViTs), Multi-head self-attention (MHSA)


- [Self-Infilling Code Generation](https://icml.cc/virtual/2024/poster/33640) (Poster)
  - **Authors:** [Lin Zheng](http://openreview.net/profile?id=~Lin_Zheng1), [Jianbo Yuan](http://openreview.net/profile?id=~Jianbo_Yuan1), [Zhi Zhang](http://openreview.net/profile?id=~Zhi_Zhang4), [Hongxia Yang](http://openreview.net/profile?id=~Hongxia_Yang2), [Lingpeng Kong](http://openreview.net/profile?id=~Lingpeng_Kong1)
  - **Affiliations:** The University of Hong Kong, ByteDance Inc., ByteDance Inc., ByteDance Inc., The University of Hong Kong
  - **TL;DR:** This study introduces a self-infilling code generation framework that enhances auto-regressive decoding by incorporating infilling operations, leading to improved output quality and regularization of code generation. The proposed mechanisms allow for better control and synchronization during the code generation process.
  - **Keywords:** self-infilling code generation, code language models, auto-regressive decoding, infilling operations, interruption mechanism, looping mechanism, code generation, code understanding, potential degeneration in code generation, need for bidirectional context in code tasks, improved output quality, regularization of code generation, enhanced control during decoding


- [Quantum Implicit Neural Representations](https://icml.cc/virtual/2024/poster/34996) (Poster)
  - **Authors:** [Jiaming Zhao](http://openreview.net/profile?id=~Jiaming_Zhao2), [Wenbo Qiao](http://openreview.net/profile?id=~Wenbo_Qiao1), [Peng Zhang](http://openreview.net/profile?id=~Peng_Zhang17), [Hui Gao](http://openreview.net/profile?id=~Hui_Gao4)
  - **Affiliations:** School of New Media and Communication, Tianjin University, China, School of New Media and Communication, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China, College of Intelligence and Computing, Tianjin University, China
  - **TL;DR:** This paper introduces the Quantum Implicit Representation Network (QIREN), a novel quantum extension of Fourier Neural Networks that effectively models high-frequency components of signals. Experimental results demonstrate QIREN's superior performance in signal representation, image superresolution, and image generation compared to state-of-the-art models.
  - **Keywords:** Implicit Neural Representations, Quantum Neural Networks, Fourier Neural Networks, Quantum Implicit Representation Network (QIREN), Signal representation, Image superresolution, Image generation, Challenges in modeling high-frequency components, Computational resource demands, Quantum advantage over classical FNNs, Improved representation of high-frequency components, ReLU-based multilayer perceptrons (MLPs), Trigonometric functions


- [Provably Efficient Exploration in Quantum Reinforcement Learning with Logarithmic Worst-Case Regret](https://icml.cc/virtual/2024/poster/33509) (Poster)
  - **Authors:** [Han Zhong](http://openreview.net/profile?id=~Han_Zhong1), [Jiachen Hu](http://openreview.net/profile?id=~Jiachen_Hu1), [Yecheng Xue](http://openreview.net/profile?id=~Yecheng_Xue1), [Tongyang Li](http://openreview.net/profile?id=~Tongyang_Li1), [Liwei Wang](http://openreview.net/profile?id=~Liwei_Wang1)
  - **Affiliations:** Center for Data Science, Peking University, School of Computer Science, Peking University, School of Computer Science, Peking University, Center on Frontiers of Computing Studies, Peking University; National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, Center for Data Science, Peking University; National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University
  - **TL;DR:** This paper presents a novel quantum reinforcement learning algorithm that efficiently addresses the exploration-exploitation trade-off in tabular Markov decision processes, achieving logarithmic worst-case regret. The proposed methods extend to linear function approximation, demonstrating significant improvements over classical approaches.
  - **Keywords:** Quantum Reinforcement Learning, Exploration-Exploitation Trade-off, UCRL-style algorithm, Value Target Regression (VTR), Quantum estimation subroutines, Tabular Markov Decision Processes (MDPs), Linear mixture MDPs, Exploration-exploitation trade-off, Regret minimization, O(poly(S, A, H, logT)) worst-case regret, O(poly(d, H, logT)) regret for linear mixture MDPs, Markov Decision Processes (MDPs), Quantum computing


- [DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max Vehicle Routing Problems](https://icml.cc/virtual/2024/poster/33667) (Poster)
  - **Authors:** [zhi Zheng](http://openreview.net/profile?id=~Zhi_Zheng2), [Shunyu Yao](http://openreview.net/profile?id=~Shunyu_Yao3), [Zhenkun Wang](http://openreview.net/profile?id=~Zhenkun_Wang1), [Tong Xialiang](http://openreview.net/profile?id=~Tong_Xialiang2), [Mingxuan Yuan](http://openreview.net/profile?id=~Mingxuan_Yuan1), [Ke Tang](http://openreview.net/profile?id=~Ke_Tang2)
  - **Affiliations:** Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China, Department of Computer Science, City University of Hong Kong, Hong Kong SAR, School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Shenzhen, China, China Huawei Noah’s Ark Lab, Shenzhen, China, China Huawei Noah’s Ark Lab, Shenzhen, China, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China
  - **TL;DR:** This paper presents a novel approach to the min-max vehicle routing problem by decoupling customer partitioning and navigation tasks using an attention-based encoder. The proposed Decoupling-Partition-Navigation method significantly outperforms existing learning-based methods in both single-depot and multi-depot scenarios.
  - **Keywords:** min-max vehicle routing problem, reinforcement learning, sequential planning, attention-based Partition-and-Navigation encoder, agent-permutation-symmetric loss function, transportation planning, disaster management, robotics, balancing route lengths, customer partitioning, navigation tasks, Decoupling-Partition-Navigation method, improved efficiency and optimality in solving min-max VRPs


- [GNNs Also Deserve Editing, and They Need It More Than Once](https://icml.cc/virtual/2024/poster/32961) (Poster)
  - **Authors:** [Shaochen (Henry) Zhong](http://openreview.net/profile?id=~Shaochen_Zhong1), [Duy Le](http://openreview.net/profile?id=~Duy_Le1), [Zirui Liu](http://openreview.net/profile?id=~Zirui_Liu1), [Zhimeng Jiang](http://openreview.net/profile?id=~Zhimeng_Jiang1), [Andrew Ye](http://openreview.net/profile?id=~Andrew_Ye1), [Jiamu Zhang](http://openreview.net/profile?id=~Jiamu_Zhang1), [Jiayi Yuan](http://openreview.net/profile?id=~Jiayi_Yuan1), [Kaixiong Zhou](http://openreview.net/profile?id=~Kaixiong_Zhou1), [Zhaozhuo Xu](http://openreview.net/profile?id=~Zhaozhuo_Xu2), [Jing Ma](http://openreview.net/profile?id=~Jing_Ma2), [Shuai Xu](http://openreview.net/profile?id=~Shuai_Xu2), [Vipin Chaudhary](http://openreview.net/profile?id=~Vipin_Chaudhary2), [Xia Hu](http://openreview.net/profile?id=~Xia_Hu4)
  - **Affiliations:** Rice University, Case Western Reserve University, Rice University, Texas A&M University, Case Western Reserve University, Case Western Reserve University, Rice University, North Carolina State University, Stevens Institute of Technology, Case Western Reserve University, Case Western Reserve University, Case Western Reserve University, Rice University
  - **TL;DR:** This paper addresses the need for effective model editing in Graph Neural Networks (GNNs) by proposing SEED-GNN, a method that enhances sequential editing robustness and mitigates model overfitting. The findings highlight the importance of efficient error correction in GNNs, which is crucial for real-world applications like autonomous driving and chatbots.
  - **Keywords:** Model Editing, Graph Neural Networks (GNNs), Sequential Editing Robustness, Overfit-prevention techniques, SEED-GNN, Autonomous driving, Chatbots, Model overfitting, Ineffective error correction in GNNs, GNN model editing method, Efficient patching of model behaviors


- [Finite-Time Convergence and Sample Complexity of Actor-Critic Multi-Objective Reinforcement Learning](https://icml.cc/virtual/2024/poster/35162) (Poster)
  - **Authors:** [Tianchen Zhou](http://openreview.net/profile?id=~Tianchen_Zhou1), [Hairi](http://openreview.net/profile?id=~FNU_Hairi1), [Haibo Yang](http://openreview.net/profile?id=~Haibo_Yang1), [Jia (Kevin) Liu](http://openreview.net/profile?id=~Jia_Liu1), [Tian Tong](http://openreview.net/profile?id=~Tian_Tong1), [Fan Yang](http://openreview.net/profile?id=~Fan_Yang36), [Michinari Momma](http://openreview.net/profile?id=~Michinari_Momma2), [Yan Gao](http://openreview.net/profile?id=~Yan_Gao8)
  - **Affiliations:** Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA; Amazon.com, Seattle, WA, USA, Department of Computer Science, University of Wisconsin-Whitewater, WI, USA, Department of Computing and Information Sciences, Rochester Institute of Technology, Rochester, NY, USA, Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA, Amazon.com, Seattle, WA, USA, Amazon.com, Seattle, WA, USA, Amazon.com, Seattle, WA, USA, Amazon.com, Seattle, WA, USA
  - **TL;DR:** This paper introduces a novel actor-critic algorithm called MOAC for solving multi-objective reinforcement learning (MORL) problems, providing the first analysis of finite-time convergence and sample complexity. The proposed method effectively addresses conflicting reward signals and enhances practical applicability through improved initialization techniques.
  - **Keywords:** Multi-Objective Reinforcement Learning (MORL), Actor-Critic Algorithms, MOAC (Multi-Objective Actor-Critic), Gradient Descent, Recommender Systems, E-commerce, Healthcare, Robotics, Conflicting Reward Objectives, Cumulative Estimation Bias, Finite-Time Convergence, Pareto-Stationary Convergence Analysis, Sample Complexity Guarantees, Real-World Dataset (specific dataset not mentioned)


- [On Prompt-Driven Safeguarding for Large Language Models](https://icml.cc/virtual/2024/poster/32814) (Poster)
  - **Authors:** [Chujie Zheng](http://openreview.net/profile?id=~Chujie_Zheng2), [Fan Yin](http://openreview.net/profile?id=~Fan_Yin1), [Hao Zhou](http://openreview.net/profile?id=~Hao_Zhou8), [Fandong Meng](http://openreview.net/profile?id=~Fandong_Meng3), [Jie Zhou](http://openreview.net/profile?id=~Jie_Zhou8), [Kai-Wei Chang](http://openreview.net/profile?id=~Kai-Wei_Chang1), [Minlie Huang](http://openreview.net/profile?id=~Minlie_Huang1), [Nanyun Peng](http://openreview.net/profile?id=~Nanyun_Peng1)
  - **Affiliations:** The CoAI Group, DCST, BNRist, Tsinghua University, University of California, Los Angeles, Pattern Recognition Center, WeChat AI, Tencent Inc., China, Pattern Recognition Center, WeChat AI, Tencent Inc., China, Pattern Recognition Center, WeChat AI, Tencent Inc., China, University of California, Los Angeles, The CoAI Group, DCST, BNRist, Tsinghua University, University of California, Los Angeles
  - **TL;DR:** This study investigates the impact of safety prompts on the behavior of large language models (LLMs) in recognizing and refusing harmful queries. The proposed method, Directed Representation Optimization (DRO), enhances the effectiveness of safety prompts without compromising the models' overall performance.
  - **Keywords:** Large Language Models, Safety Prompts, Directed Representation Optimization (DRO), AI Safety, Distinguishing harmful and harmless queries, compliance with harmful queries, Improved safeguarding performance of safety prompts


- [GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting](https://icml.cc/virtual/2024/poster/32635) (Poster)
  - **Authors:** [Xiaoyu Zhou](http://openreview.net/profile?id=~Xiaoyu_Zhou1), [Xingjian Ran](http://openreview.net/profile?id=~Xingjian_Ran1), [Yajiao Xiong](http://openreview.net/profile?id=~Yajiao_Xiong1), [Jinlin He](http://openreview.net/profile?id=~Jinlin_He1), [Zhiwei Lin](http://openreview.net/profile?id=~Zhiwei_Lin1), [Yongtao Wang](http://openreview.net/profile?id=~Yongtao_Wang1), [Deqing Sun](http://openreview.net/profile?id=~Deqing_Sun2), [Ming-Hsuan Yang](http://openreview.net/profile?id=~Ming-Hsuan_Yang1)
  - **Affiliations:** Wangxuan Institute of Computer Technology, Peking University, Wangxuan Institute of Computer Technology, Peking University, Wangxuan Institute of Computer Technology, Peking University, Wangxuan Institute of Computer Technology, Peking University, Wangxuan Institute of Computer Technology, Peking University, Wangxuan Institute of Computer Technology, Peking University; National Key Laboratory for Multimedia Information Processing, Google DeepMind, University of California, Merced; Google DeepMind
  - **TL;DR:** The study presents GALA3D, a method for generating high-quality complex 3D scenes from text descriptions using layout-guided generative Gaussian splatting. It addresses challenges in existing methods, such as low-quality textures and inaccurate object interactions, by leveraging large language models for initial layout generation.
  - **Keywords:** Text-to-3D generation, Complex scene generation, Generative Gaussian Splatting, Layout-guided control, Large Language Models (LLMs), 3D content generation, Interactive editing, Low-quality textures, Visual artifacts, Geometric distortions, Accurate object interactions, Layout-guided 3D Gaussian representation


- [Learning Latent Space Hierarchical EBM Diffusion Models](https://icml.cc/virtual/2024/poster/33094) (Poster)
  - **Authors:** [Jiali Cui](http://openreview.net/profile?id=~Jiali_Cui1), [Tian Han](http://openreview.net/profile?id=~Tian_Han1)
  - **Affiliations:** Department of Computer Science, Stevens Institute of Technology, Department of Computer Science, Stevens Institute of Technology
  - **TL;DR:** This study addresses the limitations of Gaussian prior models in hierarchical generative models by proposing a diffusion probabilistic scheme to learn energy-based priors effectively. The results demonstrate improved performance in sampling and learning EBM priors, bridging the gap between generator posteriors and prior models.
  - **Keywords:** hierarchical generative models, energy-based models, diffusion probabilistic scheme, Markov Chain Monte Carlo (MCMC) sampling, prior hole problem, multi-modal generator posterior, EBM sampling challenges, diffusion-learned EBM prior, conditional EBM prior, Gaussian prior model, latent variables, Two-Stage learning scheme


- [CurBench: Curriculum Learning Benchmark](https://icml.cc/virtual/2024/poster/34442) (Poster)
  - **Authors:** [Yuwei Zhou](http://openreview.net/profile?id=~Yuwei_Zhou1), [Zirui Pan](http://openreview.net/profile?id=~Zirui_Pan1), [Xin Wang](http://openreview.net/profile?id=~Xin_Wang17), [Hong Chen](http://openreview.net/profile?id=~Hong_Chen9), [Haoyang Li](http://openreview.net/profile?id=~Haoyang_Li1), [Yanwen Huang](http://openreview.net/profile?id=~Yanwen_Huang1), [Zhixiao Xiong](http://openreview.net/profile?id=~Zhixiao_Xiong1), [Fangzhou Xiong](http://openreview.net/profile?id=~Fangzhou_Xiong1), [Peiyang Xu](http://openreview.net/profile?id=~Peiyang_Xu2), [Shengnan liu](http://openreview.net/profile?id=~Shengnan_liu3), [Wenwu Zhu](http://openreview.net/profile?id=~Wenwu_Zhu1)
  - **Affiliations:** Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University, Department of Computer Science and Technology, BNRIST, Tsinghua University
  - **TL;DR:** This paper introduces CurBench, a benchmark for curriculum learning that facilitates systematic evaluations across various datasets and settings. The study highlights the importance of a structured training approach to enhance model generalization and convergence in machine learning.
  - **Keywords:** Curriculum Learning, Machine Learning, Computer Vision, Natural Language Processing, Graph Machine Learning, Benchmarking, Model Generalization, Convergence, CurBench, Evaluation Metrics, Core Curriculum Learning Methods, 15 datasets, Open-source


- [Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process](https://icml.cc/virtual/2024/poster/32649) (Poster)
  - **Authors:** [Xiangxin Zhou](http://openreview.net/profile?id=~Xiangxin_Zhou1), [Liang Wang](http://openreview.net/profile?id=~Liang_Wang3), [Yichi Zhou](http://openreview.net/profile?id=~Yichi_Zhou2)
  - **Affiliations:** School of Artificial Intelligence, University of Chinese Academy of Sciences; New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), School of Artificial Intelligence, University of Chinese Academy of Sciences; New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), ByteDance Research
  - **TL;DR:** This study proposes a method to stabilize policy gradients for stochastic differential equations (SDEs) by ensuring consistency with a perturbation process, addressing issues of data sparsity and instability. The approach is evaluated in the context of structure-based drug design, achieving a notable improvement in binding affinity scores.
  - **Keywords:** Stochastic Differential Equations (SDEs), Generative Modeling, Policy Gradient, Perturbation Process, Structure-based Drug Design, Molecule Generation, Data Sparsity, Ill-defined Policy Gradients, Stability Issues, Optimization of Binding Affinity, Best Vina Score, CrossDocked2020 Dataset


- [ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL](https://icml.cc/virtual/2024/poster/33654) (Poster)
  - **Authors:** [Yifei Zhou](http://openreview.net/profile?id=~Yifei_Zhou1), [Andrea Zanette](http://openreview.net/profile?id=~Andrea_Zanette1), [Jiayi Pan](http://openreview.net/profile?id=~Jiayi_Pan1), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1), [Aviral Kumar](http://openreview.net/profile?id=~Aviral_Kumar2)
  - **Affiliations:** University of California, Berkeley, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley, Google Deepmind
  - **TL;DR:** This paper presents ArCHer, a hierarchical reinforcement learning framework designed to enhance the training of large language models in multi-turn decision-making tasks. The framework significantly improves sample efficiency and performance, achieving a 100x efficiency gain over existing methods while accommodating long-term interactions and delayed rewards.
  - **Keywords:** large language models, goal-directed decision-making, multi-turn interactions, reinforcement learning, hierarchical RL, off-policy value-based RL, policy gradient RL, agent tasks, human interaction, tool usage, credit assignment, long-term rewards, online interaction challenges, ArCHer framework, improved sample efficiency, hierarchical structure


- [DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection](https://icml.cc/virtual/2024/poster/34225) (Poster)
  - **Authors:** [Zhi Zhou](http://openreview.net/profile?id=~Zhi_Zhou2), [Ming Yang](http://openreview.net/profile?id=~Ming_Yang22), [Jiang-Xin Shi](http://openreview.net/profile?id=~Jiang-Xin_Shi1), [Lan-Zhe Guo](http://openreview.net/profile?id=~Lan-Zhe_Guo2), [Yu-Feng Li](http://openreview.net/profile?id=~Yu-Feng_Li1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Intelligence Science and Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This study introduces Open-world Prompt Tuning (OPT) to improve the performance of vision-language models by tuning prompts on base classes and evaluating them on a mix of base and new classes. The proposed Decomposed Context Optimization (DECOOP) method enhances discriminability and achieves a significant 2% accuracy improvement over state-of-the-art methods.
  - **Keywords:** Vision-language models, Open-world Prompt Tuning, Few-shot prompt tuning, Decomposed Prompt Tuning, Decomposed Context Optimization, Downstream tasks in image and language processing, Base-to-new class generalization, out-of-distribution detection, Enhanced discriminability between base and new classes, performance improvement, 11 benchmark datasets, CLIP, zero-shot capabilities


- [Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation](https://icml.cc/virtual/2024/poster/34068) (Poster)
  - **Authors:** [Mingyuan Zhou](http://openreview.net/profile?id=~Mingyuan_Zhou1), [Huangjie Zheng](http://openreview.net/profile?id=~Huangjie_Zheng1), [Zhendong Wang](http://openreview.net/profile?id=~Zhendong_Wang1), [Mingzhang Yin](http://openreview.net/profile?id=~Mingzhang_Yin1), [Hai Huang](http://openreview.net/profile?id=~Hai_Huang5)
  - **Affiliations:** The University of Texas at Austin; Google, The University of Texas at Austin, The University of Texas at Austin, The University of Florida, Google; The University of Texas at Austin
  - **TL;DR:** This study introduces Score identity Distillation (SiD), a data-free method that distills pretrained diffusion models into a single-step generator, achieving rapid reductions in Fréchet inception distance (FID) and surpassing existing distillation methods in generation quality. The proposed approach leverages innovative loss mechanisms and semi-implicit distributions, significantly improving efficiency in diffusion-based generation.
  - **Keywords:** diffusion models, generative models, single-step generation, Score identity Distillation (SiD), semi-implicit distributions, score-matching loss, image generation, high-dimensional data generation, multi-step generation, high inference cost, intractable score-matching loss, rapid FID reduction, efficient distillation, innovative loss mechanism, CIFAR-10, ImageNet, PyTorch, Fréchet inception distance (FID), Fisher divergence, Monte Carlo estimation


- [Graphon Mean Field Games with a Representative Player: Analysis and Learning Algorithm](https://icml.cc/virtual/2024/poster/34891) (Poster)
  - **Authors:** [Fuzhong Zhou](http://openreview.net/profile?id=~Fuzhong_Zhou1), [Chenyu Zhang](http://openreview.net/profile?id=~Chenyu_Zhang2), [Xu Chen](http://openreview.net/profile?id=~Xu_Chen29), [Xuan Di](http://openreview.net/profile?id=~Xuan_Di1)
  - **Affiliations:** Department of Industrial Engineering and Operations Research, Columbia University, New York, NY, USA, Data Science Institute, Columbia University, New York, NY, USA, Department of Civil Engineering and Engineering Mechanics, Columbia University, New York, NY, USA, Department of Civil Engineering and Engineering Mechanics, Columbia University, New York, NY, USA
  - **TL;DR:** This study introduces a discrete time graphon game formulation to analyze stochastic games with heterogeneous interactions among agents, proving the existence and uniqueness of the graphon equilibrium. The findings suggest that this equilibrium can effectively approximate solutions for finite player games on networks, addressing challenges posed by the curse of dimensionality.
  - **Keywords:** Graphon Mean Field Games, Stochastic Games, Heterogeneous Interaction, Graphon Equilibrium, Online Oracle-Free Learning Algorithm, Multiagent Systems, Autonomous Driving, Epidemiology, Flocking, Curse of Dimensionality, Heterogeneous Interaction, Existence and Uniqueness of Graphon Equilibrium, Approximate Solution for Finite Player Games, Mean Field Games (MFGs), Nash Equilibrium (NE), Weighted Graphs


- [RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation](https://icml.cc/virtual/2024/poster/33051) (Poster)
  - **Authors:** [Jiawei Zhou](http://openreview.net/profile?id=~Jiawei_Zhou4), [Linye Lyu](http://openreview.net/profile?id=~Linye_Lyu1), [Daojing He](http://openreview.net/profile?id=~Daojing_He1), [YU LI](http://openreview.net/profile?id=~YU_LI10)
  - **Affiliations:** School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen 518055, P.R. China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen 518055, P.R. China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen 518055, P.R. China, School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen 518055, P.R. China
  - **TL;DR:** This study presents RAUCA, a novel method for generating robust and accurate adversarial camouflage against vehicle detectors, addressing challenges related to environmental characteristics and weather conditions. Experimental results demonstrate that RAUCA outperforms existing methods in both simulated and real-world scenarios.
  - **Keywords:** Adversarial camouflage, Physical adversarial attacks, Vehicle detection, Differentiable neural renderers, Neural Renderer Plus (NRP), Autonomous driving, Vehicle detection systems, Suboptimal attack performance, Environmental characteristics, Diverse weather conditions, Robust and accurate camouflage generation, Enhanced attack robustness, Multi-weather dataset


- [Exploring Training on Heterogeneous Data with Mixture of Low-rank Adapters](https://icml.cc/virtual/2024/poster/34206) (Poster)
  - **Authors:** [Yuhang Zhou](http://openreview.net/profile?id=~Yuhang_Zhou4), [Zhao Zihua](http://openreview.net/profile?id=~Zihua_Zhao3), [Siyuan Du](http://openreview.net/profile?id=~Siyuan_Du1), [Haolin li](http://openreview.net/profile?id=~Haolin_li2), [Jiangchao Yao](http://openreview.net/profile?id=~Jiangchao_Yao1), [Ya Zhang](http://openreview.net/profile?id=~Ya_Zhang1), [Yanfeng Wang](http://openreview.net/profile?id=~Yanfeng_Wang1)
  - **Affiliations:** Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory, Fudan University, Fudan University, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University; Shanghai Artificial Intelligence Laboratory
  - **TL;DR:** This study investigates the use of Mixture of Low-rank Adapters (MoLA) to address training conflicts in heterogeneous data, introducing two variants, MoLA-Grad and MoLA-Router, to enhance model performance across diverse tasks. The findings demonstrate that MoLA effectively mitigates conflicts and improves training outcomes compared to existing methods.
  - **Keywords:** heterogeneous data training, artificial general intelligence, Mixture of Low-rank Adapters (MoLA), MoLA-Grad, MoLA-Router, Task-wise Decorrelation (TwD) loss, multi-task learning (MTL), training conflicts, data heterogeneity, joint training of low-rank adapters, mitigation of conflicts in heterogeneous data, low-rank adaptation (LoRA), Mixture of Experts (MoE)


- [Sequential Kernel Goodness-of-fit Testing](https://icml.cc/virtual/2024/poster/33623) (Poster)
  - **Authors:** [Zhengyu Zhou](http://openreview.net/profile?id=~Zhengyu_Zhou1), [Weiwei Liu](http://openreview.net/profile?id=~Weiwei_Liu1)
  - **Affiliations:** School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China, School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China
  - **TL;DR:** This paper presents a novel approach to sequential goodness-of-fit testing that adapts to streaming data, addressing the limitations of traditional batch tests. The proposed method maintains control over type-I errors while allowing for flexible data collection and hypothesis testing.
  - **Keywords:** Goodness-of-fit testing, Sequential testing, Kernel Stein Discrepancy (KSD), Statistical power, Streaming data analysis, Sample size determination, Data peeking, Type-I error control, Consistent sequential goodness-of-fit tests, Kolmogorov–Smirnov test, Stein discrepancy, Cramér–von Mises criterion


- [Generative Active Learning for Long-tailed Instance Segmentation](https://icml.cc/virtual/2024/poster/33076) (Poster)
  - **Authors:** [Muzhi Zhu](http://openreview.net/profile?id=~Muzhi_Zhu1), [Chengxiang Fan](http://openreview.net/profile?id=~Chengxiang_Fan1), [Hao Chen](http://openreview.net/profile?id=~Hao_Chen17), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu83), [Weian Mao](http://openreview.net/profile?id=~Weian_Mao2), [Xiaogang Xu](http://openreview.net/profile?id=~Xiaogang_Xu2), [Chunhua Shen](http://openreview.net/profile?id=~Chunhua_Shen2)
  - **Affiliations:** Zhejiang University, China, Zhejiang University, China, Zhejiang University, China, Zhejiang University, China, The University of Adelaide, Australia, The Chinese University of Hong Kong, China, Ant Group
  - **TL;DR:** This paper introduces BSGAL, a novel algorithm for generative active learning aimed at enhancing long-tailed instance segmentation by effectively utilizing generated data. The results demonstrate that BSGAL significantly outperforms baseline methods, addressing challenges related to data quality and selection in segmentation tasks.
  - **Keywords:** Generative Active Learning, Long-tailed Instance Segmentation, BSGAL (new algorithm), Gradient Cache, Visual Perception, Instance Segmentation, Data sparsity in long-tailed categories, Quality of generated data, Improved performance in long-tailed segmentation, Effective utilization of generated data


- [Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model](https://icml.cc/virtual/2024/poster/33768) (Poster)
  - **Authors:** [Lianghui Zhu](http://openreview.net/profile?id=~Lianghui_Zhu3), [Bencheng Liao](http://openreview.net/profile?id=~Bencheng_Liao1), [Qian Zhang](http://openreview.net/profile?id=~Qian_Zhang7), [Xinlong Wang](http://openreview.net/profile?id=~Xinlong_Wang2), [Wenyu Liu](http://openreview.net/profile?id=~Wenyu_Liu3), [Xinggang Wang](http://openreview.net/profile?id=~Xinggang_Wang1)
  - **Affiliations:** School of EIC, Huazhong University of Science & Technology, Institute of Artificial Intelligence, Huazhong University of Science & Technology, Horizon Robotics, Beijing Academy of Artificial Intelligence, School of EIC, Huazhong University of Science & Technology, School of EIC, Huazhong University of Science & Technology
  - **TL;DR:** This paper introduces Vim, a new vision backbone utilizing bidirectional state space models for efficient visual representation learning, demonstrating superior performance and memory efficiency compared to existing models like DeiT on various tasks. The findings suggest Vim's potential as a next-generation backbone for vision foundation models.
  - **Keywords:** visual representation learning, state space models, bidirectional state space models, Mamba deep learning model, image classification, object detection, semantic segmentation, position-sensitivity of visual data, global context requirement, new vision backbone (Vim), improved computation and memory efficiency, ImageNet, COCO, ADE20k


- [Switched Flow Matching: Eliminating Singularities via Switching ODEs](https://icml.cc/virtual/2024/poster/35090) (Poster)
  - **Authors:** [Qunxi Zhu](http://openreview.net/profile?id=~Qunxi_Zhu1), [Wei Lin](http://openreview.net/profile?id=~Wei_Lin1)
  - **Affiliations:** Research Institute of Intelligent Complex Systems, Fudan University, China; School of Mathematical Sciences, LMNS, and SCMS, Fudan University, China; State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Institutes of Brain Science, Fudan University, China; Shanghai Artificial Intelligence Laboratory, China, Research Institute of Intelligent Complex Systems, Fudan University, China; School of Mathematical Sciences, LMNS, and SCMS, Fudan University, China; State Key Laboratory of Medical Neurobiology and MOE Frontiers Center for Brain Science, Institutes of Brain Science, Fudan University, China; Shanghai Artificial Intelligence Laboratory, China
  - **TL;DR:** This paper introduces Switched Flow Matching (SFM) as a framework to address the singularity problem in continuous-time generative models, enhancing the efficiency of sampling processes. The authors demonstrate that SFM can effectively transport between distributions where traditional Flow Matching (FM) fails, leading to improved integration of neural ODEs.
  - **Keywords:** generative modeling, probability distribution transformation, Flow Matching (FM), Switched Flow Matching (SFM), neural ordinary differential equations (ODEs), singularity problem, joint heterogeneity of distributions, efficient sampling process, enhanced flow straightness


- [Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF](https://icml.cc/virtual/2024/poster/33860) (Poster)
  - **Authors:** [Banghua Zhu](http://openreview.net/profile?id=~Banghua_Zhu1), [Michael Jordan](http://openreview.net/profile?id=~Michael_Jordan1), [Jiantao Jiao](http://openreview.net/profile?id=~Jiantao_Jiao1)
  - **Affiliations:** Department of EECS, University of California, Berkeley, Department of EECS, University of California, Berkeley, Department of EECS, University of California, Berkeley
  - **TL;DR:** This paper addresses the issues of reward overfitting and overoptimization in Reinforcement Learning from Human Feedback (RLHF) by proposing an improved reward learning algorithm called Iterative Data Smoothing (IDS), which replaces hard labels with soft labels during training. The findings demonstrate that this approach significantly enhances performance compared to traditional methods.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Large Language Models (LLMs), Reward model, Iterative Data Smoothing (IDS), Natural Language Processing, AI applications, Reward overfitting, Reward overoptimization, Value-reward mismatches, Improved reward learning algorithm, Soft labels


- [Iterative Search Attribution for Deep Neural Networks](https://icml.cc/virtual/2024/poster/34976) (Poster)
  - **Authors:** [Zhiyu Zhu](http://openreview.net/profile?id=~Zhiyu_Zhu2), [Huaming Chen](http://openreview.net/profile?id=~Huaming_Chen1), [Xinyi Wang](http://openreview.net/profile?id=~Xinyi_Wang9), [Jiayu Zhang](http://openreview.net/profile?id=~Jiayu_Zhang1), [Zhibo Jin](http://openreview.net/profile?id=~Zhibo_Jin1), [Minhui Xue](http://openreview.net/profile?id=~Jason_Xue1), [Jun Shen](http://openreview.net/profile?id=~Jun_Shen3)
  - **Affiliations:** School of Electrical and Computer Engineering, University of Sydney, Sydney, NSW, Australia, School of Electrical and Computer Engineering, University of Sydney, Sydney, NSW, Australia, Faculty of Computer Science & Information Technology, University of Malaya, Suzhou Yierqi, Suzhou, China, School of Electrical and Computer Engineering, University of Sydney, Sydney, NSW, Australia, Data61, CSIRO, Sydney, NSW, Australia, University of Wollongong, Australia
  - **TL;DR:** This paper proposes an Iterative Search Attribution (ISA) method to enhance the interpretability of deep neural networks by iteratively distinguishing the importance of features during gradient ascent and descent. The method demonstrates superior performance in image recognition tasks compared to existing attribution algorithms.
  - **Keywords:** Explainable Artificial Intelligence (XAI), Deep Neural Networks (DNNs), Interpretability, Iterative Search Attribution (ISA), Gradient Ascent, Gradient Descent, Adversarial Gradient Integration (AGI), Integrated Gradient (IG), More Faithful and Accelerated Boundary-based Attribution (MFABA), Image Recognition, Computer Vision, Natural Language Processing, Speech Recognition, Interpretability of DNNs, Complexity of Nonlinear Layers, Decision-Making Errors, AI Safety, Enhanced Attribution Accuracy, Faithful Feature Identification


- [Conformalized Adaptive Forecasting of Heterogeneous Trajectories](https://icml.cc/virtual/2024/poster/33115) (Poster)
  - **Authors:** [Yanfei Zhou](http://openreview.net/profile?id=~Yanfei_Zhou1), [Lars Lindemann](http://openreview.net/profile?id=~Lars_Lindemann1), [Matteo Sesia](http://openreview.net/profile?id=~Matteo_Sesia1)
  - **Affiliations:** Department of Data Sciences and Operations, University of Southern California, Los Angeles, CA, USA, Department of Computer Science, University of Southern California, Los Angeles, CA, USA, Department of Data Sciences and Operations, University of Southern California, Los Angeles, CA, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA
  - **TL;DR:** This paper introduces a new conformal method for generating reliable forecasting bands for random trajectories, addressing the challenges of data heterogeneity and unpredictability in motion planning applications. The proposed solution offers precise finite-sample guarantees and improves upon previous methods by providing more informative predictions.
  - **Keywords:** conformal prediction, time series forecasting, uncertainty estimation, conformal methods, online conformal prediction, motion planning, autonomous driving, wildfire forecasting, data heterogeneity, heteroscedasticity, unpredictability, simultaneous forecasting bands, finite-sample guarantees


- [Causal-IQA: Towards the Generalization of Image Quality Assessment Based on Causal Inference](https://icml.cc/virtual/2024/poster/33433) (Poster)
  - **Authors:** [Yan Zhong](http://openreview.net/profile?id=~Yan_Zhong2), [Xingyu Wu](http://openreview.net/profile?id=~Xingyu_Wu3), [Li Zhang](http://openreview.net/profile?id=~Li_Zhang25), [Chenxi Yang](http://openreview.net/profile?id=~Chenxi_Yang2), [Tingting Jiang](http://openreview.net/profile?id=~Tingting_Jiang2)
  - **Affiliations:** School of Mathematical Sciences, Peking University, Beijing, China; National Engineering Research Center of Visual Technology, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing, China, Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China, Hefei Institute of Physical Science, Chinese Academy of Sciences, University of Science and Technology of China, Hefei, China, School of Mathematical Sciences, Peking University, Beijing, China; National Engineering Research Center of Visual Technology, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Beijing, China, National Biomedical Imaging Center, Peking University, Beijing, China
  - **TL;DR:** This paper introduces Causal-IQA, a novel end-to-end blind image quality assessment method that leverages causal inference to improve the accuracy of image quality score estimation by addressing confounding effects. The proposed method demonstrates superior performance in generalizing across diverse distortions and complex scenes compared to traditional deep learning-based IQA methods.
  - **Keywords:** Image Quality Assessment (IQA), Causal Inference, Causal graph, Causality-based optimization strategy, Counterfactual Division process, Image denoising, Image restoration, Image generation, Limited generalization, Overfitting, High cost of IQA datasets, Causal-IQA method, Improved estimation accuracy of image quality scores, Blind IQA (BIQA), Full-Reference IQA (FR-IQA), Reduced-Reference IQA (RR-IQA)


- [Language Models Represent Beliefs of Self and Others](https://icml.cc/virtual/2024/poster/33665) (Poster)
  - **Authors:** [Wentao Zhu](http://openreview.net/profile?id=~Wentao_Zhu3), [Zhining Zhang](http://openreview.net/profile?id=~Zhining_Zhang1), [Yizhou Wang](http://openreview.net/profile?id=~Yizhou_Wang1)
  - **Affiliations:** Center on Frontiers of Computing Studies, School of Computer Science, Peking University; Inst. for Artificial Intelligence, Peking University; Nat’l Eng. Research Center of Visual Technology, Peking University; Nat’l Key Lab of General Artificial Intelligence, Peking University, Center on Frontiers of Computing Studies, School of Computer Science, Peking University, Center on Frontiers of Computing Studies, School of Computer Science, Peking University; Inst. for Artificial Intelligence, Peking University; Nat’l Eng. Research Center of Visual Technology, Peking University; Nat’l Key Lab of General Artificial Intelligence, Peking University
  - **TL;DR:** This study investigates the Theory of Mind (ToM) capabilities of Large Language Models (LLMs) by exploring their internal representations of beliefs. The findings reveal that LLMs can linearly decode belief statuses from various perspectives, indicating a significant role of these representations in social reasoning tasks.
  - **Keywords:** Theory of Mind (ToM), social reasoning, mental states, linear decoding, neural activations, artificial intelligence, social interactions, understanding mental states, distinguishing between self and others' beliefs, internal representations of beliefs, changes in ToM performance, Large Language Models (LLMs), causal inference


- [On the Emergence of Cross-Task Linearity in Pretraining-Finetuning Paradigm](https://icml.cc/virtual/2024/poster/32982) (Poster)
  - **Authors:** [Zhanpeng Zhou](http://openreview.net/profile?id=~Zhanpeng_Zhou1), [Zijun Chen](http://openreview.net/profile?id=~Zijun_Chen1), [Yilan Chen](http://openreview.net/profile?id=~Yilan_Chen1), [Bo Zhang](http://openreview.net/profile?id=~Bo_Zhang17), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Computer Science and Engineering, University of California San Diego, Shanghai Artificial Intelligence Laboratory, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This study investigates the phenomenon of Cross-Task Linearity (CTL) in the pretraining-finetuning paradigm, revealing that models initialized from a common checkpoint exhibit linear behavior when interpolating weights. The findings provide insights into the underlying mechanisms of model merging and editing, emphasizing the role of pretraining in neural network functionality.
  - **Keywords:** pretraining-finetuning, deep learning, Cross-Task Linearity (CTL), linear interpolation, neural networks, understanding pretraining and finetuning dynamics, insights into model merging/editing, linear maps in parameter and feature space, Linear Mode Connectivity (LMC), Layerwise Linear Feature Connectivity (LLFC)


- [Online Learning in Betting Markets: Profit versus Prediction](https://icml.cc/virtual/2024/poster/34217) (Poster)
  - **Authors:** [Haiqing Zhu](http://openreview.net/profile?id=~Haiqing_Zhu1), [Alexander Soen](http://openreview.net/profile?id=~Alexander_Soen1), [Yun Kuen Cheung](http://openreview.net/profile?id=~Yun_Kuen_Cheung1), [Lexing Xie](http://openreview.net/profile?id=~Lexing_Xie1)
  - **Affiliations:** School of Computing, The Australian National University, Canberra, Australia, School of Computing, The Australian National University, Canberra, Australia; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan, School of Computing, The Australian National University, Canberra, Australia, School of Computing, The Australian National University, Canberra, Australia
  - **TL;DR:** This study explores the dynamics of binary betting markets, highlighting the conflict between profit maximization and information gathering. It introduces online learning algorithms for price-setting that can exploit bettor biases and belief distributions to enhance bookmaker profits.
  - **Keywords:** binary betting markets, profit maximization, information elicitation, online learning methods, price-setting algorithms, sports gambling, prediction markets, incompatibility between profit maximization and information elicitation, bettor belief distribution, stochastic regret analysis, exploitation of bettor biases and belief distributions


- [Pedestrian Attribute Recognition as Label-balanced Multi-label Learning](https://icml.cc/virtual/2024/poster/34695) (Poster)
  - **Authors:** [Yibo Zhou](http://openreview.net/profile?id=~Yibo_Zhou1), [Hai-Miao Hu](http://openreview.net/profile?id=~Hai-Miao_Hu1), [Yirong Xiang](http://openreview.net/profile?id=~Yirong_Xiang1), [Xiaokang Zhang](http://openreview.net/profile?id=~Xiaokang_Zhang2), [Haotian Wu](http://openreview.net/profile?id=~Haotian_Wu5)
  - **Affiliations:** Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing 100191, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing 100191, China, Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing 100191, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing 100191, China; Hangzhou Innovation Institute of Beihang University, Hangzhou 310051, China, The University of Manchester, UK, Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing 100191, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing 100191, China, Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing 100191, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing 100191, China
  - **TL;DR:** This study addresses the challenges of label and semantics imbalance in pedestrian attribute recognition by proposing a novel framework for label-balanced multi-label learning. The approach effectively improves model accuracy on various benchmarks while maintaining a minimal computational budget.
  - **Keywords:** Pedestrian Attribute Recognition, Multi-label Learning, Bayesian Feature Augmentation, Label-balanced Data Re-sampling, Label Imbalance, Semantics Imbalance, Data Selection Bias, Improved Accuracy, Novel Framework, PETA, RAP


- [Dynamic Evaluation of Large Language Models by Meta Probing Agents](https://icml.cc/virtual/2024/poster/34607) (Poster)
  - **Authors:** [Kaijie Zhu](http://openreview.net/profile?id=~Kaijie_Zhu1), [Jindong Wang](http://openreview.net/profile?id=~Jindong_Wang1), [Qinlin Zhao](http://openreview.net/profile?id=~Qinlin_Zhao1), [Ruochen Xu](http://openreview.net/profile?id=~Ruochen_Xu2), [Xing Xie](http://openreview.net/profile?id=~Xing_Xie3)
  - **Affiliations:** Microsoft Research, Microsoft Research, University of Science and Technology of China, Microsoft Research, Microsoft Research
  - **TL;DR:** This paper introduces meta probing agents (MPA) as a dynamic evaluation protocol for large language models (LLMs), addressing issues of data contamination and interpretability. The findings reveal that most LLMs perform poorly in evaluations, highlighting the need for improvement and demonstrating a correlation between model size and cognitive abilities.
  - **Keywords:** Evaluation of large language models, psychometrics, Meta probing agents (MPA), dynamic evaluation protocol, Natural language processing, model evaluation, Data contamination, model interpretability, overfitting benchmarks, Multifaceted analysis of LLMs, correlation of cognitive abilities, data augmentation approach, Large language models (LLMs), language understanding, problem solving, domain knowledge, Matthew effect


- [Translation Equivariant Transformer Neural Processes](https://icml.cc/virtual/2024/poster/33034) (Poster)
  - **Authors:** [Matthew Ashman](http://openreview.net/profile?id=~Matthew_Ashman1), [Cristiana Diaconu](http://openreview.net/profile?id=~Cristiana_Diaconu1), [Junhyuck Kim](http://openreview.net/profile?id=~Junhyuck_Kim1), [Lakee Sivaraya](http://openreview.net/profile?id=~Lakee_Sivaraya1), [Stratis Markou](http://openreview.net/profile?id=~Stratis_Markou1), [James Requeima](http://openreview.net/profile?id=~James_Requeima1), [Wessel Bruinsma](http://openreview.net/profile?id=~Wessel_P_Bruinsma1), [Richard E Turner](http://openreview.net/profile?id=~Richard_E_Turner1)
  - **Affiliations:** Department of Engineering, University of Cambridge, Cambridge, UK, Department of Engineering, University of Cambridge, Cambridge, UK, Department of Engineering, University of Cambridge, Cambridge, UK, Department of Engineering, University of Cambridge, Cambridge, UK, Department of Engineering, University of Cambridge, Cambridge, UK, Vector Institute, University of Toronto, Toronto, Canada, Microsoft Research AI for Science, Cambridge, UK, Department of Engineering, University of Cambridge, Cambridge, UK; Microsoft Research AI for Science, Cambridge, UK
  - **TL;DR:** This paper introduces a new family of translation equivariant transformer neural processes (TE-TNPs) that enhance the modeling of posterior prediction maps for stationary data. Through extensive experiments, TE-TNPs demonstrate improved effectiveness compared to non-translation-equivariant models and other neural process baselines.
  - **Keywords:** Neural Processes, Translation Equivariance, Spatio-Temporal Modelling, Transformer Neural Processes (TNPs), Permutation Invariant Set Functions, Spatio-Temporal Data, Healthcare, Few-Shot Learning, Stationary Data, Posterior Prediction Maps, Translation Equivariant TNPs (TE-TNPs), Improved Posterior Predictive Distributions


- [Antibody Design Using a Score-based Diffusion Model Guided by Evolutionary, Physical and Geometric Constraints](https://icml.cc/virtual/2024/poster/35143) (Poster)
  - **Authors:** [Tian Zhu](http://openreview.net/profile?id=~Tian_Zhu1), [Milong Ren](http://openreview.net/profile?id=~Milong_Ren2), [Haicang Zhang](http://openreview.net/profile?id=~Haicang_Zhang1)
  - **Affiliations:** Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This study presents AbX, a novel score-based diffusion model for antibody design that incorporates evolutionary, physical, and geometric constraints to optimize antibody sequences and structures. The model demonstrates superior performance in generating antibodies with enhanced binding affinity compared to existing methods, addressing the challenge of limited antibody-antigen complex data.
  - **Keywords:** Antibody design, Generative models, Score-based diffusion model, Protein language model, Immunology, Drug design, Data scarcity, Binding affinity optimization, Enhanced antibody-antigen binding affinity, Improved sequence and structure generation, SAbDab database, Complementarity-determining regions (CDRs), SE(3) structure space, Evolutionary, physical, and geometric constraints


- [Stealthy Imitation: Reward-guided Environment-free Policy Stealing](https://icml.cc/virtual/2024/poster/34479) (Poster)
  - **Authors:** [Zhixiong Zhuang](http://openreview.net/profile?id=~Zhixiong_Zhuang1), [Maria-Irina Nicolae](http://openreview.net/profile?id=~Maria-Irina_Nicolae1), [Mario Fritz](http://openreview.net/profile?id=~Mario_Fritz1)
  - **Affiliations:** Graduate School of Computer Science, Saarland University, Saarbrücken, Germany; Bosch Center for Artificial Intelligence, Robert Bosch GmbH, Renningen, Germany; CISPA Helmholtz Center for Information Security, Saarbrücken, Germany, Bosch Center for Artificial Intelligence, Robert Bosch GmbH, Renningen, Germany, CISPA Helmholtz Center for Information Security, Saarbrücken, Germany
  - **TL;DR:** This paper introduces Stealthy Imitation, a novel attack method for stealing deep reinforcement learning policies without access to the environment or input range. The approach demonstrates improved effectiveness over previous methods and proposes a countermeasure to mitigate the attack's impact.
  - **Keywords:** deep reinforcement learning, model stealing attacks, control systems, reward model, data-free model stealing, industrial automation, robotics, vulnerability to model stealing, unauthorized model usage, exposure of sensitive information, Stealthy Imitation attack, countermeasure for model stealing


- [Toward Availability Attacks in 3D Point Clouds](https://icml.cc/virtual/2024/poster/34703) (Poster)
  - **Authors:** [Yifan Zhu](http://openreview.net/profile?id=~Yifan_Zhu6), [Yibo Miao](http://openreview.net/profile?id=~Yibo_Miao1), [Yinpeng Dong](http://openreview.net/profile?id=~Yinpeng_Dong2), [Xiao-Shan Gao](http://openreview.net/profile?id=~Xiao-Shan_Gao2)
  - **Affiliations:** Academy of Mathematics and Systems Science, Chinese Academy of Sciences; University of Chinese Academy of Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences; University of Chinese Academy of Sciences, Tsinghua University; RealAI, Academy of Mathematics and Systems Science, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Kaiyuan International Mathematical Sciences Institute
  - **TL;DR:** This paper addresses the lack of systematic exploration of data privacy and security in 3D deep learning by proposing a novel availability attack method called Feature Collision Error-Minimization (FC-EM) to overcome challenges posed by the unordered nature of point clouds. The effectiveness of the FC-EM method is theoretically analyzed and validated through extensive experiments on various 3D datasets.
  - **Keywords:** 3D vision, data privacy, security in 3D deep learning, availability attacks, Feature Collision Error-Minimization (FC-EM), bi-level optimization, Autonomous driving, medical image processing, scene reconstruction, face recognition, Data privacy and security issues, challenges in designing availability attacks for 3D point clouds, degeneracy in bi-level optimization, Novel methods for availability attacks, theoretical analysis of FC-EM effectiveness, 3D intracranial aneurysm medical dataset, 3D face dataset, typical point cloud datasets, Point clouds, unstructured sets, adversarial attacks


- [Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption](https://icml.cc/virtual/2024/poster/34811) (Poster)
  - **Authors:** [Itamar Zimerman](http://openreview.net/profile?id=~Itamar_Zimerman1), [Moran Baruch](http://openreview.net/profile?id=~Moran_Baruch1), [Nir Drucker](http://openreview.net/profile?id=~Nir_Drucker1), [Gilad Ezov](http://openreview.net/profile?id=~Gilad_Ezov1), [Omri Soceanu](http://openreview.net/profile?id=~Omri_Soceanu1), [Lior Wolf](http://openreview.net/profile?id=~Lior_Wolf1)
  - **Affiliations:** IBM Research, Israel; Tel-Aviv University, IBM Research, Israel; Bar-Ilan University, IBM Research, Israel, IBM Research, Israel, IBM Research, Israel, Tel-Aviv University
  - **TL;DR:** This study introduces the first polynomial transformer, enabling secure inference over homomorphic encryption for deep learning models. The proposed methods demonstrate comparable performance to traditional models while maintaining data privacy throughout the inference process.
  - **Keywords:** Privacy-Preserving Machine Learning, Homomorphic Encryption, Polynomial Transformer, Secure Inference, Deep Learning, Natural Language Processing, Vision Transformers, Challenges in converting models to polynomial form, privacy concerns in data processing, First polynomial transformer, secure inference with full transformers, methods for converting operators to polynomial equivalents, Homomorphic Encryption (HE), CKKS, GELU, Softmax


- [Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models](https://icml.cc/virtual/2024/poster/33636) (Poster)
  - **Authors:** [Yongshuo Zong](http://openreview.net/profile?id=~Yongshuo_Zong1), [Ondrej Bohdal](http://openreview.net/profile?id=~Ondrej_Bohdal1), [Tingyang Yu](http://openreview.net/profile?id=~Tingyang_Yu1), [Yongxin Yang](http://openreview.net/profile?id=~Yongxin_Yang1), [Timothy Hospedales](http://openreview.net/profile?id=~Timothy_Hospedales1)
  - **Affiliations:** University of Edinburgh, University of Edinburgh, EPFL, University of Edinburgh, University of Edinburgh
  - **TL;DR:** This study addresses the safety vulnerabilities of Vision Large Language Models (VLLMs) by introducing the VLGuard dataset for effective safety fine-tuning. The findings demonstrate that fine-tuned VLLMs significantly reduce harmful outputs and resist adversarial attacks, enhancing their overall helpfulness.
  - **Keywords:** Vision Large Language Models (VLLMs), AI Safety, AI Alignment, Fine-tuning, Safety instruction-following, Generation of harmful content, Vulnerability to adversarial attacks, VLGuard dataset, Safety alignment of VLLMs, VLGuard, Jailbreaking, Red-teaming


- [Viewing Transformers Through the Lens of Long Convolutions Layers](https://icml.cc/virtual/2024/poster/33124) (Poster)
  - **Authors:** [Itamar Zimerman](http://openreview.net/profile?id=~Itamar_Zimerman1), [Lior Wolf](http://openreview.net/profile?id=~Lior_Wolf1)
  - **Affiliations:** The Blavatnik School of Computer Science, Tel Aviv University, The Blavatnik School of Computer Science, Tel Aviv University
  - **TL;DR:** This study investigates the limitations of transformer architectures in capturing long-range dependencies and introduces a novel attention mechanism, Local and Smooth Attention (LaS-Attention), which significantly enhances performance on long-range tasks with minimal computational overhead. The findings highlight the importance of smoothness and locality in improving the effectiveness of transformers for long-range applications.
  - **Keywords:** long-range dependencies, transformer architectures, deep learning, Local and Smooth Attention (LaS-Attention), state-space layers, linear RNN layers, global convolution layers, natural language processing (NLP), time-series analysis, long-form data modalities, sub-optimal performance on long-range tasks, effectiveness and efficiency in capturing long-range dependencies, improved transformer performance on long-range tasks, insights into long-range sequence modeling, Long Range Arena (LRA) benchmark


- [Reinformer: Max-Return Sequence Modeling for Offline RL](https://icml.cc/virtual/2024/poster/33185) (Poster)
  - **Authors:** [Zifeng Zhuang](http://openreview.net/profile?id=~Zifeng_Zhuang1), [Dengyun Peng](http://openreview.net/profile?id=~Dengyun_Peng1), [Jinxin Liu](http://openreview.net/profile?id=~Jinxin_Liu1), [Ziqi Zhang](http://openreview.net/profile?id=~Ziqi_Zhang7), [Donglin Wang](http://openreview.net/profile?id=~Donglin_Wang1)
  - **Affiliations:** Zhejiang University; School of Engineering, Westlake University; Harbin Institute of Technology, Zhejiang University; School of Engineering, Westlake University; Harbin Institute of Technology, Zhejiang University; School of Engineering, Westlake University; Harbin Institute of Technology, Zhejiang University; School of Engineering, Westlake University; Harbin Institute of Technology, Zhejiang University; School of Engineering, Westlake University; Harbin Institute of Technology
  - **TL;DR:** This study introduces the concept of max-return sequence modeling to enhance offline reinforcement learning by integrating the objective of maximizing returns into sequence models. The proposed Reinforced Transformer (Reinformer) demonstrates improved trajectory stitching capabilities and competitive performance against classical RL methods on the D4RL benchmark.
  - **Keywords:** offline reinforcement learning, sequence modeling, max-return sequence modeling, expectile regression, trajectory stitching, maximizing returns, Reinforced Transformer (Reinformer), improved trajectory stitching ability, D4RL benchmark


- [Compositional Few-Shot Class-Incremental Learning](https://icml.cc/virtual/2024/poster/32882) (Poster)
  - **Authors:** [Yixiong Zou](http://openreview.net/profile?id=~Yixiong_Zou1), [Shanghang Zhang](http://openreview.net/profile?id=~Shanghang_Zhang4), [haichen zhou](http://openreview.net/profile?id=~haichen_zhou1), [Yuhua Li](http://openreview.net/profile?id=~Yuhua_Li2), [Ruixuan Li](http://openreview.net/profile?id=~Ruixuan_Li1)
  - **Affiliations:** School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China, School of Computer Science, Peking University, Beijing, China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science, Peking University, Beijing, China
  - **TL;DR:** This study proposes a cognitive-inspired method for Few-Shot Class-Incremental Learning (FSCIL) that utilizes compositional learning to enhance the model's ability to recognize novel classes with limited data. The method demonstrates improved interpretability and reusability of learned primitives, outperforming existing state-of-the-art approaches.
  - **Keywords:** Few-shot class-incremental learning (FSCIL), Compositional learning, Centered Kernel Alignment (CKA), Primitive composition module, Primitive reuse module, Data scarcity, Catastrophic forgetting, Improved interpretability, Enhanced primitive reusability, Three datasets (specific names not provided), Visual primitives, Incremental learning, Cognitive-inspired method


- [CRoFT: Robust Fine-Tuning with Concurrent Optimization for OOD Generalization and Open-Set OOD Detection](https://icml.cc/virtual/2024/poster/32722) (Poster)
  - **Authors:** [Lin Zhu](http://openreview.net/profile?id=~Lin_Zhu10), [Yifeng Yang](http://openreview.net/profile?id=~Yifeng_Yang1), [Qinying Gu](http://openreview.net/profile?id=~Qinying_Gu1), [Xinbing Wang](http://openreview.net/profile?id=~Xinbing_Wang1), [Chenghu Zhou](http://openreview.net/profile?id=~Chenghu_Zhou3), [Nanyang Ye](http://openreview.net/profile?id=~Nanyang_Ye1)
  - **Affiliations:** Shanghai Jiao Tong University, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China, Shanghai Jiao Tong University, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China, Shanghai Artificial Intelligence Laboratory, Shanghai, China, Shanghai Jiao Tong University, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China, Shanghai Jiao Tong University, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China, Shanghai Jiao Tong University, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China
  - **TL;DR:** This paper proposes a novel objective function for OOD detection that enhances the generalization ability of vision-language pre-trained models (VL-PTMs) during fine-tuning. The authors develop a unified framework that concurrently optimizes OOD generalization and open-set detection, demonstrating its effectiveness through extensive experiments.
  - **Keywords:** Out-of-Distribution (OOD) generalization, Open-Set OOD detection, Vision-Language Pre-trained Models (VL-PTMs), Fine-tuning, Objective function optimization, Covariate shifts, Semantic shifts, Distribution shifts, Unified fine-tuning framework, Domain-consistent Hessians


- [Towards Efficient Spiking Transformer: a Token Sparsification Framework for Training and Inference Acceleration](https://icml.cc/virtual/2024/poster/32674) (Poster)
  - **Authors:** [Zhengyang Zhuge](http://openreview.net/profile?id=~Zhengyang_Zhuge1), [Peisong Wang](http://openreview.net/profile?id=~Peisong_Wang1), [Xingting Yao](http://openreview.net/profile?id=~Xingting_Yao1), [Jian Cheng](http://openreview.net/profile?id=~Jian_Cheng7)
  - **Affiliations:** Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; AiRiA; School of Future Technology, University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; AiRiA
  - **TL;DR:** This paper presents a token sparsification framework called STATA to enhance the training efficiency of Spiking Transformers, achieving up to 1.53× training speedup and 48% energy reduction while maintaining comparable performance. The study addresses the challenges of time-consuming training and high energy consumption associated with Spiking Transformers.
  - **Keywords:** Spiking Transformers, Spiking Neural Networks (SNNs), energy efficiency, Token sparsification, Timestep-wise Anchor Token, dual Alignments, Edge computing, Time-consuming training, performance degradation, energy consumption, STATA framework, training speedup, energy reduction, ImageNet, Artificial Neural Networks (ANNs), Transformer


- [BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization](https://icml.cc/virtual/2024/poster/34619) (Poster)
  - **Authors:** [Lancheng Zou](http://openreview.net/profile?id=~Lancheng_Zou1), [Wenqian Zhao](http://openreview.net/profile?id=~Wenqian_Zhao2), [Shuo Yin](http://openreview.net/profile?id=~Shuo_Yin2), [Chen Bai](http://openreview.net/profile?id=~Chen_Bai1), [Qi Sun](http://openreview.net/profile?id=~Qi_Sun4), [Bei Yu](http://openreview.net/profile?id=~Bei_Yu2)
  - **Affiliations:** The Chinese University of Hong Kong, China, The Chinese University of Hong Kong, China, The Chinese University of Hong Kong, China, The Chinese University of Hong Kong, China, Zhejiang University, China, The Chinese University of Hong Kong, China
  - **TL;DR:** This paper introduces Bi-Exponent Block Floating Point (BiE) as a novel quantization method for Large Language Models (LLMs) to address challenges related to data distribution outliers and hardware efficiency. The proposed method demonstrates superior accuracy and hardware friendliness across various models and benchmarks.
  - **Keywords:** Large Language Models, Quantization, Bi-Exponent Block Floating Point (BiE), low-bit quantization, Natural Language Processing (NLP), Multi-modal tasks, Data distribution outliers, hardware efficiency, computational and memory requirements, Novel numerical representation, improved quantization flow, accuracy superiority


- [Improving Equivariant Graph Neural Networks on Large Geometric Graphs via Virtual Nodes Learning](https://icml.cc/virtual/2024/poster/32746) (Poster)
  - **Authors:** [Yuelin Zhang](http://openreview.net/profile?id=~Yuelin_Zhang2), [Jiacheng Cen](http://openreview.net/profile?id=~Jiacheng_Cen1), [Jiaqi Han](http://openreview.net/profile?id=~Jiaqi_Han2), [Zhiqiang Zhang](http://openreview.net/profile?id=~Zhiqiang_Zhang4), [JUN ZHOU](http://openreview.net/profile?id=~JUN_ZHOU6), [Wenbing Huang](http://openreview.net/profile?id=~Wenbing_Huang1)
  - **Affiliations:** Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Stanford University, Ant Group, Ant Group, Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods
  - **TL;DR:** This study introduces FastEGNN, an enhanced equivariant Graph Neural Network model designed to improve efficiency and accuracy on large geometric graphs by utilizing virtual nodes. Experiments demonstrate that FastEGNN outperforms existing models, achieving a balance between accuracy and efficiency even in sparse graph scenarios.
  - **Keywords:** Equivariant Graph Neural Networks, Large Geometric Graphs, FastEGNN, Maximum Mean Discrepancy (MMD), N-body systems, Protein generation, Fluid dynamics simulation, Efficiency issues in large geometric graphs, Quadratic complexity in message exchange, Enhanced model for equivariant GNNs, Balance between accuracy and efficiency, E(3) symmetries, Geometric graphs, Virtual nodes learning


- [Visual Representation Learning with Stochastic Frame Prediction](https://icml.cc/virtual/2024/poster/32962) (Poster)
  - **Authors:** [Huiwon Jang](http://openreview.net/profile?id=~Huiwon_Jang1), [Dongyoung Kim](http://openreview.net/profile?id=~Dongyoung_Kim3), [Junsu Kim](http://openreview.net/profile?id=~Junsu_Kim1), [Jinwoo Shin](http://openreview.net/profile?id=~Jinwoo_Shin1), [Pieter Abbeel](http://openreview.net/profile?id=~Pieter_Abbeel2), [Younggyo Seo](http://openreview.net/profile?id=~Younggyo_Seo1)
  - **Affiliations:** KAIST, KAIST, KAIST, KAIST, UC Berkeley, KAIST; Dyson Robot Learning Lab
  - **TL;DR:** This paper presents a framework for visual representation learning through stochastic frame prediction, addressing the challenges of under-determined future frame prediction. The proposed method effectively combines temporal information learning and masked image modeling, demonstrating its utility across various video-related tasks.
  - **Keywords:** self-supervised learning, video representation learning, stochastic frame prediction, masked image modeling, video segmentation, pose tracking, vision-based robotic locomotion, manipulation tasks, under-determined nature of frame prediction, multi-modal distribution of future frames, framework for visual representation learning, effective combination of objectives


- [Amend to Alignment: Decoupled Prompt Tuning for Mitigating Spurious Correlation in Vision-Language Models](https://icml.cc/virtual/2024/poster/33470) (Poster)
  - **Authors:** [Jie ZHANG](http://openreview.net/profile?id=~Jie_ZHANG18), [Xiaosong Ma](http://openreview.net/profile?id=~Xiaosong_Ma4), [Song Guo](http://openreview.net/profile?id=~Song_Guo5), [Peng Li](http://openreview.net/profile?id=~Peng_Li20), [Wenchao Xu](http://openreview.net/profile?id=~Wenchao_Xu1), [Xueyang Tang](http://openreview.net/profile?id=~Xueyang_Tang1), [Zicong Hong](http://openreview.net/profile?id=~Zicong_Hong1)
  - **Affiliations:** Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China, Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China, School of Cyber Science and Engineering, Xi’an Jiaotong University, Shaanxi, China, Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China, Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China, Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China
  - **TL;DR:** This paper presents CoOPood, a novel fine-grained prompt tuning method for vision-language models that effectively mitigates spurious correlations and enhances out-of-distribution generalization by aligning text with invariant features. Extensive experiments demonstrate its superiority over existing methods in various tasks.
  - **Keywords:** Vision-Language Models, Prompt Tuning, Out-of-Distribution Generalization, CoOPood, Contrastive Loss, Fine-Grained Prompt Tuning, Image Classification, Object Detection, Vision-Language Answering, Spurious Correlation, Poor OOD Generalization, Feature Alignment, Improved Cross-Modal Alignment, Enhanced Generalization Performance, CLIP, Invariant Features, Spurious Features


- [On the Embedding Collapse when Scaling up Recommendation Models](https://icml.cc/virtual/2024/poster/33689) (Poster)
  - **Authors:** [Xingzhuo Guo](http://openreview.net/profile?id=~Xingzhuo_Guo1), [Junwei Pan](http://openreview.net/profile?id=~Junwei_Pan1), [Ximei Wang](http://openreview.net/profile?id=~Ximei_Wang1), [Baixu Chen](http://openreview.net/profile?id=~Baixu_Chen2), [Jie Jiang](http://openreview.net/profile?id=~Jie_Jiang3), [Mingsheng Long](http://openreview.net/profile?id=~Mingsheng_Long5)
  - **Affiliations:** School of Software, BNRist, Tsinghua University, China; Tencent Inc, China, Tencent Inc, China, Tencent Inc, China, School of Software, BNRist, Tsinghua University, China, Tencent Inc, China, School of Software, BNRist, Tsinghua University, China
  - **TL;DR:** This study investigates the embedding collapse phenomenon in recommendation models, which inhibits scalability and performance. The authors propose a multi-embedding design that incorporates interaction modules to enhance embedding diversity and mitigate collapse, demonstrating consistent scalability across various models.
  - **Keywords:** recommendation models, embedding collapse, model scalability, empirical analysis, theoretical analysis, singular value decomposition, recommender systems, e-commerce, social media, news feeds, music streaming, embedding collapse, model overfitting, ineffective parameter utilization, multi-embedding design, embedding-set-specific interaction modules


- [Position: Is machine learning good or bad for the natural sciences?](https://icml.cc/virtual/2024/poster/32948) (Poster)
  - **Authors:** [David W. Hogg](http://openreview.net/profile?id=~David_W_Hogg1), [Soledad Villar](http://openreview.net/profile?id=~Soledad_Villar2)
  - **Affiliations:** Center for Cosmology and Particle Physics, Department of Physics, New York University, USA; Max-Planck-Institut für Astronomie, Heidelberg, Germany; Flatiron Institute, New York, USA, Flatiron Institute, New York, USA; Department of Applied Mathematics and Statistics, Johns Hopkins University, USA; Mathematical Institute for Data Science, Johns Hopkins University, USA
  - **TL;DR:** The paper discusses the impact of machine learning on the natural sciences, highlighting both its potential benefits in causal inference and the risks of introducing statistical biases. It calls for the scientific community to critically evaluate the role of machine learning in their research practices.
  - **Keywords:** Machine Learning, Natural Sciences, Causal Inference, Expressive Machine Learning Models, Physics, Astrophysics, Statistical Biases, Confirmation Biases, Trustworthy Results, Understanding of Latent Structures, Ontology, Epistemology


- [Differentially Private Sum-Product Networks](https://icml.cc/virtual/2024/poster/32625) (Poster)
  - **Authors:** [Xenia Heilmann](http://openreview.net/profile?id=~Xenia_Heilmann1), [Mattia Cerrato](http://openreview.net/profile?id=~Mattia_Cerrato1), [Ernst Althaus](http://openreview.net/profile?id=~Ernst_Althaus1)
  - **Affiliations:** Institute of Computer Science, Johannes Gutenberg University Mainz, Saarstraße 21, Mainz 55122, Rhineland-Palatinate, Germany, Institute of Computer Science, Johannes Gutenberg University Mainz, Saarstraße 21, Mainz 55122, Rhineland-Palatinate, Germany, Institute of Computer Science, Johannes Gutenberg University Mainz, Saarstraße 21, Mainz 55122, Rhineland-Palatinate, Germany
  - **TL;DR:** This paper presents a novel method called Differentiably Private Sum-Product Networks (DPSPNs) that enables both privacy-preserving classification and synthetic data generation in a single model. The approach outperforms existing methods in terms of stability and utility of the generated data.
  - **Keywords:** Differential Privacy, Machine Learning, Sum-Product Networks, Privacy-Preserving Data Generation, Classification, Privacy Budget Cost, Utility-Privacy Tradeoff, Differentiably Private Sum-Product Networks (DPSPNs), Stability in Training


- [REST: Efficient and Accelerated EEG Seizure Analysis through Residual State Updates](https://icml.cc/virtual/2024/poster/34812) (Poster)
  - **Authors:** [Arshia Afzal](http://openreview.net/profile?id=~Arshia_Afzal1), [Grigorios Chrysos](http://openreview.net/profile?id=~Grigorios_Chrysos1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1), [Mahsa Shoaran](http://openreview.net/profile?id=~Mahsa_Shoaran1)
  - **Affiliations:** INL, EPFL, Switzerland; LIONS, EPFL, Switzerland, Department of Electrical and Computer Engineering, University of Wisconsin-Madison, USA, LIONS, EPFL, Switzerland, INL, EPFL, Switzerland
  - **TL;DR:** This study presents REST, a novel graph-based residual state update mechanism for real-time EEG seizure detection, achieving a 9-fold increase in inference speed and reduced memory usage compared to existing models. The findings suggest REST's potential for effective implementation in clinical devices for seizure management.
  - **Keywords:** EEG seizure detection, real-time analysis, Graph neural networks, recurrent structures, residual state update mechanism, Clinical devices, responsive neurostimulation, seizure alert systems, Inference speed, memory efficiency, real-time implementation challenges, 9-fold acceleration in inference speed, high accuracy in seizure detection and classification


- [AquaLoRA: Toward White-box Protection for Customized Stable Diffusion Models via Watermark LoRA](https://icml.cc/virtual/2024/poster/34825) (Poster)
  - **Authors:** [Weitao Feng](http://openreview.net/profile?id=~George_fe1), [Wenbo Zhou](http://openreview.net/profile?id=~Wenbo_Zhou1), [Jiyan He](http://openreview.net/profile?id=~Jiyan_He1), [Jie Zhang](http://openreview.net/profile?id=~Jie_Zhang11), [Tianyi Wei](http://openreview.net/profile?id=~Tianyi_Wei1), [Guanlin Li](http://openreview.net/profile?id=~Guanlin_Li2), [Tianwei Zhang](http://openreview.net/profile?id=~Tianwei_Zhang1), [Weiming Zhang](http://openreview.net/profile?id=~Weiming_Zhang2), [Nenghai Yu](http://openreview.net/profile?id=~Nenghai_Yu1)
  - **Affiliations:** University of Science and Technology of China, Hefei, China, University of Science and Technology of China, Hefei, China, University of Science and Technology of China, Hefei, China, Nanyang Technological University, Singapore, University of Science and Technology of China, Hefei, China, Nanyang Technological University, Singapore, Nanyang Technological University, Singapore, University of Science and Technology of China, Hefei, China, University of Science and Technology of China, Hefei, China
  - **TL;DR:** The study introduces AquaLoRA, a novel watermarking method integrated into the U-Net of Stable Diffusion models to provide robust white-box protection against unauthorized use and distribution. The proposed method ensures high fidelity in generated images while maintaining watermark integrity, addressing significant copyright concerns in the generative AI landscape.
  - **Keywords:** Watermarking, Copyright Protection, Generative AI, Low-Rank Adaptation (LoRA), U-Net, Prior Preserving Fine-Tuning (PPFT), Image Generation, Customized Models, Unauthorized Model Distribution, Unconsented Commercial Use, White-box Protection, Watermark Integration, Robust Watermarking Techniques, Stable Diffusion, Text-to-Image Synthesis


- [COPAL: Continual Pruning in Large Language Generative Models](https://icml.cc/virtual/2024/poster/34266) (Poster)
  - **Authors:** [Srikanth Malla](http://openreview.net/profile?id=~Srikanth_Malla1), [Joon Hee Choi](http://openreview.net/profile?id=~Joon_Hee_Choi2), [Chiho Choi](http://openreview.net/profile?id=~Chiho_Choi2)
  - **Affiliations:** Samsung Semiconductor, San Jose, USA, Samsung Semiconductor, San Jose, USA, Samsung Semiconductor, San Jose, USA
  - **TL;DR:** This paper introduces COPAL, an algorithm for continual pruning of large language generative models, addressing the challenges of high computational demands and limited adaptability. The empirical evaluation demonstrates that COPAL enhances resource efficiency and outperforms baseline models in adapting to new domains.
  - **Keywords:** Large Language Models, Continual Adaptation, Pruning, Sensitivity Analysis, Natural Language Processing, High Computational Demands, Catastrophic Forgetting, COPAL Algorithm, Resource Efficiency, LLaMA-7B


- [A3S: A General Active Clustering Method with Pairwise Constraints](https://icml.cc/virtual/2024/poster/33610) (Poster)
  - **Authors:** [Xun Deng](http://openreview.net/profile?id=~Xun_Deng1), [Junlong Liu](http://openreview.net/profile?id=~Junlong_Liu2), [Han Zhong](http://openreview.net/profile?id=~Han_Zhong1), [Fuli Feng](http://openreview.net/profile?id=~Fuli_Feng1), [Chen Shen](http://openreview.net/profile?id=~Chen_Shen7), [Xiangnan He](http://openreview.net/profile?id=~Xiangnan_He1), [Jieping Ye](http://openreview.net/profile?id=~Jieping_Ye4), [Zheng Wang](http://openreview.net/profile?id=~Zheng_Wang32)
  - **Affiliations:** University of Science and Technology of China, Alibaba Group, Peking University, University of Science and Technology of China, Alibaba Group, University of Science and Technology of China, Alibaba Group, Alibaba Group
  - **TL;DR:** This study introduces the A3S framework for active clustering, which enhances clustering performance by strategically integrating human-annotated pairwise constraints. The framework significantly reduces the number of human queries needed while improving clustering quality, particularly in large datasets with many classes.
  - **Keywords:** Active clustering, Semi-supervised clustering, Adaptive Active Aggregation and Splitting (A3S), Cluster-adjustment scheme, High query costs, Large datasets, Class imbalance, Improved clustering quality, Reduced human queries, Normalized mutual information (NMI)


- [Understanding Inter-Concept Relationships in Concept-Based Models](https://icml.cc/virtual/2024/poster/34396) (Poster)
  - **Authors:** [Naveen Raman](http://openreview.net/profile?id=~Naveen_Janaki_Raman1), [Mateo Espinosa Zarlenga](http://openreview.net/profile?id=~Mateo_Espinosa_Zarlenga1), [Mateja Jamnik](http://openreview.net/profile?id=~Mateja_Jamnik1)
  - **Affiliations:** Carnegie Mellon University, University of Cambridge, University of Cambridge
  - **TL;DR:** This study investigates the ability of concept-based explainability methods to capture inter-concept relationships in deep learning systems and finds that existing models often fail to do so. A novel algorithm is proposed to leverage these relationships, improving the accuracy of concept interventions and enhancing downstream task performance.
  - **Keywords:** Explainability, Concept-based models, Inter-concept relationships, Human-AI teaming, Model debugging, Lack of stability and robustness in concept representations, Noisy concept labels, Inherent instability of explainability methods, Novel algorithm for concept intervention accuracy


- [Position: Quo Vadis, Unsupervised Time Series Anomaly Detection?](https://icml.cc/virtual/2024/poster/33889) (Poster)
  - **Authors:** [M. Saquib Sarfraz](http://openreview.net/profile?id=~M._Saquib_Sarfraz1), [Mei-Yen Chen](http://openreview.net/profile?id=~Mei-Yen_Chen1), [Lukas Layer](http://openreview.net/profile?id=~Lukas_Layer1), [Kunyu Peng](http://openreview.net/profile?id=~Kunyu_Peng1), [Marios Koulakis](http://openreview.net/profile?id=~Marios_Koulakis1)
  - **Affiliations:** Mercedes-Benz Tech Innovation, Ulm, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany, Mercedes-Benz Tech Innovation, Ulm, Germany, Mercedes-Benz Tech Innovation, Ulm, Germany, Karlsruhe Institute of Technology, Karlsruhe, Germany, Karlsruhe Institute of Technology, Karlsruhe, Germany
  - **TL;DR:** The paper critically analyzes the current state of Time Series Anomaly Detection (TAD), highlighting issues with evaluation metrics and benchmarking practices, and advocates for a shift towards simpler, more interpretable methods. It concludes that the complexity of state-of-the-art deep learning models offers minimal improvement over simpler approaches.
  - **Keywords:** Time Series Anomaly Detection (TAD), Machine Learning, Deep Learning, LSTM, Transformer, Graph Neural Networks, Vehicles, Manufacturing Plants, Robotics, Patient Monitoring Systems, Flawed Evaluation Metrics, Inconsistent Benchmarking Practices, Model Complexity, Need for Rigorous Evaluation Protocols, Creation of Simple Baselines, Exploration of Interpretable TAD Methods


- [Sample Complexity Bounds for Estimating Probability Divergences under Invariances](https://icml.cc/virtual/2024/poster/32915) (Poster)
  - **Authors:** [Behrooz Tahmasebi](http://openreview.net/profile?id=~Behrooz_Tahmasebi1), [Stefanie Jegelka](http://openreview.net/profile?id=~Stefanie_Jegelka3)
  - **Affiliations:** MIT CSAIL, TU Munich; MIT CSAIL
  - **TL;DR:** This study investigates how group invariances can enhance sample complexity in estimating probability divergences, specifically the 1-Wasserstein distance and related metrics. The findings reveal a significant reduction in sample complexity and improved convergence rates for distributions invariant under group actions.
  - **Keywords:** Probability divergences, Sample complexity, Group invariance, 1-Wasserstein distance, Sobolev Integral Probability Metrics (Sobolev IPMs), Maximum Mean Discrepancy (MMD), Generative models, Domain adaptation, Geometric data processing, Biomedical research, Estimation of probability measures, Curse of dimensionality, Slow convergence rate, Improved sample complexity bounds, Convergence rate enhancement, Lie group, Compact smooth manifold, Invariant distributions


- [Scaling Beyond the GPU Memory Limit for Large Mixture-of-Experts Model Training](https://icml.cc/virtual/2024/poster/32831) (Poster)
  - **Authors:** [Yechan Kim](http://openreview.net/profile?id=~Yechan_Kim1), [Hwijoon Lim](http://openreview.net/profile?id=~Hwijoon_Lim1), [Dongsu Han](http://openreview.net/profile?id=~Dongsu_Han1)
  - **Affiliations:** Kim Jaechul Graduate School of AI, KAIST, Daejeon, Republic of Korea, School of Electrical Engineering, KAIST, Daejeon, Republic of Korea, Kim Jaechul Graduate School of AI, KAIST, Daejeon, Republic of Korea; School of Electrical Engineering, KAIST, Daejeon, Republic of Korea
  - **TL;DR:** The study presents ES-MoE, a method for efficiently scaling Mixture-of-Experts model training beyond GPU memory limits while improving computational efficiency. It demonstrates significant improvements in scalability and throughput compared to existing frameworks.
  - **Keywords:** Mixture-of-Experts (MoE), neural networks, scalability, ES-MoE, expert parallelism, dynamic scheduling, GPU memory limit, load imbalance, computational efficiency, Improved scalability, enhanced training efficiency, better throughput


- [Multimodal Prototyping for cancer survival prediction](https://icml.cc/virtual/2024/poster/35066) (Poster)
  - **Authors:** [Andrew Song](http://openreview.net/profile?id=~Andrew_H._Song1), [Richard Chen](http://openreview.net/profile?id=~Richard_J._Chen1), [Guillaume Jaume](http://openreview.net/profile?id=~Guillaume_Jaume2), [Anurag Vaidya](http://openreview.net/profile?id=~Anurag_Jayant_Vaidya1), [Alexander Baras](http://openreview.net/profile?id=~Alexander_Baras1), [Faisal Mahmood](http://openreview.net/profile?id=~Faisal_Mahmood1)
  - **Affiliations:** Mass General Brigham, Boston, MA, USA; Harvard Medical School, Boston, MA, USA, Mass General Brigham, Boston, MA, USA; Harvard Medical School, Boston, MA, USA, Mass General Brigham, Boston, MA, USA; Harvard Medical School, Boston, MA, USA, Mass General Brigham, Boston, MA, USA; Harvard Medical School, Boston, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA, Johns Hopkins University School of Medicine, Baltimore, MD, USA, Mass General Brigham, Boston, MA, USA; Harvard Medical School, Boston, MA, USA
  - **TL;DR:** This study presents a novel multimodal framework for cancer survival prediction that effectively integrates gigapixel histology whole-slide images and transcriptomic profiles using morphological and biological pathway prototypes. The proposed method significantly reduces computational requirements while enhancing interpretability and outperforms existing state-of-the-art approaches across multiple cancer types.
  - **Keywords:** cancer survival prediction, patient prognostication, multimodal approaches, Transformer, multiple instance learning (MIL), unsupervised learning, oncology, medical imaging, transcriptomics, high memory requirements, computational expense, integration of multimodal data, morphological prototypes, biological pathway prototypes, improved interpretability analyses, whole-slide images (WSIs), RNA sequencing


- [Exploration and Anti-Exploration with Distributional Random Network Distillation](https://icml.cc/virtual/2024/poster/32960) (Poster)
  - **Authors:** [Kai Yang](http://openreview.net/profile?id=~Kai_Yang6), [jian tao](http://openreview.net/profile?id=~Jian_Tao4), [Jiafei Lyu](http://openreview.net/profile?id=~Jiafei_Lyu1), [Xiu Li](http://openreview.net/profile?id=~Xiu_Li1)
  - **Affiliations:** Tsinghua Shenzhen International Graduate School, Tsinghua University, Tsinghua Shenzhen International Graduate School, Tsinghua University, Tsinghua Shenzhen International Graduate School, Tsinghua University, Tsinghua Shenzhen International Graduate School, Tsinghua University
  - **TL;DR:** This paper introduces Distributional Random Network Distillation (DRND) to address the bonus inconsistency issue in the Random Network Distillation (RND) algorithm, enhancing exploration in reinforcement learning. The proposed method improves bonus allocation precision and encourages more extensive exploration without significant computational overhead, demonstrating superior performance in challenging environments.
  - **Keywords:** Exploration, Reinforcement Learning, Random Network Distillation (RND), Distributional Random Network Distillation (DRND), Online Exploration, Offline Tasks (D4RL), Bonus Inconsistency, Sparse Rewards, Improved Bonus Allocation, Enhanced Exploration, Pseudo Counts, Curiosity-driven Methods


- [Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game](https://icml.cc/virtual/2024/poster/32805) (Poster)
  - **Authors:** [Zelai Xu](http://openreview.net/profile?id=~Zelai_Xu1), [Chao Yu](http://openreview.net/profile?id=~Chao_Yu1), [Fei Fang](http://openreview.net/profile?id=~Fei_Fang1), [Yu Wang](http://openreview.net/profile?id=~Yu_Wang3), [Yi Wu](http://openreview.net/profile?id=~Yi_Wu1)
  - **Affiliations:** Tsinghua University, Beijing, China, Tsinghua University, Beijing, China, Carnegie Mellon University, Pittsburgh, United States, Tsinghua University, Beijing, China, Tsinghua University, Beijing, China; Shanghai Qi Zhi Institute, Shanghai, China
  - **TL;DR:** This study proposes a novel framework that enhances large language model-based agents with reinforcement learning to improve their strategic decision-making in the Werewolf game. The agents demonstrate human-level performance and effectively mitigate intrinsic biases in action selection.
  - **Keywords:** strategic language agents, decision-making, social deduction games, reinforcement learning, large language models (LLMs), Werewolf game, multi-agent scenarios, intrinsic bias in language actions, suboptimal performance in decision-making tasks, novel framework for strategic play, human-level performance


- [Online Isolation Forest](https://icml.cc/virtual/2024/poster/34674) (Poster)
  - **Authors:** [Filippo Leveni](http://openreview.net/profile?id=~Filippo_Leveni1), [Guilherme Weigert Cassales](http://openreview.net/profile?id=~Guilherme_Weigert_Cassales1), [Bernhard Pfahringer](http://openreview.net/profile?id=~Bernhard_Pfahringer1), [Albert Bifet](http://openreview.net/profile?id=~Albert_Bifet1), [Giacomo Boracchi](http://openreview.net/profile?id=~Giacomo_Boracchi2)
  - **Affiliations:** Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy, Artificial Intelligence Institute, University of Waikato, Hamilton, New Zealand, Artificial Intelligence Institute, University of Waikato, Hamilton, New Zealand, Artificial Intelligence Institute, University of Waikato, Hamilton, New Zealand, Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy
  - **TL;DR:** The study introduces ONLINE-IFOREST, a novel online anomaly detection method designed for streaming data that adapts to evolving data distributions. Experimental results show that it outperforms existing online methods and rivals state-of-the-art offline techniques in efficiency, making it suitable for critical applications like cybersecurity and fraud detection.
  - **Keywords:** anomaly detection, online anomaly detection, Isolation Forest, ONLINE-IFOREST, cybersecurity, fraud detection, fault detection, streaming data, model obsolescence, continuous adaptability, efficient anomaly detection, dynamic learning mechanism


- [Position: Understanding LLMs Requires More Than Statistical Generalization](https://icml.cc/virtual/2024/poster/33038) (Spotlight Poster)
  - **Authors:** [Patrik Reizinger](http://openreview.net/profile?id=~Patrik_Reizinger1), [Szilvia Ujváry](http://openreview.net/profile?id=~Szilvia_Ujv%C3%A1ry1), [Anna Mészáros](http://openreview.net/profile?id=~Anna_M%C3%A9sz%C3%A1ros1), [Anna Kerekes](http://openreview.net/profile?id=~Anna_Kerekes1), [Wieland Brendel](http://openreview.net/profile?id=~Wieland_Brendel1), [Ferenc Huszár](http://openreview.net/profile?id=~Ferenc_Husz%C3%A1r1)
  - **Affiliations:** Max Planck Institute for Intelligent Systems, Tübingen, Germany, University of Cambridge, UK; AI Center, UCL, London, UK, University of Cambridge, UK; AI Center, UCL, London, UK, Department of Computer Science, ETH Zurich; Max Planck ETH Center for Learning Systems, Max Planck Institute for Intelligent Systems, Tübingen, Germany; ELLIS Institute Tübingen, Germany; Tübingen AI Center, Germany, University of Cambridge, UK
  - **TL;DR:** This paper argues that understanding the desirable qualities of Large Language Models (LLMs) requires a shift in perspective beyond statistical generalization, focusing on the non-identifiability of autoregressive models. The authors present case studies illustrating the implications of this non-identifiability for zero-shot extrapolation, in-context learning, and fine-tunability, suggesting new research directions in LLM generalization.
  - **Keywords:** Large Language Models, Statistical Generalization, Deep Learning Theory, Autoregressive Models, Next-Token Prediction, Non-identifiability, Zero-shot Rule Extrapolation, In-context Learning, Fine-tunability, Generalization Measures, Transferability, Inductive Biases, KL Divergence, Interpolation Regime, Saturation Regime


- [Revisiting Character-level Adversarial Attacks for Language Models](https://icml.cc/virtual/2024/poster/34762) (Poster)
  - **Authors:** [Elias Abad Rocamora](http://openreview.net/profile?id=~Elias_Abad_Rocamora1), [Yongtao Wu](http://openreview.net/profile?id=~Yongtao_Wu1), [Fanghui Liu](http://openreview.net/profile?id=~Fanghui_Liu1), [Grigorios Chrysos](http://openreview.net/profile?id=~Grigorios_Chrysos1), [Volkan Cevher](http://openreview.net/profile?id=~Volkan_Cevher1)
  - **Affiliations:** LIONS, École Polytechnique Fédérale de Lausanne, Switzerland, LIONS, École Polytechnique Fédérale de Lausanne, Switzerland, Department of Computer Science, University of Warwick, United Kingdom, Department of Electrical and Computer Engineering, University of Wisconsin-Madison, USA, LIONS, École Polytechnique Fédérale de Lausanne, Switzerland
  - **TL;DR:** This paper introduces Charmer, an efficient query-based adversarial attack method for language models that improves attack success rates while maintaining semantic similarity. The method demonstrates significant improvements on BERT and Llama 2 models, addressing challenges in adversarial robustness in natural language processing.
  - **Keywords:** Adversarial attacks, Natural Language Processing, Character-level attacks, Token-level attacks, Query-based adversarial attack, Language Models, Sentiment classification, Robustness to adversarial noise, Invalid adversarial examples, NP-hard problem, Charmer method, Attack success rate (ASR), USE similarity, BERT, Llama 2, SST-2


- [MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation](https://icml.cc/virtual/2024/poster/33736) (Oral)
  - **Authors:** [Nianzu Yang](http://openreview.net/profile?id=~Nianzu_Yang1), [Kaipeng Zeng](http://openreview.net/profile?id=~Kaipeng_Zeng1), [Haotian Lu](http://openreview.net/profile?id=~Haotian_Lu1), [Yexin Wu](http://openreview.net/profile?id=~Yexin_Wu2), [Zexin Yuan](http://openreview.net/profile?id=~Zexin_Yuan1), [Danni Chen](http://openreview.net/profile?id=~Danni_Chen1), [Shengdian Jiang](http://openreview.net/profile?id=~Shengdian_Jiang1), [Jiaxiang Wu](http://openreview.net/profile?id=~Jiaxiang_Wu1), [Yimin Wang](http://openreview.net/profile?id=~Yimin_Wang3), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Guangdong Institute of Intelligence Science and Technology, Guangdong Institute of Intelligence Science and Technology, SEU-ALLEN Joint Center, Institute for Brain and Intelligence, Southeast University, XVERSE Technology, Guangdong Institute of Intelligence Science and Technology, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University
  - **TL;DR:** This study introduces MorphGrower, a novel method for generating plausible neuronal morphologies by mimicking natural growth mechanisms, which significantly outperforms the previous method MorphV AE. The results demonstrate enhanced realism and topological validity in the generated samples, with implications for neuroscience research.
  - **Keywords:** neuronal morphology generation, brain functioning, neurodegenerative disorders, MorphGrower, MorphV AE, layer-by-layer generation, sibling branches, neuroscience, computational biology, high cost of acquiring real-world morphology data, topological validity in generated morphologies, improved plausibility of generated neuronal morphologies, electrophysiological response simulation, four real-world datasets


- [REMEDI: Corrective Transformations for Improved Neural Entropy Estimation](https://icml.cc/virtual/2024/poster/35084) (Poster)
  - **Authors:** [Viktor Nilsson](http://openreview.net/profile?id=~Viktor_Nilsson1), [Anirban Samaddar](http://openreview.net/profile?id=~Anirban_Samaddar1), [Sandeep Madireddy](http://openreview.net/profile?id=~Sandeep_Madireddy1), [Pierre Nyquist](http://openreview.net/profile?id=~Pierre_Nyquist1)
  - **Affiliations:** Department of Mathematics, KTH Royal Institute of Technology, Stockholm, Sweden, Mathematics and Computer Science Division, Argonne National Laboratory, Chicago IL, USA, Mathematics and Computer Science Division, Argonne National Laboratory, Chicago IL, USA, Department of Mathematical Sciences, Chalmers University of Technology and University of Gothenburg, Gothenburg, Sweden
  - **TL;DR:** This paper introduces REMEDI, a method for efficient and accurate estimation of differential entropy, addressing challenges in high-dimensional data. The approach shows improved performance in various estimation tasks and extends to information theoretic supervised learning models, particularly enhancing the Information Bottleneck method.
  - **Keywords:** Information theory, Differential entropy, Machine learning, Cross-entropy minimization, Relative entropy estimation, Information Bottleneck, Reinforcement learning, Unsupervised learning, Dimensionality reduction, High-dimensional data estimation challenges, Data inefficiency in estimators, REMEDI method for entropy estimation, Improved accuracy in Information Bottleneck, Generative modeling, Rejection sampling, Langevin dynamics


- [An Online Optimization Perspective on First-Order and Zero-Order Decentralized Nonsmooth Nonconvex Stochastic Optimization](https://icml.cc/virtual/2024/poster/34120) (Poster)
  - **Authors:** [Emre Sahinoglu](http://openreview.net/profile?id=~Emre_Sahinoglu1), [Shahin Shahrampour](http://openreview.net/profile?id=~Shahin_Shahrampour2)
  - **Affiliations:** Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA 02115, Department of Mechanical and Industrial Engineering, Northeastern University, Boston, MA 02115
  - **TL;DR:** This paper presents a novel algorithm, ME-DOL, for decentralized nonsmooth nonconvex stochastic optimization, achieving optimal sample complexity for finding (δ, ϵ)-stationary points. The findings establish the first finite-time guarantees for decentralized optimization in the first-order setting, matching centralized results.
  - **Keywords:** decentralized optimization, nonsmooth nonconvex optimization, Multi Epoch Decentralized Online Learning (ME-DOL), randomized smoothing, Goldstein-subdifferential, (δ, ϵ)-stationary points, finite-time convergence guarantees, sample complexity O(δ−1ϵ−3), optimal convergence rate, first-order oracle, zero-order oracle


- [Expand-and-Cluster: Parameter Recovery of Neural Networks](https://icml.cc/virtual/2024/poster/35069) (Poster)
  - **Authors:** [Flavio Martinelli](http://openreview.net/profile?id=~Flavio_Martinelli1), [Berfin Simsek](http://openreview.net/profile?id=~Berfin_Simsek1), [Wulfram Gerstner](http://openreview.net/profile?id=~Wulfram_Gerstner1), [Johanni Brea](http://openreview.net/profile?id=~Johanni_Brea1)
  - **Affiliations:** Department of Life Sciences and Computer Sciences, EPFL, Lausanne, Switzerland, Department of Life Sciences and Computer Sciences, EPFL, Lausanne, Switzerland; Center for Data Science, NYU, New York, United States, Department of Life Sciences and Computer Sciences, EPFL, Lausanne, Switzerland, Department of Life Sciences and Computer Sciences, EPFL, Lausanne, Switzerland
  - **TL;DR:** This study presents the 'Expand-and-Cluster' method for recovering the weights and layer sizes of neural networks by training overparameterised student networks to imitate a target network. The method successfully identifies weight vectors and demonstrates effective recovery with minimal overhead, addressing challenges related to non-convex optimization and loss landscape complexity.
  - **Keywords:** Parameter recovery, Neural networks, Overparameterisation, Expand-and-Cluster method, Non-convex optimization, Clustering procedure, Identifying weights of neural networks, Input-output mapping, Loss landscape complexity, Successful weight and size recovery, Ease-of-identifiability analysis, Activation function symmetries, Permutation symmetries, Overparameterisation symmetries


- [A Dense Reward View on Aligning Text-to-Image Diffusion with Preference](https://icml.cc/virtual/2024/poster/32707) (Poster)
  - **Authors:** [Shentao Yang](http://openreview.net/profile?id=~Shentao_Yang1), [Tianqi Chen](http://openreview.net/profile?id=~Tianqi_Chen2), [Mingyuan Zhou](http://openreview.net/profile?id=~Mingyuan_Zhou1)
  - **Affiliations:** The University of Texas at Austin, The University of Texas at Austin, The University of Texas at Austin
  - **TL;DR:** This paper proposes a dense reward perspective for aligning text-to-image diffusion models with user preferences, addressing the limitations of existing methods that treat the generation process as a single action. The authors introduce temporal discounting to enhance the learning efficiency and efficacy of the alignment process, demonstrating competitive results in experiments.
  - **Keywords:** text-to-image diffusion models, preference alignment, direct preference optimization (DPO), reinforcement learning (RL), image generation, sparse reward, sequential generation process, dense reward perspective, alignment objective


- [Log Neural Controlled Differential Equations: The Lie Brackets Make A Difference](https://icml.cc/virtual/2024/poster/35176) (Poster)
  - **Authors:** [Benjamin Walker](http://openreview.net/profile?id=~Benjamin_Walker1), [Andrew McLeod](http://openreview.net/profile?id=~Andrew_Donald_McLeod1), [Tiexin QIN](http://openreview.net/profile?id=~Tiexin_Qin1), [Yichuan Cheng](http://openreview.net/profile?id=~Yichuan_Cheng1), [Haoliang Li](http://openreview.net/profile?id=~Haoliang_Li2), [Terry Lyons](http://openreview.net/profile?id=~Terry_Lyons2)
  - **Affiliations:** Mathematical Institute, University of Oxford, UK, Mathematical Institute, University of Oxford, UK, Department of Electrical Engineering, City University of Hong Kong, Department of Electrical Engineering, City University of Hong Kong, Department of Electrical Engineering, City University of Hong Kong, Mathematical Institute, University of Oxford, UK
  - **TL;DR:** This paper introduces Log-NCDEs, a novel method for training Neural Controlled Differential Equations (NCDEs) that outperforms existing state-of-the-art approaches in multivariate time series modelling by utilizing the Log-ODE method. The findings highlight the robustness of NCDEs to irregular sampling rates and their effectiveness in handling large datasets.
  - **Keywords:** Neural Controlled Differential Equations, Multivariate Time Series Modelling, Log-ODE method, Neural networks, Controlled Differential Equations (CDEs), Time series data analysis, Real-world data modelling, Irregular sampling rates, Performance gap in time series modelling, Log-NCDEs, Improved training methods for NCDEs, Neural Rough Differential Equations (NRDEs), Universal approximators


- [Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning](https://icml.cc/virtual/2024/poster/33519) (Poster)
  - **Authors:** [Long Qian](http://openreview.net/profile?id=~Long_Qian2), [Juncheng Li](http://openreview.net/profile?id=~Juncheng_Li3), [Yu Wu](http://openreview.net/profile?id=~Yu_Wu3), [Yaobo Ye](http://openreview.net/profile?id=~Yaobo_Ye1), [Hao Fei](http://openreview.net/profile?id=~Hao_Fei1), [Tat-Seng Chua](http://openreview.net/profile?id=~Tat-Seng_Chua2), [Yueting Zhuang](http://openreview.net/profile?id=~Yueting_Zhuang1), [Siliang Tang](http://openreview.net/profile?id=~Siliang_Tang1)
  - **Affiliations:** Zhejiang University, Zhejiang University, Wuhan University, Zhejiang University, National University of Singapore, National University of Singapore, Zhejiang University, Zhejiang University
  - **TL;DR:** The study introduces Momentor, a Video-LLM designed to enhance fine-grained temporal understanding and segment-level reasoning in video comprehension tasks. It addresses limitations of existing Video-LLMs by providing effective temporal representation and segment-level modeling, validated through zero-shot evaluations.
  - **Keywords:** Video Large Language Models, fine-grained temporal reasoning, Video comprehension, video localization, Lack of effective temporal representation, lack of segment-level modeling, Momentor, Moment-10M dataset, Moment-10M, Video-LLMs, segment-level reasoning


- [Counterfactual Metarules for Local and Global Recourse](https://icml.cc/virtual/2024/poster/34759) (Poster)
  - **Authors:** [Tom Bewley](http://openreview.net/profile?id=~Tom_Bewley1), [Salim I. Amoukou](http://openreview.net/profile?id=~Salim_I._Amoukou1), [Saumitra Mishra](http://openreview.net/profile?id=~Saumitra_Mishra1), [Daniele Magazzeni](http://openreview.net/profile?id=~Daniele_Magazzeni1), [Manuela Veloso](http://openreview.net/profile?id=~Manuela_Veloso1)
  - **Affiliations:** J.P. Morgan AI Research, J.P. Morgan AI Research, J.P. Morgan AI Research, J.P. Morgan AI Research, J.P. Morgan AI Research
  - **TL;DR:** The study introduces T-CREx, a model-agnostic method for generating local and global counterfactual explanations that summarize recourse options in human-readable rules. It demonstrates superior performance over existing methods while being significantly faster, enhancing the understanding of model behavior and providing actionable recourse options for users.
  - **Keywords:** Counterfactual Explanation, Explainable AI, Tree-based Surrogate Models, Model Behavior Analysis, Recourse Options, Adverse Outputs, Subgroup Fairness, Recourse Noise, T-CREx Method, Optimal Counterfactual Rules


- [UGrid: An Efficient-And-Rigorous Neural Multigrid Solver for Linear PDEs](https://icml.cc/virtual/2024/poster/32790) (Poster)
  - **Authors:** [Xi Han](http://openreview.net/profile?id=~Xi_Han1), [Fei Hou](http://openreview.net/profile?id=~Fei_Hou1), [Hong Qin](http://openreview.net/profile?id=~Hong_Qin1)
  - **Affiliations:** Department of Computer Science, Stony Brook University (SUNY), Stony Brook, NY 11794, USA, Key Laboratory of System Software (Chinese Academy of Sciences); State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China, Department of Computer Science, Stony Brook University (SUNY), Stony Brook, NY 11794, USA
  - **TL;DR:** This paper presents UGrid, a mathematically rigorous neural solver for linear PDEs that integrates U-Net and multigrid methods, ensuring convergence and correctness while achieving high numerical accuracy. The proposed framework also introduces a new residual loss metric for self-supervised training, enhancing stability and solution space compared to traditional methods.
  - **Keywords:** neural PDE solver, multigrid methods, explainable AI, U-Net, Multi Grid, convolutional neural networks, scientific computing, simulation, modeling, convergence and correctness of neural methods, efficiency of numerical solvers, UGrid framework, residual loss metric, self-supervised training


- [RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://icml.cc/virtual/2024/poster/32802) (Poster)
  - **Authors:** [Harrison Lee](http://openreview.net/profile?id=~Harrison_Lee1), [Samrat Phatale](http://openreview.net/profile?id=~Samrat_Phatale1), [Hassan Mansoor](http://openreview.net/profile?id=~Hassan_Mansoor1), [Thomas Mesnard](http://openreview.net/profile?id=~Thomas_Mesnard2), [Johan Ferret](http://openreview.net/profile?id=~Johan_Ferret1), [Kellie Lu](http://openreview.net/profile?id=~Kellie_Ren_Lu1), [Colton Bishop](http://openreview.net/profile?id=~Colton_Bishop1), [Ethan Hall](http://openreview.net/profile?id=~Ethan_Hall1), [Victor Carbune](http://openreview.net/profile?id=~Victor_Carbune1), [Abhinav Rastogi](http://openreview.net/profile?id=~Abhinav_Rastogi2), [Sushant Prakash](http://openreview.net/profile?id=~Sushant_Prakash1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google, Google DeepMind, Google, Google DeepMind, Google DeepMind, Google
  - **TL;DR:** This study explores Reinforcement Learning from AI Feedback (RLAIF) as a scalable alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning large language models with human preferences. The findings indicate that RLAIF can achieve performance comparable to RLHF while addressing the challenges of obtaining high-quality human preference labels.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Reinforcement Learning from AI Feedback (RLAIF), AI alignment, Reward Model (RM), direct-RLAIF (d-RLAIF), Summarization, Dialogue Generation, High-quality preference labels, Scalability limitations of RLHF, Comparable performance to RLHF, Self-improvement in RL, Performance on-par with human feedback, Large Language Models (LLMs), Supervised Fine-Tuning (SFT)


- [Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF](https://icml.cc/virtual/2024/poster/33816) (Poster)
  - **Authors:** [Han Shen](http://openreview.net/profile?id=~Han_Shen3), [Zhuoran Yang](http://openreview.net/profile?id=~Zhuoran_Yang1), [Tianyi Chen](http://openreview.net/profile?id=~Tianyi_Chen5)
  - **Affiliations:** Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, United States, Department of Statistics and Data Science, Yale University, United States, Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, United States
  - **TL;DR:** This paper introduces a principled algorithmic framework for solving bilevel reinforcement learning problems using penalty formulations, addressing the challenges posed by dynamic objective functions. The effectiveness of the proposed methods is demonstrated through simulations in Stackelberg games and RL from human feedback.
  - **Keywords:** Bilevel optimization, Reinforcement learning (RL), RL from human feedback (RLHF), Penalty formulation, Policy gradient algorithms, Stackelberg games, Incentive design, Inverse reinforcement learning, Dynamic objective functions, Handling constraints in bilevel RL, Algorithmic framework for bilevel RL, Theoretical studies of problem landscape, Markov decision process (MDP), Best response policy


- [Improved Differentially Private and Lazy Online Convex Optimization: Lower Regret without Smoothness Requirements](https://icml.cc/virtual/2024/poster/32698) (Poster)
  - **Authors:** [Naman Agarwal](http://openreview.net/profile?id=~Naman_Agarwal1), [Satyen Kale](http://openreview.net/profile?id=~Satyen_Kale2), [Karan Singh](http://openreview.net/profile?id=~Karan_Singh1), [Abhradeep Guha Thakurta](http://openreview.net/profile?id=~Abhradeep_Guha_Thakurta1)
  - **Affiliations:** Google DeepMind, Google Research, Tepper School of Business, Carnegie Mellon University, Google DeepMind
  - **TL;DR:** This paper presents differentially private algorithms for online convex optimization that achieve optimal regret bounds without requiring smoothness in loss functions. The proposed methods improve upon previous results, particularly in terms of dimension dependence, and also address privacy concerns effectively.
  - **Keywords:** Differential Privacy, Online Convex Optimization, Regret-minimizing algorithms, Log-Sobolev Inequality, Rejection sampling, Non-smooth loss functions, Privacy leakage, Improved regret bounds, Optimal leading-order term, Lipschitz convex loss functions, Strongly log-concave densities


- [StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation](https://icml.cc/virtual/2024/poster/33412) (Spotlight Poster)
  - **Authors:** [Weike Fang](http://openreview.net/profile?id=~Weike_Fang1), [Zhejian Zhou](http://openreview.net/profile?id=~Zhejian_Zhou1), [Junzhou He](http://openreview.net/profile?id=~Junzhou_He1), [Weihang Wang](http://openreview.net/profile?id=~Weihang_Wang3)
  - **Affiliations:** Department of Computer Science, University of Southern California, Los Angeles, CA, United States, Department of Computer Science, University of Southern California, Los Angeles, CA, United States, Department of Computer Science, University of Southern California, Los Angeles, CA, United States, Department of Computer Science, University of Southern California, Los Angeles, CA, United States
  - **TL;DR:** This paper presents StackSight, a novel neurosymbolic approach that combines Large Language Models with advanced program analysis to effectively decompile complex WebAssembly code into readable C++ snippets. The evaluation shows significant improvements in decompilation performance and user understanding of code semantics.
  - **Keywords:** WebAssembly, Decompilation, Large Language Models, Neurosymbolic approach, Static analysis algorithm, Chain-of-thought prompting, Web applications, High-performance computing, Security, Understanding assembly-like syntax, Reverse engineering techniques, Tracking virtual stack behaviors, Improved WebAssembly decompilation, Readable C++ snippets


- [Optimal Recurrent Network Topologies for Dynamical Systems Reconstruction](https://icml.cc/virtual/2024/poster/34459) (Poster)
  - **Authors:** [Christoph Jürgen Hemmer](http://openreview.net/profile?id=~Christoph_J%C3%BCrgen_Hemmer1), [Manuel Brenner](http://openreview.net/profile?id=~Manuel_Brenner1), [Florian Hess](http://openreview.net/profile?id=~Florian_Hess1), [Daniel Durstewitz](http://openreview.net/profile?id=~Daniel_Durstewitz1)
  - **Affiliations:** Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany, Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany, Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany, Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany; Interdisciplinary Center for Scientific Computing, Heidelberg University, Heidelberg, Germany
  - **TL;DR:** This study investigates optimal recurrent network topologies for dynamical systems reconstruction, revealing that geometric pruning can significantly reduce parameter load without compromising model quality. The findings emphasize the importance of topological structure over weight magnitude in influencing system dynamics.
  - **Keywords:** dynamical systems reconstruction, generative modeling, recurrent neural networks (RNNs), geometric pruning, magnitude-based pruning, low parameter load, influence of small parameters on dynamics, topology in dynamical systems, algorithm for generating topologies, comparison of network topologies (small-world, scale-free), LSTM (Long Short-Term Memory networks), neural ordinary differential equations (Neural ODEs)


- [Unmasking Vulnerabilities: Cardinality Sketches under Adaptive Inputs](https://icml.cc/virtual/2024/poster/33296) (Poster)
  - **Authors:** [Sara Ahmadian](http://openreview.net/profile?id=~Sara_Ahmadian1), [Edith Cohen](http://openreview.net/profile?id=~Edith_Cohen1)
  - **Affiliations:** Google Research, United States, Google Research, United States; Department of Computer Science, Tel Aviv University, Israel
  - **TL;DR:** This study investigates the performance of cardinality sketches under adaptive query conditions, revealing inherent vulnerabilities in standard estimators. The authors demonstrate that an adversarial input can be constructed using a limited number of queries, highlighting the need for improved robustness in cardinality estimation methods.
  - **Keywords:** Cardinality sketches, Adaptive inputs, Data structures, HyperLogLog (HLL++), Randomized representations, Large data sets, Streaming applications, Vulnerabilities in cardinality estimation, Adaptive query responses, Attack against standard estimators, Inherent vulnerabilities in estimators, Composable sketches, Normalized Root Mean Squared Error (NRMSE), Set union operations


- [DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency](https://icml.cc/virtual/2024/poster/33347) (Poster)
  - **Authors:** [Zalan Fabian](http://openreview.net/profile?id=~Zalan_Fabian1), [Berk Tinaz](http://openreview.net/profile?id=~Berk_Tinaz1), [Mahdi Soltanolkotabi](http://openreview.net/profile?id=~Mahdi_Soltanolkotabi1)
  - **Affiliations:** Dept. of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, Dept. of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, Dept. of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA
  - **TL;DR:** This study presents a novel framework for solving inverse problems using diffusion models, focusing on reversing a stochastic degradation process to recover clean images while maintaining data consistency. The method achieves significant improvements in both perceptual quality and distortion metrics compared to existing diffusion-based approaches.
  - **Keywords:** Diffusion models, image restoration, inverse problems, Denoising Diffusion Probabilistic Models (DDPM), Score-Based Models, Stochastic Differential Equations (SDEs), Computer vision, image generation, Perception-distortion trade-off, data consistency in inverse problems, Novel framework for inverse problem solving, improved distortion metrics, sampling speedup via early-stopping, Gaussian noise, stochastic degradation process


- [I/O Complexity of Attention, or How Optimal is FlashAttention?](https://icml.cc/virtual/2024/poster/34232) (Oral)
  - **Authors:** [Barna Saha](http://openreview.net/profile?id=~Barna_Saha3), [Christopher Ye](http://openreview.net/profile?id=~Christopher_Ye1)
  - **Affiliations:** Department of Computer Science and Engineering, University of California San Diego, La Jolla, CA, Department of Computer Science and Engineering, University of California San Diego, La Jolla, CA
  - **TL;DR:** This paper investigates the I/O complexity of the FlashAttention algorithm for self-attention in Transformers, demonstrating that its complexity is optimal for cache sizes greater than or equal to the square of the head dimension. Additionally, it presents a more efficient algorithm for smaller cache sizes and establishes a connection between communication complexity and I/O complexity.
  - **Keywords:** Attention mechanisms, I/O complexity, Transformers, FlashAttention, matrix multiplication, communication complexity, Natural language processing, computer vision, Quadratic time and memory complexity, I/O complexity bottleneck, Optimal I/O complexity algorithms, new communication complexity protocol, Self-attention, attention matrix, cache size


- [Disentangled Graph Self-supervised Learning for Out-of-Distribution Generalization](https://icml.cc/virtual/2024/poster/34163) (Poster)
  - **Authors:** [Haoyang Li](http://openreview.net/profile?id=~Haoyang_Li1), [Xin Wang](http://openreview.net/profile?id=~Xin_Wang17), [Zeyang Zhang](http://openreview.net/profile?id=~Zeyang_Zhang1), [Haibo Chen](http://openreview.net/profile?id=~Haibo_Chen7), [Ziwei Zhang](http://openreview.net/profile?id=~Ziwei_Zhang1), [Wenwu Zhu](http://openreview.net/profile?id=~Wenwu_Zhu1)
  - **Affiliations:** Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China
  - **TL;DR:** This paper addresses the challenge of out-of-distribution generalization in graph neural networks by proposing a self-supervised learning approach that learns disentangled graph representations without relying on annotated labels. The proposed model demonstrates superior performance under distribution shifts compared to existing methods.
  - **Keywords:** Graph out-of-distribution (OOD) generalization, self-supervised learning, Graph neural networks (GNNs), disentangled graph contrastive learning model (OOD-GCL), disentangled graph encoder, Drug discovery, graph classification, Distribution shifts, lack of annotated labels, entangled invariant and variant information, Disentangled graph representations, stable performance under distribution shifts


- [Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models](https://icml.cc/virtual/2024/poster/33927) (Spotlight Poster)
  - **Authors:** [Zalan Fabian](http://openreview.net/profile?id=~Zalan_Fabian1), [Berk Tinaz](http://openreview.net/profile?id=~Berk_Tinaz1), [Mahdi Soltanolkotabi](http://openreview.net/profile?id=~Mahdi_Soltanolkotabi1)
  - **Affiliations:** Dept. of Electrical and Comp. Eng., Univ. of Southern California, Los Angeles, CA, Dept. of Electrical and Comp. Eng., Univ. of Southern California, Los Angeles, CA, Dept. of Electrical and Comp. Eng., Univ. of Southern California, Los Angeles, CA
  - **TL;DR:** This study introduces a novel method called Flash-Diffusion for sample-adaptive reconstruction of signals from noisy observations, leveraging latent diffusion models and severity encoding to improve performance and accelerate inference times. The proposed framework demonstrates significant improvements in reconstruction tasks, achieving up to 10× acceleration in mean sampling speed.
  - **Keywords:** Inverse problems, Sample-adaptive reconstruction, Latent diffusion models, Severity encoding, Autoencoder, Computer vision, Biomedical imaging, Scientific applications, Difficulty of reconstruction, Corruption severity, Information loss, Flash-Diffusion framework, Sample-adaptive inference


- [On the Effectiveness of Supervision in Asymmetric Non-Contrastive Learning](https://icml.cc/virtual/2024/poster/33367) (Poster)
  - **Authors:** [Jeongheon Oh](http://openreview.net/profile?id=~Jeongheon_Oh1), [Kibok Lee](http://openreview.net/profile?id=~Kibok_Lee1)
  - **Affiliations:** Department of Statistics and Data Science, Yonsei University, Department of Statistics and Data Science, Yonsei University
  - **TL;DR:** This study introduces a supervised asymmetric non-contrastive learning framework (SUPSIAM and SUPBYOL) that leverages labels to enhance representation learning while preventing collapse. The findings indicate that supervision reduces intra-class variance and improves performance across various datasets and tasks.
  - **Keywords:** Supervised contrastive learning, Asymmetric non-contrastive learning, SUPSIAM, SUPBYOL, SIMSIAM, BYOL, Transfer learning, Self-supervised representation learning, Intra-class variance, Representation collapse, Supervised ANCL framework, Improved representation learning


- [Disentangled Continual Graph Neural Architecture Search with Invariant Modular Supernet](https://icml.cc/virtual/2024/poster/34455) (Poster)
  - **Authors:** [Zeyang Zhang](http://openreview.net/profile?id=~Zeyang_Zhang1), [Xin Wang](http://openreview.net/profile?id=~Xin_Wang17), [Yijian Qin](http://openreview.net/profile?id=~Yijian_Qin2), [Hong Chen](http://openreview.net/profile?id=~Hong_Chen9), [Ziwei Zhang](http://openreview.net/profile?id=~Ziwei_Zhang1), [Xu Chu](http://openreview.net/profile?id=~Xu_Chu1), [Wenwu Zhu](http://openreview.net/profile?id=~Wenwu_Zhu1)
  - **Affiliations:** Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China, Department of Computer Science and Technology, BN-RIST, Tsinghua University, Beijing, China
  - **TL;DR:** This paper addresses the challenge of continual graph neural architecture search (GNAS) by proposing a novel method called GASIM, which enables the continual learning of new graph tasks without forgetting previous knowledge. The method effectively resolves architecture conflicts and demonstrates state-of-the-art performance in GNAS.
  - **Keywords:** Continual Graph Neural Architecture Search, Graph Neural Networks, Disentangled Continual Graph Neural Architecture Search, Invariant Modular Supernet, Graph-structured data, Drug design, Citation networks, Catastrophic forgetting, Architecture conflicts, Performance deterioration, Modular graph architecture super-network, Factor-based task-module router, Invariant architecture search mechanism, GNAS (Graph Neural Architecture Search), GNNs (Graph Neural Networks)


- [A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks](https://icml.cc/virtual/2024/poster/33997) (Spotlight Poster)
  - **Authors:** [Boqi Li](http://openreview.net/profile?id=~Boqi_Li2), [Weiwei Liu](http://openreview.net/profile?id=~Weiwei_Liu1)
  - **Affiliations:** School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China, School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China
  - **TL;DR:** This study provides a theoretical analysis of backdoor poisoning attacks in convolutional neural networks, focusing on dirty-label attacks. The findings reveal that the effectiveness of these attacks depends on the number of feature vectors, the norm ratio of trigger patterns to feature vectors, and the percentage of poisoned data in the training set.
  - **Keywords:** backdoor poisoning attacks, deep neural networks, security of DNNs, convolutional neural networks, dirty-label attacks, data poisoning attacks, model compromise, theoretical insights on backdoor learning, effectiveness of BPA, two real-world datasets, trigger pattern, clean-label attacks, dirty-label attacks


- [Conformal prediction for multi-dimensional time series by ellipsoidal sets](https://icml.cc/virtual/2024/poster/32830) (Spotlight Poster)
  - **Authors:** [Chen Xu](http://openreview.net/profile?id=~Chen_Xu12), [Hanyang Jiang](http://openreview.net/profile?id=~Hanyang_Jiang2), [Yao Xie](http://openreview.net/profile?id=~Yao_Xie2)
  - **Affiliations:** H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology
  - **TL;DR:** This study introduces a sequential conformal prediction method called MultiDimSPCI for multivariate time series, addressing the challenges of non-exchangeable data. The method demonstrates valid coverage and produces smaller prediction regions compared to existing conformal prediction and non-conformal prediction baselines.
  - **Keywords:** Conformal prediction, uncertainty quantification, multivariate time series, MultiDimSPCI, prediction regions, non-conformity score, Time series forecasting, anomaly detection, classification, regression, Non-exchangeable data, complex dependence in multivariate outputs, Valid coverage, smaller prediction regions, finite-sample high-probability bounds


- [Lookbehind-SAM: k steps back, 1 step forward](https://icml.cc/virtual/2024/poster/32791) (Poster)
  - **Authors:** [Gonçalo Mordido](http://openreview.net/profile?id=~Goncalo_Mordido1), [Pranshu Malviya](http://openreview.net/profile?id=~Pranshu_Malviya1), [Aristide Baratin](http://openreview.net/profile?id=~Aristide_Baratin1), [Sarath Chandar](http://openreview.net/profile?id=~Sarath_Chandar1)
  - **Affiliations:** Mila - Quebec AI Institute; Polytechnique Montreal, Mila - Quebec AI Institute; Polytechnique Montreal, Samsung SAIT AI Lab Montreal, Mila - Quebec AI Institute; Polytechnique Montreal; Canada CIFAR AI Chair
  - **TL;DR:** This study introduces Lookbehind, an enhancement to sharpness-aware minimization (SAM) that improves the efficiency of both maximization and minimization steps, leading to better generalization performance and robustness in deep learning models. The proposed method mitigates issues related to gradient instability and catastrophic forgetting in lifelong learning settings.
  - **Keywords:** sharpness-aware minimization, optimization methods, deep learning, Lookbehind, minimax optimization, gradient ascent, linear interpolation, lifelong learning, generalization performance, loss sharpness, training loss, gradient instability, improved loss-sharpness trade-off, increased robustness, reduced catastrophic forgetting, SAM (Sharpness-Aware Minimization), perturbations


- [Understanding the Training Speedup from Sampling with Approximate Losses](https://icml.cc/virtual/2024/poster/32803) (Poster)
  - **Authors:** [Rudrajit Das](http://openreview.net/profile?id=~Rudrajit_Das1), [Xi Chen](http://openreview.net/profile?id=~Xi_Chen37), [Bertram Ieong](http://openreview.net/profile?id=~Bertram_Ieong1), [Parikshit Bansal](http://openreview.net/profile?id=~Parikshit_Bansal1), [Sujay Sanghavi](http://openreview.net/profile?id=~sujay_sanghavi1)
  - **Affiliations:** UT Austin, Amazon, Amazon, UT Austin, UT Austin; Amazon
  - **TL;DR:** This study introduces a greedy approach for selecting samples with large approximate losses to accelerate training in machine learning models, demonstrating significant time savings in training a BERT model. The proposed method, SIFT, shows improved efficiency in convergence compared to standard random sampling techniques.
  - **Keywords:** sample selection, training speedup, approximate losses, Stochastic Gradient Descent (SGD), importance sampling, greedy strategy, machine learning model training, BERT model training, high selection overhead, convergence speed, SIFT method, early exiting for approximate losses, BERT base model


- [A Distributional Analogue to the Successor Representation](https://icml.cc/virtual/2024/poster/32627) (Spotlight Poster)
  - **Authors:** [Harley Wiltzer](http://openreview.net/profile?id=~Harley_Wiltzer1), [Jesse Farebrother](http://openreview.net/profile?id=~Jesse_Farebrother1), [Arthur Gretton](http://openreview.net/profile?id=~Arthur_Gretton1), [Yunhao Tang](http://openreview.net/profile?id=~Yunhao_Tang1), [Andre Barreto](http://openreview.net/profile?id=~Andre_Barreto1), [Will Dabney](http://openreview.net/profile?id=~Will_Dabney1), [Marc Bellemare](http://openreview.net/profile?id=~Marc_G_Bellemare1), [Mark Rowland](http://openreview.net/profile?id=~Mark_Rowland1)
  - **Affiliations:** McGill University; Mila - Québec AI Institute, McGill University; Mila - Québec AI Institute; Google DeepMind, Google DeepMind; Gatsby Unit, University College London, Google DeepMind, Google DeepMind, Google DeepMind, McGill University; Mila - Québec AI Institute; CIFAR AI Chair, Google DeepMind
  - **TL;DR:** This paper introduces a distributional successor measure (DSM) that allows for zero-shot evaluation of novel reward functions in distributional reinforcement learning, overcoming the need for retraining on new tasks. The proposed δ-model effectively generalizes across tasks and risk-sensitive criteria without additional data collection or training.
  - **Keywords:** distributional reinforcement learning, successor representation, distributional successor measure, δ-model, maximum mean discrepancy, risk-sensitive policy evaluation, zero-shot evaluation of novel reward functions, separation of transition structure and reward, new approach for distributional RL, generative models of state


- [From Neurons to Neutrons: A Case Study in Interpretability](https://icml.cc/virtual/2024/poster/33828) (Poster)
  - **Authors:** [Ouail Kitouni](http://openreview.net/profile?id=~Ouail_Kitouni1), [Niklas Nolte](http://openreview.net/profile?id=~Niklas_Nolte1), [Víctor Samuel Pérez-Díaz](http://openreview.net/profile?id=~V%C3%ADctor_Samuel_P%C3%A9rez-D%C3%ADaz1), [Sokratis Trifinopoulos](http://openreview.net/profile?id=~Sokratis_Trifinopoulos1), [Mike Williams](http://openreview.net/profile?id=~Mike_Williams1)
  - **Affiliations:** NSF Institute for Artificial Intelligence and Fundamental Interactions (IAIFI); Massachusetts Institute of Technology, FAIR at Meta, NSF Institute for Artificial Intelligence and Fundamental Interactions (IAIFI); Harvard John A. Paulson School of Engineering and Applied Sciences; Center for Astrophysics | Harvard & Smithsonian; School of Engineering, Science and Technology, Universidad del Rosario, NSF Institute for Artificial Intelligence and Fundamental Interactions (IAIFI); Massachusetts Institute of Technology, NSF Institute for Artificial Intelligence and Fundamental Interactions (IAIFI); Massachusetts Institute of Technology
  - **TL;DR:** This study explores mechanistic interpretability in neural networks, demonstrating that they can derive low-dimensional representations of high-dimensional data, particularly in the context of nuclear physics. The findings suggest that these models can provide valuable insights that align with human-derived domain knowledge.
  - **Keywords:** Mechanistic Interpretability, Neural Networks, Low-dimensional Representations, Nuclear Physics, High-dimensional Data, Interpretability Challenges, Insights from Machine Learning Models, Nuclear Data


- [Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills](https://icml.cc/virtual/2024/poster/34795) (Poster)
  - **Authors:** [Kolby Nottingham](http://openreview.net/profile?id=~Kolby_Nottingham1), [Bodhisattwa Prasad Majumder](http://openreview.net/profile?id=~Bodhisattwa_Prasad_Majumder1), [Bhavana Dalvi](http://openreview.net/profile?id=~Bhavana_Dalvi_Mishra2), [Sameer Singh](http://openreview.net/profile?id=~Sameer_Singh1), [Peter Clark](http://openreview.net/profile?id=~Peter_Clark1), [Roy Fox](http://openreview.net/profile?id=~Roy_Fox1)
  - **Affiliations:** Department of Computer Science, University of California Irvine, Irvine CA, United States, Allen Institute for AI, Seattle Washington, United States, Allen Institute for AI, Seattle Washington, United States, Department of Computer Science, University of California Irvine, Irvine CA, United States, Allen Institute for AI, Seattle Washington, United States, Department of Computer Science, University of California Irvine, Irvine CA, United States
  - **TL;DR:** This paper introduces Skill Set Optimization (SSO) to enhance the performance of large language model actors in interactive environments by constructing and refining transferable skills. The proposed method significantly outperforms existing baselines in both the NetHack game and the ScienceWorld text environment, demonstrating effective in-context policy improvement.
  - **Keywords:** Skill Set Optimization, Large Language Models, Reinforcement Learning, In-context learning, Policy improvement, Interactive environments, Video games, Robotics, Sequential decision making, Credit assignment, Transferable skills, Performance optimization, NetHack, ScienceWorld


- [MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations](https://icml.cc/virtual/2024/poster/33709) (Poster)
  - **Authors:** [Zhangyu Wang](http://openreview.net/profile?id=~Zhangyu_Wang1), [Gengchen Mai](http://openreview.net/profile?id=~Gengchen_Mai1), [Krzysztof Janowicz](http://openreview.net/profile?id=~Krzysztof_Janowicz1), [Ni Lao](http://openreview.net/profile?id=~Ni_Lao1)
  - **Affiliations:** Department of Geography, University of California Santa Barbara, CA, USA, Department of Geography, University of Georgia, GA, USA; SEAI Lab, Department of Geography and the Environment, University of Texas at Austin, TX, USA, Faculty of Geosciences, Geography and Astronomy, University of Vienna, Vienna, Austria, Google, Mountain View, CA, USA
  - **TL;DR:** This study introduces MC-GTA, a novel clustering algorithm that effectively incorporates metric autocorrelation to improve clustering performance in temporal and spatial data analysis. The algorithm outperforms existing methods significantly while achieving faster and more stable optimization.
  - **Keywords:** metric-constrained clustering, temporal and spatial data analysis, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations), Expectation-Maximization, square Wasserstein-2 distance, semivariogram, vehicle sensor trajectories, temporal continuity, geospatial proximity, computational instability, complexity in clustering algorithms, feature similarity vs. metric distance, improved clustering performance, faster and stabler optimization, minimized total hinge loss, synthetic datasets, real-world datasets, metric autocorrelation, model-based clustering


- [DNCs Require More Planning Steps](https://icml.cc/virtual/2024/poster/32852) (Poster)
  - **Authors:** [Yara Shamshoum](http://openreview.net/profile?id=~Yara_Shamshoum1), [Nitzan Hodos](http://openreview.net/profile?id=~Nitzan_Hodos1), [Yuval Sieradzki](http://openreview.net/profile?id=~Yuval_Sieradzki1), [Assaf Schuster](http://openreview.net/profile?id=~Assaf_Schuster2)
  - **Affiliations:** Department of Computer Science, Technion- Israel Institute of Technology, Haifa, Israel, Department of Computer Science, Technion- Israel Institute of Technology, Haifa, Israel, Department of Computer Science, Technion- Israel Institute of Technology, Haifa, Israel, Department of Computer Science, Technion- Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This study investigates the impact of computational time and memory on the generalization of implicit algorithmic solvers, specifically focusing on the Differentiable Neural Computer (DNC). The findings reveal that the planning budget significantly affects the model's ability to generalize and utilize external memory effectively.
  - **Keywords:** machine learning, algorithmic reasoning, generalization, Differentiable Neural Computer (DNC), Recurrent Neural Network (RNN), LSTM (Long Short-Term Memory), computational complexity, generalization to unseen inputs, planning budget, learned time complexity, training time, stability, generalization, Graph Shortest Path, Convex Hull, Graph MinCut, Associative Recall


- [Knowledge-aware Reinforced Language Models for Protein Directed Evolution](https://icml.cc/virtual/2024/poster/34229) (Poster)
  - **Authors:** [Yuhao Wang](http://openreview.net/profile?id=~Yuhao_Wang12), [Qiang Zhang](http://openreview.net/profile?id=~Qiang_Zhang6), [Ming Qin](http://openreview.net/profile?id=~Ming_Qin3), [Xiang Zhuang](http://openreview.net/profile?id=~Xiang_Zhuang1), [Xiaotong Li](http://openreview.net/profile?id=~Xiaotong_Li3), [Zhichen Gong](http://openreview.net/profile?id=~Zhichen_Gong1), [Zeyuan Wang](http://openreview.net/profile?id=~Zeyuan_Wang3), [Yu Zhao](http://openreview.net/profile?id=~Yu_Zhao8), [Jianhua Yao](http://openreview.net/profile?id=~Jianhua_Yao3), [Keyan Ding](http://openreview.net/profile?id=~Keyan_Ding1), [Huajun Chen](http://openreview.net/profile?id=~Huajun_Chen1)
  - **Affiliations:** Zhejiang University; ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University; ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University; ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University; ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University; ZJU-Hangzhou Global Scientific and Technological Innovation Center, ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University; ZJU-Hangzhou Global Scientific and Technological Innovation Center, Tencent AI Lab, Tencent AI Lab, Zhejiang University; ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University; ZJU-Hangzhou Global Scientific and Technological Innovation Center
  - **TL;DR:** This study introduces a Knowledge-aware Reinforced Language Model (KnowRLM) for protein directed evolution, leveraging an Amino Acid Knowledge Graph to enhance the identification of high-fitness mutants. The proposed method demonstrates superior efficiency in optimizing protein functionality compared to existing techniques.
  - **Keywords:** Directed evolution, protein optimization, machine learning-assisted directed evolution, Knowledge-aware Reinforced Language Model (KnowRLM), Protein Language Model (PLM), Amino Acid Knowledge Graph (AAKG), reinforcement learning, Protein engineering, biochemical relationships, protein sequence-function relationships, Exploration of protein sequence space, identification of optimal mutants, Novel mutants sampling, fitness predictor fine-tuning, active learning approach


- [Enforcing Constraints in RNA Secondary Structure Predictions: A Post-Processing Framework Based on the Assignment Problem](https://icml.cc/virtual/2024/poster/33832) (Poster)
  - **Authors:** [Geewon Suh](http://openreview.net/profile?id=~Geewon_Suh1), [Gyeongjo Hwang](http://openreview.net/profile?id=~Gyeongjo_Hwang1), [SeokjunKang](http://openreview.net/profile?id=~Seokjun_Kang1), [Doojin Baek](http://openreview.net/profile?id=~Doojin_Baek1), [Mingeun Kang](http://openreview.net/profile?id=~Mingeun_Kang2)
  - **Affiliations:** Spidercore Inc., Daejeon, South Korea; Department of Electrical Engineering, KAIST, Daejeon, South Korea, Spidercore Inc., Daejeon, South Korea, Spidercore Inc., Daejeon, South Korea, School of Computing, KAIST, Daejeon, South Korea, Spidercore Inc., Daejeon, South Korea; Department of Electrical Engineering, KAIST, Daejeon, South Korea
  - **TL;DR:** This study presents a novel post-processing framework for RNA secondary structure predictions that ensures compliance with biological constraints, significantly improving predictive performance without constraint violations. The approach is inspired by the assignment problem in integer linear programming, demonstrating efficiency and theoretical guarantees.
  - **Keywords:** RNA secondary structure prediction, computational models, dynamic programming, machine learning (ML), integer linear programming, biotechnology, RNA structure analysis, constraint violations in RNA predictions, biological relevance, post-processing framework for RNA predictions, improved predictive performance, Watson-Crick pairs, Wobble pairs, pseudoknotted structures


- [Low-Rank Similarity Mining for Multimodal Dataset Distillation](https://icml.cc/virtual/2024/poster/33168) (Poster)
  - **Authors:** [Yue Xu](http://openreview.net/profile?id=~Yue_Xu4), [Zhilin Lin](http://openreview.net/profile?id=~Zhilin_Lin1), [Yusong Qiu](http://openreview.net/profile?id=~Yusong_Qiu1), [Cewu Lu](http://openreview.net/profile?id=~Cewu_Lu3), [Yong-Lu Li](http://openreview.net/profile?id=~Yong-Lu_Li1)
  - **Affiliations:** Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai Jiao Tong University
  - **TL;DR:** This study introduces Low-Rank Similarity Mining (LoRS) for effective multimodal dataset distillation, focusing on image-text pairs and addressing the challenges of modality correspondence. The proposed method significantly enhances existing algorithms and contributes to the field of visual-language dataset distillation.
  - **Keywords:** multimodal dataset distillation, image-text pairs, Low-Rank Similarity Mining (LoRS), low-rank factorization, visual-language dataset distillation, challenges in distilling multimodal data, modality correspondence, high sample variance, significant improvement over existing algorithms, image-text contrastive learning (ITC), vision-language pre-training models (VLP), multimodal large language models (MLLM)


- [Adaptive Horizon Actor-Critic for Policy Learning in Contact-Rich Differentiable Simulation](https://icml.cc/virtual/2024/poster/35115) (Poster)
  - **Authors:** [Ignat Georgiev](http://openreview.net/profile?id=~Ignat_Georgiev1), [Krishnan Srinivasan](http://openreview.net/profile?id=~Krishnan_Srinivasan1), [Jie Xu](http://openreview.net/profile?id=~Jie_Xu7), [Eric Heiden](http://openreview.net/profile?id=~Eric_Heiden1), [Animesh Garg](http://openreview.net/profile?id=~Animesh_Garg1)
  - **Affiliations:** Georgia Institute of Technology, Stanford University, Nvidia, Nvidia, Georgia Institute of Technology; University of Toronto
  - **TL;DR:** This paper introduces the Adaptive Horizon Actor-Critic (AHAC) algorithm, which enhances policy learning in contact-rich environments by adapting the model-based horizon to mitigate gradient errors. The empirical results demonstrate that AHAC significantly outperforms traditional Model-Free Reinforcement Learning methods, achieving 40% more reward in locomotion tasks while improving efficiency in high-dimensional control settings.
  - **Keywords:** Model-Free Reinforcement Learning (MFRL), First-Order Model-Based Reinforcement Learning (FO-MBRL), differentiable simulation, Adaptive Horizon Actor-Critic (AHAC), policy gradient theorem, locomotion tasks, continuous motor control, high gradient variance, sampling error, stiff dynamics, suboptimal policies, improved reward performance, efficient scaling to high-dimensional control environments


- [How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?](https://icml.cc/virtual/2024/poster/34435) (Poster)
  - **Authors:** [Hongkang Li](http://openreview.net/profile?id=~Hongkang_Li1), [Meng Wang](http://openreview.net/profile?id=~Meng_Wang4), [Songtao Lu](http://openreview.net/profile?id=~Songtao_Lu1), [Xiaodong Cui](http://openreview.net/profile?id=~Xiaodong_Cui1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1)
  - **Affiliations:** Department of Electrical, Computer, and System Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA, Department of Electrical, Computer, and System Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA
  - **TL;DR:** This paper provides a theoretical analysis of how Transformers can be trained to achieve in-context learning (ICL) capabilities, focusing on the dynamics of nonlinear self-attention and MLPs. It quantifies the impact of various factors on ICL performance and demonstrates that proper model pruning minimally affects ICL while reducing inference costs.
  - **Keywords:** In-Context Learning (ICL), Transformer Models, Nonlinear Self-Attention, Nonlinear MLP, Model Pruning, Binary Classification Tasks, Large Language Models (LLMs), Nonconvex Training Problems, Data Distribution Shifts, Theoretical Analysis of Training Dynamics, Impact of Model Components on ICL Performance, Transformers, ICL, Magnitude-Based Pruning


- [Controllable Prompt Tuning For Balancing Group Distributional Robustness](https://icml.cc/virtual/2024/poster/34157) (Poster)
  - **Authors:** [Hoang Phan](http://openreview.net/profile?id=~Hoang_Phan1), [Andrew Wilson](http://openreview.net/profile?id=~Andrew_Gordon_Wilson1), [Qi Lei](http://openreview.net/profile?id=~Qi_Lei1)
  - **Affiliations:** New York University, New York University, New York University
  - **TL;DR:** This study introduces Controllable Prompt Tuning (CPT) to enhance model performance across different groups while addressing the challenges of distribution shifts and spurious correlations. The proposed method achieves state-of-the-art results with minimal tunable parameters, demonstrating effective balancing of group performance.
  - **Keywords:** distributional robustness, group performance, optimization, Controllable Prompt Tuning (CPT), Group Distributionally Robust Optimization (GroupDRO), spurious correlation benchmarks, multimodal data, performance degradation under distribution shifts, spurious correlation, model overfitting, state-of-the-art results, balancing mechanism, Waterbirds benchmark, ResNet50


- [Position: Scarce Resource Allocations That Rely On Machine Learning Should Be Randomized](https://icml.cc/virtual/2024/poster/35038) (Poster)
  - **Authors:** [Shomik Jain](http://openreview.net/profile?id=~Shomik_Jain1), [Kathleen A. Creel](http://openreview.net/profile?id=~Kathleen_Creel1), [Ashia Wilson](http://openreview.net/profile?id=~Ashia_Camage_Wilson1)
  - **Affiliations:** Institute for Data, Systems, and Society, MIT; Department of Electrical Engineering and Computer Science, MIT, Department of Philosophy & Religion and Khoury College of Computer Sciences, Northeastern University, Institute for Data, Systems, and Society, MIT; Department of Electrical Engineering and Computer Science, MIT
  - **TL;DR:** This paper argues that fairly allocating scarce resources using machine learning often requires randomness to respect individuals' claims to resources. It highlights the limitations of deterministic algorithms in achieving fairness and proposes stochastic methods to improve decision-making in resource allocation contexts.
  - **Keywords:** algorithmic fairness, resource allocation, randomness, stochastic procedures, job allocation, healthcare resource allocation, algorithmic bias, predictive multiplicity, fairness in decision-making


- [Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems](https://icml.cc/virtual/2024/poster/33469) (Poster)
  - **Authors:** [Bonan Zhang](http://openreview.net/profile?id=~Bonan_Zhang1), [Chia-Yu Chen](http://openreview.net/profile?id=~Chia-Yu_Chen2), [Naveen Verma](http://openreview.net/profile?id=~Naveen_Verma1)
  - **Affiliations:** Princeton University, NJ, USA; EnCharge AI, CA, USA, Princeton University, NJ, USA; EnCharge AI, CA, USA, Princeton University, NJ, USA; EnCharge AI, CA, USA
  - **TL;DR:** This study introduces RAOQ, a method designed to mitigate quantization errors in in-memory computing systems by adjusting activation and weight statistics and adapting AI models for better tolerance to quantization. The proposed techniques demonstrate high performance across various neural network models in computer vision and NLP tasks, achieving state-of-the-art results.
  - **Keywords:** In-memory computing, Quantization-aware training, Activation-shifting approach (A-shift), Weight reshaping technique (W-reshape), Bit augmentation method (BitAug), Low-rank approximation (ADC-LoRA), Computer vision, Natural language processing (NLP), Quantization errors, Data movement challenges, Computational efficiency, RAOQ (Reshape and Adapt for Output Quantization), State-of-the-art results in IMC implementations, Analog-to-digital converters (ADCs), IMC parallelism


- [Position: Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?](https://icml.cc/virtual/2024/poster/35050) (Spotlight Poster)
  - **Authors:** [Shayne Longpre](http://openreview.net/profile?id=~Shayne_Longpre1), [Robert Mahari](http://openreview.net/profile?id=~Robert_Mahari1), [Naana Obeng-Marnu](http://openreview.net/profile?id=~Naana_Obeng-Marnu1), [William Brannon](http://openreview.net/profile?id=~William_Brannon1), [Tobin South](http://openreview.net/profile?id=~Tobin_South1), [Katy Gero](http://openreview.net/profile?id=~Katy_Ilonka_Gero1), [Alex Pentland](http://openreview.net/profile?id=~Alex_Pentland1), [Jad Kabbara](http://openreview.net/profile?id=~Jad_Kabbara1)
  - **Affiliations:** Media Lab, Massachusetts Institute of Technology, Cambridge, USA; MIT Center for Constructive Communication, Cambridge, USA, Media Lab, Massachusetts Institute of Technology, Cambridge, USA; MIT Center for Constructive Communication, Cambridge, USA, Media Lab, Massachusetts Institute of Technology, Cambridge, USA; MIT Center for Constructive Communication, Cambridge, USA, Media Lab, Massachusetts Institute of Technology, Cambridge, USA; MIT Center for Constructive Communication, Cambridge, USA, Media Lab, Massachusetts Institute of Technology, Cambridge, USA; MIT Center for Constructive Communication, Cambridge, USA, Harvard University, Cambridge, USA, Media Lab, Massachusetts Institute of Technology, Cambridge, USA, Media Lab, Massachusetts Institute of Technology, Cambridge, USA; MIT Center for Constructive Communication, Cambridge, USA
  - **TL;DR:** The paper addresses the critical issues of data authenticity, consent, and provenance in the development of foundation models, highlighting the ethical and legal challenges arising from current data collection practices. It calls for the establishment of universal data provenance standards to facilitate responsible AI development and mitigate risks associated with under-documented training data.
  - **Keywords:** Data authenticity, Consent, Provenance, Foundation models, Generative AI, Consumer technologies, Tracing authenticity, Verifying consent, Preserving privacy, Addressing representation and bias, Copyright issues, Responsible foundation model development practices, Universal data provenance standards, LAION-5B dataset, HuggingFace, Foundation models, Generative AI, Ethical and trustworthy AI, Data transparency


- [Conditionally-Conjugate Gaussian Process Factor Analysis for Spike Count Data via Data Augmentation](https://icml.cc/virtual/2024/poster/32621) (Poster)
  - **Authors:** [Yididiya Nadew](http://openreview.net/profile?id=~Yididiya_Y._Nadew1), [Xuhui Fan](http://openreview.net/profile?id=~Xuhui_Fan1), [Christopher J Quinn](http://openreview.net/profile?id=~Christopher_John_Quinn1)
  - **Affiliations:** Department of Computer Science, Iowa State University, Ames, IA, USA, School of Computing, Macquarie University, Sydney, NSW, Australia, Department of Computer Science, Iowa State University, Ames, IA, USA; School of Computing, Macquarie University, Sydney, NSW, Australia
  - **TL;DR:** This study introduces a conditionally-conjugate Gaussian process factor analysis (ccGPFA) method for modeling spike count data, addressing the intractability of inference due to non-conjugacy. The proposed method enables tractable inference through data augmentation and offers scalable solutions using sparse Gaussian Processes.
  - **Keywords:** Gaussian process factor analysis, neural recordings, latent variable modeling, conditionally-conjugate Gaussian process factor analysis (ccGPFA), variational EM algorithm, data augmentation, neuroscience, neural activity modeling, intractable inference, non-conjugacy of likelihood, high-dimensional data, analytically and computationally tractable inference, closed-form updates, scalable models using sparse Gaussian Processes, Gaussian processes, Poisson distributions, negative binomial distributions, latent variable models


- [A Neural-Preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions](https://icml.cc/virtual/2024/poster/33924) (Poster)
  - **Authors:** [Kai Weixian Lan](http://openreview.net/profile?id=~Kai_Weixian_Lan1), [Elias Gueidon](http://openreview.net/profile?id=~Elias_Gueidon1), [Ayano Kaneda](http://openreview.net/profile?id=~Ayano_Kaneda1), [Julian Panetta](http://openreview.net/profile?id=~Julian_Panetta1), [Joseph Teran](http://openreview.net/profile?id=~Joseph_Teran1)
  - **Affiliations:** University of California, Davis, USA, University of California, Los Angeles, USA, Waseda University, Tokyo, Japan, University of California, Davis, USA, University of California, Davis, USA
  - **TL;DR:** This paper presents a neural-preconditioned iterative solver for Poisson equations with mixed Dirichlet and Neumann boundary conditions, addressing the challenges of ill-conditioned linear systems and evolving domains. The proposed method outperforms existing techniques, including algebraic multigrid and previous neural preconditioners, demonstrating its effectiveness in incompressible fluid simulations.
  - **Keywords:** neural preconditioning, Poisson equations, mixed boundary conditions, neural network architecture, iterative solver, preconditioned conjugate gradient (PCG), multigrid, incompressible fluid simulations, scientific computing, ill-conditioned linear systems, evolving boundaries, costly setup phases, efficient approximation of the inverse of the discrete Laplacian, lightweight neural network for fast inference, discrete Laplacian, elliptic PDEs


- [What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding](https://icml.cc/virtual/2024/poster/33179) (Poster)
  - **Authors:** [Hongkang Li](http://openreview.net/profile?id=~Hongkang_Li1), [Meng Wang](http://openreview.net/profile?id=~Meng_Wang4), [Tengfei Ma](http://openreview.net/profile?id=~Tengfei_Ma1), [Sijia Liu](http://openreview.net/profile?id=~Sijia_Liu1), [Zaixi Zhang](http://openreview.net/profile?id=~ZAIXI_ZHANG2), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1)
  - **Affiliations:** Department of Electrical, Computer, and System Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA, Department of Electrical, Computer, and System Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA, Department of Biomedical Informatics, Stony Brook University, Stony Brook, NY, USA, Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; MIT-IBM Watson AI Lab, IBM Research, MA, USA, Department of Computer Science and Engineering, University of Science and Technology of China, Hefei, Anhui, China, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA
  - **TL;DR:** This study theoretically investigates the generalization capabilities of Graph Transformers in semi-supervised node classification, revealing that self-attention and positional encoding enhance generalization by promoting sparsity in attention maps. The findings provide a quantitative framework for understanding sample complexity and convergence in graph learning tasks.
  - **Keywords:** Graph Transformers, Generalization, Theoretical Investigation, Self-attention, Positional Encoding, Stochastic Gradient Descent (SGD), Semi-supervised Node Classification, Graph Learning Tasks, Sample Complexity, Generalization Error, Non-convex Interactions, Quantitative Characterization of Sample Complexity, Attention Map Sparsity, Graph Neural Networks (GNN), Graph Convolutional Networks (GCN)


- [Do Transformer World Models Give Better Policy Gradients?](https://icml.cc/virtual/2024/poster/33932) (Poster)
  - **Authors:** [Michel Ma](http://openreview.net/profile?id=~Michel_Ma1), [Tianwei Ni](http://openreview.net/profile?id=~Tianwei_Ni1), [Clement Gehring](http://openreview.net/profile?id=~Clement_Gehring1), [Pierluca D'Oro](http://openreview.net/profile?id=~Pierluca_D%27Oro1), [Pierre-Luc Bacon](http://openreview.net/profile?id=~Pierre-Luc_Bacon1)
  - **Affiliations:** Mila, Montreal, Canada; Department of Computer Science, University of Montreal, Canada, Mila, Montreal, Canada; Department of Computer Science, University of Montreal, Canada, Mila, Montreal, Canada; Department of Computer Science, University of Montreal, Canada, Mila, Montreal, Canada; Department of Computer Science, University of Montreal, Canada, Mila, Montreal, Canada; Department of Computer Science, University of Montreal, Canada
  - **TL;DR:** This study investigates whether transformer world models improve policy gradients in reinforcement learning. The findings reveal that conditioning on a sequence of actions rather than the full history leads to better policy performance and more stable gradient propagation.
  - **Keywords:** reinforcement learning, world models, policy optimization, transformers, Action-conditioned World Models (AWMs), backpropagation, long-horizon policy gradients, hard-to-optimize loss landscapes, stability in differentiating through world models, better policy gradients, easier optimization landscapes


- [PerceptAnon: Exploring the Human Perception of Image Anonymization Beyond Pseudonymization for GDPR](https://icml.cc/virtual/2024/poster/35031) (Poster)
  - **Authors:** [Kartik Patwari](http://openreview.net/profile?id=~Kartik_Patwari1), [Chen-Nee Chuah](http://openreview.net/profile?id=~Chen-Nee_Chuah1), [Lingjuan Lyu](http://openreview.net/profile?id=~Lingjuan_Lyu1), [Vivek Sharma](http://openreview.net/profile?id=~Vivek_Sharma1)
  - **Affiliations:** None, Department of Electrical and Computer Engineering, University of California Davis, CA, USA, Sony AI, Sony AI
  - **TL;DR:** This study explores the limitations of current image anonymization techniques focused on localized pseudonymization and introduces PerceptAnon, a learning-based metric that evaluates the privacy vulnerabilities of anonymized images in accordance with GDPR. The findings highlight the importance of considering the entire image context for effective privacy protection.
  - **Keywords:** image anonymization, privacy assessment, GDPR compliance, PerceptAnon metric, learning-based evaluation, data privacy, image processing, privacy vulnerabilities, identifiable features, localized pseudonymization, comprehensive evaluation of anonymized images, dataset for assessing anonymization, pseudonymization, anonymization, Personally Identifiable Information (PII)


- [Partially Stochastic Infinitely Deep Bayesian Neural Networks](https://icml.cc/virtual/2024/poster/33312) (Poster)
  - **Authors:** [Sergio Calvo Ordoñez](http://openreview.net/profile?id=~Sergio_Calvo_Ordo%C3%B1ez1), [Matthieu Meunier](http://openreview.net/profile?id=~Matthieu_Meunier1), [Francesco Piatti](http://openreview.net/profile?id=~Francesco_Piatti1), [Yuantao Shi](http://openreview.net/profile?id=~YUANTAO_SHI1)
  - **Affiliations:** Oxford-Man Institute of Quantitative Finance, University of Oxford; Mathematical Institute, University of Oxford, Mathematical Institute, University of Oxford, Department of Mathematics, Imperial College London, Oxford-Man Institute of Quantitative Finance, University of Oxford; Mathematical Institute, University of Oxford
  - **TL;DR:** This paper introduces Partially Stochastic Infinitely Deep Bayesian Neural Networks, a new architecture that enhances computational efficiency while maintaining the benefits of Bayesian approaches. Empirical evaluations demonstrate improved performance and uncertainty quantification compared to existing models.
  - **Keywords:** Bayesian Neural Networks, Infinite-depth neural networks, Partial stochasticity, Neural ODEs, Neural SDEs, Bayesian inference, Financial forecasting, Healthcare, Computational complexity, Model uncertainty, Universal Conditional Distribution Approximators, Improved computational efficiency


- [Do Topological Characteristics Help in Knowledge Distillation?](https://icml.cc/virtual/2024/poster/35096) (Poster)
  - **Authors:** [Jungeun Kim](http://openreview.net/profile?id=~Jungeun_Kim2), [Junwon You](http://openreview.net/profile?id=~Junwon_You1), [Dongjin Lee](http://openreview.net/profile?id=~Dongjin_Lee3), [Ha Young Kim](http://openreview.net/profile?id=~Ha_Young_Kim2), [Jae-Hun Jung](http://openreview.net/profile?id=~Jae-Hun_Jung1)
  - **Affiliations:** Department of AI, Yonsei University, Seoul, South Korea, Department of Mathematics, POSTECH, Pohang, South Korea, Graduate School of AI, POSTECH, Pohang, South Korea, Graduate School of Information, Yonsei University, Seoul, South Korea, Graduate School of AI, POSTECH, Pohang, South Korea
  - **TL;DR:** This study introduces TopKD, a novel knowledge distillation method that leverages global topology of latent spaces to enhance knowledge transfer from larger to smaller networks. The findings demonstrate the effectiveness of using topological characteristics to improve the performance of student models in deep learning applications.
  - **Keywords:** Knowledge Distillation, Model Compression, TopKD, Persistence Diagram (PD), Topology Distillation Loss, Deep Learning, Neural Networks, Efficient transfer of knowledge, Relationships in complex latent spaces, Novel KD method, Benefits of global topology in knowledge transfer


- [Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning](https://icml.cc/virtual/2024/poster/33318) (Poster)
  - **Authors:** [Libin Zhu](http://openreview.net/profile?id=~Libin_Zhu1), [Chaoyue Liu](http://openreview.net/profile?id=~Chaoyue_Liu2), [Adityanarayanan Radhakrishnan](http://openreview.net/profile?id=~Adityanarayanan_Radhakrishnan1), [Misha Belkin](http://openreview.net/profile?id=~Mikhail_Belkin1)
  - **Affiliations:** Department of Computer Science, UCSD; Halicioğlu Data Science Institute, UCSD, Halicioğlu Data Science Institute, UCSD, Harvard University; Broad Institute of MIT and Harvard, Department of Computer Science, UCSD; Halicioğlu Data Science Institute, UCSD
  - **TL;DR:** This study investigates the phenomenon of spikes in training loss during neural network training with stochastic gradient descent (SGD) and explains how these spikes, termed "catapults," enhance generalization by promoting feature learning. The findings suggest that using smaller batch sizes in SGD leads to more frequent catapults, resulting in improved model performance.
  - **Keywords:** Stochastic Gradient Descent (SGD), training loss spikes, generalization, Catapult dynamics, Average Gradient Outer Product (AGOP), Deep learning, neural networks, Spikes in training loss, generalization performance, Improved generalization through feature learning, optimization phenomena, Gradient Descent (GD), learning rates


- [ProtoGate: Prototype-based Neural Networks with Global-to-local Feature Selection for Tabular Biomedical Data](https://icml.cc/virtual/2024/poster/35215) (Poster)
  - **Authors:** [Xiangjian Jiang](http://openreview.net/profile?id=~Xiangjian_Jiang1), [Andrei Margeloiu](http://openreview.net/profile?id=~Andrei_Margeloiu1), [Nikola Simidjievski](http://openreview.net/profile?id=~Nikola_Simidjievski1), [Mateja Jamnik](http://openreview.net/profile?id=~Mateja_Jamnik1)
  - **Affiliations:** Department of Computer Science and Technology, University of Cambridge, UK; Department of Oncology, University of Cambridge, UK, Department of Computer Science and Technology, University of Cambridge, UK; Department of Oncology, University of Cambridge, UK, Department of Computer Science and Technology, University of Cambridge, UK; Department of Oncology, University of Cambridge, UK, Department of Computer Science and Technology, University of Cambridge, UK
  - **TL;DR:** The study introduces ProtoGate, a prototype-based neural model designed for feature selection in high-dimensional low-sample-size biomedical data. It effectively balances global and local feature selection while addressing the co-adaptation problem, resulting in improved prediction accuracy and explainability compared to existing methods.
  - **Keywords:** tabular biomedical data, feature selection, machine learning, prototype-based neural model, non-parametric prediction, biomedical research, clinical trials, high-dimensional low-sample-size (HDLSS) data, co-adaptation problem, local feature selection, ProtoGate model, explainable predictions, high-fidelity feature selection, synthetic datasets, real-world datasets, global-to-local feature selection, instance-wise features, explainability


- [Deep Functional Factor Models: Forecasting High-Dimensional Functional Time Series via Bayesian Nonparametric Factorization](https://icml.cc/virtual/2024/poster/33562) (Poster)
  - **Authors:** [Yirui Liu](http://openreview.net/profile?id=~Yirui_Liu1), [Xinghao Qiao](http://openreview.net/profile?id=~Xinghao_Qiao1), [Yulong Pei](http://openreview.net/profile?id=~Yulong_Pei1), [Liying Wang](http://openreview.net/profile?id=~Liying_Wang3)
  - **Affiliations:** J.P. Morgan; London School of Economics and Political Science, Faculty of Business and Economics, The University of Hong Kong, J.P. Morgan, Management School, University of Liverpool
  - **TL;DR:** This paper presents the Deep Functional Factor Model (DF2M), a Bayesian nonparametric approach for analyzing high-dimensional functional time series that captures nonlinear and non-Markovian dynamics. DF2M demonstrates superior predictive accuracy and explainability compared to traditional deep learning models, making it particularly useful in fields like finance and healthcare.
  - **Keywords:** high-dimensional functional time series, explainability, Deep Functional Factor Model (DF2M), Bayesian nonparametric model, Indian Buffet Process, multi-task Gaussian Process, deep kernel function, finance, healthcare, climate change, high-dimensional data, nonlinear dynamics, non-Markovian dynamics, temporal dependence, improved predictive accuracy, explainable neural networks


- [Neuro-Visualizer: A Novel Auto-Encoder-Based Loss Landscape Visualization Method With an Application in Knowledge-Guided Machine Learning](https://icml.cc/virtual/2024/poster/33809) (Poster)
  - **Authors:** [Mohannad Elhamod](http://openreview.net/profile?id=~Mohannad_Elhamod1), [Anuj Karpatne](http://openreview.net/profile?id=~Anuj_Karpatne1)
  - **Affiliations:** Department of Computer Science, Virginia Tech, Blacksburg, VA, USA, Department of Computer Science, Virginia Tech, Blacksburg, VA, USA
  - **TL;DR:** This paper introduces Neuro-Visualizer, a novel auto-encoder-based method for visualizing the loss landscape of neural networks, addressing limitations of linear methods. The findings demonstrate that Neuro-Visualizer outperforms existing techniques and provides valuable insights into neural network training and generalization.
  - **Keywords:** loss landscape visualization, neural networks, knowledge-guided machine learning, auto-encoder, principal component analysis (PCA), high-dimensional landscape representation, training trajectory visualization, Neuro-Visualizer, non-linear landscape visualization method


- [Learning High-Frequency Functions Made Easy with Sinusoidal Positional Encoding](https://icml.cc/virtual/2024/poster/32975) (Poster)
  - **Authors:** [Chuanhao Sun](http://openreview.net/profile?id=~Chuanhao_Sun2), [Zhihang Yuan](http://openreview.net/profile?id=~Zhihang_Yuan4), [Kai Xu](http://openreview.net/profile?id=~Kai_Xu4), [Luo Mai](http://openreview.net/profile?id=~Luo_Mai1), [Siddharth N](http://openreview.net/profile?id=~Siddharth_N1), [Shuo Chen](http://openreview.net/profile?id=~Shuo_Chen17), [Mahesh Marina](http://openreview.net/profile?id=~Mahesh_K._Marina1)
  - **Affiliations:** The University of Edinburgh, Edinburgh, UK, The University of Edinburgh, Edinburgh, UK, MIT-IBM Watson AI Lab, Cambridge, MA, US, The University of Edinburgh, Edinburgh, UK, The University of Edinburgh, Edinburgh, UK, The University of Edinburgh, Edinburgh, UK, The University of Edinburgh, Edinburgh, UK; MIT-IBM Watson AI Lab, Cambridge, MA, US
  - **TL;DR:** This study introduces sinusoidal positional encoding (SPE) as a method to efficiently learn adaptive frequency features for high-frequency functions without the need for manual hyperparameter tuning. SPE demonstrates improved fidelity and faster training across various tasks, including 3D view synthesis and Text-to-Speech generation.
  - **Keywords:** high-frequency functions, positional encoding, sinusoidal positional encoding (SPE), Fourier series regression, multilayer perceptrons (MLPs), 3D view synthesis, Text-to-Speech generation, 1D regression, manual hyperparameter tuning, learning high-frequency functions, data limitations, adaptive frequency features, enhanced fidelity, faster training, Fourier features, neural radiance fields (NeRFs)


- [Discovering Bias in Latent Space: An Unsupervised Debiasing Approach](https://icml.cc/virtual/2024/poster/33523) (Poster)
  - **Authors:** [Dyah Adila](http://openreview.net/profile?id=~Dyah_Adila1), [Shuai Zhang](http://openreview.net/profile?id=~Shuai_Zhang7), [Boran Han](http://openreview.net/profile?id=~Boran_Han1), [Yuyang Wang](http://openreview.net/profile?id=~Bernie_Wang1)
  - **Affiliations:** Department of Computer Science, University of Wisconsin-Madison, Amazon Web Services, Amazon Web Services, Amazon Web Services
  - **TL;DR:** This study introduces STEER FAIR, an unsupervised approach to mitigate bias in question-answering models by steering activation values away from identified bias directions. The method significantly reduces performance variance across prompt modifications and outperforms supervised baselines in accuracy.
  - **Keywords:** Bias in machine learning, Question-answering systems, Unsupervised debiasing, STEER FAIR, Large Language Models, Vision-Language Models, Multi-modal settings, Sensitivity to prompt variations, Model bias, Performance instability, Reduction in performance variance, Identification of bias directions, ScienceQA, Visual Genome Relation


- [Diffusion Posterior Sampling is Computationally Intractable](https://icml.cc/virtual/2024/poster/32856) (Poster)
  - **Authors:** [Shivam Gupta](http://openreview.net/profile?id=~Shivam_Gupta1), [Ajil Jalal](http://openreview.net/profile?id=~Ajil_Jalal1), [Aditya Parulekar](http://openreview.net/profile?id=~Aditya_Parulekar1), [Eric Price](http://openreview.net/profile?id=~Eric_Price1), [Zhiyang Xun](http://openreview.net/profile?id=~Zhiyang_Xun1)
  - **Affiliations:** Department of Computer Science, University of Texas at Austin, Department of Electrical Engineering and Computer Science, University of California, Berkeley, Department of Computer Science, University of Texas at Austin, Department of Computer Science, University of Texas at Austin, Department of Computer Science, University of Texas at Austin
  - **TL;DR:** This paper demonstrates that posterior sampling using diffusion models is computationally intractable under the assumption that one-way functions exist, indicating that all algorithms may take superpolynomial time. It also presents the exponential-time rejection sampling algorithm as essentially optimal under stronger assumptions regarding one-way functions.
  - **Keywords:** Diffusion models, Posterior sampling, SDE (Stochastic Differential Equations), ODE (Ordinary Differential Equations), Score matching, Inpainting, Super-resolution, MRI reconstruction, Computational intractability, Superpolynomial time complexity, Exponential-time rejection sampling algorithm, Heuristic approximation methods, One-way functions, Smoothed scores


- [Centralized Selection with Preferences in the Presence of Biases](https://icml.cc/virtual/2024/poster/34807) (Poster)
  - **Authors:** [L. Elisa Celis](http://openreview.net/profile?id=~L._Elisa_Celis2), [Amit Kumar](http://openreview.net/profile?id=~Amit_Kumar7), [Nisheeth K. Vishnoi](http://openreview.net/profile?id=~Nisheeth_K._Vishnoi2), [Shangyu Andrew Xu](http://openreview.net/profile?id=~Shangyu_Andrew_Xu1)
  - **Affiliations:** Yale University, IIT Delhi, Yale University, Yale University
  - **TL;DR:** This paper addresses the centralized selection problem in contexts like education and employment, focusing on maximizing utility while considering candidate preferences amidst biased utility estimates. The authors propose an algorithm that achieves near-optimal group fairness and nearly maximizes true utility, validated through empirical studies.
  - **Keywords:** Centralized selection, Candidate preferences, Utility maximization, Gale-Shapley algorithm, Education, Employment, Centralized admissions systems, Biased utility estimates, Group fairness, Preference discrepancies, Near-optimal group fairness, Maximization of true utility


- [Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration](https://icml.cc/virtual/2024/poster/34629) (Poster)
  - **Authors:** [Zhongzhi Yu](http://openreview.net/profile?id=~Zhongzhi_Yu1), [Zheng Wang](http://openreview.net/profile?id=~Zheng_Wang38), [Yonggan Fu](http://openreview.net/profile?id=~Yonggan_Fu1), [Shi Huihong](http://openreview.net/profile?id=~Huihong_Shi1), [Khalid Shaikh](http://openreview.net/profile?id=~Khalid_Shaikh1), [Yingyan (Celine) Lin](http://openreview.net/profile?id=~Yingyan_Celine_Lin1)
  - **Affiliations:** Georgia Institute of Technology, Georgia Institute of Technology, Georgia Institute of Technology, Georgia Institute of Technology, Georgia Institute of Technology, Georgia Institute of Technology
  - **TL;DR:** This study investigates the phenomenon of attention sinks in large language models (LLMs) and proposes a training-free Attention Calibration Technique (ACT) to optimize attention distributions during inference. The findings demonstrate that ACT can enhance the accuracy of LLMs by an average of up to 7.30% across various datasets.
  - **Keywords:** Large Language Models, Attention Mechanism, Attention Calibration Technique (ACT), Natural Language Processing, Attention Sinks, Attention Distributions, Optimization of Attention Distributions, Accuracy Improvement, Llama-30B, Attention Sinks, Inference


- [Compositional Text-to-Image Generation with Dense Blob Representations](https://icml.cc/virtual/2024/poster/33559) (Poster)
  - **Authors:** [Weili Nie](http://openreview.net/profile?id=~Weili_Nie1), [Sifei Liu](http://openreview.net/profile?id=~Sifei_Liu2), [Morteza Mardani](http://openreview.net/profile?id=~Morteza_Mardani1), [Chao Liu](http://openreview.net/profile?id=~Chao_Liu11), [Benjamin Eckart](http://openreview.net/profile?id=~Benjamin_Eckart1), [Arash Vahdat](http://openreview.net/profile?id=~Arash_Vahdat3)
  - **Affiliations:** NVIDIA Corporation, NVIDIA Corporation, NVIDIA Corporation, NVIDIA Corporation, NVIDIA Corporation, NVIDIA Corporation
  - **TL;DR:** This study introduces BlobGEN, a blob-grounded text-to-image diffusion model that enhances the generation of images from complex text prompts by utilizing dense blob representations. The method demonstrates improved controllability and accuracy in image generation tasks, particularly when augmented by large language models.
  - **Keywords:** text-to-image generation, compositional generation, blob-grounded text-to-image diffusion model, masked cross-attention module, image generation, visual representation, complex text prompts, fine-grained controllability, misunderstanding context, superior zero-shot generation quality, layout-guided controllability, numerical and spatial correctness, MS-COCO, dense blob representations, large language models (LLMs)


- [Symmetric Replay Training: Enhancing Sample Efficiency in Deep Reinforcement Learning for Combinatorial Optimization](https://icml.cc/virtual/2024/poster/33579) (Poster)
  - **Authors:** [Hyeonah Kim](http://openreview.net/profile?id=~Hyeonah_Kim1), [Minsu Kim](http://openreview.net/profile?id=~Minsu_Kim2), [Sungsoo Ahn](http://openreview.net/profile?id=~Sungsoo_Ahn1), [Jinkyoo Park](http://openreview.net/profile?id=~Jinkyoo_Park1)
  - **Affiliations:** Department of Industrial & Systems Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, Department of Industrial & Systems Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, Pohang University of Science and Technology (POSTECH), Pohang, South Korea, Department of Industrial & Systems Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; OMELET, Daejeon, South Korea
  - **TL;DR:** This study introduces symmetric replay training (SRT) to improve sample efficiency in deep reinforcement learning for combinatorial optimization problems, addressing the challenge of costly reward evaluations. Experimental results show that SRT consistently enhances sample efficiency across various DRL methods in real-world applications like molecular optimization and hardware design.
  - **Keywords:** Deep Reinforcement Learning, Combinatorial Optimization, Symmetric Replay Training (SRT), Reinforcement Learning, Molecular Optimization, Hardware Design, Sample Efficiency, Reward Evaluations, NP-hardness, Enhanced Sample Efficiency, Exploration of Symmetric Regions


- [Stochastic Optimization with Arbitrary Recurrent Data Sampling](https://icml.cc/virtual/2024/poster/34369) (Poster)
  - **Authors:** [William Powell](http://openreview.net/profile?id=~William_Powell1), [Hanbaek Lyu](http://openreview.net/profile?id=~Hanbaek_Lyu1)
  - **Affiliations:** Department of Mathematics, University of Wisconsin-Madison, WI, USA, Department of Mathematics, University of Wisconsin-Madison, WI, USA
  - **TL;DR:** This paper presents a stochastic optimization framework that guarantees optimal first-order convergence using recurrent data sampling without requiring additional properties like independence or reshuffling. The findings highlight the importance of the speed of recurrence in achieving convergence for non-convex and possibly non-smooth objective functions.
  - **Keywords:** Stochastic Optimization, Recurrent Data Sampling, Minimization by Incremental Surrogate Optimization (MISO), Markov Chain Monte-Carlo (MCMC), Decentralized Optimization, Distributed Non-Negative Matrix Factorization, Non-convex optimization, Non-smooth objective functions, Constraints, Optimal first-order convergence guarantees, Expected optimality gap convergence


- [Embodied CoT Distillation From LLM To Off-the-shelf Agents](https://icml.cc/virtual/2024/poster/34255) (Poster)
  - **Authors:** [Wonje Choi](http://openreview.net/profile?id=~Wonje_Choi2), [Woo Kyung Kim](http://openreview.net/profile?id=~Woo_Kyung_Kim1), [Minjong Yoo](http://openreview.net/profile?id=~Minjong_Yoo2), [Honguk Woo](http://openreview.net/profile?id=~Honguk_Woo1)
  - **Affiliations:** Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, Republic of Korea, Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, Republic of Korea, Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, Republic of Korea, Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, Republic of Korea
  - **TL;DR:** This study presents DEDER, a framework for distilling the reasoning capabilities of large language models into efficient small language model-based policies for embodied tasks. The approach effectively addresses the challenges of high computational requirements and decision-making in capacity-limited environments, demonstrating superior performance on the ALFRED benchmark.
  - **Keywords:** embodied AI, large language models (LLMs), decision-making, distillation, reasoning-policy, planning-policy, embodied in-context learning, self-verification, robotics, task planning, high computational requirements, capacity-limited devices, complex embodied tasks, DEDER framework, optimized plans, embodied knowledge graph, contrastively prompted attention model, ALFRED benchmark, small language models (sLMs), reinforcement learning (RL)


- [A General Framework for Sequential Decision-Making under Adaptivity Constraints](https://icml.cc/virtual/2024/poster/34424) (Poster)
  - **Authors:** [Nuoya Xiong](http://openreview.net/profile?id=~Nuoya_Xiong1), [Zhaoran Wang](http://openreview.net/profile?id=~Zhaoran_Wang1), [Zhuoran Yang](http://openreview.net/profile?id=~Zhuoran_Yang1)
  - **Affiliations:** IIIS, Tsinghua University, China, Department of Industrial Engineering and Management Sciences, Northwestern University, USA, Department of Statistics and Data Science, Yale University, USA
  - **TL;DR:** This paper introduces a framework for sequential decision-making under adaptivity constraints, focusing on rare policy switches and batch learning. It presents algorithms that achieve efficient performance while minimizing policy switching costs across various reinforcement learning models.
  - **Keywords:** Sequential decision-making, Reinforcement learning, Eluder Condition class, Rare policy switch, Batch learning, Recommender systems, Robotic control, Healthcare, Adaptivity constraints, Policy switching costs, Algorithms for rare policy switch and batch learning, Markov decision process (MDP), Low eluder dimension, Kernelized nonlinear regulator, Partially observed Markov decision process (POMDP)


- [How Does Goal Relabeling Improve Sample Efficiency?](https://icml.cc/virtual/2024/poster/34821) (Poster)
  - **Authors:** [Sirui Zheng](http://openreview.net/profile?id=~Sirui_Zheng2), [Chenjia Bai](http://openreview.net/profile?id=~Chenjia_Bai2), [Zhuoran Yang](http://openreview.net/profile?id=~Zhuoran_Yang1), [Zhaoran Wang](http://openreview.net/profile?id=~Zhaoran_Wang1)
  - **Affiliations:** Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL 60208, USA, Shanghai Artificial Intelligence Laboratory, Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA, Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL 60208, USA
  - **TL;DR:** This study investigates the theoretical underpinnings of goal relabeling in reinforcement learning, demonstrating that it can significantly improve sample efficiency through better hypothesis elimination. The authors introduce a new algorithm, GOALIVE, and a complexity measure that shows exponential improvements in sample complexity for goal-conditioned problems.
  - **Keywords:** reinforcement learning, goal relabeling, sample efficiency, hindsight experience replay, GOALIVE algorithm, goal-conditioned value functions, sparse rewards, vast state spaces, lack of meaningful feedback, goal-conditioned Bellman-Eluder dimension, polynomial sample complexity


- [DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems](https://icml.cc/virtual/2024/poster/35055) (Poster)
  - **Authors:** [Yair Schiff](http://openreview.net/profile?id=~Yair_Schiff1), [Zhong Yi Wan](http://openreview.net/profile?id=~Zhong_Yi_Wan1), [Jeffrey Parker](http://openreview.net/profile?id=~Jeffrey_B._Parker1), [Stephan Hoyer](http://openreview.net/profile?id=~Stephan_Hoyer1), [Volodymyr Kuleshov](http://openreview.net/profile?id=~Volodymyr_Kuleshov1), [Fei Sha](http://openreview.net/profile?id=~Fei_Sha3), [Leonardo Zepeda-Nunez](http://openreview.net/profile?id=~Leonardo_Zepeda-N%C3%BA%C3%B1ez1)
  - **Affiliations:** Department of Computer Sciences, Cornell Tech, New York, NY, USA, Google Research, Mountain View, CA, USA, Google Research, Mountain View, CA, USA, Google Research, Mountain View, CA, USA, Department of Computer Sciences, Cornell Tech, New York, NY, USA, Google Research, Mountain View, CA, USA, Google Research, Mountain View, CA, USA; Department of Mathematics, University of Wisconsin-Madison, WI, USA
  - **TL;DR:** This study introduces the Dynamics Stable Learning by Invariant Measure (DySLIM) framework, which effectively learns the invariant measure and dynamics of dissipative chaotic systems. The proposed method demonstrates improved tracking and statistical accuracy compared to traditional trajectory-based learning approaches, with potential applications in complex systems like weather and climate models.
  - **Keywords:** dynamics learning, chaotic systems, invariant measure, sample efficient objective, regularization term, weather modeling, climate modeling, fluid dynamics, molecular dynamics, quantum chemistry, plasma physics, instability in dynamics, divergence in trajectory learning, Dynamics Stable Learning by Invariant Measure (DySLIM), improved point-wise tracking, long-term statistical accuracy, Lyapunov exponents, ergodicity, attractor


- [Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization](https://icml.cc/virtual/2024/poster/33986) (Poster)
  - **Authors:** [Rui Li](http://openreview.net/profile?id=~Rui_Li16), [Chaozhuo Li](http://openreview.net/profile?id=~Chaozhuo_Li1), [Yanming Shen](http://openreview.net/profile?id=~Yanming_Shen1), [Zeyu Zhang](http://openreview.net/profile?id=~Zeyu_Zhang6), [Xu Chen](http://openreview.net/profile?id=~Xu_Chen13)
  - **Affiliations:** Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China, Key Laboratory of Trustworthy Distributed Computing and Service (MOE), Beijing University of Posts and Telecommunications, China, School of Computer Science and Technology, Dalian University of Technology, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China
  - **TL;DR:** This study introduces GoldE, a novel framework for knowledge graph embedding that utilizes universal orthogonal parameterization to enhance modeling capabilities by addressing dimensional and geometric limitations. GoldE demonstrates state-of-the-art performance on standard benchmarks, effectively capturing logical patterns and topological heterogeneity in knowledge graphs.
  - **Keywords:** Knowledge Graph Embedding, Orthogonal Relation Transformations, Universal Orthogonal Parameterization, Householder Reflection, Knowledge Graphs, Link Prediction, Incompleteness of Knowledge Graphs, Deficient Modeling Capability, GoldE Framework, State-of-the-Art Performance, Euclidean Geometry, Hyperbolic Geometry, Isometries, Logical Patterns, Topological Structures


- [Theory of Consistency Diffusion Models: Distribution Estimation Meets Fast Sampling](https://icml.cc/virtual/2024/poster/33055) (Poster)
  - **Authors:** [Zehao Dou](http://openreview.net/profile?id=~Zehao_Dou2), [Minshuo Chen](http://openreview.net/profile?id=~Minshuo_Chen1), [Mengdi Wang](http://openreview.net/profile?id=~Mengdi_Wang1), [Zhuoran Yang](http://openreview.net/profile?id=~Zhuoran_Yang1)
  - **Affiliations:** Department of Statistics and Data Science, Yale University, New Haven, US, Electrical and Computer Engineering, Princeton University, Princeton, US, Electrical and Computer Engineering, Princeton University, Princeton, US, Department of Statistics and Data Science, Yale University, New Haven, US
  - **TL;DR:** This paper presents a statistical theory for consistency models in diffusion processes, addressing the slow sample generation issue by formulating their training as a distribution discrepancy minimization problem. The findings indicate that consistency models can achieve fast sampling without sacrificing quality, matching the statistical estimation rates of traditional diffusion models.
  - **Keywords:** diffusion models, consistency models, statistical theory, distribution discrepancy minimization, Wasserstein distance, distillation, isolation, computer vision, audio generation, language generation, reinforcement learning, computational biology, slow sample generation, extensive number of steps, quality compromise in sampling speed, statistical estimation rates, training methods for consistency models, score neural network, Generative Adversarial Networks (GANs), AutoEncoders


- [From Biased Selective Labels to Pseudo-Labels: An Expectation-Maximization Framework for Learning from Biased Decisions](https://icml.cc/virtual/2024/poster/33393) (Poster)
  - **Authors:** [Trenton Chang](http://openreview.net/profile?id=~Trenton_Chang1), [Jenna Wiens](http://openreview.net/profile?id=~Jenna_Wiens1)
  - **Affiliations:** Division of Computer Science & Engineering, University of Michigan, Ann Arbor, MI, USA, Division of Computer Science & Engineering, University of Michigan, Ann Arbor, MI, USA
  - **TL;DR:** This study addresses the issue of disparate censorship in selective labeling, proposing the Disparate Censorship Expectation-Maximization (DCEM) algorithm to mitigate bias in machine learning models. The results demonstrate that DCEM improves bias mitigation without sacrificing discriminative performance, validated through synthetic and clinical data.
  - **Keywords:** selective labels, disparate censorship, machine learning bias, Disparate Censorship Expectation-Maximization (DCEM), causal models, healthcare, sepsis classification, labeling biases, imputation of unlabeled individuals, inequity in diagnostic testing, bias mitigation, improved model performance (AUC), synthetic data, clinical data


- [BAT: Learning to Reason about Spatial Sounds with Large Language Models](https://icml.cc/virtual/2024/poster/33244) (Poster)
  - **Authors:** [Zhisheng Zheng](http://openreview.net/profile?id=~Zhisheng_Zheng1), [Puyuan Peng](http://openreview.net/profile?id=~Puyuan_Peng1), [Ziyang Ma](http://openreview.net/profile?id=~Ziyang_Ma3), [Xie Chen](http://openreview.net/profile?id=~Xie_Chen2), [Eunsol Choi](http://openreview.net/profile?id=~Eunsol_Choi1), [David Harwath](http://openreview.net/profile?id=~David_Harwath1)
  - **Affiliations:** Department of Computer Science, University of Texas at Austin, USA; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China, Department of Computer Science, University of Texas at Austin, USA, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China, Department of Computer Science, University of Texas at Austin, USA, Department of Computer Science, University of Texas at Austin, USA
  - **TL;DR:** This paper introduces BAT, a novel spatial audio-based large language model that integrates binaural sound perception with natural language reasoning to enhance spatial sound understanding. The study demonstrates BAT's superior performance in spatial sound perception and reasoning, addressing the challenges of interpreting complex spatial audio environments.
  - **Keywords:** Spatial sound reasoning, Large Language Models (LLMs), Spatial Audio Spectrogram Transformer (SPATIAL-AST), binaural acoustic scene analysis, Spatial audio perception, question answering, Lack of datasets for in-the-wild spatial sounds, challenges in spatial audio input handling, Development of BAT, synthesis of binaural audio dataset, SPATIAL SOUND QA, AudioSet, SoundSpaces 2.0, Sound Event Localization and Detection (SELD), binaural hearing


- [Rethinking Transformers in Solving POMDPs](https://icml.cc/virtual/2024/poster/33983) (Poster)
  - **Authors:** [Chenhao Lu](http://openreview.net/profile?id=~Chenhao_Lu1), [Ruizhe Shi](http://openreview.net/profile?id=~Ruizhe_Shi1), [Yuyao Liu](http://openreview.net/profile?id=~Yuyao_Liu1), [Kaizhe Hu](http://openreview.net/profile?id=~Kaizhe_Hu1), [Simon Du](http://openreview.net/profile?id=~Simon_Shaolei_Du1), [Huazhe Xu](http://openreview.net/profile?id=~Huazhe_Xu1)
  - **Affiliations:** IIIS, Tsinghua University, IIIS, Tsinghua University, IIIS, Tsinghua University, IIIS, Tsinghua University, University of Washington, IIIS, Tsinghua University; Shanghai Qi Zhi Institute; Shanghai AI Lab
  - **TL;DR:** This study critically examines the effectiveness of Transformers in solving Partially Observable Markov Decision Processes (POMDPs) and argues that they are inadequate for this task, even with large datasets. The authors propose the Deep Linear Recurrent Unit (LRU) as a more suitable alternative, demonstrating its superior performance in this context.
  - **Keywords:** Partially Observable Markov Decision Processes (POMDPs), Reinforcement Learning (RL), Transformers, Deep Linear Recurrent Unit (LRU), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), Partial observability, sample inefficiency, learning inductive biases, Introduction of LRU as an alternative to Transformers in POMDPs, Regular languages, Hidden Markov Models (HMMs)


- [RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis](https://icml.cc/virtual/2024/poster/32693) (Poster)
  - **Authors:** [Yao Mu](http://openreview.net/profile?id=~Yao_Mu1), [Junting Chen](http://openreview.net/profile?id=~Junting_Chen2), [Qing-Long Zhang](http://openreview.net/profile?id=~Qing-Long_Zhang1), [Shoufa Chen](http://openreview.net/profile?id=~Shoufa_Chen1), [Qiaojun Yu](http://openreview.net/profile?id=~Qiaojun_Yu1), [Chongjian GE](http://openreview.net/profile?id=~Chongjian_GE1), [Runjian Chen](http://openreview.net/profile?id=~Runjian_Chen1), [Zhixuan Liang](http://openreview.net/profile?id=~Zhixuan_Liang2), [Mengkang Hu](http://openreview.net/profile?id=~Mengkang_Hu1), [Chaofan Tao](http://openreview.net/profile?id=~Chaofan_Tao1), [Peize Sun](http://openreview.net/profile?id=~Peize_Sun1), [Haibao Yu](http://openreview.net/profile?id=~Haibao_Yu2), [Chao Yang](http://openreview.net/profile?id=~Chao_Yang3), [WENQI SHAO](http://openreview.net/profile?id=~Wenqi_Shao2), [Wenhai Wang](http://openreview.net/profile?id=~Wenhai_Wang2), [Jifeng Dai](http://openreview.net/profile?id=~Jifeng_Dai1), [Yu Qiao](http://openreview.net/profile?id=~Yu_Qiao1), [Mingyu Ding](http://openreview.net/profile?id=~Mingyu_Ding1), [Ping Luo](http://openreview.net/profile?id=~Ping_Luo2)
  - **Affiliations:** Department of Computer Science, The University of Hong Kong, Hong Kong; None, OpenGVLab, Shanghai AI Laboratory; ETH Zurich, OpenGVLab, Shanghai AI Laboratory, Department of Computer Science, The University of Hong Kong, Hong Kong; None, Shanghai Jiao Tong University, Department of Computer Science, The University of Hong Kong, Hong Kong; None, Department of Computer Science, The University of Hong Kong, Hong Kong; OpenGVLab, Shanghai AI Laboratory, Department of Computer Science, The University of Hong Kong, Hong Kong; None, Department of Computer Science, The University of Hong Kong, Hong Kong; OpenGVLab, Shanghai AI Laboratory, Department of Computer Science, The University of Hong Kong, Hong Kong; None, Department of Computer Science, The University of Hong Kong, Hong Kong; None, Department of Computer Science, The University of Hong Kong, Hong Kong; None, OpenGVLab, Shanghai AI Laboratory; None, OpenGVLab, Shanghai AI Laboratory; None, OpenGVLab, Shanghai AI Laboratory; The Chinese University of Hong Kong, OpenGVLab, Shanghai AI Laboratory; Tsinghua University, OpenGVLab, Shanghai AI Laboratory; Tsinghua University, UC Berkeley, Department of Computer Science, The University of Hong Kong, Hong Kong; None
  - **TL;DR:** This paper presents RoboCodeX, a tree-structured multimodal code generation framework designed for robotic behavior synthesis, which effectively translates high-level human instructions into detailed robotic actions. Extensive experiments show that RoboCodeX achieves state-of-the-art performance across various manipulation and navigation tasks in both simulators and real robots.
  - **Keywords:** Robotic behavior synthesis, Embodied AI, Multimodal code generation, Tree-structured reasoning, Robotics, Manipulation tasks, Navigation tasks, Generalization across various scenarios, Mapping conceptual understanding to robotic actions, State-of-the-art performance in manipulation and navigation tasks, Iterative refining methodology, Specialized multimodal reasoning dataset, Multimodal large language models, Physical constraints, Affordance


- [On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis](https://icml.cc/virtual/2024/poster/32779) (Poster)
  - **Authors:** [Jerry Yao-Chieh Hu](http://openreview.net/profile?id=~Jerry_Yao-Chieh_Hu1), [Thomas Lin](http://openreview.net/profile?id=~Thomas_Lin2), [Zhao Song](http://openreview.net/profile?id=~Zhao_Song3), [Han Liu](http://openreview.net/profile?id=~Han_Liu4)
  - **Affiliations:** Department of Computer Science, Northwestern University, Evanston, IL, USA, Department of Physics, National Taiwan University, Taipei, Taiwan, Adobe Research, Seattle, WA, USA, Department of Statistics and Data Science, Northwestern University, Evanston, IL, USA
  - **TL;DR:** This study characterizes the computational limits of modern Hopfield models through a fine-grained complexity analysis, establishing a phase transition behavior based on the norm of patterns. The findings include efficient constructions of these models and insights into their memory retrieval dynamics, with implications for large-scale applications in various scientific fields.
  - **Keywords:** computational limits, memory retrieval dynamics, modern Hopfield models, fine-grained complexity analysis, low-rank approximation, energy minimization algorithms, large-scale applications, deep learning, natural language processing, financial analytics, genomic research, medical science, efficiency of memory retrieval, computational time, memory retrieval error bound, phase transition behavior, exponential memory capacity, energy-based associative memory models, log-sum-exponential, transformer attention


- [Enhancing Adversarial Robustness in SNNs with Sparse Gradients](https://icml.cc/virtual/2024/poster/34066) (Poster)
  - **Authors:** [Yujia Liu](http://openreview.net/profile?id=~Yujia_Liu1), [Tong Bu](http://openreview.net/profile?id=~Tong_Bu1), [Ding Jianhao](http://openreview.net/profile?id=~Jianhao_Ding1), [Zecheng Hao](http://openreview.net/profile?id=~Zecheng_Hao1), [Tiejun Huang](http://openreview.net/profile?id=~Tiejun_Huang1), [Zhaofei Yu](http://openreview.net/profile?id=~Zhaofei_Yu1)
  - **Affiliations:** NERCVT, School of Computer Science, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University, China; School of Computer Science, Peking University, China, NERCVT, School of Computer Science, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University, China; Institution for Artificial Intelligence, Peking University, China, NERCVT, School of Computer Science, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University, China; School of Computer Science, Peking University, China, NERCVT, School of Computer Science, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University, China; School of Computer Science, Peking University, China, NERCVT, School of Computer Science, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University, China; School of Computer Science, Peking University, China, NERCVT, School of Computer Science, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University, China; Institution for Artificial Intelligence, Peking University, China
  - **TL;DR:** This paper proposes a novel approach to enhance the robustness of Spiking Neural Networks (SNNs) against adversarial attacks by utilizing gradient sparsity regularization. The findings demonstrate that SNNs can achieve improved resilience to adversarial perturbations, narrowing the performance gap compared to random perturbations.
  - **Keywords:** Spiking Neural Networks (SNNs), Adversarial Robustness, Gradient Sparsity Regularization, Image-based datasets, Event-based datasets, Robustness to adversarial attacks, Performance gap between adversarial and random perturbations, Enhanced robustness of SNNs, Practical strategy for training robust SNNs, Energy-efficient operations, Biologically inspired structures


- [Layerwise Change of Knowledge in Neural Networks](https://icml.cc/virtual/2024/poster/34861) (Poster)
  - **Authors:** [Xu Cheng](http://openreview.net/profile?id=~Xu_Cheng1), [Lei Cheng](http://openreview.net/profile?id=~Lei_Cheng2), [Zhaoran Peng](http://openreview.net/profile?id=~Zhaoran_Peng2), [Yang Xu](http://openreview.net/profile?id=~Yang_Xu19), [Tian Han](http://openreview.net/profile?id=~Tian_Han1), [Quanshi Zhang](http://openreview.net/profile?id=~Quanshi_Zhang1)
  - **Affiliations:** Nanjing University of Science and Technology, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Zhejiang University, Stevens Institute of Technology, Shanghai Jiao Tong University
  - **TL;DR:** This paper investigates how deep neural networks (DNNs) extract and forget knowledge through layers during forward propagation, proposing a method to quantify and track these changes. The findings reveal insights into the learning behavior of DNNs and the dynamics of feature representation stability.
  - **Keywords:** deep neural networks, knowledge extraction, forward propagation, information bottleneck theory, mutual information, knowledge definition, learning of new knowledge, forgetting of old knowledge, quantification of interactions, tracking of knowledge changes, interaction primitives, interaction concepts, sparsity property, universal-matching property


- [Analysis for Abductive Learning and Neural-Symbolic Reasoning Shortcuts](https://icml.cc/virtual/2024/poster/34769) (Poster)
  - **Authors:** [Xiao-Wen Yang](http://openreview.net/profile?id=~Xiao-Wen_Yang4), [Wen-Da Wei](http://openreview.net/profile?id=~Wen-Da_Wei1), [Jie-Jing Shao](http://openreview.net/profile?id=~Jie-Jing_Shao1), [Yu-Feng Li](http://openreview.net/profile?id=~Yu-Feng_Li1), [Zhi-Hua Zhou](http://openreview.net/profile?id=~Zhi-Hua_Zhou2)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This paper analyzes the impact of reasoning shortcuts on the generalization ability of abductive learning and neural-symbolic models, proposing methods to quantify and mitigate these issues. The findings suggest that specific distance functions in consistency optimization can help reduce shortcut risks, enhancing the effectiveness of these models.
  - **Keywords:** Abductive Learning, Neural-Symbolic Learning, Inconsistency Minimization, Distance Functions, Autonomous Driving, Reasoning Shortcuts, Generalization Ability, Analysis of Reasoning Shortcuts, Mitigation Strategies, NeSy (Neural-Symbolic), ABL (Abductive Learning)


- [Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers](https://icml.cc/virtual/2024/poster/34052) (Poster)
  - **Authors:** [Xiaoqiang Lin](http://openreview.net/profile?id=~Xiaoqiang_Lin1), [Zhaoxuan Wu](http://openreview.net/profile?id=~Zhaoxuan_Wu1), [Zhongxiang Dai](http://openreview.net/profile?id=~Zhongxiang_Dai1), [Wenyang Hu](http://openreview.net/profile?id=~Wenyang_Hu1), [Yao Shu](http://openreview.net/profile?id=~Yao_Shu1), [See-Kiong Ng](http://openreview.net/profile?id=~See-Kiong_Ng1), [Patrick Jaillet](http://openreview.net/profile?id=~Patrick_Jaillet1), [Bryan Kian Hsiang Low](http://openreview.net/profile?id=~Bryan_Kian_Hsiang_Low1)
  - **Affiliations:** Department of Computer Science, National University of Singapore; Institute of Data Science, National University of Singapore; None, Department of Computer Science, National University of Singapore; Institute of Data Science, National University of Singapore; Integrative Sciences and Engineering Programme, National University of Singapore, LIDS and EECS, Massachusetts Institute of Technology, Department of Computer Science, National University of Singapore; Institute of Data Science, National University of Singapore, Guangdong Lab of AI and Digital Economy (SZ), Department of Computer Science, National University of Singapore; Institute of Data Science, National University of Singapore, LIDS and EECS, Massachusetts Institute of Technology, Department of Computer Science, National University of Singapore
  - **TL;DR:** The study proposes the INSTINCT algorithm, which utilizes a neural bandit approach to optimize instructions for large language models (LLMs) by replacing traditional Gaussian processes with neural network surrogates. Extensive experiments demonstrate that INSTINCT consistently outperforms existing methods in various instruction-related tasks.
  - **Keywords:** instruction optimization, large language models (LLMs), neural bandit algorithm, Bayesian optimization (BO), Gaussian process (GP), neural networks (NNs), pre-trained transformers, black-box LLMs, instruction induction tasks, zero-shot chain-of-thought, dependency on instructions, manual tuning, optimization of high-dimensional objective functions, INSTINCT algorithm, improved performance of LLMs


- [EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence](https://icml.cc/virtual/2024/poster/34482) (Poster)
  - **Authors:** [Chung-Yiu Yau](http://openreview.net/profile?id=~Chung-Yiu_Yau1), [Hoi To Wai](http://openreview.net/profile?id=~Hoi_To_Wai1), [Parameswaran Raman](http://openreview.net/profile?id=~Parameswaran_Raman1), [Soumajyoti Sarkar](http://openreview.net/profile?id=~Soumajyoti_Sarkar1), [Mingyi Hong](http://openreview.net/profile?id=~Mingyi_Hong1)
  - **Affiliations:** Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong SAR of China; Amazon Web Services, USA, Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong SAR of China, Amazon Web Services, USA, Amazon Web Services, USA, Department of Electrical and Computer Engineering, University of Minnesota, USA; Amazon Scholar
  - **TL;DR:** This paper introduces EMC2, an efficient MCMC negative sampling method for contrastive learning that achieves global convergence and low computational costs. The method effectively generates hardness-aware negative samples, demonstrating strong performance in small batch training on datasets like STL-10 and ImageNet-100.
  - **Keywords:** Contrastive learning, Self-supervised learning, Markov Chain Monte Carlo, Metropolis-Hastings algorithm, Image encoding, Pre-training of models, Generating negative samples, High computational costs, Optimization of contrastive loss, Efficient negative sampling method, Global convergence, STL-10, ImageNet-100, Hardness-aware negative samples


- [Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection](https://icml.cc/virtual/2024/poster/32704) (Poster)
  - **Authors:** [Chentao Cao](http://openreview.net/profile?id=~Chentao_Cao1), [Zhun Zhong](http://openreview.net/profile?id=~Zhun_Zhong1), [Zhanke Zhou](http://openreview.net/profile?id=~Zhanke_Zhou1), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu3), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1), [Bo Han](http://openreview.net/profile?id=~Bo_Han1)
  - **Affiliations:** TMLR Group, Department of Computer Science, Hong Kong Baptist University, School of Computer Science and Information Engineering, Hefei University of Technology; School of Computer Science, University of Nottingham, TMLR Group, Department of Computer Science, Hong Kong Baptist University, Computer Science and Engineering, University of California, Santa Cruz, Sydney AI Centre, The University of Sydney, TMLR Group, Department of Computer Science, Hong Kong Baptist University
  - **TL;DR:** This paper introduces a novel method called Envisioning Outlier Exposure (EOE) that leverages large language models for effective out-of-distribution detection without requiring access to actual OOD data. EOE demonstrates state-of-the-art performance across various OOD tasks, addressing the limitations of existing methods that rely solely on closed-set labels.
  - **Keywords:** Out-of-Distribution Detection, Zero-shot Learning, Large Language Models, Vision-Language Models, CLIP, Autonomous Driving, Open-World Scenarios, Out-of-Distribution Samples, Model Error, Closed-set vs Open-set Label Space, Envisioning Outlier Exposure (EOE), Potential Outlier Penalty, ImageNet-1K


- [Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection](https://icml.cc/virtual/2024/poster/35026) (Spotlight Poster)
  - **Authors:** [Feiran Li](http://openreview.net/profile?id=~Feiran_Li3), [Qianqian Xu](http://openreview.net/profile?id=~Qianqian_Xu2), [Shilong Bao](http://openreview.net/profile?id=~Shilong_Bao1), [Zhiyong Yang](http://openreview.net/profile?id=~Zhiyong_Yang1), [Runmin Cong](http://openreview.net/profile?id=~Runmin_Cong1), [Xiaochun Cao](http://openreview.net/profile?id=~Xiaochun_Cao3), [Qingming Huang](http://openreview.net/profile?id=~Qingming_Huang1)
  - **Affiliations:** Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China, Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China, School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China, Institute of Information Science, Beijing Jiaotong University, Beijing, China; School of Control Science and Engineering, Shandong University, Jinan, China; Key Laboratory of Machine Intelligence and System Control, Ministry of Education, Jinan, China, School of Cyber Science and Tech., Shenzhen Campus, Sun Yat-sen University, School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Big Data Mining and Knowledge Management, Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** This study addresses the size sensitivity of evaluation metrics in Salient Object Detection (SOD) and proposes a size-invariant approach to improve the detection of objects of varying sizes. The proposed method demonstrates significant improvements in performance and provides a theoretical foundation for the new metrics introduced.
  - **Keywords:** Salient Object Detection (SOD), Size-Invariance, Evaluation metrics, Optimization framework, Image processing, Object detection, Size sensitivity in metrics, Imbalance in object detection, New size-invariant metrics, Improved detection of objects of different sizes, MSOD (Multi-object Salient Object Detection dataset)


- [Recurrent Early Exits for Federated Learning with Heterogeneous Clients](https://icml.cc/virtual/2024/poster/32762) (Poster)
  - **Authors:** [Royson Lee](http://openreview.net/profile?id=~Royson_Lee1), [Javier Fernandez-Marques](http://openreview.net/profile?id=~Javier_Fernandez-Marques1), [Xu Hu](http://openreview.net/profile?id=~Shell_Xu_Hu1), [Da Li](http://openreview.net/profile?id=~Da_Li3), [Stefanos Laskaridis](http://openreview.net/profile?id=~Stefanos_Laskaridis1), [Łukasz Dudziak](http://openreview.net/profile?id=~%C5%81ukasz_Dudziak1), [Timothy Hospedales](http://openreview.net/profile?id=~Timothy_Hospedales1), [Ferenc Huszár](http://openreview.net/profile?id=~Ferenc_Husz%C3%A1r1), [Nicholas Lane](http://openreview.net/profile?id=~Nicholas_Donald_Lane1)
  - **Affiliations:** Samsung AI Center, Cambridge, UK; University of Cambridge, Cambridge, UK, Flower Labs, Cambridge, UK, Samsung AI Center, Cambridge, UK, Samsung AI Center, Cambridge, UK, Brave Software, London, UK, Samsung AI Center, Cambridge, UK, University of Edinburgh, Edinburgh, UK; Samsung AI Center, Cambridge, UK, University of Cambridge, Cambridge, UK, University of Cambridge, Cambridge, UK; Flower Labs, Cambridge, UK
  - **TL;DR:** This paper introduces ReeFL, a recurrent early exit approach for federated learning that addresses the challenges of heterogeneous clients by fusing features from sub-models into a single shared classifier. The proposed method demonstrates improved performance on image and speech classification benchmarks compared to previous approaches.
  - **Keywords:** Federated Learning, Heterogeneous Clients, Recurrent Early Exit, Transformer-based Module, Self-Distillation, Image Classification, Speech Classification, Varying Hardware Capacities, Convergence Issues, Fairness Issues, ReeFL Method, Feature Fusion, Shared Classifier


- [Smoothness Adaptive Hypothesis Transfer Learning](https://icml.cc/virtual/2024/poster/32800) (Poster)
  - **Authors:** [Haotian Lin](http://openreview.net/profile?id=~Haotian_Lin1), [Matthew Reimherr](http://openreview.net/profile?id=~Matthew_Reimherr1)
  - **Affiliations:** Department of Statistics, Pennsylvania State University, University Park, PA, USA, Department of Statistics, Pennsylvania State University, University Park, PA, USA
  - **TL;DR:** This paper introduces Smoothness Adaptive Transfer Learning (SATL), a two-phase kernel ridge regression algorithm that adapts to varying and unknown smoothness in hypothesis transfer learning. The study demonstrates that SATL achieves minimax optimality and provides insights into the factors influencing transfer dynamics and efficacy.
  - **Keywords:** Hypothesis Transfer Learning, Nonparametric Regression, Kernel Ridge Regression (KRR), Gaussian Kernel, Data Scarcity, Model Shift, Smoothness Adaptation, Smoothness Adaptive Transfer Learning (SATL), Minimax Optimality, Sobolev Spaces, RKHS (Reproducing Kernel Hilbert Space)


- [Interacting Diffusion Processes for Event Sequence Forecasting](https://icml.cc/virtual/2024/poster/34685) (Poster)
  - **Authors:** [Mai Zeng](http://openreview.net/profile?id=~Mai_Zeng1), [Florence Regol](http://openreview.net/profile?id=~florence_regol1), [Mark Coates](http://openreview.net/profile?id=~Mark_Coates1)
  - **Affiliations:** Department of Electrical and Computer Engineering, McGill University, Montreal QC, Canada; International Laboratory on Learning Systems (ILLS), Montreal, QC, Canada; Mila Québec AI Institute, Montreal, QC, Canada, Department of Electrical and Computer Engineering, McGill University, Montreal QC, Canada; International Laboratory on Learning Systems (ILLS), Montreal, QC, Canada; Mila Québec AI Institute, Montreal, QC, Canada, Department of Electrical and Computer Engineering, McGill University, Montreal QC, Canada; International Laboratory on Learning Systems (ILLS), Montreal, QC, Canada; Mila Québec AI Institute, Montreal, QC, Canada
  - **TL;DR:** This study introduces a novel approach using a diffusion generative model to enhance long-horizon forecasting of event sequences, addressing the limitations of traditional Neural Temporal Point Processes. The proposed model demonstrates superior performance compared to existing state-of-the-art methods.
  - **Keywords:** Event sequence forecasting, Neural Temporal Point Processes (TPPs), Diffusion generative model, sequence-to-sequence prediction, Forecasting purchase times, modeling transaction patterns, social media activity, Long-horizon forecasting, joint modeling of inter-arrival times and event types, Outperforms state-of-the-art baselines for long-horizon forecasting, Hawkes process, intensity-based models


- [DFD: Distilling the Feature Disparity Differently for Detectors](https://icml.cc/virtual/2024/poster/34349) (Poster)
  - **Authors:** [Kang Liu](http://openreview.net/profile?id=~Kang_Liu4), [Yingyi Zhang](http://openreview.net/profile?id=~Yingyi_Zhang2), [Jingyun Zhang](http://openreview.net/profile?id=~Jingyun_Zhang1), [Jinmin Li](http://openreview.net/profile?id=~Jinmin_Li1), [Jun Wang](http://openreview.net/profile?id=~Jun_Wang43), [ShaoMing Wang](http://openreview.net/profile?id=~ShaoMing_Wang1), [Chun Yuan](http://openreview.net/profile?id=~Chun_Yuan1), [Rizen Guo](http://openreview.net/profile?id=~Rizen_Guo1)
  - **Affiliations:** Tsinghua University; Tencent WeChat Pay Lab, Tencent Youtu Lab, Tencent WeChat Pay Lab, Tencent WeChat Pay Lab, Tencent WeChat Pay Lab, Tencent WeChat Pay Lab, Tsinghua University, Tencent WeChat Pay Lab; Tencent Youtu Lab
  - **TL;DR:** This paper introduces Disparity Feature Distillation (DFD), a novel knowledge distillation method that addresses the inconsistencies in feature disparities between teacher and student models in object detection. The proposed method significantly improves the performance of various detectors, achieving notable increases in mean Average Precision (mAP).
  - **Keywords:** Knowledge Distillation, Object Detection, Feature Distillation, Disparity Feature Distillation (DFD), Real-time Object Detection, Disparity between teacher and student models, computational complexity, Improved mAP for detectors, enhanced performance of student models, ResNet50, RetinaNet, FasterRCNN, RepPoints, YOLO, ViT


- [Smooth Tchebycheff Scalarization for Multi-Objective Optimization](https://icml.cc/virtual/2024/poster/33189) (Poster)
  - **Authors:** [Xi Lin](http://openreview.net/profile?id=~Xi_Lin2), [Xiaoyuan Zhang](http://openreview.net/profile?id=~Xiaoyuan_Zhang2), [Zhiyuan Yang](http://openreview.net/profile?id=~Zhiyuan_Yang2), [Fei Liu](http://openreview.net/profile?id=~Fei_Liu14), [Zhenkun Wang](http://openreview.net/profile?id=~Zhenkun_Wang1), [Qingfu Zhang](http://openreview.net/profile?id=~Qingfu_Zhang1)
  - **Affiliations:** City University of Hong Kong, City University of Hong Kong, City University of Hong Kong, City University of Hong Kong, Southern University of Science and Technology, City University of Hong Kong; Southern University of Science and Technology
  - **TL;DR:** This study introduces a smooth Tchebycheff scalarization approach for multi-objective optimization, which effectively balances conflicting objectives while maintaining low computational complexity. The proposed method demonstrates promising theoretical properties and is validated through experimental results on various real-world applications.
  - **Keywords:** Multi-objective optimization, Pareto solutions, Smooth Tchebycheff scalarization, gradient-based optimization, Real-world applications, energy-efficient agents, drug design, Conflicting objectives, computational complexity, non-convex Pareto front, Efficient scalarization approach, theoretical analyses, Pareto set learning


- [WISER: Weak Supervision and Supervised Representation Learning to Improve Drug Response Prediction in Cancer](https://icml.cc/virtual/2024/poster/34824) (Poster)
  - **Authors:** [Kumar Shubham](http://openreview.net/profile?id=~Kumar_Shubham1), [Aishwarya Jayagopal](http://openreview.net/profile?id=~Aishwarya_Jayagopal1), [Syed Danish](http://openreview.net/profile?id=~Syed_Mohammed_Danish1), [Prathosh AP](http://openreview.net/profile?id=~Prathosh_AP1), [Vaibhav Rajan](http://openreview.net/profile?id=~Vaibhav_Rajan2)
  - **Affiliations:** Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, Karnataka, India; Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, Karnataka, India, Department of Information Systems and Analytics, School of Computing, National University of Singapore, Singapore, Indian Institute of Technology, Patna, Bihar, India, Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, Karnataka, India, Department of Information Systems and Analytics, School of Computing, National University of Singapore, Singapore
  - **TL;DR:** This study presents WISER, a novel method that combines weak supervision and supervised representation learning to enhance drug response prediction in cancer. The experimental results indicate that WISER outperforms existing state-of-the-art methods, addressing challenges related to data scarcity and variability in patient responses.
  - **Keywords:** Cancer treatment, Personalized medicine, Drug response prediction, Weak supervision, Supervised representation learning, Cancer research, Drug response modeling, Data scarcity, Variability in drug response, Domain adaptation, WISER method for drug response prediction, The Cancer Genome Atlas (TCGA), Cell line datasets


- [Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling](https://icml.cc/virtual/2024/poster/34858) (Poster)
  - **Authors:** [Guoqi Yu](http://openreview.net/profile?id=~Guoqi_Yu1), [Jing Zou](http://openreview.net/profile?id=~Jing_Zou2), [Xiaowei Hu](http://openreview.net/profile?id=~Xiaowei_Hu3), [Angelica I Aviles-Rivero](http://openreview.net/profile?id=~Angelica_I_Aviles-Rivero1), [Jing Qin](http://openreview.net/profile?id=~Jing_Qin3), [Shujun Wang](http://openreview.net/profile?id=~Shujun_Wang1)
  - **Affiliations:** Department of Biomedical Engineering, The Hong Kong Polytechnic University, Hong Kong SAR, China; University of Electronic Science and Technology of China, Sichuan, China, School of Nursing, The Hong Kong Polytechnic University, Hong Kong SAR, China, Shanghai Artificial Intelligence Laboratory, Shanghai, China, DAMTP, University of Cambridge, Cambridge, UK, School of Nursing, The Hong Kong Polytechnic University, Hong Kong SAR, China, Department of Biomedical Engineering, The Hong Kong Polytechnic University, Hong Kong SAR, China
  - **TL;DR:** This study introduces a learnable decomposition strategy and a dual attention module to enhance multivariate time series forecasting by effectively capturing inter-series dependencies and intra-series variations. The proposed method, Leddam, shows significant improvements in predictive performance across various datasets, outperforming state-of-the-art techniques.
  - **Keywords:** Multivariate time series forecasting, Learnable decomposition, Dual attention module, Channel-wise self-attention, Autoregressive self-attention, Energy management, Weather forecasting, Disease control, Traffic planning, Inter-series dependencies, Intra-series variations, Non-linear structure, Complex trends, Leddam (LEarnable Decomposition and Dual Attention Module), Performance improvement in predictive performance, Eight open-source datasets


- [NeuralIndicator: Implicit Surface Reconstruction from Neural Indicator Priors](https://icml.cc/virtual/2024/poster/34877) (Poster)
  - **Authors:** [Shi-Sheng Huang](http://openreview.net/profile?id=~Shi-Sheng_Huang2), [Guo Chen](http://openreview.net/profile?id=~Guo_Chen6), [Li-heng Chen](http://openreview.net/profile?id=~CHEN_LI_HENG1), [Hua Huang](http://openreview.net/profile?id=~Hua_Huang1)
  - **Affiliations:** School of Artificial Intelligence, Beijing Normal University, Beijing, China, School of Artificial Intelligence, Beijing Normal University, Beijing, China, School of Artificial Intelligence, Beijing Normal University, Beijing, China, School of Artificial Intelligence, Beijing Normal University, Beijing, China
  - **TL;DR:** This paper presents NeuralIndicator, a framework for neural implicit surface reconstruction that utilizes global shape priors to improve reliability and accuracy, particularly in the presence of incomplete and noisy point clouds. The proposed method outperforms existing approaches, demonstrating significant advancements in surface reconstruction quality.
  - **Keywords:** neural implicit surface reconstruction, global shape priors, point cloud processing, smooth indicator function, neural implicit function, signed distance function (SDF), computer graphics, computer vision, 3D surface reconstruction, incomplete point clouds, noisy point clouds, complex topology structure, reliable surface reconstruction, high-fidelity reconstruction, synthetic datasets, real-scan datasets


- [Tilt your Head: Activating the Hidden Spatial-Invariance of Classifiers](https://icml.cc/virtual/2024/poster/34980) (Poster)
  - **Authors:** [Johann Schmidt](http://openreview.net/profile?id=~Johann_Schmidt1), [Sebastian Stober](http://openreview.net/profile?id=~Sebastian_Stober1)
  - **Affiliations:** Artificial Intelligence Lab, Otto-von-Guericke University, Magdeburg, Germany, Artificial Intelligence Lab, Otto-von-Guericke University, Magdeburg, Germany
  - **TL;DR:** This study introduces the Inverse Transformation Search (ITS) algorithm, which enhances the robustness of deep learning models by enabling zero-shot pseudo-invariance to spatially transformed inputs. The method outperforms existing baselines on various benchmark datasets, addressing significant challenges in generalization and robustness in neural networks.
  - **Keywords:** deep learning, robust generalization, spatial invariance, Inverse Transformation Search (ITS), data augmentation, image classification, neural networks, robustness issues, spatially transformed input signals, combinatorial explosion, curse of dimensionality, zero-shot pseudo-invariance, improved inference process, ImageNet


- [Agent-Specific Effects: A Causal Effect Propagation Analysis in Multi-Agent MDPs](https://icml.cc/virtual/2024/poster/33025) (Poster)
  - **Authors:** [Stelios Triantafyllou](http://openreview.net/profile?id=~Stelios_Triantafyllou1), [Aleksa Sukovic](http://openreview.net/profile?id=~Aleksa_Sukovic1), [Debmalya Mandal](http://openreview.net/profile?id=~Debmalya_Mandal2), [Goran Radanovic](http://openreview.net/profile?id=~Goran_Radanovic1)
  - **Affiliations:** Max Planck Institute for Software Systems, Saarbrücken, Germany; Saarland University, Saarbrücken, Germany, Max Planck Institute for Software Systems, Saarbrücken, Germany; Saarland University, Saarbrücken, Germany, University of Warwick, Department of Computer Science, UK, Max Planck Institute for Software Systems, Saarbrücken, Germany
  - **TL;DR:** This paper presents a systematic approach to attribute causal effects of agents' actions in multi-agent decision-making contexts, particularly focusing on multi-agent Markov decision processes. The authors introduce a novel causal quantity, agent-specific effects (ASE), and its counterfactual counterpart (cf-ASE), and demonstrate its utility through a simulation-based testbed in sepsis management.
  - **Keywords:** Causal relationships, Multi-agent decision-making, Accountability, Causal effect propagation, Counterfactual analysis, Sampling-based algorithms, Sepsis management, Healthcare decision-making, Inter-agent action interdependence, Causal contribution quantification, Agent-specific effects (ASE), Counterfactual agent-specific effects (cf-ASE), Multi-agent Markov decision processes (MDPs), Causal graphs


- [Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences](https://icml.cc/virtual/2024/poster/34376) (Poster)
  - **Authors:** [Andi Nika](http://openreview.net/profile?id=~Andi_Nika1), [Debmalya Mandal](http://openreview.net/profile?id=~Debmalya_Mandal2), [Parameswaran Kamalaruban](http://openreview.net/profile?id=~Parameswaran_Kamalaruban2), [Georgios Tzannetos](http://openreview.net/profile?id=~Georgios_Tzannetos1), [Goran Radanovic](http://openreview.net/profile?id=~Goran_Radanovic1), [Adish Singla](http://openreview.net/profile?id=~Adish_Singla2)
  - **Affiliations:** Max Planck Institute for Software Systems, Saarbrücken, Germany, University of Warwick, Coventry, UK, Independent Researcher, London, UK, Max Planck Institute for Software Systems, Saarbrücken, Germany, Max Planck Institute for Software Systems, Saarbrücken, Germany, Max Planck Institute for Software Systems, Saarbrücken, Germany
  - **TL;DR:** This paper compares reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) to understand their effectiveness in learning from human preferences. The study derives statistical bounds on suboptimality and highlights the advantages of DPO in certain contexts, particularly when the ground-truth reward is not realizable.
  - **Keywords:** Learning from human preferences, Reinforcement learning from human feedback (RLHF), Direct preference optimization (DPO), Loglinear policy parametrization, Linear reward functions, Minimax statistical bounds, Game-playing, Robotics, Large language models (LLMs), Suboptimality gap, Ground-truth reward realizability, Sample complexity, Comparative analysis of RLHF and DPO, Exponentially decaying convergence rates


- [On Interpolating Experts and Multi-Armed Bandits](https://icml.cc/virtual/2024/poster/32997) (Poster)
  - **Authors:** [Houshuang Chen](http://openreview.net/profile?id=~Houshuang_Chen1), [Yuchen He](http://openreview.net/profile?id=~Yuchen_He2), [Chihao Zhang](http://openreview.net/profile?id=~Chihao_Zhang1)
  - **Affiliations:** Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, John Hopcroft Center for Computer Science, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This paper studies a family of problems that interpolate between multi-armed bandits and learning with expert advice, introducing the m-MAB framework and proving tight minimax regret bounds. It also presents an optimal PAC algorithm for the pure exploration version, m-BAI, aimed at identifying the arm with minimum loss efficiently.
  - **Keywords:** Multi-Armed Bandits, Learning with Expert Advice, Minimax Regret Bounds, PAC Algorithm, Online Decision Problems, Regret Minimization, Best Arm Identification, Optimal PAC Algorithm for m-BAI, Tight Minimax Regret Bounds, m-MAB, m-BAI, Feedback Graph


- [KernelSHAP-IQ: Weighted Least Square Optimization for Shapley Interactions](https://icml.cc/virtual/2024/poster/33568) (Poster)
  - **Authors:** [Fabian Fumagalli](http://openreview.net/profile?id=~Fabian_Fumagalli1), [Maximilian Muschalik](http://openreview.net/profile?id=~Maximilian_Muschalik1), [Patrick Kolpaczki](http://openreview.net/profile?id=~Patrick_Kolpaczki1), [Eyke Hüllermeier](http://openreview.net/profile?id=~Eyke_H%C3%BCllermeier1), [CITEC Barbara Hammer](http://openreview.net/profile?id=~Barbara_Hammer4)
  - **Affiliations:** Bielefeld University, CITEC, D-33619 Bielefeld, Germany, LMU Munich, D-80539 Munich, Germany; MCML, Munich, Paderborn University, D-33098, Paderborn, Germany, LMU Munich, D-80539 Munich, Germany; MCML, Munich, Bielefeld University, CITEC, D-33619 Bielefeld, Germany
  - **TL;DR:** This study introduces KernelSHAP-IQ, an extension of KernelSHAP that optimally approximates Shapley interactions using weighted least square optimization. The findings demonstrate its effectiveness in enhancing interpretability of feature interactions in machine learning models.
  - **Keywords:** Shapley value, Shapley Interaction Index, machine learning interpretability, Weighted least square optimization, k-Shapley values, Feature attribution, global interpretability, data valuation, Higher-order interactions, interpretability challenges in complex systems, KernelSHAP-IQ, optimal approximation for feature interactions


- [Individual Contributions as Intrinsic Exploration Scaffolds for Multi-agent Reinforcement Learning](https://icml.cc/virtual/2024/poster/32640) (Poster)
  - **Authors:** [Xinran Li](http://openreview.net/profile?id=~Xinran_Li3), [Zifan LIU](http://openreview.net/profile?id=~Zifan_LIU1), [Shibo Chen](http://openreview.net/profile?id=~Shibo_Chen2), [Jun Zhang](http://openreview.net/profile?id=~Jun_Zhang25)
  - **Affiliations:** Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China, Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China, Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China, Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong SAR, China
  - **TL;DR:** This study introduces Individual Contributions as Intrinsic Exploration Scaffolds (ICES) to enhance exploration in multi-agent reinforcement learning (MARL) environments with sparse rewards. The proposed method effectively utilizes global information during training to guide agents towards impactful actions, demonstrating superior exploration capabilities in benchmark tasks.
  - **Keywords:** Multi-agent reinforcement learning (MARL), Exploration, Individual Contributions as Intrinsic Exploration Scaffolds (ICES), Bayesian surprise, Cooperative benchmark tasks, Google Research Football (GRF), StarCraft Multi-agent Challenge (SMAC), Sparse rewards, Credit assignment, Non-stationarity, Partial observability, Improved exploration capabilities, Separation of exploration and exploitation policies, Centralized training decentralized execution (CTDE), Extrinsic rewards, Intrinsic rewards


- [The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling](https://icml.cc/virtual/2024/poster/35092) (Poster)
  - **Authors:** [Jiajun Ma](http://openreview.net/profile?id=~Jiajun_Ma1), [Shuchen Xue](http://openreview.net/profile?id=~Shuchen_Xue1), [Tianyang Hu](http://openreview.net/profile?id=~Tianyang_Hu1), [Wenjia Wang](http://openreview.net/profile?id=~Wenjia_Wang2), [Zhaoqiang Liu](http://openreview.net/profile?id=~Zhaoqiang_Liu1), [Zhenguo Li](http://openreview.net/profile?id=~Zhenguo_Li1), [Zhiming Ma](http://openreview.net/profile?id=~Zhi-Ming_Ma1), [Kenji Kawaguchi](http://openreview.net/profile?id=~Kenji_Kawaguchi1)
  - **Affiliations:** The Hong Kong University of Science and Technology; Hong Kong University of Science and Technology (Guangzhou), University of Chinese Academy of Sciences; Academy of Mathematics and Systems Science, Huawei Noah’s Ark Lab, The Hong Kong University of Science and Technology; Hong Kong University of Science and Technology (Guangzhou), University of Electronic Science and Technology of China, Huawei Noah’s Ark Lab, University of Chinese Academy of Sciences; Academy of Mathematics and Systems Science, National University of Singapore
  - **TL;DR:** This study introduces Skip-Tuning, a training-free method that enhances the performance of diffusion probabilistic models by optimizing skip connections in the UNet architecture, achieving significant improvements in image generation quality. The findings reveal that while Skip-Tuning increases pixel space losses, it reduces feature space losses, particularly at intermediate noise levels, leading to better image quality.
  - **Keywords:** Diffusion Probabilistic Models, Image Generation, UNet architecture, Skip-Tuning, Denoising Score Matching, High-resolution image generation, Complexity of transformation in diffusion sampling, limitations of skip connections, 100% FID improvement, training-free tuning method, ImageNet, ODE samplers, score-matching losses


- [RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning](https://icml.cc/virtual/2024/poster/32727) (Poster)
  - **Authors:** [Yukinari Hisaki](http://openreview.net/profile?id=~Yukinari_Hisaki1), [Isao Ono](http://openreview.net/profile?id=~Isao_Ono1)
  - **Affiliations:** Tokyo Institute of Technology Yokohama, Kanagawa, Japan, Tokyo Institute of Technology Yokohama, Kanagawa, Japan
  - **TL;DR:** This paper introduces RVI-SAC, an off-policy deep reinforcement learning method that utilizes the average reward criterion to address discrepancies in training objectives for continuing tasks. The method demonstrates competitive performance in Mujoco tasks compared to existing approaches.
  - **Keywords:** Off-Policy Deep Reinforcement Learning, Average Reward Criterion, RVI Q-learning, Soft Actor-Critic (SAC), Mujoco tasks, Robot locomotion, Discrepancy between training objective and performance metrics, Discounted reward criterion issues, RVI-SAC method, Competitive performance in continuing tasks, Gymnasium’s Mujoco


- [Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model](https://icml.cc/virtual/2024/poster/34705) (Oral)
  - **Authors:** [Fei Liu](http://openreview.net/profile?id=~Fei_Liu14), [Tong Xialiang](http://openreview.net/profile?id=~Tong_Xialiang2), [Mingxuan Yuan](http://openreview.net/profile?id=~Mingxuan_Yuan1), [Xi Lin](http://openreview.net/profile?id=~Xi_Lin2), [Fu Luo](http://openreview.net/profile?id=~Fu_Luo1), [Zhenkun Wang](http://openreview.net/profile?id=~Zhenkun_Wang1), [Zhichao Lu](http://openreview.net/profile?id=~Zhichao_Lu1), [Qingfu Zhang](http://openreview.net/profile?id=~Qingfu_Zhang1)
  - **Affiliations:** Department of Computer Science, City University of Hong Kong, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, Department of Computer Science, City University of Hong Kong, School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, School of System Design and Intelligent Manufacturing, Southern University of Science and Technology, Department of Computer Science, City University of Hong Kong, Department of Computer Science, City University of Hong Kong
  - **TL;DR:** This paper introduces the Evolution of Heuristic (EoH), an innovative approach that combines Large Language Models and Evolutionary Computation for Automatic Heuristic Design, demonstrating superior performance over traditional handcrafted heuristics in combinatorial optimization tasks. The findings suggest that EoH can efficiently generate high-performance heuristics with a low computational budget.
  - **Keywords:** Automatic Heuristic Design, Evolutionary Computation, Large Language Models, Evolutionary search framework, Combinatorial optimization, Online bin packing problem, Manual design of heuristics, Labor-intensive heuristic configuration, Evolution of heuristics, High-performance heuristics generation, Heuristics, Thoughts, Genetic Programming


- [On the Calibration of Human Pose Estimation](https://icml.cc/virtual/2024/poster/35199) (Poster)
  - **Authors:** [Kerui Gu](http://openreview.net/profile?id=~Kerui_Gu1), [Rongyu Chen](http://openreview.net/profile?id=~Rongyu_Chen1), [Xuanlong Yu](http://openreview.net/profile?id=~Xuanlong_Yu1), [Angela Yao](http://openreview.net/profile?id=~Angela_Yao1)
  - **Affiliations:** School of Computing, National University of Singapore, School of Computing, National University of Singapore, U2IS, ENSTA Paris, IP Paris, School of Computing, National University of Singapore
  - **TL;DR:** This study addresses the calibration of confidence in 2D human pose estimation, revealing a gap between estimated confidence and pose accuracy. The authors propose a new method, Calibrated ConfidenceNet (CCNet), which enhances confidence estimation in existing frameworks, improving reliability for applications in robotics and autonomous systems.
  - **Keywords:** Human Pose Estimation, Calibration, Calibrated ConfidenceNet (CCNet), Theoretical Analysis, Empirical Verification, Robotics, Autonomous Driving, 3D Mesh Recovery, Calibration Gap, Confidence Estimation, Alignment of Confidence and Accuracy, Improved Calibration of Pose Estimation Frameworks


- [In-Context Unlearning: Language Models as Few-Shot Unlearners](https://icml.cc/virtual/2024/poster/34503) (Poster)
  - **Authors:** [Martin Pawelczyk](http://openreview.net/profile?id=~Martin_Pawelczyk1), [Seth Neel](http://openreview.net/profile?id=~Seth_Neel2), [Himabindu Lakkaraju](http://openreview.net/profile?id=~Himabindu_Lakkaraju1)
  - **Affiliations:** Harvard University, US, Harvard University, US, Harvard University, US
  - **TL;DR:** This study introduces "In-Context Unlearning," a novel method for efficiently removing the influence of specific training instances from Large Language Models without retraining. The experimental results show that this approach can match or exceed the performance of existing methods that require access to model parameters, while also addressing privacy and copyright concerns.
  - **Keywords:** Machine unlearning, Large Language Models, In-Context Unlearning, User privacy, Copyright compliance, Right to be Forgotten, Data removal from models, Efficient unlearning methods, Performance comparison with state-of-the-art methods


- [On the Minimal Degree Bias in Generalization on the Unseen for non-Boolean Functions](https://icml.cc/virtual/2024/poster/33812) (Poster)
  - **Authors:** [Denys Pushkin](http://openreview.net/profile?id=~Denys_Pushkin1), [Raphaël Berthier](http://openreview.net/profile?id=~Rapha%C3%ABl_Berthier1), [Emmanuel Abbe](http://openreview.net/profile?id=~Emmanuel_Abbe1)
  - **Affiliations:** School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland; Apple, Machine Learning Research (MLR), Switzerland, Inria Sorbonne Université, Paris, France; School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland, School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland; Apple, Machine Learning Research (MLR), Switzerland
  - **TL;DR:** This study investigates the out-of-domain generalization of random feature models and Transformers, focusing on the generalization on the unseen (GOTU) setting. It finds that in certain conditions, these models converge to minimal degree interpolators, while in other cases, they may not, highlighting the complexity of generalization in non-Boolean functions.
  - **Keywords:** out-of-domain generalization, generalization on the unseen (GOTU), minimal degree bias, random feature models, Transformers, reasoning tasks, combinatorial data, distribution shift, data sparsity, length generalization, minimal degree interpolators, convergence in small feature regime, q-ary data tokens, roots of unities, Fourier coefficients


- [DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exemplified as A Video Agent)](https://icml.cc/virtual/2024/poster/34086) (Poster)
  - **Authors:** [Zongxin Yang](http://openreview.net/profile?id=~Zongxin_Yang1), [Guikun Chen](http://openreview.net/profile?id=~Guikun_Chen1), [Xiaodi Li](http://openreview.net/profile?id=~Xiaodi_Li2), [Wenguan Wang](http://openreview.net/profile?id=~Wenguan_Wang4), [Yi Yang](http://openreview.net/profile?id=~Yi_Yang4)
  - **Affiliations:** ReLER, CCAI, Zhejiang University, Hangzhou, China, ReLER, CCAI, Zhejiang University, Hangzhou, China, ReLER, CCAI, Zhejiang University, Hangzhou, China, ReLER, CCAI, Zhejiang University, Hangzhou, China, ReLER, CCAI, Zhejiang University, Hangzhou, China
  - **TL;DR:** This paper presents DoraemonGPT, a system leveraging large language models to understand dynamic scenes through video analysis, addressing challenges in spatial-temporal reasoning and task decomposition. The study introduces a novel planner based on Monte Carlo Tree Search to enhance the system's effectiveness in real-world applications.
  - **Keywords:** Large Language Models, Dynamic Scene Understanding, Monte Carlo Tree Search, Spatial-Temporal Reasoning, Video Analysis, Laboratory Experiment Guidance, Limited Internal Knowledge, Task Decomposition Challenges, Symbolic Memory Representation, Tool Scheduling, LLM-driven Agents


- [Acquiring Diverse Skills using Curriculum Reinforcement Learning with Mixture of Experts](https://icml.cc/virtual/2024/poster/34802) (Poster)
  - **Authors:** [Onur Celik](http://openreview.net/profile?id=~Onur_Celik1), [Aleksandar Taranovic](http://openreview.net/profile?id=~Aleksandar_Taranovic1), [Gerhard Neumann](http://openreview.net/profile?id=~Gerhard_Neumann2)
  - **Affiliations:** Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany, Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany, Autonomous Learning Robots, Karlsruhe Institute of Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany
  - **TL;DR:** The study presents Diverse Skill Learning (Di-SkilL), a reinforcement learning method that utilizes a Mixture of Experts to learn diverse skills as contextual motion primitives. The approach effectively addresses challenges in learning multi-modal behaviors and demonstrates improved performance in robot simulation tasks.
  - **Keywords:** Reinforcement Learning, Diverse Skill Learning, Mixture of Experts, Maximum Entropy Objective, Energy-based Models, Policy Gradient, Robot Simulation, Contextual Reinforcement Learning, Learning diverse skills, Multi-modality in behavior space, Data sparsity, Di-SkilL method, Automatic curriculum learning, Contextual motion primitive, Context distribution


- [Distributed Bilevel Optimization with Communication Compression](https://icml.cc/virtual/2024/poster/34969) (Poster)
  - **Authors:** [Yutong He](http://openreview.net/profile?id=~Yutong_He2), [Jie Hu](http://openreview.net/profile?id=~Jie_Hu12), [Xinmeng Huang](http://openreview.net/profile?id=~Xinmeng_Huang1), [Songtao Lu](http://openreview.net/profile?id=~Songtao_Lu1), [Bin Wang](http://openreview.net/profile?id=~Bin_Wang35), [Kun Yuan](http://openreview.net/profile?id=~Kun_Yuan4)
  - **Affiliations:** Peking University, Peking University, University of Pennsylvania, IBM Research, Zhejiang University, Peking University; National Engineering Laboratory for Big Data Analytics and Applications; AI for Science Institute, Beijing, China
  - **TL;DR:** This paper introduces a family of distributed bilevel optimization algorithms that utilize communication compression to address the significant overhead associated with transmitting full-dimensional gradients. The proposed methods demonstrate improved convergence properties and can achieve a 10× reduction in communication overhead without severe performance degradation.
  - **Keywords:** Distributed bilevel optimization, Stochastic optimization, Communication compression, C-SOBA, Moving average, Error feedback, Multi-step compression, Large-scale optimization, Machine learning, Communication overhead, Nested optimization structures, Advanced algorithms with improved convergence properties, 10× reduction in communication overhead


- [Auto-Linear Phenomenon in Subsurface Imaging](https://icml.cc/virtual/2024/poster/35056) (Poster)
  - **Authors:** [Yinan Feng](http://openreview.net/profile?id=~Yinan_Feng1), [Yinpeng Chen](http://openreview.net/profile?id=~Yinpeng_Chen1), [Peng Jin](http://openreview.net/profile?id=~Peng_Jin6), [Shihang Feng](http://openreview.net/profile?id=~Shihang_Feng1), [Youzuo Lin](http://openreview.net/profile?id=~Youzuo_Lin1)
  - **Affiliations:** Department of Computer Science, The University of North Carolina at Chapel Hill, USA, Google Research, USA, College of Information Sciences and Technology, The Pennsylvania State University, USA, Earth and Environmental Sciences Division, Los Alamos National Laboratory, USA, School of Data Science and Society, The University of North Carolina at Chapel Hill, USA
  - **TL;DR:** This study introduces the Auto-Linear phenomenon in subsurface imaging, demonstrating that self-supervised learning can effectively learn encoders and decoders from separate domains without requiring paired data. The proposed method outperforms existing techniques, particularly in scenarios with limited or noisy data, while simultaneously addressing both forward and inverse modeling tasks.
  - **Keywords:** Subsurface imaging, Full waveform inversion (FWI), Encoder-decoder network, Self-supervised learning, Energy exploration, Carbon capture, Earthquake early warning systems, Limited paired data, Noisy data, Auto-Linear method, Enhanced performance, Strong generalization ability, Linear mapping, Image-to-image translation


- [Projecting Molecules into Synthesizable Chemical Spaces](https://icml.cc/virtual/2024/poster/32903) (Poster)
  - **Authors:** [Shitong Luo](http://openreview.net/profile?id=~Shitong_Luo1), [Wenhao Gao](http://openreview.net/profile?id=~Wenhao_Gao1), [Zuofan Wu](http://openreview.net/profile?id=~Zuofan_Wu1), [Jian Peng](http://openreview.net/profile?id=~Jian_Peng1), [Connor Coley](http://openreview.net/profile?id=~Connor_W._Coley1), [Jianzhu Ma](http://openreview.net/profile?id=~Jianzhu_Ma2)
  - **Affiliations:** Helixon Research, Massachusetts Institute of Technology, Helixon Research, Helixon Research, Massachusetts Institute of Technology, Tsinghua University
  - **TL;DR:** This study presents a novel framework for generating new drug molecules that ensures synthetic accessibility, addressing the limitations of existing generative models. The proposed transformer-based model effectively translates molecular graphs into synthesizable chemical structures, enhancing the drug discovery process.
  - **Keywords:** drug discovery, synthetic accessibility, generative models, transformer-based model, postfix notation, synthesis planning, medicinal science, pharmaceutical industry, synthetic accessibility, synthetically infeasible molecules, chemical space exploration, novel framework for generating chemical structures, structurally similar analogs


- [Accelerating Parallel Sampling of Diffusion Models](https://icml.cc/virtual/2024/poster/34665) (Poster)
  - **Authors:** [Zhiwei Tang](http://openreview.net/profile?id=~Zhiwei_Tang1), [Jiasheng Tang](http://openreview.net/profile?id=~Jiasheng_Tang1), [Hao Luo](http://openreview.net/profile?id=~Hao_Luo1), [Fan Wang](http://openreview.net/profile?id=~Fan_Wang6), [Tsung-Hui Chang](http://openreview.net/profile?id=~Tsung-Hui_Chang1)
  - **Affiliations:** School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; None, DAMO Academy, Alibaba Group; Hupan Lab, Zhejiang Province, DAMO Academy, Alibaba Group, DAMO Academy, Alibaba Group, Shenzhen Research Institute of Big Data, Shenzhen, China
  - **TL;DR:** This study introduces ParaTAA, a novel parallel sampling algorithm that significantly accelerates the sampling process of diffusion models by reformulating it as solving triangular nonlinear equations. Experiments show that ParaTAA can reduce the inference steps of existing algorithms by a factor of 4 to 14, achieving comparable image quality in just 7 steps with Stable Diffusion.
  - **Keywords:** diffusion models, generative models, image generation, parallel sampling, autoregressive process, fixed-point iteration, triangular nonlinear equations, text-to-image generation, image synthesis, time-consuming sampling, inference steps reduction, ParaTAA (parallel sampling algorithm), reduced inference steps, DDIM (Denoising Diffusion Implicit Models), DDPM (Denoising Diffusion Probabilistic Models), Stable Diffusion


- [Causally Motivated Personalized Federated Invariant Learning with Shortcut-Averse Information-Theoretic Regularization](https://icml.cc/virtual/2024/poster/34335) (Poster)
  - **Authors:** [Xueyang Tang](http://openreview.net/profile?id=~Xueyang_Tang1), [Song Guo](http://openreview.net/profile?id=~Song_Guo5), [Jingcai Guo](http://openreview.net/profile?id=~Jingcai_Guo1), [Jie ZHANG](http://openreview.net/profile?id=~Jie_ZHANG18), [Yue Yu](http://openreview.net/profile?id=~Yue_Yu8)
  - **Affiliations:** The Hong Kong Polytechnic University; Peng Cheng Laboratory, The Hong Kong University of Science and Technology, The Hong Kong Polytechnic University; Hong Kong Polytechnic University Shenzhen Research Institute, The Hong Kong Polytechnic University; Hong Kong Polytechnic University Shenzhen Research Institute, Peng Cheng Laboratory
  - **TL;DR:** This study presents FedPIN, a method for personalized federated invariant learning that addresses the challenge of spurious correlations in heterogeneous data distributions. The method demonstrates improved out-of-distribution generalization performance and provides theoretical guarantees on generalization error and convergence rate.
  - **Keywords:** Personalized Federated Learning, Out-of-Distribution Generalization, Invariant Learning, Causal Signature, Information-Theoretic Regularization, Spurious Correlation, Data Distribution Shift, FedPIN method, Generalization Error Bound, Convergence Rate Guarantee, Shortcut-Averse Learning


- [A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts](https://icml.cc/virtual/2024/poster/34161) (Poster)
  - **Authors:** [Kuang-Huei Lee](http://openreview.net/profile?id=~Kuang-Huei_Lee1), [Xinyun Chen](http://openreview.net/profile?id=~Xinyun_Chen1), [Hiroki Furuta](http://openreview.net/profile?id=~Hiroki_Furuta1), [John Canny](http://openreview.net/profile?id=~John_Canny1), [Ian Fischer](http://openreview.net/profile?id=~Ian_Fischer1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** The study introduces ReadAgent, a system designed to enhance the effective context length of Large Language Models (LLMs) by utilizing gist memory to improve reading comprehension of long documents. The results demonstrate that ReadAgent significantly outperforms existing baselines across multiple long-document comprehension tasks.
  - **Keywords:** Large Language Models, Reading Comprehension, Gist Memory, Memory Episodes, Retrieval Methods, Long-document Reading Comprehension, Context Length Limitation, Performance Decline with Long Inputs, ReadAgent, Effective Context Length Extension, QuALITY, NarrativeQA, QMSum, Fuzzy-trace theory, Episodic Memory


- [Privacy Attacks in Decentralized Learning](https://icml.cc/virtual/2024/poster/33161) (Poster)
  - **Authors:** [Abdellah El Mrini](http://openreview.net/profile?id=~Abdellah_El_Mrini1), [Edwige Cyffers](http://openreview.net/profile?id=~Edwige_Cyffers1), [Aurélien Bellet](http://openreview.net/profile?id=~Aur%C3%A9lien_Bellet1)
  - **Affiliations:** School of Computer and Communication Sciences, EPFL, Switzerland, Université de Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France, Inria, Univ Montpellier, Montpellier, France
  - **TL;DR:** This study investigates privacy vulnerabilities in decentralized learning, specifically through attacks on the Decentralized Gradient Descent (D-GD) algorithm. The authors demonstrate that users can reconstruct private data from non-neighboring nodes, challenging the belief that decentralized learning inherently preserves privacy.
  - **Keywords:** Decentralized learning, privacy attacks, Decentralized Gradient Descent (D-GD), gossip averaging, Collaborative learning, federated learning, Privacy concerns, data reconstruction, Attack methods against D-GD, effectiveness validation, Honest-but-curious nodes, network graph


- [On the Weight Dynamics of Deep Normalized Networks](https://icml.cc/virtual/2024/poster/34743) (Poster)
  - **Authors:** [Christian H.X. Ali Mehmeti-Göpel](http://openreview.net/profile?id=~Christian_H.X._Ali_Mehmeti-G%C3%B6pel1), [Michael Wand](http://openreview.net/profile?id=~Michael_Wand1)
  - **Affiliations:** Department of Computer Science, Johannes-Gutenberg University, Mainz, Germany, Department of Computer Science, Johannes-Gutenberg University, Mainz, Germany
  - **TL;DR:** This study investigates the dynamics of effective learning rates (ELRs) in deep normalized networks, demonstrating that ELR ratios converge to 1 over time despite initial gradient explosions. The authors propose a hyper-parameter-free warm-up method to minimize ELR disparities, enhancing trainability in very deep networks.
  - **Keywords:** deep learning, effective learning rates (ELRs), weight dynamics, normalization layers, batch normalization (BN), residual connections, vanishing gradients, exploding gradients, trainability issues, hyper-parameter-free warm-up method, convergence of ELR ratios, effective learning rates (ELR), gradient magnitude excursions


- [Energy-based Backdoor Defense without Task-Specific Samples and Model Retraining](https://icml.cc/virtual/2024/poster/33978) (Poster)
  - **Authors:** [Yudong Gao](http://openreview.net/profile?id=~Yudong_Gao2), [Honglong Chen](http://openreview.net/profile?id=~Honglong_Chen1), [Peng Sun](http://openreview.net/profile?id=~Peng_Sun9), [Zhe Li](http://openreview.net/profile?id=~Zhe_Li14), [Junjian Li](http://openreview.net/profile?id=~Junjian_Li1), [Huajie Shao](http://openreview.net/profile?id=~Huajie_Shao1)
  - **Affiliations:** China University of Petroleum, China University of Petroleum, Hunan University, China University of Petroleum, China University of Petroleum, College of William & Mary
  - **TL;DR:** This study presents two energy-based methods, EBBA and EBBA+, for effective detection and removal of backdoors in machine learning models without requiring task-specific samples or model retraining. The proposed methods demonstrate superior performance in addressing backdoor attacks, enhancing the safety and robustness of machine learning applications.
  - **Keywords:** Backdoor defense, Machine learning security, Energy-Based methods, EBBA, EBBA+, Safety-critical applications, Health care, Autonomous driving, Backdoor attacks, Model detection, Model removal, Detection and removal of backdoors, Theoretical analysis of model predictions, Benchmark datasets


- [Denoising Autoregressive Representation Learning](https://icml.cc/virtual/2024/poster/33550) (Poster)
  - **Authors:** [Yazhe Li](http://openreview.net/profile?id=~Yazhe_Li2), [Jorg Bornschein](http://openreview.net/profile?id=~Jorg_Bornschein1), [Ting Chen](http://openreview.net/profile?id=~Ting_Chen1)
  - **Affiliations:** Google DeepMind, London, UK; xAI, San Francisco, US, Google DeepMind, London, UK, Google DeepMind, London, UK; xAI, San Francisco, US
  - **TL;DR:** This paper presents DARL, a generative approach using a decoder-only Transformer for learning visual representations by predicting image patches autoregressively. The study finds that replacing the MSE loss with a diffusion objective and employing tailored noise schedules significantly enhances representation learning and image generation capabilities.
  - **Keywords:** generative representation learning, visual perception and generation, decoder-only Transformer, denoising patch decoder, Mean Squared Error (MSE), diffusion objective, relative positional encodings, decomposed rotary position embedding (2D RoPE), image generation, visual representation learning, lack of generation capabilities in existing representation learning methods, pre-training loss as an unreliable performance indicator, improved representation learning, enhanced image generation ability, optimal noise schedule for training, autoregressive models, denoising diffusion models


- [From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems](https://icml.cc/virtual/2024/poster/32980) (Poster)
  - **Authors:** [Jianliang He](http://openreview.net/profile?id=~Jianliang_He1), [Siyu Chen](http://openreview.net/profile?id=~Siyu_Chen2), [Fengzhuo Zhang](http://openreview.net/profile?id=~Fengzhuo_Zhang1), [Zhuoran Yang](http://openreview.net/profile?id=~Zhuoran_Yang1)
  - **Affiliations:** Fudan University, Yale University, National University of Singapore, Yale University
  - **TL;DR:** This study investigates the theoretical foundations of LLM-driven autonomous systems, demonstrating how LLM planners can effectively solve decision-making problems through hierarchical reinforcement learning and Bayesian aggregated imitation learning. The findings highlight the importance of exploration strategies and the potential for coordination among multiple agents in complex environments.
  - **Keywords:** Large Language Models, Autonomous Systems, Decision-Making, Hierarchical Reinforcement Learning, Bayesian Aggregated Imitation Learning, In-Context Learning, Autonomous Driving, Robotics, Personal Assistance, Exploration in Decision-Making, Linear Regret, Coordination in Multi-Agent Settings, Subgoal Generation, Exploration Strategy, World Model Inference, Partially Observable Markov Decision Process (POMDP), LLM-empowered Agents


- [Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](https://icml.cc/virtual/2024/poster/33512) (Poster)
  - **Authors:** [Yichao Fu](http://openreview.net/profile?id=~Yichao_Fu1), [Peter Bailis](http://openreview.net/profile?id=~Peter_Bailis2), [Ion Stoica](http://openreview.net/profile?id=~Ion_Stoica1), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang2)
  - **Affiliations:** UCSD; None, Google, UC Berkeley, UCSD
  - **TL;DR:** This paper introduces LOOKAHEAD DECODING, a parallel decoding algorithm that enhances the efficiency of autoregressive decoding in large language models by reducing the number of decoding steps and improving parallelization. The method achieves significant speedups in code completion tasks without the need for auxiliary models or data stores.
  - **Keywords:** Large Language Models, Autoregressive Decoding, LOOKAHEAD DECODING, Jacobi Decoding, Code Completion, Low-Latency Applications, High Latency, Memory Bandwidth Limitations, Underutilization of Parallel Processing, Speedup in Decoding, Parallel Decoding Algorithm, MT-bench, FlashAttention


- [On a Neural Implementation of Brenier's Polar Factorization](https://icml.cc/virtual/2024/poster/32639) (Spotlight Poster)
  - **Authors:** [Nina Vesseron](http://openreview.net/profile?id=~Nina_Vesseron1), [Marco Cuturi](http://openreview.net/profile?id=~marco_cuturi2)
  - **Affiliations:** CREST-ENSAE, IP Paris, Apple
  - **TL;DR:** This paper presents a neural implementation of Brenier's polar factorization theorem, demonstrating how to recover approximations of a convex potential and a measure-preserving map using input convex neural networks. The findings suggest potential applications in non-convex optimization and density sampling, enhancing the understanding of gradient fields in machine learning contexts.
  - **Keywords:** Brenier's polar factorization, optimal transport, machine learning, input convex neural networks (ICNN), stochastic generator, non-convex optimization problems, sampling of densities, ill-posed inverse map, measure-preserving map, practical implementation of polar factorization, gradient field analysis, convex potential, measure-preserving map


- [Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations](https://icml.cc/virtual/2024/poster/35039) (Poster)
  - **Authors:** [Jonas Beck](http://openreview.net/profile?id=~Jonas_Beck1), [Nathanael Bosch](http://openreview.net/profile?id=~Nathanael_Bosch1), [Michael Deistler](http://openreview.net/profile?id=~Michael_Deistler1), [Kyra Kadhim](http://openreview.net/profile?id=~Kyra_L._Kadhim1), [Jakob Macke](http://openreview.net/profile?id=~Jakob_H._Macke1), [Philipp Hennig](http://openreview.net/profile?id=~Philipp_Hennig1), [Philipp Berens](http://openreview.net/profile?id=~Philipp_Berens1)
  - **Affiliations:** Hertie Institute for AI in Brain Health, University of Tübingen; AI Center, University of Tübingen, AI Center, University of Tübingen, AI Center, University of Tübingen, Hertie Institute for AI in Brain Health, University of Tübingen; AI Center, University of Tübingen, AI Center, University of Tübingen, AI Center, University of Tübingen, Hertie Institute for AI in Brain Health, University of Tübingen; AI Center, University of Tübingen
  - **TL;DR:** This study introduces diffusion tempering, a novel regularization technique for probabilistic numerical methods, to enhance gradient-based parameter optimization in ordinary differential equations (ODEs). The method demonstrates improved convergence and reliable parameter estimates across various dynamical systems, including a Hodgkin–Huxley model.
  - **Keywords:** Ordinary Differential Equations (ODEs), Parameter Estimation, Probabilistic Numerical Methods, Diffusion Tempering, Gradient-Based Optimization, Fenrir (Physics-Enhanced Regression for Initial Value Problems), Dynamical Systems, Neuroscience (Hodgkin–Huxley Model), Parameter Identification, Nonlinear Dynamics, Local Minima, Sensitivity to Initial Conditions, Improved Convergence, Reliable Parameter Estimates


- [Causal Inference from Competing Treatments](https://icml.cc/virtual/2024/poster/34460) (Poster)
  - **Authors:** [Ana-Andreea Stoica](http://openreview.net/profile?id=~Ana-Andreea_Stoica1), [Vivian Y. Nastl](http://openreview.net/profile?id=~Vivian_Yvonne_Nastl1), [Moritz Hardt](http://openreview.net/profile?id=~Moritz_Hardt1)
  - **Affiliations:** Social Foundations of Computation, Max Planck Institute for Intelligent Systems, Tübingen, Germany; Tübingen AI Center, Germany, Department of Mathematics, ETH Zürich, Zürich, Switzerland, Social Foundations of Computation, Max Planck Institute for Intelligent Systems, Tübingen, Germany; Tübingen AI Center, Germany
  - **TL;DR:** This paper develops a game-theoretic model to estimate causal effects in the presence of competition among treatment administrators, particularly in online advertising. It establishes a tractable objective that maximizes sample value and demonstrates the existence of a Nash equilibrium, providing insights into optimal budget allocation for causal effect estimation.
  - **Keywords:** Causal inference, competition in treatment administration, Game-theoretic model, bidding system, utility function, Online advertising, randomized controlled trials (RCTs), Estimation of causal effects under competition, external validity issues, Approximation with a tractable objective, equilibrium behavior in experimentation, Nash equilibrium, estimation error


- [Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks](https://icml.cc/virtual/2024/poster/33446) (Poster)
  - **Authors:** [Guanhua Zhang](http://openreview.net/profile?id=~Guanhua_Zhang1), [Moritz Hardt](http://openreview.net/profile?id=~Moritz_Hardt1)
  - **Affiliations:** Max Planck Institute for Intelligent Systems, Tübingen; Tübingen AI Center, Max Planck Institute for Intelligent Systems, Tübingen; Tübingen AI Center
  - **TL;DR:** This study investigates the trade-off between diversity and stability in multi-task benchmarks in machine learning using social choice theory. It reveals that increased diversity in benchmarks leads to greater sensitivity to irrelevant changes, highlighting the instability of aggregated rankings.
  - **Keywords:** multi-task benchmarks, social choice theory, Arrow’s impossibility theorem, quantitative measures, machine learning, sensitivity to irrelevant changes, diversity vs. stability trade-off, approximation algorithms for diversity and sensitivity measures, cardinal benchmarks, ordinal benchmarks


- [Don’t Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget](https://icml.cc/virtual/2024/poster/32618) (Poster)
  - **Authors:** [Florian Dorner](http://openreview.net/profile?id=~Florian_E._Dorner1), [Moritz Hardt](http://openreview.net/profile?id=~Moritz_Hardt1)
  - **Affiliations:** Max Planck Institute for Intelligent Systems, Tübingen; Tübingen AI Center; ETH Zürich, Max Planck Institute for Intelligent Systems, Tübingen; Tübingen AI Center
  - **TL;DR:** The study investigates the optimal allocation of a budget for noisy labels to compare the accuracy of two binary classifiers, revealing that collecting a single label for more samples is more effective than aggregating multiple labels. This finding challenges conventional practices in machine learning benchmarks and provides improved sample size bounds.
  - **Keywords:** binary classifiers, noisy labels, machine learning benchmarks, majority vote, Cramér’s theorem, comparing classifier accuracy, label noise, sample size bounds, empirical accuracy


- [Membership Inference Attacks on Diffusion Models via Quantile Regression](https://icml.cc/virtual/2024/poster/32691) (Poster)
  - **Authors:** [Shuai Tang](http://openreview.net/profile?id=~Shuai_Tang1), [Steven Wu](http://openreview.net/profile?id=~Steven_Wu1), [Sergul Aydore](http://openreview.net/profile?id=~Sergul_Aydore1), [Michael Kearns](http://openreview.net/profile?id=~Michael_Kearns2), [Aaron Roth](http://openreview.net/profile?id=~Aaron_Roth1)
  - **Affiliations:** Amazon AWS AI/ML, Amazon AWS AI/ML; Carnegie Mellon University, Amazon AWS AI/ML, University of Pennsylvania, University of Pennsylvania
  - **TL;DR:** This study investigates the privacy vulnerabilities of diffusion models through membership inference attacks, proposing a novel approach using quantile regression to predict reconstruction loss. The findings indicate that the new attack method is more efficient and effective compared to previous state-of-the-art methods.
  - **Keywords:** privacy vulnerability, membership inference attacks, diffusion models, quantile regression, hypothesis testing, image synthesis, generative models, privacy concerns, data leakage, improved membership inference attack, reduced computational expense, GANs (Generative Adversarial Networks), VAE (Variational Autoencoders)


- [Jacobian Regularizer-based Neural Granger Causality](https://icml.cc/virtual/2024/poster/34544) (Poster)
  - **Authors:** [Wanqi Zhou](http://openreview.net/profile?id=~Wanqi_Zhou2), [Shuanghao Bai](http://openreview.net/profile?id=~Shuanghao_Bai1), [Shujian Yu](http://openreview.net/profile?id=~Shujian_Yu1), [Qibin Zhao](http://openreview.net/profile?id=~Qibin_Zhao1), [Badong Chen](http://openreview.net/profile?id=~Badong_Chen1)
  - **Affiliations:** Institute of Artificial Intelligence and Robotics, Xi’an Jiao Tong University, China; RIKEN AIP, Japan, Institute of Artificial Intelligence and Robotics, Xi’an Jiao Tong University, China, Vrije Universiteit Amsterdam, Netherlands, RIKEN AIP, Japan, Institute of Artificial Intelligence and Robotics, Xi’an Jiao Tong University, China
  - **TL;DR:** This study introduces a Jacobian Regularizer-based Neural Granger Causality (JRNGC) approach that effectively learns multivariate and full-time Granger causality using a single model, overcoming limitations of existing methods. The proposed method demonstrates competitive performance while maintaining lower complexity and high scalability.
  - **Keywords:** Neural Granger causality, time-series analysis, Jacobian Regularizer, neural networks, multi-layer perceptron (MLP), recurrent neural networks (RNNs), Limitations of existing neural Granger causality methods, computational inefficiencies, scalability challenges, JRNGC approach, competitive performance with state-of-the-art methods, lower model complexity, high scalability, Granger causality, causal matrix, input-output Jacobian matrix


- [Hybrid Neural Representations for Spherical Data](https://icml.cc/virtual/2024/poster/33324) (Poster)
  - **Authors:** [Hyomin Kim](http://openreview.net/profile?id=~Hyomin_Kim4), [Yunhui Jang](http://openreview.net/profile?id=~Yunhui_Jang1), [Jaeho Lee](http://openreview.net/profile?id=~Jaeho_Lee3), [Sungsoo Ahn](http://openreview.net/profile?id=~Sungsoo_Ahn1)
  - **Affiliations:** Pohang University of Science and Technology, Pohang University of Science and Technology, Pohang University of Science and Technology, Pohang University of Science and Technology
  - **TL;DR:** This study introduces Hybrid Neural Representations for Spherical data (HNeR-S), a novel approach that combines spherical feature-grids with multi-layer perceptrons to effectively model and predict spherical signals, particularly in weather and climate data as well as cosmic microwave background data. The method demonstrates significant improvements in regression, super-resolution, temporal interpolation, and compression tasks compared to existing techniques.
  - **Keywords:** hybrid neural representations, spherical data, coordinate-based neural representations, multi-layer perceptron (MLP), spherical feature-grids, weather data, climate data, cosmic microwave background (CMB) data, capturing nonlinear signals, interpolation, increasing resolution, HNeR-S method, regression, super-resolution, temporal interpolation, compression tasks, spherical harmonics, equirectangular pixelization, hierarchical equal area isolatitude pixelization


- [Graph As Point Set](https://icml.cc/virtual/2024/poster/33653) (Poster)
  - **Authors:** [Xiyuan Wang](http://openreview.net/profile?id=~Xiyuan_Wang1), [Pan Li](http://openreview.net/profile?id=~Pan_Li2), [Muhan Zhang](http://openreview.net/profile?id=~Muhan_Zhang1)
  - **Affiliations:** Institute for Artificial Intelligence, Peking University, Georgia Institute of Technology, Institute for Artificial Intelligence, Peking University
  - **TL;DR:** This paper presents a novel graph-to-set conversion method that transforms interconnected nodes into independent points, enabling the use of set encoders for graph representation learning. The proposed Point Set Transformer (PST) demonstrates superior performance in various graph-related tasks compared to existing methods.
  - **Keywords:** Graph Neural Networks, Graph Representation Learning, Message Passing, Graph-to-Set Conversion, Set Encoder, Point Set Transformer (PST), Interconnections between entities, Information exchange among nodes, Novel graph-to-set conversion method, Improved graph representation learning, Graph Transformers, Deepset-based set encoder


- [Premise Order Matters in Reasoning with Large Language Models](https://icml.cc/virtual/2024/poster/34999) (Poster)
  - **Authors:** [Xinyun Chen](http://openreview.net/profile?id=~Xinyun_Chen1), [Ryan Chi](http://openreview.net/profile?id=~Ryan_Andrew_Chi1), [Xuezhi Wang](http://openreview.net/profile?id=~Xuezhi_Wang3), [Denny Zhou](http://openreview.net/profile?id=~Denny_Zhou1)
  - **Affiliations:** Google DeepMind, Stanford University, Google DeepMind, Google DeepMind
  - **TL;DR:** This study investigates the significant impact of premise order on the reasoning performance of large language models (LLMs), revealing that LLMs perform best when premises are presented in the same order as the ground-truth proof. The findings highlight a performance drop of over 30% when the premise order is permuted, emphasizing the importance of ordering in deductive reasoning tasks.
  - **Keywords:** Large Language Models, Reasoning Performance, Deductive Reasoning, Mathematical Problem-Solving, Premise Ordering, Performance Drop, R-GSM Benchmark, Performance Evaluation, GSM8K, Cognitive Biases, Reversal Curse, Distractibility


- [Adaptive Stabilization Based on Machine Learning for Column Generation](https://icml.cc/virtual/2024/poster/34873) (Poster)
  - **Authors:** [Yunzhuang Shen](http://openreview.net/profile?id=~Yunzhuang_Shen1), [Yuan Sun](http://openreview.net/profile?id=~Yuan_Sun1), [Xiaodong Li](http://openreview.net/profile?id=~Xiaodong_Li5), [Zhiguang Cao](http://openreview.net/profile?id=~Zhiguang_Cao1), [Andrew Eberhard](http://openreview.net/profile?id=~Andrew_Eberhard1), [Guangquan Zhang](http://openreview.net/profile?id=~Guangquan_Zhang2)
  - **Affiliations:** Australian Artificial Intelligence Institute, University of Technology Sydney, Australia; None, La Trobe Business School, La Trobe University, Australia, School of Computing Technologies, Royal Melbourne Institute of Technology, Australia, School of Computing and Information Systems, Singapore Management University, Singapore, School of Computing Technologies, Royal Melbourne Institute of Technology, Australia, Australian Artificial Intelligence Institute, University of Technology Sydney, Australia
  - **TL;DR:** This paper presents a novel approach that combines machine learning for predicting optimal dual solutions with an adaptive stabilization technique to enhance the convergence rate of column generation methods in linear programming. The proposed method significantly improves convergence compared to traditional techniques, particularly in the context of the graph coloring problem.
  - **Keywords:** Column Generation, Linear Programming, Machine Learning, Adaptive Stabilization, Dual Value Prediction, Graph Coloring Problem, Combinatorial Optimization, Oscillation of Dual Values, Convergence Rate Slowdown, Improved Convergence Rate, Accurate Dual Value Prediction


- [Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning](https://icml.cc/virtual/2024/poster/32719) (Poster)
  - **Authors:** [Jiachen Li](http://openreview.net/profile?id=~Jiachen_Li6), [Qiaozi Gao](http://openreview.net/profile?id=~Qiaozi_Gao1), [Michael Johnston](http://openreview.net/profile?id=~Michael_Johnston1), [Xiaofeng Gao](http://openreview.net/profile?id=~Xiaofeng_Gao1), [Xuehai He](http://openreview.net/profile?id=~Xuehai_He1), [Hangjie Shi](http://openreview.net/profile?id=~Hangjie_Shi1), [Suhaila Shakiah](http://openreview.net/profile?id=~Suhaila_Shakiah1), [Reza Ghanadan](http://openreview.net/profile?id=~Reza_Ghanadan1), [William Wang](http://openreview.net/profile?id=~William_Yang_Wang2)
  - **Affiliations:** Department of Computer Science, University of California, Santa Barbara, USA, Amazon AGI, Amazon AGI, Amazon AGI, Department of Computer Science, University of California, Santa Cruz, USA, Amazon AGI, Amazon AGI, Amazon AGI, Department of Computer Science, University of California, Santa Barbara, USA
  - **TL;DR:** This study presents a framework for training robots to understand multimodal prompts that combine vision and language, addressing the challenges of interpreting complex task instructions. The proposed method achieves a state-of-the-art success rate improvement and demonstrates strong in-context learning capabilities.
  - **Keywords:** Robot manipulation, Multimodal prompts, Instruction following, Inverse dynamics pretraining, Multi-task fine-tuning, Multimodal prompt encoder, Robotics, AI, Human-robot interaction, Understanding interconnection between vision and language signals, Inverse dynamic prediction, New framework for robot manipulation, State-of-the-art success rate improvement, VIMA-BENCH, Large Language Models (LLMs), Action planners


- [An Analysis of Linear Time Series Forecasting Models](https://icml.cc/virtual/2024/poster/32697) (Poster)
  - **Authors:** [William Toner](http://openreview.net/profile?id=~William_Toner1), [Luke Darlow](http://openreview.net/profile?id=~Luke_Nicholas_Darlow1)
  - **Affiliations:** Department of Informatics, University of Edinburgh, Edinburgh; Systems Infrastructure Research, Huawei Research Centre, Edinburgh, Systems Infrastructure Research, Huawei Research Centre, Edinburgh
  - **TL;DR:** This paper analyzes various linear time series forecasting models, demonstrating that many popular variants are functionally equivalent to standard linear regression. The findings indicate that simpler closed-form solutions often outperform more complex models in practical forecasting scenarios.
  - **Keywords:** Time series forecasting, Linear models, Model equivalence, Linear regression, Feature augmentation, Finance, Meteorology, Healthcare, Cloud infrastructure, Traffic flow management, Model generalisation, Performance comparison with deep learning models, Closed-form solutions, Experimental evidence of model performance, Explainability, Efficiency


- [SparQ Attention: Bandwidth-Efficient LLM Inference](https://icml.cc/virtual/2024/poster/34162) (Poster)
  - **Authors:** [Luka Ribar](http://openreview.net/profile?id=~Luka_Ribar1), [Ivan Chelombiev](http://openreview.net/profile?id=~Ivan_Chelombiev1), [Luke Hudlass-Galley](http://openreview.net/profile?email=lukehg%40graphcore.ai), [Charlie Blake](http://openreview.net/profile?id=~Charlie_Blake1), [Carlo Luschi](http://openreview.net/profile?id=~Carlo_Luschi1), [Douglas Orr](http://openreview.net/profile?id=~Douglas_Orr1)
  - **Affiliations:** Graphcore Research, United Kingdom, Synthesia, United Kingdom; Graphcore Research, United Kingdom, Graphcore Research, United Kingdom, Graphcore Research, United Kingdom, Graphcore Research, United Kingdom, Graphcore Research, United Kingdom
  - **TL;DR:** This paper introduces SparQ Attention, a technique designed to enhance the inference throughput of large language models by improving memory bandwidth efficiency during attention layer processing. The method achieves significant reductions in data transfer requirements while maintaining accuracy across various models and tasks.
  - **Keywords:** Large Language Models, Inference Optimization, SparQ Attention, Key-Value Caching, Natural Language Processing, Memory Bandwidth Limitations, Data Transfer Bottlenecks, Increased Inference Throughput, Efficient Memory Utilization, Llama 2, Llama 3, Mistral, Gemma, Pythia, Transformer Models, In-Context Learning


- [Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games](https://icml.cc/virtual/2024/poster/33991) (Poster)
  - **Authors:** [Yannik Mahlau](http://openreview.net/profile?id=~Yannik_Mahlau1), [Frederik Schubert](http://openreview.net/profile?id=~Frederik_Schubert1), [Bodo Rosenhahn](http://openreview.net/profile?id=~Bodo_Rosenhahn1)
  - **Affiliations:** Department for Information Processing, Leibniz University Hannover, Germany, Department for Information Processing, Leibniz University Hannover, Germany, Department for Information Processing, Leibniz University Hannover, Germany
  - **TL;DR:** The study introduces Albatross, an algorithm designed for zero-shot interactions in simultaneous games, which effectively models opponent behavior and adapts to various playing strengths. It demonstrates significant improvements over existing methods, achieving a 37.6% performance increase in cooperative game benchmarks and successfully exploiting weaker agents in competitive settings.
  - **Keywords:** zero-shot interaction, simultaneous games, opponent modeling, AlphaZero, Smooth Best Response Logit Equilibrium (SBRLE), Monte-Carlo Tree Search (MCTS), cooperative games, competitive games, game theory, missing information about concurrent actions, adapting to different Nash equilibria, Albatross algorithm, improvement in performance metrics, Overcooked benchmark, Battlesnake


- [A Probabilistic Approach to Learning the Degree of Equivariance in Steerable CNNs](https://icml.cc/virtual/2024/poster/35033) (Poster)
  - **Authors:** [Lars Veefkind](http://openreview.net/profile?id=~Lars_Veefkind1), [Gabriele Cesa](http://openreview.net/profile?id=~Gabriele_Cesa1)
  - **Affiliations:** University of Amsterdam, The Netherlands; QUV A-Lab, None, University of Amsterdam, The Netherlands; Qualcomm AI Research, None
  - **TL;DR:** This paper presents a probabilistic method for learning the degree of equivariance in steerable convolutional neural networks (SCNNs), addressing the challenges of overconstrained weights due to unknown or varying symmetries. The proposed approach demonstrates competitive performance on datasets with mixed symmetries and provides an interpretable degree of equivariance across the network.
  - **Keywords:** steerable convolutional neural networks, equivariance, probabilistic method, likelihood distribution, Fourier coefficients, image processing, deep learning, overconstrained weights, varying symmetries, selection of transformation group, interpretable degree of equivariance, competitive performance on datasets, equivariance constraints, geometric symmetries


- [Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup](https://icml.cc/virtual/2024/poster/33233) (Poster)
  - **Authors:** [Damien Teney](http://openreview.net/profile?id=~Damien_Teney1), [Jindong Wang](http://openreview.net/profile?id=~Jindong_Wang1), [Ehsan Abbasnejad](http://openreview.net/profile?id=~Ehsan_Abbasnejad3)
  - **Affiliations:** Idiap Research Institute, Martigny, Switzerland, Microsoft Research Asia, Beijing, China, AIML, University of Adelaide, Australia
  - **TL;DR:** This study investigates selective mixup, a technique that enhances generalization in the presence of distribution shifts by non-randomly selecting training pairs. The findings reveal that this selection implicitly biases the training distribution, leading to improvements that are not solely due to the mixing process itself.
  - **Keywords:** Selective mixup, Distribution shifts, Generalization, Mixup, Resampling, Image classification, Semantic segmentation, Natural language processing, Speech processing, Label shift, Out-of-distribution generalization, Improved understanding of selective mixup, Variants of mixing and resampling, DomainBed, WILDS, Wild-Time


- [PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning](https://icml.cc/virtual/2024/poster/32911) (Poster)
  - **Authors:** [Jaejun Lee](http://openreview.net/profile?id=~Jaejun_Lee1), [Minsung Hwang](http://openreview.net/profile?id=~Minsung_Hwang1), [Joyce Whang](http://openreview.net/profile?id=~Joyce_Jiyoung_Whang2)
  - **Affiliations:** School of Computing, KAIST, Daejeon, South Korea, School of Computing, KAIST, Daejeon, South Korea, School of Computing, KAIST, Daejeon, South Korea
  - **TL;DR:** This paper presents the first PAC-Bayesian generalization bounds for knowledge graph representation learning (KGRL) methods, introducing a framework called ReED that encompasses various existing KGRL models. The findings provide theoretical insights into common practices in KGRL and explain generalization errors observed in real-world knowledge graphs.
  - **Keywords:** Knowledge Graph Representation Learning (KGRL), Generalization Bounds, PAC-Bayesian approaches, Relation-aware Encoder-Decoder, Graph Neural Networks (GNNs), Message Passing Neural Networks (MPNNs), Knowledge Graph Completion, Theoretical analysis of KGRL methods, Generalization errors, Generalization bounds for KGRL, Framework for analyzing KGRL models, R-GCN, CompGCN, RotatE, ANALOGY


- [Optimizing Watermarks for Large Language Models](https://icml.cc/virtual/2024/poster/34093) (Poster)
  - **Authors:** [Bram Wouters](http://openreview.net/profile?id=~Bram_Wouters1)
  - **Affiliations:** University of Amsterdam
  - **TL;DR:** This paper addresses the trade-off between identifiability and stealthiness in watermarking large language models (LLMs) by introducing a systematic multi-objective optimization approach. The study identifies Pareto optimal solutions that outperform existing watermarking strategies, enhancing the ability to distinguish LLM-generated text from human-generated text.
  - **Keywords:** Large Language Models, Watermarking, Multi-objective optimization, Hypothesis testing, Text generation, AI safety, Identifiability vs. stealthiness trade-off, Quality degradation, Pareto optimal solutions, Robust and efficient watermarks, Green-red split, LLM-generated text


- [Accelerating Legacy Numerical Solvers by Non-intrusive Gradient-based Meta-solving](https://icml.cc/virtual/2024/poster/32660) (Poster)
  - **Authors:** [Sohei Arisaka](http://openreview.net/profile?id=~Sohei_Arisaka1), [Qianxiao Li](http://openreview.net/profile?id=~Qianxiao_Li1)
  - **Affiliations:** Department of Mathematics, National University of Singapore; Kajima Corporation, Japan, Department of Mathematics, National University of Singapore
  - **TL;DR:** This study proposes a non-intrusive gradient-based methodology to enhance legacy numerical solvers using machine learning techniques, addressing the limitations of existing methods that require automatic differentiation. The proposed approach demonstrates significant advantages in computational efficiency and applicability to established numerical codes.
  - **Keywords:** Scientific computing, Machine learning, Meta-learning, Gradient estimation, Non-intrusive methodology, Numerical solvers, Legacy codes, Computational cost, Non-automatic-differentiable methods, ML-enhanced simulation, Hyperparameter selection, PETSc, Scientific Machine Learning (SciML), Preconditioners


- [Is In-Context Learning in Large Language Models Bayesian? A Martingale Perspective](https://icml.cc/virtual/2024/poster/33659) (Poster)
  - **Authors:** [Fabian Falck](http://openreview.net/profile?id=~Fabian_Falck1), [Ziyu Wang](http://openreview.net/profile?id=~Ziyu_Wang2), [Christopher Holmes](http://openreview.net/profile?id=~Christopher_C._Holmes1)
  - **Affiliations:** Department of Statistics, University of Oxford, Oxford, UK, Department of Statistics, University of Oxford, Oxford, UK, Department of Statistics, University of Oxford, Oxford, UK
  - **TL;DR:** This study investigates whether in-context learning in large language models can be considered approximately Bayesian by analyzing the martingale property. The findings indicate violations of this property and deviations from expected Bayesian behavior regarding uncertainty, challenging the hypothesis that ICL is Bayesian.
  - **Keywords:** In-Context Learning, Large Language Models, Bayesian Inference, Martingale Property, Uncertainty in predictions, Exchangeable data, Evidence for violations of the martingale property, Bayesian scaling behaviour of uncertainty, AI Safety, Trustworthy Systems


- [Hieros: Hierarchical Imagination on Structured State Space Sequence World Models](https://icml.cc/virtual/2024/poster/34428) (Poster)
  - **Authors:** [Paul Mattes](http://openreview.net/profile?id=~Paul_Mattes1), [Rainer Schlosser](http://openreview.net/profile?id=~Rainer_Schlosser1), [Ralf Herbrich](http://openreview.net/profile?id=~Ralf_Herbrich1)
  - **Affiliations:** Digital Engineering Faculty, Hasso Plattner Institute, University of Potsdam, Germany, Digital Engineering Faculty, Hasso Plattner Institute, University of Potsdam, Germany, None
  - **TL;DR:** The study introduces HIEROS, a hierarchical policy that enhances sample efficiency in deep reinforcement learning by learning time abstracted world representations and imagining trajectories at multiple time scales. The proposed method outperforms existing approaches in terms of training efficiency and exploration capabilities while accurately predicting complex dynamics.
  - **Keywords:** deep reinforcement learning, world models, hierarchical policy, S5 layer-based world model, recurrent state space model (RSSM), Atari 100k benchmark, simulated environments, sample efficiency, imagination accuracy, exploration capabilities, runtime efficiency, HIEROS, improved training efficiency, accurate prediction of complex dynamics, Atari 100k


- [Online Variational Sequential Monte Carlo](https://icml.cc/virtual/2024/poster/33294) (Poster)
  - **Authors:** [Alessandro Mastrototaro](http://openreview.net/profile?id=~Alessandro_Mastrototaro1), [Jimmy Olsson](http://openreview.net/profile?id=~Jimmy_Olsson1)
  - **Affiliations:** Department of Mathematics, KTH Royal Institute of Technology, Stockholm, Sweden, Department of Mathematics, KTH Royal Institute of Technology, Stockholm, Sweden
  - **TL;DR:** This paper presents an online version of the variational sequential Monte Carlo (VSMC) method for efficient parameter estimation and Bayesian latent-state inference in state-space models, enabling real-time learning from data streams. The authors demonstrate the algorithm's convergence properties and its effectiveness in both online and batch-processing settings.
  - **Keywords:** state-space models, generative models, online learning, variational sequential Monte Carlo (VSMC), particle methods, variational inference, stochastic approximation, AI, statistical machine learning, complex latent-state posteriors, joint posterior distributions, parameter learning, state inference, efficient model parameter estimation, Bayesian latent-state inference, convergence properties, joint-smoothing distributions, Kullback–Leibler divergence (KLD), importance-weighted autoencoder (IWAE), evidence lower bound (ELBO)


- [MS$^3$D: A RG Flow-Based Regularization for GAN Training with Limited Data](https://icml.cc/virtual/2024/poster/33961) (Poster)
  - **Authors:** [Jian Wang](http://openreview.net/profile?id=~Jian_Wang12), [Xin Lan](http://openreview.net/profile?id=~Xin_Lan1), [Yuxin Tian](http://openreview.net/profile?id=~Yuxin_Tian3), [Jiancheng Lv](http://openreview.net/profile?id=~Jiancheng_Lv2)
  - **Affiliations:** College of Computer Science, Sichuan University; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, P.R. China, College of Computer Science, Sichuan University; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, P.R. China, College of Computer Science, Sichuan University; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, P.R. China, College of Computer Science, Sichuan University; Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education, Chengdu 610065, P.R. China
  - **TL;DR:** This study proposes a novel regularization method called multi-scale structural self-dissimilarity (MS3D) to improve the training of GANs with limited data, addressing issues of discriminator overfitting. The method enhances the performance and stability of GANs, enabling them to generate high-quality images even with very few training samples.
  - **Keywords:** GANs (Generative Adversarial Networks), limited data training, Multi-scale structural self-dissimilarity (MS3D) regularization, renormalization group (RG), Image generation, Discriminator overfitting, data sparsity, Enhanced performance and stability of GANs, high-quality image generation with limited data


- [Improved Stability and Generalization Guarantees of the Decentralized SGD Algorithm](https://icml.cc/virtual/2024/poster/34384) (Poster)
  - **Authors:** [Batiste Le Bars](http://openreview.net/profile?id=~Batiste_Le_bars1), [Aurélien Bellet](http://openreview.net/profile?id=~Aur%C3%A9lien_Bellet1), [Marc Tommasi](http://openreview.net/profile?id=~Marc_Tommasi1), [Kevin Scaman](http://openreview.net/profile?id=~Kevin_Scaman1), [Giovanni Neglia](http://openreview.net/profile?id=~Giovanni_Neglia1)
  - **Affiliations:** Inria Paris - Ecole Normale Supérieure, PSL Research University, Inria, Université de Montpellier, Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189, CRIStAL, F-59000 Lille, Inria Paris - Ecole Normale Supérieure, PSL Research University, Inria, Université Côte d’Azur
  - **TL;DR:** This paper analyzes the generalization error of Decentralized Stochastic Gradient Descent (D-SGD) and demonstrates that it can achieve generalization bounds similar to classical SGD, regardless of the communication graph structure. The findings suggest that a poorly-connected graph may even enhance generalization in certain scenarios.
  - **Keywords:** Decentralized Learning, Generalization Error, Stochastic Gradient Descent, Decentralized Stochastic Gradient Descent (D-SGD), Algorithmic Stability, Machine Learning, Optimization Algorithms, Generalization Error, Communication Graphs, Data Heterogeneity, Generalization Bounds, Optimization-Dependent Generalization Bound, Convex Functions, Strongly Convex Functions, Non-Convex Functions


- [Understanding Heterophily for Graph Neural Networks](https://icml.cc/virtual/2024/poster/32750) (Poster)
  - **Authors:** [Junfu Wang](http://openreview.net/profile?id=~Junfu_Wang1), [Yuanfang Guo](http://openreview.net/profile?id=~Yuanfang_Guo1), [Liang Yang](http://openreview.net/profile?id=~Liang_Yang2), [Yunhong Wang](http://openreview.net/profile?id=~Yunhong_Wang1)
  - **Affiliations:** State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Shen Yuan Honors College, Beihang University, Beijing, China, State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China, School of Artificial Intelligence, Hebei University of Technology, Tianjin, China, School of Computer Science and Engineering, Beihang University, Beijing, China
  - **TL;DR:** This study investigates the theoretical implications of heterophily in Graph Neural Networks (GNNs) and presents a new model, Heterophilous Stochastic Block Models (HSBM), to analyze the impact of various heterophily patterns on node separability. The findings reveal that both low and high heterophily ratios can enhance classification performance, while neighborhood inconsistency and over-smoothing present significant challenges.
  - **Keywords:** Heterophily, Graph Neural Networks (GNNs), Graph Convolution (GC), Heterophilous Stochastic Block Models (HSBM), Challenges of heterophily in GNNs, neighborhood inconsistency, over-smoothing, Theoretical understandings of heterophily, separability gains, impact of neighborhood distributions


- [Leveraging VLM-Based Pipelines to Annotate 3D Objects](https://icml.cc/virtual/2024/poster/34981) (Poster)
  - **Authors:** [Rishabh Kabra](http://openreview.net/profile?id=~Rishabh_Kabra1), [Loic Matthey](http://openreview.net/profile?id=~Loic_Matthey1), [Alexander Lerchner](http://openreview.net/profile?id=~Alexander_Lerchner1), [Niloy Mitra](http://openreview.net/profile?id=~Niloy_Mitra1)
  - **Affiliations:** Google DeepMind; University College London, Google DeepMind, Google DeepMind, University College London
  - **TL;DR:** This study proposes a novel pipeline leveraging vision language models to annotate 3D objects reliably by utilizing probabilistic aggregation methods, addressing issues like hallucination and viewpoint variability. The approach demonstrates improved accuracy in inferring object types and enhances downstream predictions, showcasing its effectiveness on a large dataset of 764K objects.
  - **Keywords:** Vision Language Models (VLMs), 3D Object Annotation, Probabilistic Aggregation, Score-based Aggregation (ScoreAgg), 3D Object Recognition, Conditional Inference, Hallucination in Text-based Aggregation, Viewpoint Variability, Improved Object Type Inference, Enhanced Downstream Predictions, Objaverse Dataset


- [The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline](https://icml.cc/virtual/2024/poster/33717) (Oral)
  - **Authors:** [Haonan Wang](http://openreview.net/profile?id=~Haonan_Wang1), [Qianli Shen](http://openreview.net/profile?id=~Qianli_Shen1), [Yao Tong](http://openreview.net/profile?id=~Yao_Tong2), [Yang Zhang](http://openreview.net/profile?id=~Yang_Zhang22), [Kenji Kawaguchi](http://openreview.net/profile?id=~Kenji_Kawaguchi1)
  - **Affiliations:** School of Computing, National University of Singapore, School of Computing, National University of Singapore, School of Computing, National University of Singapore, School of Computing, National University of Singapore, School of Computing, National University of Singapore
  - **TL;DR:** This study formalizes a copyright infringement attack on generative AI models and introduces a backdoor attack method, SilentBadDiffusion, which can induce copyright breaches without access to training processes. The findings reveal that stronger diffusion models make it easier to execute such attacks, highlighting vulnerabilities in current copyright protection strategies.
  - **Keywords:** copyright infringement, generative AI models, text-to-image diffusion models, backdoor attack, SilentBadDiffusion, copyright concerns, vulnerabilities in copyright protection, method for inducing copyright infringement, effectiveness of poisoning data


- [On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning](https://icml.cc/virtual/2024/poster/34844) (Spotlight Poster)
  - **Authors:** [Ari Karchmer](http://openreview.net/profile?id=~Ari_Karchmer1)
  - **Affiliations:** Department of Computer Science, Boston University, Boston, MA, USA
  - **TL;DR:** This paper investigates the computational separations between multimodal and unimodal machine learning, demonstrating that unimodal learning is computationally hard for typical instances while multimodal learning is easier. The findings suggest that strong computational advantages of multimodal learning may be infrequent in practice, as they are tied to specific cryptographic distributions.
  - **Keywords:** multimodal machine learning, unimodal machine learning, computational separations, computational hardness, average-case vs worst-case instances, stronger average-case computational separation, cryptographic key agreement protocol


- [Unifying Bayesian Flow Networks and Diffusion Models through Stochastic Differential Equations](https://icml.cc/virtual/2024/poster/35139) (Poster)
  - **Authors:** [Kaiwen Xue](http://openreview.net/profile?id=~Kaiwen_Xue1), [Yuhao Zhou](http://openreview.net/profile?id=~Yuhao_Zhou2), [Shen Nie](http://openreview.net/profile?id=~Shen_Nie2), [Xu Min](http://openreview.net/profile?id=~Xu_Min1), [Xiaolu Zhang](http://openreview.net/profile?id=~Xiaolu_Zhang2), [JUN ZHOU](http://openreview.net/profile?id=~JUN_ZHOU6), [Chongxuan Li](http://openreview.net/profile?id=~Chongxuan_Li1)
  - **Affiliations:** Gaoling School of AI, Renmin University of China, Beijing, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China, Gaoling School of AI, Renmin University of China, Beijing, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Gaoling School of AI, Renmin University of China, Beijing, China
  - **TL;DR:** This paper connects Bayesian Flow Networks (BFNs) with Diffusion Models (DMs) through stochastic differential equations (SDEs) to enhance BFN's sampling capabilities. The proposed specialized solvers significantly improve sample quality and speed, achieving a 5 to 20 times increase in efficiency with limited function evaluations.
  - **Keywords:** Bayesian Flow Networks, Diffusion Models, Deep Generative Models, Stochastic Differential Equations, Denoise Score Matching, Image Generation, Language Modeling, Handling Discrete Variables, Efficient Inference, Specialized Solvers for BFNs, Improved Sampling Quality


- [Ai-sampler: Adversarial Learning of Markov kernels with involutive maps](https://icml.cc/virtual/2024/poster/32723) (Poster)
  - **Authors:** [Evgenii Egorov](http://openreview.net/profile?id=~Evgenii_Egorov1), [Riccardo Valperga](http://openreview.net/profile?id=~Riccardo_Valperga1), [Efstratios Gavves](http://openreview.net/profile?id=~Stratis_Gavves1)
  - **Affiliations:** Amsterdam Machine Learning Lab, University of Amsterdam, the Netherlands, Riccardo Valperga VIS-Lab, University of Amsterdam, the Netherlands, University of Amsterdam, the Netherlands
  - **TL;DR:** This study proposes a novel MCMC method utilizing time-reversible neural networks to efficiently sample from complex unnormalized distributions by minimizing the total variation distance to the target distribution. The approach leverages an adversarial training objective and demonstrates effective sampling through various synthetic and real-world examples.
  - **Keywords:** Markov Chain Monte Carlo (MCMC), sampling from complex distributions, Metropolis-Hastings kernels, reversible neural networks, adversarial objective, Bayesian inference, probabilistic modeling, integral estimation, time-series analysis, Efficient sampling, good mixing, convergence to target distribution, measuring sample quality, Training methods for transition kernels, upper bound to total variation distance, C2-equivariance, implicit generative model


- [Equivariant Diffusion for Crystal Structure Prediction](https://icml.cc/virtual/2024/poster/33915) (Poster)
  - **Authors:** [Peijia Lin](http://openreview.net/profile?id=~Peijia_Lin1), [Pin Chen](http://openreview.net/profile?id=~Pin_Chen1), [Rui Jiao](http://openreview.net/profile?id=~Rui_Jiao1), [Qing Mo](http://openreview.net/profile?id=~Qing_Mo1), [Jianhuan Cen](http://openreview.net/profile?id=~Cen_Jianhuan1), [Wenbing Huang](http://openreview.net/profile?id=~Wenbing_Huang1), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu19), [Dan Huang](http://openreview.net/profile?id=~Dan_Huang3), [Yutong Lu](http://openreview.net/profile?id=~Yutong_Lu1)
  - **Affiliations:** School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; National Supercomputer Center in Guangzhou, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; National Supercomputer Center in Guangzhou, China, Dept. of Comp. Sci. Tech., Institute for AI, Tsinghua University, Beijing, China; Institute for AIR, Tsinghua University, Beijing, China, National Supercomputer Center in Guangzhou, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China, Dept. of Comp. Sci. Tech., Institute for AI, Tsinghua University, Beijing, China; Institute for AIR, Tsinghua University, Beijing, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; National Supercomputer Center in Guangzhou, China
  - **TL;DR:** This study introduces EquiCSP, an equivariant diffusion model for Crystal Structure Prediction (CSP) that effectively addresses lattice permutation and periodic translation equivariance. The model demonstrates superior accuracy in generating crystal structures and faster training convergence compared to existing methods.
  - **Keywords:** Crystal Structure Prediction (CSP), symmetry-aware deep learning, generative models, Equivariant diffusion models, denoising diffusion probabilistic models (DDPMs), score-based generative models, stochastic differential equations (SDEs), Physics, chemistry, materials science, Lattice permutation equivariance, periodic translation equivariance, complexity of potential energy landscape, EquiCSP model, unique noising algorithm, improved structure generation accuracy, faster convergence, E(3) transformations, atomic fractional coordinates


- [Sequential Asynchronous Action Coordination in Multi-Agent Systems: A Stackelberg Decision Transformer Approach](https://icml.cc/virtual/2024/poster/34258) (Poster)
  - **Authors:** [Bin Zhang](http://openreview.net/profile?id=~Bin_Zhang12), [Hangyu Mao](http://openreview.net/profile?id=~Hangyu_Mao2), [Lijuan Li](http://openreview.net/profile?id=~Lijuan_Li2), [Zhiwei Xu](http://openreview.net/profile?id=~Zhiwei_Xu3), [dapeng Li](http://openreview.net/profile?id=~Dapeng_Li2), [Rui Zhao](http://openreview.net/profile?id=~Rui_Zhao6), [Guoliang Fan](http://openreview.net/profile?id=~Guoliang_Fan3)
  - **Affiliations:** Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, SenseTime Research, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, School of Computer Science and Technology, Shandong University, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, SenseTime Research, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences
  - **TL;DR:** This study introduces the Stackelberg Decision Transformer (STEER) to enhance decision-making in Multi-Agent Systems by addressing the challenges of asynchronous action coordination and scalability in existing MARL methods. Experimental results show that STEER converges towards Stackelberg equilibrium strategies and outperforms strong baselines in complex scenarios.
  - **Keywords:** Multi-Agent Systems (MAS), Asynchronous Action Coordination, Stackelberg Game (SG), Multi-Agent Reinforcement Learning (MARL), Stackelberg Decision Transformer (STEER), Autoregressive Sequence Models, Scalability issues in MARL, Complex interactions among agents, Limitations of existing methods in mixed tasks, Stackelberg equilibrium strategies, Efficient decision-making processes


- [Sparse Dimensionality Reduction Revisited](https://icml.cc/virtual/2024/poster/32816) (Poster)
  - **Authors:** [Mikael Møller Høgsgaard](http://openreview.net/profile?id=~Mikael_M%C3%B8ller_H%C3%B8gsgaard1), [Lior Kamma](http://openreview.net/profile?id=~Lior_Kamma1), [Kasper Green Larsen](http://openreview.net/profile?id=~Kasper_Green_Larsen1), [Jelani Nelson](http://openreview.net/profile?id=~Jelani_Nelson2), [Chris Schwiegelshohn](http://openreview.net/profile?id=~Chris_Schwiegelshohn1)
  - **Affiliations:** Computer Science Department, Aarhus University, Aarhus, Denmark, School of Computer Science, Academic College of Tel-Aviv Yaffo, Tel-Aviv, Israel, Computer Science Department, Aarhus University, Aarhus, Denmark, Department of EECS, UC Berkeley, Berkeley, CA, USA, Computer Science Department, Aarhus University, Aarhus, Denmark
  - **TL;DR:** This paper revisits sparse embeddings in dimensionality reduction, presenting a new embedding method that achieves improved sparsity for cases where the dimensionality is low. The findings include a strengthened lower bound for embeddings and enhancements to the best oblivious subspace embeddings.
  - **Keywords:** Dimensionality reduction, Sparse embeddings, Sparse Johnson-Lindenstrauss transform, Oblivious subspace embeddings, Large scale data analysis, Memory consumption reduction, High-dimensional data, Embedding time optimization, Improved sparsity in embeddings, Strengthened lower bounds for embeddings, Johnson-Lindenstrauss transform, ε-JL matrix


- [DiffDA: a Diffusion model for weather-scale Data Assimilation](https://icml.cc/virtual/2024/poster/32775) (Poster)
  - **Authors:** [Langwen Huang](http://openreview.net/profile?id=~Langwen_Huang1), [Lukas Gianinazzi](http://openreview.net/profile?id=~Lukas_Gianinazzi1), [Yuejiang Yu](http://openreview.net/profile?email=yuejyu%40student.ethz.ch), [Peter Dueben](http://openreview.net/profile?id=~Peter_Dominik_Dueben1), [Torsten Hoefler](http://openreview.net/profile?id=~Torsten_Hoefler1)
  - **Affiliations:** Department of Computer Science, ETH Zürich, Switzerland, Department of Computer Science, ETH Zürich, Switzerland, Department of Computer Science, ETH Zürich, Switzerland, European Centre for Medium-Range Weather Forecasts (ECMWF), Reading, United Kingdom, Department of Computer Science, ETH Zürich, Switzerland
  - **TL;DR:** The study introduces DiffDA, a denoising diffusion model for assimilating atmospheric data using sparse observations, achieving the highest resolution in ML data assimilation models. It demonstrates that initial conditions from sparse data can effectively support weather forecasting with minimal lead time loss.
  - **Keywords:** weather forecasting, climate modeling, data assimilation, denoising diffusion model, GraphCast neural network, atmospheric data assimilation, reanalysis datasets, data sparsity, forecast error, assimilated global atmospheric data, autoregressive data assimilation, ERA5 reanalysis dataset


- [PEARL: Zero-shot Cross-task Preference Alignment and Robust Reward Learning for Robotic Manipulation](https://icml.cc/virtual/2024/poster/35173) (Poster)
  - **Authors:** [Runze Liu](http://openreview.net/profile?id=~Runze_Liu2), [Yali Du](http://openreview.net/profile?id=~Yali_Du1), [Fengshuo Bai](http://openreview.net/profile?id=~Fengshuo_Bai1), [Jiafei Lyu](http://openreview.net/profile?id=~Jiafei_Lyu1), [Xiu Li](http://openreview.net/profile?id=~Xiu_Li1)
  - **Affiliations:** Tsinghua Shenzhen International Graduate School, Tsinghua University, King’s College London, Beijing Institute for General AI; Shanghai Jiao Tong University, Tsinghua Shenzhen International Graduate School, Tsinghua University, Tsinghua Shenzhen International Graduate School, Tsinghua University
  - **TL;DR:** This paper presents PEARL, a method for learning policies in robotic manipulation through zero-shot cross-task preference alignment and robust reward learning, eliminating the need for human preference labels in new tasks. The empirical results demonstrate that PEARL effectively transfers preference labels across tasks and outperforms existing methods, especially when human preferences are limited.
  - **Keywords:** Preference-based Reinforcement Learning, Robotic Manipulation, Cross-task Preference Alignment, Robust Reward Learning, Gromov-Wasserstein distance, Data sparsity in preference labels, Reward engineering challenges, Reward hacking, Transfer of preference labels, Learning policies without human labels, Meta-World, Robomimic, Optimal transport, Gaussian distributions


- [Subhomogeneous Deep Equilibrium Models](https://icml.cc/virtual/2024/poster/33781) (Poster)
  - **Authors:** [Pietro Sittoni](http://openreview.net/profile?id=~Pietro_Sittoni1), [Francesco Tudisco](http://openreview.net/profile?id=~Francesco_Tudisco1)
  - **Affiliations:** School of Mathematics, Gran Sasso Science Institute, L’Aquila, Italy, School of Mathematics and Maxwell Institute, University of Edinburgh, Edinburgh, UK
  - **TL;DR:** This paper presents a new analysis of the existence and uniqueness of fixed points in implicit-depth neural networks using subhomogeneous operators and nonlinear Perron-Frobenius theory. The findings provide a more flexible framework for designing stable Deep Equilibrium Models, addressing issues of stability and reproducibility.
  - **Keywords:** Implicit-depth neural networks, Deep Equilibrium Models, Subhomogeneous operators, Nonlinear Perron-Frobenius theory, Feedforward neural networks, Convolutional neural networks, Graph neural networks, Existence and uniqueness of fixed points, Stability, Performance, Reproducibility issues, New analysis framework for implicit networks, Unique fixed points for a broad class of operators, Neural ODEs, Fixed-point equation, Activation functions


- [ContPhy: Continuum Physical Concept Learning and Reasoning from Videos](https://icml.cc/virtual/2024/poster/32864) (Poster)
  - **Authors:** [Zhicheng Zheng](http://openreview.net/profile?id=~Zhicheng_Zheng2), [Xin Yan](http://openreview.net/profile?id=~Xin_Yan3), [Zhenfang Chen](http://openreview.net/profile?id=~Zhenfang_Chen1), [Jingzhou Wang](http://openreview.net/profile?id=~Jingzhou_Wang3), [Qin Zhi Eddie Lim](http://openreview.net/profile?id=~Qin_Zhi_Eddie_Lim1), [Josh Tenenbaum](http://openreview.net/profile?id=~Joshua_B._Tenenbaum1), [Chuang Gan](http://openreview.net/profile?id=~Chuang_Gan1)
  - **Affiliations:** Tsinghua University, Wuhan University, MIT-IBM Watson AI Lab, None, Massachusetts Institute of Technology, Massachusetts Institute of Technology, MIT-IBM Watson AI Lab; UMass Amherst
  - **TL;DR:** The study introduces the Continuum Physical Dataset (ContPhy) to evaluate machine physical commonsense, highlighting the challenges AI models face in understanding diverse physical properties and dynamics. It also presents an oracle model (ContPRO) that combines particle-based dynamics with large language models to improve predictions and reasoning in physical scenarios.
  - **Keywords:** physical commonsense, physical reasoning, continuum, particle-based physical dynamic models, large language models, machine learning, physical property inference, lack of physical commonsense in AI models, challenges in predicting dynamics, Continuum Physical Dataset (ContPhy), oracle model (ContPRO), Continuum Physical Dataset (ContPhy), soft bodies, rigid bodies, dynamic interactions


- [A New Theoretical Perspective on Data Heterogeneity in Federated Optimization](https://icml.cc/virtual/2024/poster/32945) (Poster)
  - **Authors:** [Jiayi Wang](http://openreview.net/profile?id=~Jiayi_Wang4), [Shiqiang Wang](http://openreview.net/profile?id=~Shiqiang_Wang1), [Rong-Rong Chen](http://openreview.net/profile?id=~Rong-Rong_Chen1), [Mingyue Ji](http://openreview.net/profile?id=~Mingyue_Ji1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA, IBM T. J. Watson Research Center, Yorktown Heights, NY, USA, Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA, Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA
  - **TL;DR:** This paper presents a new theoretical perspective on data heterogeneity in federated learning, proposing a heterogeneity-driven pseudo-Lipschitz assumption that better characterizes the impact of data heterogeneity on convergence rates. The findings indicate that increasing local updates can improve convergence even under high gradient divergence, challenging existing pessimistic theoretical analyses.
  - **Keywords:** Federated Learning, Data Heterogeneity, Federated Averaging (FedAvg), Local Stochastic Gradient Descent (local SGD), Convergence Rate, Gradient Divergence, Non-IID Data, Heterogeneity-driven Pseudo-Lipschitz Constant, Convergence Upper Bound


- [3D-VLA: A 3D Vision-Language-Action Generative World Model](https://icml.cc/virtual/2024/poster/34575) (Poster)
  - **Authors:** [Haoyu Zhen](http://openreview.net/profile?id=~Haoyu_Zhen1), [Xiaowen Qiu](http://openreview.net/profile?id=~Xiaowen_Qiu1), [Peihao Chen](http://openreview.net/profile?id=~Peihao_Chen1), [Jincheng Yang](http://openreview.net/profile?id=~Jincheng_Yang2), [Xin Yan](http://openreview.net/profile?id=~Xin_Yan3), [Yilun Du](http://openreview.net/profile?id=~Yilun_Du1), [Yining Hong](http://openreview.net/profile?id=~Yining_Hong1), [Chuang Gan](http://openreview.net/profile?id=~Chuang_Gan1)
  - **Affiliations:** University of Massachusetts Amherst; Shanghai Jiao Tong University, University of Massachusetts Amherst, South China University of Technology, Shanghai Jiao Tong University, Wuhan University, Massachusetts Institute of Technology, University of California, Los Angeles, University of Massachusetts Amherst; MIT-IBM Watson AI Lab
  - **TL;DR:** The study introduces 3D-VLA, a generative world model that integrates 3D perception, reasoning, and action, addressing the limitations of existing 2D-based models. It demonstrates significant improvements in reasoning, multimodal generation, and planning capabilities in embodied environments, highlighting its potential for real-world applications.
  - **Keywords:** 3D Vision-Language-Action, embodied foundation models, generative world model, large language model (LLM), embodied diffusion models, robotics, embodied environments, lack of 3D understanding in existing models, direct mapping from perception to action, absence of 3D-related annotations, improved reasoning, multimodal generation, planning capabilities, large-scale 3D embodied instruction dataset, existing robotics datasets


- [Rich-Observation Reinforcement Learning with Continuous Latent Dynamics](https://icml.cc/virtual/2024/poster/34487) (Poster)
  - **Authors:** [Yuda Song](http://openreview.net/profile?id=~Yuda_Song2), [Lili Wu](http://openreview.net/profile?id=~Lili_Wu1), [Dylan Foster](http://openreview.net/profile?id=~Dylan_J_Foster1), [Akshay Krishnamurthy](http://openreview.net/profile?id=~Akshay_Krishnamurthy1)
  - **Affiliations:** Carnegie Mellon University, Microsoft Research, Microsoft Research, Microsoft Research
  - **TL;DR:** This paper introduces the RichCLD framework for reinforcement learning, addressing the challenges of sample-efficiency and reliability in continuous settings with high-dimensional observations. The authors present a new algorithm that is statistically and computationally efficient, providing insights into the complexities of representation learning in this context.
  - **Keywords:** reinforcement learning, continuous latent dynamics, rich observations, RichCLD framework, representation learning, robotic control, online resource allocation, sample-efficiency, reliability, exploration challenges, new algorithm for RichCLD, insights into statistical complexity, Lipschitz continuous dynamics


- [Generalized Preference Optimization: A Unified Approach to Offline Alignment](https://icml.cc/virtual/2024/poster/33409) (Poster)
  - **Authors:** [Yunhao Tang](http://openreview.net/profile?id=~Yunhao_Tang1), [Zhaohan Guo](http://openreview.net/profile?id=~Zhaohan_Daniel_Guo1), [Zeyu Zheng](http://openreview.net/profile?id=~Zeyu_Zheng1), [Daniele Calandriello](http://openreview.net/profile?id=~Daniele_Calandriello1), [REMI MUNOS](http://openreview.net/profile?id=~Remi_Munos1), [Mark Rowland](http://openreview.net/profile?id=~Mark_Rowland1), [Pierre Richemond](http://openreview.net/profile?id=~Pierre_Harvey_Richemond1), [Michal Valko](http://openreview.net/profile?id=~Michal_Valko1), [Bernardo Avila Pires](http://openreview.net/profile?id=~Bernardo_Avila_Pires1), [Bilal Piot](http://openreview.net/profile?id=~Bilal_Piot1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** The paper introduces generalized preference optimization (GPO) as a unified framework for offline preference optimization, encompassing existing algorithms and providing new variants. It highlights the trade-offs between regularization and performance in aligning AI systems with human values through reinforcement learning from human feedback (RLHF).
  - **Keywords:** Offline preference optimization, AI alignment, Generalized preference optimization (GPO), DPO, IPO, SLiC, reinforcement learning from human feedback (RLHF), Large language models (LLMs), Regularization in offline algorithms, trade-offs between regularization and performance, New algorithmic toolkits, empirical insights for alignment practitioners, Convex functions, pairwise comparison datasets


- [Diffusion Rejection Sampling](https://icml.cc/virtual/2024/poster/34559) (Poster)
  - **Authors:** [Byeonghu Na](http://openreview.net/profile?id=~Byeonghu_Na1), [Yeongmin Kim](http://openreview.net/profile?id=~Yeongmin_Kim1), [Minsang Park](http://openreview.net/profile?id=~Minsang_Park1), [Donghyeok Shin](http://openreview.net/profile?id=~Donghyeok_Shin2), [Wanmo Kang](http://openreview.net/profile?id=~Wanmo_Kang1), [IL CHUL MOON](http://openreview.net/profile?id=~Il-chul_Moon1)
  - **Affiliations:** Department of Industrial & Systems Engineering, KAIST, Daejeon, Republic of Korea, Department of Industrial & Systems Engineering, KAIST, Daejeon, Republic of Korea, Department of Industrial & Systems Engineering, KAIST, Daejeon, Republic of Korea, Department of Industrial & Systems Engineering, KAIST, Daejeon, Republic of Korea, Department of Industrial & Systems Engineering, KAIST, Daejeon, Republic of Korea, Department of Industrial & Systems Engineering, KAIST, Daejeon, Republic of Korea; summary.ai, Daejeon, Republic of Korea
  - **TL;DR:** This paper introduces Diffusion Rejection Sampling (DiffRS), a method that enhances sampling performance in diffusion models by evaluating and refining sample quality at each timestep. The results show that DiffRS achieves state-of-the-art performance on benchmark datasets while maintaining efficiency in sampling speed.
  - **Keywords:** diffusion models, sampling performance, Diffusion Rejection Sampling (DiffRS), rejection sampling scheme, time-dependent discriminator, image generation, video generation, text-to-image generation, sampling error, sampling speed, quality of samples, state-of-the-art performance, tighter upper bound on sampling error, CIFAR-10, ImageNet 64 × 64, DPM-Solver++, Stable Diffusion


- [Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models](https://icml.cc/virtual/2024/poster/33030) (Poster)
  - **Authors:** [Didi Zhu](http://openreview.net/profile?id=~Didi_Zhu1), [Zhongyi Sun](http://openreview.net/profile?id=~Zhongyisun_Sun1), [Zexi Li](http://openreview.net/profile?id=~Zexi_Li1), [tao shen](http://openreview.net/profile?id=~Tao_Shen4), [Ke Yan](http://openreview.net/profile?id=~Ke_Yan2), [Shouhong Ding](http://openreview.net/profile?id=~Shouhong_Ding3), [Chao Wu](http://openreview.net/profile?id=~Chao_Wu1), [Kun Kuang](http://openreview.net/profile?id=~Kun_Kuang1)
  - **Affiliations:** Department of Computer and Science, Zhejiang University; Tencent YouTu Lab, Tencent YouTu Lab, Department of Computer and Science, Zhejiang University, Department of Computer and Science, Zhejiang University, Tencent YouTu Lab, Tencent YouTu Lab, Department of Computer and Science, Zhejiang University, Department of Computer and Science, Zhejiang University
  - **TL;DR:** This paper addresses the issue of catastrophic forgetting in multi-modal large language models (MLLMs) by introducing a method called Model Tailor, which preserves pre-trained parameters while fine-tuning. The approach maintains high effectiveness on original tasks and achieves significant performance on new tasks, demonstrating adaptability in multi-task scenarios.
  - **Keywords:** Catastrophic forgetting, Multi-modal large language models (MLLMs), Model Tailor, Fine-tuning, Sparse mask, Salience and sensitivity analysis, Image captioning, Visual question answering, Catastrophic forgetting, Task generalization challenges, Post-training adjustment method, Parameter preservation, Compensation mechanism, InstructBLIP, LLaV A-1.5


- [Dynamic Spectral Clustering with Provable Approximation Guarantee](https://icml.cc/virtual/2024/poster/33577) (Poster)
  - **Authors:** [Steinar Laenen](http://openreview.net/profile?id=~Steinar_Laenen1), [He Sun](http://openreview.net/profile?id=~He_Sun5)
  - **Affiliations:** School of Informatics, University of Edinburgh, United Kingdom, School of Informatics, University of Edinburgh, United Kingdom
  - **TL;DR:** This paper presents a dynamic spectral clustering algorithm for evolving graphs that can efficiently approximate the cluster structure with provable guarantees. The algorithm operates with an amortized update time of O(1) and demonstrates practical effectiveness on large-scale datasets.
  - **Keywords:** dynamic graph clustering, evolving graphs, spectral clustering, randomized algorithms, machine learning, graph analysis, cluster structure changes, efficient clustering algorithms, provable approximation guarantee, amortized update time, synthetic datasets, real-world datasets


- [Adversarially Robust Deep Multi-View Clustering: A Novel Attack and Defense Framework](https://icml.cc/virtual/2024/poster/34642) (Poster)
  - **Authors:** [Haonan Huang](http://openreview.net/profile?id=~Haonan_Huang1), [Guoxu Zhou](http://openreview.net/profile?id=~Guoxu_Zhou1), [Yanghang Zheng](http://openreview.net/profile?id=~Yanghang_Zheng1), [Yuning Qiu](http://openreview.net/profile?id=~Yuning_Qiu1), [Andong Wang](http://openreview.net/profile?id=~Andong_Wang1), [Qibin Zhao](http://openreview.net/profile?id=~Qibin_Zhao1)
  - **Affiliations:** School of Automation, Guangdong University of Technology, Guangzhou, CHINA, School of Automation, Guangdong University of Technology, Guangzhou, CHINA; RIKEN AIP, Tokyo, JAPAN; Key Laboratory of Intelligent Detection and the Internet of Things in Manufacturing, Ministry of Education, Guangzhou, CHINA, School of Automation, Guangdong University of Technology, Guangzhou, CHINA, RIKEN AIP, Tokyo, JAPAN, RIKEN AIP, Tokyo, JAPAN, School of Automation, Guangdong University of Technology, Guangzhou, CHINA
  - **TL;DR:** This paper investigates adversarial attacks on Deep Multi-view Clustering (DMVC) models and proposes a novel defense mechanism through adversarial training, demonstrating significant improvements in robustness against such attacks. The findings highlight the critical need for addressing vulnerabilities in DMVC, especially in safety-critical applications.
  - **Keywords:** Adversarial attacks, Deep Multi-view Clustering (DMVC), Generative Adversarial Networks (GANs), adversarial training, Clustering, machine learning, Vulnerability to adversarial attacks, high-dimensional data challenges, Adversarially Robust Deep Multi-View Clustering, Attack Mitigator


- [RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation](https://icml.cc/virtual/2024/poster/34008) (Poster)
  - **Authors:** [Yufei Wang](http://openreview.net/profile?id=~Yufei_Wang4), [Zhou Xian](http://openreview.net/profile?id=~Zhou_Xian1), [Feng Chen](http://openreview.net/profile?id=~Feng_Chen16), [Johnson Tsun-Hsuan Wang](http://openreview.net/profile?id=~Tsun-Hsuan_Wang2), [Yian Wang](http://openreview.net/profile?id=~Yian_Wang1), [Katerina Fragkiadaki](http://openreview.net/profile?id=~Katerina_Fragkiadaki1), [Zackory Erickson](http://openreview.net/profile?id=~Zackory_Erickson1), [David Held](http://openreview.net/profile?id=~David_Held1), [Chuang Gan](http://openreview.net/profile?id=~Chuang_Gan1)
  - **Affiliations:** CMU, CMU, Tsinghua IIIS, MIT CSAIL, UMass Amherst, CMU, CMU, CMU, UMass Amherst; MIT-IBM AI Lab
  - **TL;DR:** The study introduces RoboGen, a generative robotic agent that autonomously learns diverse skills through a self-guided propose-generate-learn cycle, significantly reducing the need for human intervention in constructing simulation environments. The approach leverages generative models to create an endless stream of skill demonstrations, enhancing the scalability of robotic skill learning.
  - **Keywords:** robotic skill learning, generative simulation, reinforcement learning, motion planning, trajectory optimization, robotics, simulation environments, scalability of robotic skill learning, human effort in environment construction, generative pipeline for skill demonstrations, self-guided propose-generate-learn cycle, foundation models, task proposals, training supervisions


- [Differentially Private Worst-group Risk Minimization](https://icml.cc/virtual/2024/poster/34566) (Poster)
  - **Authors:** [Xinyu Zhou](http://openreview.net/profile?id=~Xinyu_Zhou4), [Raef Bassily](http://openreview.net/profile?id=~Raef_Bassily2)
  - **Affiliations:** Department of Computer Science & Engineering, The Ohio State University, Department of Computer Science & Engineering and the Translational Data Analytics Institute (TDAI), The Ohio State University
  - **TL;DR:** This study presents a systematic approach to worst-group risk minimization under differential privacy, aiming to find models that minimize maximal risk across multiple sub-populations. The authors introduce new algorithms and provide nearly optimal excess risk bounds, highlighting the importance of robustness and fairness in machine learning.
  - **Keywords:** worst-group risk minimization, differential privacy, multi-distribution learning, robustness, fairness, stability-based analysis, online convex optimization, group distributionally robust optimization, good-intent fairness, min-max group fairness, maximal risk across sub-populations, generalization error, new algorithms for worst-group risk minimization, excess risk bounds, (ϵ, δ)-differential privacy, empirical risk minimization


- [Speech Self-Supervised Learning Using Diffusion Model Synthetic Data](https://icml.cc/virtual/2024/poster/33487) (Oral)
  - **Authors:** [Heting Gao](http://openreview.net/profile?id=~Heting_Gao1), [Kaizhi Qian](http://openreview.net/profile?id=~Kaizhi_Qian1), [Junrui Ni](http://openreview.net/profile?id=~Junrui_Ni1), [Chuang Gan](http://openreview.net/profile?id=~Chuang_Gan1), [Mark Hasegawa-Johnson](http://openreview.net/profile?id=~Mark_A._Hasegawa-Johnson1), [Shiyu Chang](http://openreview.net/profile?id=~Shiyu_Chang2), [Yang Zhang](http://openreview.net/profile?id=~Yang_Zhang3)
  - **Affiliations:** University of Illinois at Urbana-Champaign, IL, USA, MIT-IBM Watson AI Lab, MA, USA, University of Illinois at Urbana-Champaign, IL, USA, MIT-IBM Watson AI Lab, MA, USA, University of Illinois at Urbana-Champaign, IL, USA, University of California, Santa Barbara, CA, USA, MIT-IBM Watson AI Lab, MA, USA
  - **TL;DR:** This study proposes DIFFS4L, a pretraining scheme that enhances self-supervised learning in speech by augmenting limited unannotated data with synthetic variations generated by a diffusion model. The approach significantly improves the performance of SSL models, evidenced by a 6.26 percentage point reduction in word error rate for the HuBERT model in English ASR tasks.
  - **Keywords:** Self-supervised learning (SSL), Speech processing, Diffusion models, Data augmentation, Speech recognition, Automatic speech recognition (ASR), Limited unannotated corpus, Data sparsity, Low-resource languages, DIFFS4L pretraining scheme, Performance improvement in SSL models, HuBERT


- [RoboDreamer: Learning Compositional World Models for Robot Imagination](https://icml.cc/virtual/2024/poster/33266) (Poster)
  - **Authors:** [Siyuan Zhou](http://openreview.net/profile?id=~Siyuan_Zhou2), [Yilun Du](http://openreview.net/profile?id=~Yilun_Du1), [Jiaben Chen](http://openreview.net/profile?id=~Jiaben_Chen1), [Yandong li](http://openreview.net/profile?id=~YANDONG_LI1), [Dit-Yan Yeung](http://openreview.net/profile?id=~Dit-Yan_Yeung2), [Chuang Gan](http://openreview.net/profile?id=~Chuang_Gan1)
  - **Affiliations:** Hong Kong University of Science and Technology, Massachusetts Institute of Technology, University of California, San Diego, University of Central Florida, Hong Kong University of Science and Technology, University of Massachusetts Amherst
  - **TL;DR:** The study introduces RoboDreamer, a novel approach for learning compositional world models that enhances robotic decision-making by enabling the synthesis of video plans for unseen tasks through factorization of video generation. The method significantly outperforms existing models by allowing for generalization to new combinations of objects and actions based on natural language instructions.
  - **Keywords:** robotic decision-making, video generation, compositional world model, video generation factorization, robotics, AI content generation, generalization in video synthesis, limitations of existing models, RoboDreamer, compositional generalization, multimodal goals, RT-X (mentioned as a context for testing), text-to-video models, natural language instructions


- [Compositional Image Decomposition with Diffusion Models](https://icml.cc/virtual/2024/poster/34860) (Poster)
  - **Authors:** [Jocelin Su](http://openreview.net/profile?id=~Jocelin_Su1), [Nan Liu](http://openreview.net/profile?id=~Nan_Liu4), [Yanbo Wang](http://openreview.net/profile?id=~Yanbo_Wang3), [Josh Tenenbaum](http://openreview.net/profile?id=~Joshua_B._Tenenbaum1), [Yilun Du](http://openreview.net/profile?id=~Yilun_Du1)
  - **Affiliations:** MIT, UIUC, TU Delft, MIT, MIT
  - **TL;DR:** This paper presents Decomp Diffusion, an unsupervised method for decomposing images into compositional components using diffusion models. The approach allows for flexible recombination of these components to generate novel scenes, demonstrating the ability to combine elements from different contexts effectively.
  - **Keywords:** compositional image decomposition, unsupervised learning, diffusion models, energy-based models, image synthesis, scene generation, combining components from different images, modeling global and local factors, Decomp Diffusion method, flexible composition of inferred factors


- [Exploring the Enigma of Neural Dynamics Through A Scattering-Transform Mixer Landscape for Riemannian Manifold](https://icml.cc/virtual/2024/poster/34579) (Poster)
  - **Authors:** [Tingting Dan](http://openreview.net/profile?id=~Tingting_Dan1), [Ziquan Wei](http://openreview.net/profile?id=~Ziquan_Wei1), [Won Hwa Kim](http://openreview.net/profile?id=~Won_Hwa_Kim4), [Guorong Wu](http://openreview.net/profile?id=~Guorong_Wu1)
  - **Affiliations:** Department of Psychiatry, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA; Department of Computer Science, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA, Department of Computer Science, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA, Computer Science and Engineering / Graduate School of AI, POSTECH, Pohang, 37673, South Korea, Department of Psychiatry, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA; Department of Computer Science, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA; Department of Statistics and Operations Research (STOR), University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA; UNC NeuroScience Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA; Carolina Institute for Developmental Disabilities, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA
  - **TL;DR:** This study presents a physics-informed deep learning model to explore the relationship between brain structure and function through geometric data analysis, proposing that neural activities are driven by brain-wide oscillation waves rather than localized areas. The findings challenge existing notions in cognitive neuroscience and highlight the importance of understanding the brain's complex interconnections.
  - **Keywords:** neural dynamics, brain structure and function, cognitive neuroscience, geometric deep model, graph-harmonic scattering transforms, MLP-Mixer architecture, neuroimaging, brain connectivity analysis, understanding the coupling mechanism between brain structure and function, self-organized patterns in functional fluctuations, uncovering manifold mapping functions, characterizing intrinsic feature representations, Riemannian manifold, functional connectivity (FC), BOLD signals, symmetric and positive-definite (SPD) matrices


- [Differentially Private Domain Adaptation with Theoretical Guarantees](https://icml.cc/virtual/2024/poster/33241) (Poster)
  - **Authors:** [Raef Bassily](http://openreview.net/profile?id=~Raef_Bassily2), [Corinna Cortes](http://openreview.net/profile?id=~Corinna_Cortes1), [Anqi Mao](http://openreview.net/profile?id=~Anqi_Mao1), [Mehryar Mohri](http://openreview.net/profile?id=~Mehryar_Mohri2)
  - **Affiliations:** The Ohio State University, Google Research, New York, NY; Courant Institute of Mathematical Sciences, New York, NY, Courant Institute of Mathematical Sciences, New York, NY, Google Research, New York, NY; Courant Institute of Mathematical Sciences, New York, NY
  - **TL;DR:** This paper presents two differentially private algorithms for supervised domain adaptation, addressing the challenge of limited labeled data under privacy constraints. The algorithms demonstrate strong performance comparable to non-private methods while providing theoretical guarantees for their effectiveness.
  - **Keywords:** Differential privacy, Domain adaptation, Differentially private adaptation algorithms, Convex optimization, Non-convex loss functions, Supervised learning, Privacy-preserving machine learning, Limited labeled data, Privacy constraints, Sensitivity of target domain data, Theoretical guarantees for adaptation algorithms, Performance comparison with non-private algorithms


- [Information-Directed Pessimism for Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/34380) (Poster)
  - **Authors:** [Alec Koppel](http://openreview.net/profile?id=~Alec_Koppel1), [Sujay Bhatt](http://openreview.net/profile?id=~Sujay_Bhatt1), [Jiacheng Guo](http://openreview.net/profile?id=~Jiacheng_Guo1), [Joe Eappen](http://openreview.net/profile?id=~Joe_Eappen2), [Mengdi Wang](http://openreview.net/profile?id=~Mengdi_Wang1), [Sumitra Ganesh](http://openreview.net/profile?id=~Sumitra_Ganesh1)
  - **Affiliations:** J.P. Morgan AI Research, 383 Madison Ave., 9th floor, New York, NY 10017, J.P. Morgan AI Research, 383 Madison Ave., 9th floor, New York, NY 10017, Dept. of ECE, Princeton University, Princeton, NJ 08544, Dept. of ECE, Purdue University, West Lafayette, IN, 47906, Dept. of ECE, Princeton University, Princeton, NJ 08544, J.P. Morgan AI Research, 383 Madison Ave., 9th floor, New York, NY 10017
  - **TL;DR:** This paper introduces Information-Directed Pessimism (IDP) for offline reinforcement learning, addressing the challenge of distribution mismatch in batch data. The proposed method utilizes Stein Discrepancy to derive a new pessimistic penalty, leading to improved performance and generalization in multimodal distributions.
  - **Keywords:** Offline Reinforcement Learning, Policy Optimization, Pessimistic Offsets, Stein Discrepancy, Concentration Bounds, Finance, Field Robotics, Games, Distribution Mismatch, Data Sparsity, Information-Directed Pessimism, Performance Gains, Regret Generalization, Markov Decision Process (MDP), Mixture Family Representation


- [Improved Modelling of Federated Datasets using Mixtures-of-Dirichlet-Multinomials](https://icml.cc/virtual/2024/poster/35220) (Poster)
  - **Authors:** [Jonathan Scott](http://openreview.net/profile?id=~Jonathan_Scott1), [Aine E Cahill](http://openreview.net/profile?id=~%C3%81ine_Cahill1)
  - **Affiliations:** Institute of Science and Technology Austria (ISTA), Apple
  - **TL;DR:** This paper addresses the challenge of partitioning centralized data to reflect the statistical heterogeneity of true federated clients, proposing a theoretically justified algorithm that improves server-side simulations for federated learning. The findings suggest that using inferred distributions from centralized data can enhance the efficiency of hyperparameter tuning and model training in federated settings.
  - **Keywords:** Federated Learning, Statistical Heterogeneity, Mixtures-of-Dirichlet-Multinomials, Mobile Devices, Healthcare, Slow training, Hyperparameter tuning, Data partitioning challenges, Improved server-side simulations, Efficient learning of client distribution


- [FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion](https://icml.cc/virtual/2024/poster/35059) (Poster)
  - **Authors:** [Zehan Wang](http://openreview.net/profile?id=~Zehan_Wang2), [Ziang Zhang](http://openreview.net/profile?id=~Ziang_Zhang1), [xize cheng](http://openreview.net/profile?id=~Xize_Cheng1), [Rongjie Huang](http://openreview.net/profile?id=~Rongjie_Huang1), [Luping Liu](http://openreview.net/profile?id=~Luping_Liu2), [Zhenhui Ye](http://openreview.net/profile?id=~Zhenhui_Ye1), [Haifeng Huang](http://openreview.net/profile?id=~Haifeng_Huang3), [Yang Zhao](http://openreview.net/profile?id=~Yang_Zhao14), [Tao Jin](http://openreview.net/profile?id=~Tao_Jin2), [Peng Gao](http://openreview.net/profile?id=~Peng_Gao3), [Zhou Zhao](http://openreview.net/profile?id=~Zhou_Zhao3)
  - **Affiliations:** Zhejiang University; Shanghai AI Lab, Zhejiang University, Zhejiang University, Zhejiang University, Zhejiang University, Zhejiang University, Zhejiang University, ByteDance, Zhejiang University, Shanghai AI Lab, Zhejiang University; Shanghai AI Lab
  - **TL;DR:** This paper introduces FreeBind, a method for enhancing pre-trained unified multimodal representation spaces by integrating knowledge from expert spaces through space bonds. The proposed approach outperforms existing models on various audio-image-text tasks and addresses challenges related to catastrophic forgetting and resource demands.
  - **Keywords:** Unified multimodal representation, multimodal understanding, multimodal generation, Space Displacement Bond, Space Combination Bond, Complex Sequential & Parallel Bonds, Audio-image-text tasks, multimodal spaces, Catastrophic forgetting, enhancement of pre-trained unified spaces, computational resource demands, FreeBind, enhanced unified space, integration of expert spaces, Knowledge fusion, modularization concept


- [Time Series Diffusion in the Frequency Domain](https://icml.cc/virtual/2024/poster/33886) (Poster)
  - **Authors:** [Jonathan Crabbé](http://openreview.net/profile?id=~Jonathan_Crabb%C3%A91), [Nicolas Huynh](http://openreview.net/profile?id=~Nicolas_Huynh1), [Jan Stanczuk](http://openreview.net/profile?id=~Jan_Pawel_Stanczuk1), [M van der Schaar](http://openreview.net/profile?id=~Mihaela_van_der_Schaar2)
  - **Affiliations:** DAMTP, University of Cambridge, DAMTP, University of Cambridge, DAMTP, University of Cambridge, DAMTP, University of Cambridge
  - **TL;DR:** This study investigates the application of Fourier analysis to time series diffusion models, proposing a frequency domain approach that outperforms traditional time domain models. The findings suggest that time series data is more effectively modeled in the frequency domain due to its localization properties.
  - **Keywords:** time series diffusion models, generative modelling, Fourier analysis, score-based diffusion models, denoising score matching, mirrored Brownian motions, healthcare, finance, modeling time series data, data localization in frequency domain, frequency diffusion models, improved training distribution capture


- [Partial Optimality in the Linear Ordering Problem](https://icml.cc/virtual/2024/poster/32778) (Poster)
  - **Authors:** [David Stein](http://openreview.net/profile?id=~David_Stein2), [Bjoern Andres](http://openreview.net/profile?id=~Bjoern_Andres6)
  - **Affiliations:** Faculty of Computer Science, TU Dresden, Germany, Faculty of Computer Science, TU Dresden, Germany; Center for Scalable Data Analytics and AI Dresden/Leipzig
  - **TL;DR:** The study addresses the NP-hard linear ordering problem by introducing algorithms that efficiently determine the optimal order of pairs to minimize associated costs. The authors establish conditions under which certain transformations of feasible solutions are improving, contributing to the understanding of partial optimality in this context.
  - **Keywords:** linear ordering problem, combinatorial optimization, binary linear programming, improving maps, NP-hard, APX-hard, cost minimization, algorithms for partial solutions, efficient decision-making for pairs


- [Memory Efficient Neural Processes via Constant Memory Attention Block](https://icml.cc/virtual/2024/poster/32689) (Poster)
  - **Authors:** [Leo Feng](http://openreview.net/profile?id=~Leo_Feng1), [Frederick Tung](http://openreview.net/profile?id=~Frederick_Tung1), [Hossein Hajimirsadeghi](http://openreview.net/profile?id=~Hossein_Hajimirsadeghi1), [Yoshua Bengio](http://openreview.net/profile?id=~Yoshua_Bengio1), [Mohamed Osama Ahmed](http://openreview.net/profile?id=~Mohamed_Osama_Ahmed2)
  - **Affiliations:** Mila – Université de Montréal, Canada; Borealis AI, Canada, Borealis AI, Canada, Borealis AI, Canada, Mila – Université de Montréal, Canada, Borealis AI, Canada
  - **TL;DR:** This paper introduces Constant Memory Attentive Neural Processes (CMANPs), a memory-efficient variant of Neural Processes that utilizes a novel attention block to achieve state-of-the-art results while requiring only constant memory. The proposed methods enhance computational efficiency and allow for effective updates in low-resource settings.
  - **Keywords:** Neural Processes, meta-learning, predictive uncertainty, memory efficiency, Constant Memory Attentive Neural Processes (CMANPs), Cross Attention, Constant Memory Attention Block (CMAB), low-resource settings, IoT devices, mobile phones, high memory consumption of attention-based methods, computational efficiency, state-of-the-art results on NP benchmarks, efficient updates for context data, Transformer Neural Processes (TNPs), Latent Bottlenecked Attentive Neural Processes (LBANPs)


- [A Statistical Framework for Data-dependent Retrieval-Augmented Models](https://icml.cc/virtual/2024/poster/34784) (Poster)
  - **Authors:** [Soumya Basu](http://openreview.net/profile?id=~Soumya_Basu2), [Ankit Singh Rawat](http://openreview.net/profile?id=~Ankit_Singh_Rawat1), [Manzil Zaheer](http://openreview.net/profile?id=~Manzil_Zaheer1)
  - **Affiliations:** Google, New York, USA, Google Research, New York, USA, Google DeepMind, New York, USA
  - **TL;DR:** This paper proposes a statistical framework for retrieval-augmented models, focusing on the end-to-end training of retriever and predictor components. The study establishes performance bounds and highlights the importance of retrieval augmentation in enhancing model predictions, particularly in open domain question answering tasks.
  - **Keywords:** retrieval-augmented models, machine learning, end-to-end training, data-dependent metric, open domain question answering, high computational cost, inefficiency in knowledge storage, lack of transparency, reduced grounding/factuality, excess risk bounds, contributions of retriever and predictor


- [Box Facets and Cut Facets of Lifted Multicut Polytopes](https://icml.cc/virtual/2024/poster/34387) (Poster)
  - **Authors:** [Lucas Fabian Naumann](http://openreview.net/profile?id=~Lucas_Fabian_Naumann1), [Jannik Irmai](http://openreview.net/profile?id=~Jannik_Irmai1), [Shengxian Zhao](http://openreview.net/profile?id=~Shengxian_Zhao1), [Bjoern Andres](http://openreview.net/profile?id=~Bjoern_Andres6)
  - **Affiliations:** Faculty of Computer Science, TU Dresden, Faculty of Computer Science, TU Dresden, Center for Scalable Data Analytics and AI, Dresden/Leipzig, Faculty of Computer Science, TU Dresden; Center for Scalable Data Analytics and AI, Dresden/Leipzig
  - **TL;DR:** This study addresses the lifted multicut problem, focusing on the conditions necessary for defining facets of lifted multicut polytopes and demonstrating the NP-hardness of determining facet-defining cut inequalities. The findings contribute to a deeper understanding of the structure of these polytopes, which is crucial for developing exact algorithms in computer vision applications.
  - **Keywords:** lifted multicut problem, combinatorial optimization, computer vision, binary linear programming, linear inequalities, multiple object tracking, defining facets of lifted multicut polytopes, NP-hardness of cut inequalities, conditions for facet-defining lower box inequalities, lifted multicut polytopes, box inequalities, cut inequalities


- [Improving Computational Complexity in Statistical Models with Local Curvature Information](https://icml.cc/virtual/2024/poster/34321) (Poster)
  - **Authors:** [Pedram Akbarian](http://openreview.net/profile?id=~Pedram_Akbarian1), [Tongzheng Ren](http://openreview.net/profile?id=~Tongzheng_Ren1), [Jiacheng Zhuo](http://openreview.net/profile?id=~Jiacheng_Zhuo1), [Sujay Sanghavi](http://openreview.net/profile?id=~sujay_sanghavi1), [Nhat Ho](http://openreview.net/profile?id=~Nhat_Ho1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Texas at Austin, Texas, USA, Computer Science, University of Texas at Austin, Texas, USA, Computer Science, University of Texas at Austin, Texas, USA, Department of Electrical and Computer Engineering, University of Texas at Austin, Texas, USA, Department of Statistics and Data Sciences, University of Texas at Austin, Texas, USA
  - **TL;DR:** This study investigates the use of local curvature information to enhance the computational complexity of statistical models, specifically through the normalized gradient descent (NormGD) algorithm. The findings indicate that NormGD can achieve optimal computational complexity O(n) for reaching a final statistical radius around the true parameter, outperforming traditional fixed step-size gradient descent methods.
  - **Keywords:** computational complexity, statistical models, optimization, normalized gradient descent (NormGD), gradient descent (GD), singular statistical models, Fisher information matrix, non-convex sample loss function, optimal computational complexity, statistical radius estimation, local curvature information


- [A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks](https://icml.cc/virtual/2024/poster/33999) (Poster)
  - **Authors:** [Nicholas Monath](http://openreview.net/profile?id=~Nicholas_Monath1), [Will Grathwohl](http://openreview.net/profile?id=~Will_Sussman_Grathwohl2), [Michael Boratko](http://openreview.net/profile?id=~Michael_Boratko1), [Rob Fergus](http://openreview.net/profile?id=~Rob_Fergus1), [Andrew McCallum](http://openreview.net/profile?id=~Andrew_McCallum1), [Manzil Zaheer](http://openreview.net/profile?id=~Manzil_Zaheer1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This paper introduces a target corrector network to address the challenges of stale embeddings in dense retrieval systems, enabling accurate softmax approximations and reducing computational costs significantly. The proposed method achieves state-of-the-art results without frequent updates to target embeddings during training.
  - **Keywords:** dense retrieval, deep learning, embeddings, softmax function, dual encoder, target corrector network, information retrieval, QA with retrieval augmented language models, staleness of cached embeddings, computational cost of target encoder, scalable method for updating embeddings, accurate softmax approximation, large benchmark dense retrieval datasets


- [Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers](https://icml.cc/virtual/2024/poster/33355) (Poster)
  - **Authors:** [Md Shamim Hussain](http://openreview.net/profile?id=~Md_Shamim_Hussain1), [Mohammed Zaki](http://openreview.net/profile?id=~Mohammed_J_Zaki1), [Dharmashankar Subramanian](http://openreview.net/profile?id=~Dharmashankar_Subramanian1)
  - **Affiliations:** Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, USA, Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY, USA, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA
  - **TL;DR:** The study introduces the Triplet Graph Transformer (TGT), which enhances molecular property prediction by enabling direct communication between pairs of nodes through triplet interactions. The model achieves state-of-the-art results on multiple benchmarks, demonstrating the importance of third-order interactions in understanding molecular geometry.
  - **Keywords:** Graph Transformers, Molecular Geometry Prediction, Triplet Graph Transformer (TGT), Triplet Attention, Aggregation Mechanisms, Molecular Property Prediction, Traveling Salesman Problem (TSP), Lack of third-order interactions, Geometric understanding limitations, State-of-the-art results on benchmarks (PCQM4Mv2, OC20 IS2RE, QM9, MOLPCBA, LIT-PCBA), 3D Molecular Geometry, Edge-augmented Graph Transformer (EGT), Graph Neural Networks (GNNs)


- [Accelerated Policy Gradient: On the Convergence Rates of the Nesterov Momentum for Reinforcement Learning](https://icml.cc/virtual/2024/poster/33678) (Poster)
  - **Authors:** [Yen-Ju Chen](http://openreview.net/profile?id=~Yen-Ju_Chen1), [Nai-Chieh Huang](http://openreview.net/profile?id=~Nai-Chieh_Huang1), [Ching-pei Lee](http://openreview.net/profile?id=~Ching-pei_Lee2), [Ping-Chun Hsieh](http://openreview.net/profile?id=~Ping-Chun_Hsieh1)
  - **Affiliations:** Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan, Department of Advanced Data Science, Institute of Statistical Mathematics, Tokyo, Japan, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan
  - **TL;DR:** This study introduces the Accelerated Policy Gradient (APG) method, adapting Nesterov’s accelerated gradient for policy optimization in reinforcement learning, and proves its convergence rates. The findings indicate that APG significantly improves convergence over standard policy gradient methods, achieving O(1/t²) and linear convergence rates under different step size conditions.
  - **Keywords:** Reinforcement Learning, Policy Gradient, Nesterov’s Accelerated Gradient (NAG), Accelerated Policy Gradient (APG), Convergence rates, policy optimization, O(1/t²) convergence rate, linear convergence rate, Atari 2600 benchmarks, PyTorch, Momentum-based acceleration


- [R2E: Turning any Github Repository into a Programming Agent Environment](https://icml.cc/virtual/2024/poster/33250) (Poster)
  - **Authors:** [Naman Jain](http://openreview.net/profile?id=~Naman_Jain2), [Manish Shetty Molahalli](http://openreview.net/profile?id=~Manish_Shetty1), [Tianjun Zhang](http://openreview.net/profile?id=~Tianjun_Zhang1), [Shangdian Han](http://openreview.net/profile?email=kingh0730%40berkeley.edu), [Koushik Sen](http://openreview.net/profile?id=~Koushik_Sen2), [Ion Stoica](http://openreview.net/profile?id=~Ion_Stoica1)
  - **Affiliations:** University of California, Berkeley, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley
  - **TL;DR:** This paper introduces the Repository to Environment (R2E) framework, which transforms any GitHub repository into a test environment for evaluating AI programming agents. The study highlights the need for interactive programming paradigms and presents R2E-Eval1, a large-scale benchmark for real-world coding problems.
  - **Keywords:** Large Language Models, AI programming agents, code generation, Program analysis, equivalence test harnesses, Software engineering, coding assistants, Evaluation benchmarks, real-world programming setups, interactive programming, R2E framework, R2E-Eval1 benchmark, GitHub repositories


- [Generalization Analysis of Deep Non-linear Matrix Completion](https://icml.cc/virtual/2024/poster/35041) (Poster)
  - **Authors:** [Antoine Ledent](http://openreview.net/profile?id=~Antoine_Ledent1), [Rodrigo Alves](http://openreview.net/profile?id=~Rodrigo_Alves1)
  - **Affiliations:** School of Computing and Information Sciences, Singapore Management University, Singapore, Department of Applied Mathematics, Czech Technical University in Prague, Prague, Czech Republic
  - **TL;DR:** This study analyzes generalization bounds for matrix completion using Schatten p-quasi-norm constraints, demonstrating that a non-linear model, Functionally Rescaled Matrix Completion (FRMC), significantly improves sample complexity and performance on real data. The findings contribute to understanding the implications of deep matrix factorization in machine learning contexts.
  - **Keywords:** matrix completion, deep matrix factorization, Schatten p-quasi-norm, Functionally Rescaled Matrix Completion (FRMC), recommender systems, community discovery, drug interaction prediction, data sparsity, rank sparsity, generalization bounds, sample complexity analysis


- [Online Algorithms with Uncertainty-Quantified Predictions](https://icml.cc/virtual/2024/poster/32724) (Poster)
  - **Authors:** [Bo Sun](http://openreview.net/profile?id=~Bo_Sun8), [Jerry Huang](http://openreview.net/profile?email=jyhuang%40caltech.edu), [Nicolas Christianson](http://openreview.net/profile?id=~Nicolas_Christianson1), [Mohammad Hajiesmaili](http://openreview.net/profile?id=~Mohammad_Hajiesmaili1), [Adam Wierman](http://openreview.net/profile?id=~Adam_Wierman1), [Raouf Boutaba](http://openreview.net/profile?id=~Raouf_Boutaba2)
  - **Affiliations:** David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada, Computing + Mathematical Sciences Department, California Institute of Technology, Pasadena, USA, Computing + Mathematical Sciences Department, California Institute of Technology, Pasadena, USA, Manning College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, USA, Computing + Mathematical Sciences Department, California Institute of Technology, Pasadena, USA, David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada
  - **TL;DR:** This paper investigates how to optimally utilize uncertainty-quantified predictions to enhance the performance of online algorithms, specifically addressing classic problems like ski rental and online search. The authors propose a new performance metric, the distributionally-robust competitive ratio, to improve algorithm design in the presence of uncertain predictions.
  - **Keywords:** online algorithms, uncertainty quantification, conformal inference, distributionally-robust competitive ratio, using imperfect machine learning predictions, decision-making under uncertainty, optimal online algorithms, new performance metric


- [Flora: Low-Rank Adapters Are Secretly Gradient Compressors](https://icml.cc/virtual/2024/poster/32804) (Poster)
  - **Authors:** [Yongchang Hao](http://openreview.net/profile?id=~Yongchang_Hao1), [Yanshuai Cao](http://openreview.net/profile?id=~Yanshuai_Cao1), [Lili Mou](http://openreview.net/profile?id=~Lili_Mou1)
  - **Affiliations:** Department of Computing Science & Alberta Machine Intelligence Institute (Amii), University of Alberta, Borealis AI, Department of Computing Science & Alberta Machine Intelligence Institute (Amii), University of Alberta; Canada CIFAR AI Chair
  - **TL;DR:** This study introduces FLORA, a novel optimization technique that enhances the low-rank adaptation (LoRA) method by allowing high-rank updates while maintaining sublinear memory complexity for gradient accumulation and momentum calculation. The findings suggest that FLORA effectively addresses the memory challenges associated with training large neural networks.
  - **Keywords:** low-rank adaptation, optimization states, memory efficiency, LoRA (Low-Rank Adaptation), stochastic gradient descent (SGD), Adafactor, deep learning, neural networks, excessive memory usage, scaling challenges in optimization, FLORA (from LoRA to high-rank updates), sublinear memory for gradient accumulation and momentum calculation


- [Towards Certified Unlearning for Deep Neural Networks](https://icml.cc/virtual/2024/poster/35136) (Poster)
  - **Authors:** [Binchi Zhang](http://openreview.net/profile?id=~Binchi_Zhang1), [Yushun Dong](http://openreview.net/profile?id=~Yushun_Dong1), [Tianhao Wang](http://openreview.net/profile?id=~Tianhao_Wang3), [Jundong Li](http://openreview.net/profile?id=~Jundong_Li2)
  - **Affiliations:** University of Virginia, Charlottesville, VA, USA, University of Virginia, Charlottesville, VA, USA, University of Virginia, Charlottesville, VA, USA, University of Virginia, Charlottesville, VA, USA
  - **TL;DR:** This study addresses the challenges of applying certified unlearning to deep neural networks by proposing techniques that extend certified unlearning methods to nonconvex objectives. The authors demonstrate the efficacy of their approach through extensive experiments, highlighting its advantages in maintaining certification guarantees while reducing computational costs.
  - **Keywords:** certified unlearning, deep neural networks, inverse Hessian approximation, Newton update, machine learning, data privacy, challenges in applying certified unlearning to nonconvex models, computational cost of retraining, efficient approximation of retrained models, theoretical certification for DNNs, right to be forgotten, GDPR, CCPA


- [Position: Optimization in SciML Should Employ the Function Space Geometry](https://icml.cc/virtual/2024/poster/34242) (Poster)
  - **Authors:** [Johannes Müller](http://openreview.net/profile?id=~Johannes_M%C3%BCller1), [Marius Zeinhofer](http://openreview.net/profile?id=~Marius_Zeinhofer1)
  - **Affiliations:** Chair of Mathematics of Information Processing, RWTH Aachen University, Aachen, Germany, Simula Research Laboratory, Oslo, Norway
  - **TL;DR:** This paper advocates for an infinite-dimensional approach to optimization in scientific machine learning (SciML), emphasizing the need to first optimize before discretizing solutions. It highlights the challenges faced by existing optimization methods in SciML and proposes the development of more efficient algorithms tailored to these unique problems.
  - **Keywords:** Scientific Machine Learning (SciML), optimization in function spaces, physics-informed neural networks (PINNs), neural operators, second-order optimization algorithms, fluid dynamics, solid mechanics, quantum mechanics, optimal design, optimization problems in function spaces, high-dimensional PDEs, optimization error dominating approximation and generalization error, efficient optimization algorithms, principled approaches to optimization, partial differential equations (PDEs), variational Monte Carlo methods, K-FAC


- [Enhancing Value Function Estimation through First-Order State-Action Dynamics in Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/33122) (Poster)
  - **Authors:** [Yun-Hsuan Lien](http://openreview.net/profile?id=~Yun-Hsuan_Lien1), [Ping-Chun Hsieh](http://openreview.net/profile?id=~Ping-Chun_Hsieh1), [Tzu-Mao Li](http://openreview.net/profile?id=~Tzu-Mao_Li1), [Yu-Shuen Wang](http://openreview.net/profile?id=~Yu-Shuen_Wang1)
  - **Affiliations:** Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan, Department of Computer Science and Engineering, University of California, San Diego, CA, USA, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan
  - **TL;DR:** This study presents a novel approach to enhance value function estimation in offline reinforcement learning by integrating first-order state-action dynamics, addressing challenges related to data limitations and generalization. Experiments demonstrate that this method significantly improves policy performance compared to traditional approaches.
  - **Keywords:** offline reinforcement learning, value function estimation, discrete-time Bellman Equation, Hamilton-Jacobi-Bellman equation, deterministic policy gradient methods, stochastic policy gradient methods, data limitation, extrapolation issues, generalization problems, novel objective function, improved policy performance, D4RL dataset


- [Verification of Machine Unlearning is Fragile](https://icml.cc/virtual/2024/poster/34147) (Poster)
  - **Authors:** [Binchi Zhang](http://openreview.net/profile?id=~Binchi_Zhang1), [Zihan Chen](http://openreview.net/profile?id=~Zihan_Chen5), [Cong Shen](http://openreview.net/profile?id=~Cong_Shen1), [Jundong Li](http://openreview.net/profile?id=~Jundong_Li2)
  - **Affiliations:** University of Virginia, Charlottesville, VA, USA, University of Virginia, Charlottesville, VA, USA, University of Virginia, Charlottesville, VA, USA, University of Virginia, Charlottesville, VA, USA
  - **TL;DR:** This study investigates the fragility of machine unlearning verification strategies, revealing that model providers can potentially circumvent these strategies while retaining unlearned data. The findings highlight significant vulnerabilities in current verification methods, emphasizing the need for improved safety measures in machine unlearning.
  - **Keywords:** Machine Unlearning, Privacy Concerns, Verification Strategies, Adversarial Unlearning Processes, Machine Learning as a Service (MLaaS), Data Privacy, Data Owner Trust, Model Provider Dishonesty, Verification Fragility, Novel Adversarial Unlearning Methods, Theoretical Analysis, Empirical Experiments, Real-world Datasets


- [Trained Random Forests Completely Reveal your Dataset](https://icml.cc/virtual/2024/poster/33585) (Oral)
  - **Authors:** [Julien Ferry](http://openreview.net/profile?id=~Julien_Ferry1), [Ricardo Fukasawa](http://openreview.net/profile?id=~Ricardo_Fukasawa1), [Timothée Pascal](http://openreview.net/profile?id=~Timoth%C3%A9e_Pascal1), [Thibaut Vidal](http://openreview.net/profile?id=~Thibaut_Vidal1)
  - **Affiliations:** CIRRELT & SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematics and Industrial Engineering, Polytechnique Montréal, Canada, Department of Combinatorics and Optimization, University of Waterloo, Canada, Ecole nationale des ponts et chaussées, Paris, France, CIRRELT & SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematics and Industrial Engineering, Polytechnique Montréal, Canada
  - **TL;DR:** This study presents an optimization-based reconstruction attack that can nearly completely reconstruct datasets used to train random forests, highlighting significant privacy vulnerabilities in ensemble methods. The findings indicate that even with bootstrap aggregation, a majority of the data can still be recovered, emphasizing the need for attention and mitigation in privacy practices.
  - **Keywords:** dataset reconstruction, privacy concerns, machine learning, random forests, optimization-based reconstruction attack, constraint programming, sensitive data analysis, ethical data usage, reconstruction attacks, data privacy vulnerabilities, maximum-likelihood dataset reconstruction, NP-hardness proof, scikit-learn, ensemble methods, bootstrap aggregation, feature randomization


- [Human Alignment of Large Language Models through Online Preference Optimisation](https://icml.cc/virtual/2024/poster/35106) (Poster)
  - **Authors:** [Daniele Calandriello](http://openreview.net/profile?id=~Daniele_Calandriello1), [Zhaohan Guo](http://openreview.net/profile?id=~Zhaohan_Daniel_Guo1), [REMI MUNOS](http://openreview.net/profile?id=~Remi_Munos1), [Mark Rowland](http://openreview.net/profile?id=~Mark_Rowland1), [Yunhao Tang](http://openreview.net/profile?id=~Yunhao_Tang1), [Bernardo Avila Pires](http://openreview.net/profile?id=~Bernardo_Avila_Pires1), [Pierre Richemond](http://openreview.net/profile?id=~Pierre_Harvey_Richemond1), [Charline Le Lan](http://openreview.net/profile?id=~Charline_Le_Lan2), [Michal Valko](http://openreview.net/profile?id=~Michal_Valko1), [Tianqi Liu](http://openreview.net/profile?id=~Tianqi_Liu1), [Rishabh Joshi](http://openreview.net/profile?id=~Rishabh_Joshi1), [Zeyu Zheng](http://openreview.net/profile?id=~Zeyu_Zheng1), [Bilal Piot](http://openreview.net/profile?id=~Bilal_Piot1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind; Inria, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This paper explores the alignment of language models with human preferences by establishing the equivalence between Identity Preference Optimisation (IPO) and Nash Mirror Descent (Nash-MD), and introduces a new algorithm, IPO-MD, that enhances preference optimisation. The findings suggest that optimising the IPO loss with online data is equivalent to finding a Nash equilibrium, providing a novel approach to improving language model alignment.
  - **Keywords:** Human alignment, Language models, Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimisation (DPO), Identity Preference Optimisation (IPO), Nash Mirror Descent (Nash-MD), IPO-MD, Alignment of language models with human preferences, Equivalence between IPO and Nash-MD, introduction of IPO-MD algorithm, Nash equilibrium, contrastive loss, preference probabilities


- [Dirichlet Flow Matching with Applications to DNA Sequence Design](https://icml.cc/virtual/2024/poster/32887) (Poster)
  - **Authors:** [Hannes Stärk](http://openreview.net/profile?id=~Hannes_Stark1), [Bowen Jing](http://openreview.net/profile?id=~Bowen_Jing1), [Chenyu Wang](http://openreview.net/profile?id=~Chenyu_Wang7), [Gabriele Corso](http://openreview.net/profile?id=~Gabriele_Corso1), [Bonnie Berger](http://openreview.net/profile?id=~Bonnie_Berger1), [Regina Barzilay](http://openreview.net/profile?id=~Regina_Barzilay1), [Tommi Jaakkola](http://openreview.net/profile?id=~Tommi_S._Jaakkola1)
  - **Affiliations:** CSAIL, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology; Dept. of Mathematics, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology
  - **TL;DR:** This study introduces Dirichlet flow matching as a novel approach for generative modeling of discrete categorical data, specifically for DNA sequence design. The method demonstrates significant performance improvements over traditional autoregressive models, enabling faster and more controllable sequence generation while addressing challenges associated with existing flow matching techniques.
  - **Keywords:** discrete diffusion models, generative modeling, sequence generation, Dirichlet flow matching, continuous normalizing flows, linear flow matching, DNA sequence design, biological sequence generation, discontinuities in training targets, modeling complex discrete distributions, one-step sequence generation, performance improvements over autoregressive models, probability simplex, mixtures of Dirichlet distributions, classifier-free guidance


- [QBMK: Quantum-based Matching Kernels for Un-attributed Graphs](https://icml.cc/virtual/2024/poster/34117) (Spotlight Poster)
  - **Authors:** [Lu Bai](http://openreview.net/profile?id=~Lu_Bai3), [Lixin Cui](http://openreview.net/profile?email=cuilixin%40cufe.edu.cn), [Ming Li](http://openreview.net/profile?id=~Ming_Li15), [Yue Wang](http://openreview.net/profile?id=~Yue_Wang41), [Edwin Hancock](http://openreview.net/profile?id=~Edwin_Hancock1)
  - **Affiliations:** School of Artificial Intelligence, Beijing Normal University, Beijing 100875, China, School of Information, Central University of Finance and Economics, Beijing 100081, China; Zhejiang Key Laboratory of Intelligent Education Technology and Application, Zhejiang Normal University, Jinhua 321004, China, Zhejiang Institute of Optoelectronics, Jinhua 321004, China; Zhejiang Key Laboratory of Intelligent Education Technology and Application, Zhejiang Normal University, Jinhua 321004, China, School of Information, Central University of Finance and Economics, Beijing 100081, China, Department of Computer Science, University of York, YO10 5GH York, UK
  - **TL;DR:** This study introduces a new Quantum-based Matching Kernel (QBMK) for un-attributed graphs that improves upon existing graph kernels by addressing structural correspondence and differences between vertices. Experimental results show that QBMK outperforms state-of-the-art graph kernels and deep learning approaches in graph classification tasks.
  - **Keywords:** Quantum-based Matching Kernels, Graph Classification, Continuous-time Quantum Walk (CTQW), Quantum Shannon Entropy, Un-attributed Graphs, Graph Kernels, Neglecting structural correspondence information, Ignoring structural differences between aligned vertices, New QBMK kernel, Improved similarity measures between graphs, Standard graph datasets, R-convolution graph kernels, Isomorphic substructures


- [Reward Shaping for Reinforcement Learning with An Assistant Reward Agent](https://icml.cc/virtual/2024/poster/33703) (Poster)
  - **Authors:** [Haozhe Ma](http://openreview.net/profile?id=~Haozhe_Ma1), [Kuankuan Sima](http://openreview.net/profile?id=~Kuankuan_Sima1), [Thanh Vinh Vo](http://openreview.net/profile?id=~Thanh_Vinh_Vo2), [Di Fu](http://openreview.net/profile?id=~Di_Fu2), [Tze-Yun Leong](http://openreview.net/profile?id=~Tze-Yun_Leong2)
  - **Affiliations:** School of Computing, National University of Singapore, Singapore, College of Design and Engineering, National University of Singapore, Singapore, School of Computing, National University of Singapore, Singapore, School of Computing, National University of Singapore, Singapore, School of Computing, National University of Singapore, Singapore
  - **TL;DR:** This paper presents a novel dual-agent framework, ReLara, for reinforcement learning that addresses the challenges of sparse and delayed rewards by automatically learning a dense assistant reward function. The framework enhances sample efficiency and convergence stability while maintaining a balance between exploration and exploitation.
  - **Keywords:** reinforcement learning, reward shaping, dual-agent framework, self-learning approach, robotic control, video games, sparse rewards, delayed rewards, exploration vs. exploitation, assistant reward function, sample efficiency, convergence stability


- [DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents](https://icml.cc/virtual/2024/poster/33019) (Poster)
  - **Authors:** [Yilun Xu](http://openreview.net/profile?id=~Yilun_Xu1), [Gabriele Corso](http://openreview.net/profile?id=~Gabriele_Corso1), [Tommi Jaakkola](http://openreview.net/profile?id=~Tommi_S._Jaakkola1), [Arash Vahdat](http://openreview.net/profile?id=~Arash_Vahdat3), [Karsten Kreis](http://openreview.net/profile?id=~Karsten_Kreis1)
  - **Affiliations:** NVIDIA; MIT, MIT, MIT, NVIDIA, NVIDIA
  - **TL;DR:** This paper introduces DisCo-Diff, a novel framework that enhances diffusion models by incorporating discrete latent variables to simplify the mapping from noise to complex data distributions. The approach significantly improves model performance across various tasks, achieving state-of-the-art results in image synthesis and molecular docking.
  - **Keywords:** generative learning, diffusion models, Discrete-Continuous Latent Variable Diffusion Models (DisCo-Diff), generative ordinary differential equation (ODE), image synthesis, molecular docking, encoding complex multimodal data distributions, noise-to-data mapping complexity, improved model performance, reduced curvature of generative ODE, state-of-the-art FID scores, ImageNet-64/128


- [Expressivity and Generalization: Fragment-Biases for Molecular GNNs](https://icml.cc/virtual/2024/poster/32952) (Oral)
  - **Authors:** [Tom Wollschläger](http://openreview.net/profile?id=~Tom_Wollschl%C3%A4ger1), [Niklas Kemper](http://openreview.net/profile?id=~Niklas_Kemper1), [Leon Hetzel](http://openreview.net/profile?id=~Leon_Hetzel1), [Johanna Sommer](http://openreview.net/profile?id=~Johanna_Sommer1), [Stephan Günnemann](http://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1)
  - **Affiliations:** School of Computation, Information & Technology, Technical University of Munich; Munich Data Science Institute, Germany, School of Computation, Information & Technology, Technical University of Munich; Munich Data Science Institute, Germany, School of Computation, Information & Technology, Technical University of Munich; Helmholtz Center for Computational Health, Munich, Germany, School of Computation, Information & Technology, Technical University of Munich; Munich Data Science Institute, Germany, School of Computation, Information & Technology, Technical University of Munich; Munich Data Science Institute, Germany
  - **TL;DR:** This study introduces the Fragment-WL test to analyze fragment-biased Graph Neural Networks (GNNs) and presents a new GNN architecture that significantly enhances expressiveness and generalization capabilities. The proposed model outperforms existing GNNs and transformer-based architectures on various molecular datasets, demonstrating its robustness for molecular modeling tasks.
  - **Keywords:** Graph Neural Networks (GNNs), molecular property prediction, expressiveness, Fragment-WL test, higher-order GNNs, message-passing framework, Chemistry, molecular modeling, protein analysis, Lack of expressiveness in GNNs, poor generalization to out-of-distribution data, New GNN architecture, fragmentation with infinite vocabulary, improved generalization capabilities, ZINC, Peptides, Weisfeiler & Leman (WL) test, fragment-biased models


- [Efficient Precision and Recall Metrics for Assessing Generative Models using Hubness-aware Sampling](https://icml.cc/virtual/2024/poster/33287) (Spotlight Poster)
  - **Authors:** [Yuanbang Liang](http://openreview.net/profile?id=~Yuanbang_Liang1), [Jing Wu](http://openreview.net/profile?id=~Jing_Wu3), [Yu-Kun Lai](http://openreview.net/profile?id=~Yu-Kun_Lai1), [Yipeng Qin](http://openreview.net/profile?id=~Yipeng_Qin1)
  - **Affiliations:** School of Computer Science and Informatics, Cardiff University, Cardiff, CF24 4AG, UK, School of Computer Science and Informatics, Cardiff University, Cardiff, CF24 4AG, UK, School of Computer Science and Informatics, Cardiff University, Cardiff, CF24 4AG, UK, School of Computer Science and Informatics, Cardiff University, Cardiff, CF24 4AG, UK
  - **TL;DR:** This paper proposes efficient precision and recall (eP&R) metrics for evaluating deep generative models, addressing the computational inefficiencies associated with traditional P&R metrics. The proposed method utilizes hubness-aware sampling to significantly reduce computational costs while maintaining accuracy.
  - **Keywords:** deep generative models, evaluation metrics, precision and recall (P&R), hubness-aware sampling, k-nearest neighbor (k-NN), computational inefficiency, large-scale datasets, efficient P&R (eP&R) metrics, redundancy removal, FFHQ dataset, LAION-400M, LAION-5B


- [SelfIE: Self-Interpretation of Large Language Model Embeddings](https://icml.cc/virtual/2024/poster/33415) (Poster)
  - **Authors:** [Haozhe Chen](http://openreview.net/profile?id=~Haozhe_Chen2), [Carl Vondrick](http://openreview.net/profile?id=~Carl_Vondrick2), [Chengzhi Mao](http://openreview.net/profile?id=~Chengzhi_Mao2)
  - **Affiliations:** Department of Computer Science, Columbia University, New York, NY; Mila, Montreal, Canada; McGill University, Montreal, Canada, Department of Computer Science, Columbia University, New York, NY, Department of Computer Science, Columbia University, New York, NY; Mila, Montreal, Canada; McGill University, Montreal, Canada
  - **TL;DR:** The study introduces SelfIE, a framework that enables large language models to interpret their own embeddings in natural language, enhancing transparency and control over their reasoning processes. Key findings include methods for ethical decision-making and erasing harmful knowledge without supervision.
  - **Keywords:** Large Language Models, Explainability, Self-Interpretation, SelfIE, Supervised Control, Reinforcement Control, gradient computation, Programming, Question Answering, Healthcare, Black-box nature of LLMs, limited transparency, ethical decision-making, harmful knowledge recall, Framework for interpreting embeddings, methods for controlling LLM reasoning


- [Non-Asymptotic Analysis for Single-Loop (Natural) Actor-Critic with Compatible Function Approximation](https://icml.cc/virtual/2024/poster/33775) (Poster)
  - **Authors:** [Yudan Wang](http://openreview.net/profile?id=~Yudan_Wang1), [Yue Wang](http://openreview.net/profile?id=~Yue_Wang16), [Yi Zhou](http://openreview.net/profile?id=~Yi_Zhou2), [Shaofeng Zou](http://openreview.net/profile?id=~Shaofeng_Zou1)
  - **Affiliations:** Electrical Engineering, University at Buffalo, Electrical and Computer Engineering, University of Central Florida, Electrical and Computer Engineering, University of Utah, Computer Science & Engineering, University at Buffalo; Electrical Engineering, University at Buffalo
  - **TL;DR:** This paper presents a non-asymptotic convergence analysis for actor-critic and natural actor-critic algorithms in reinforcement learning, focusing on compatible function approximation. The findings eliminate the approximation error of the critic from the error bounds while maintaining optimal sample complexities, particularly in a single-loop setting.
  - **Keywords:** reinforcement learning, actor-critic methods, temporal difference (TD) learning, natural actor-critic (NAC), compatible function approximation, convergence analysis, sample complexity, approximation errors, non-asymptotic convergence bounds, error bounds elimination, single-loop setting, Markovian sample trajectory


- [Sparsest Models Elude Pruning: An Exposé of Pruning’s Current Capabilities](https://icml.cc/virtual/2024/poster/34625) (Poster)
  - **Authors:** [Stephen Zhang](http://openreview.net/profile?id=~Stephen_Zhang4), [Vardan Papyan](http://openreview.net/profile?id=~Vardan_Papyan1)
  - **Affiliations:** Department of Mathematics, University of Toronto, Toronto, Canada, None
  - **TL;DR:** This study investigates the effectiveness of various pruning algorithms in achieving the sparsest models, revealing a significant performance gap compared to ideal sparse networks. The findings highlight the limitations of current pruning techniques, particularly under conditions of overparameterization and network disconnection.
  - **Keywords:** model compression, network pruning, sparsity, pruning algorithms, combinatorial search algorithm, recovering the sparsest models, performance gap, overparameterization, disconnected paths, suboptimal solutions, Cubist Spiral


- [An Empirical Study Into What Matters for Calibrating Vision-Language Models](https://icml.cc/virtual/2024/poster/32976) (Poster)
  - **Authors:** [Weijie Tu](http://openreview.net/profile?id=~Weijie_Tu1), [Weijian Deng](http://openreview.net/profile?id=~Weijian_Deng1), [Dylan Campbell](http://openreview.net/profile?id=~Dylan_Campbell1), [Stephen Gould](http://openreview.net/profile?id=~Stephen_Gould1), [Tom Gedeon](http://openreview.net/profile?id=~Tom_Gedeon1)
  - **Affiliations:** The Australian National University; Curtin University; University of Óbuda, The Australian National University; Curtin University; University of Óbuda, The Australian National University; Curtin University; University of Óbuda, The Australian National University; Curtin University; University of Óbuda, The Australian National University; Curtin University; University of Óbuda
  - **TL;DR:** This study investigates the calibration properties of Vision-Language Models (VLMs) and finds that while they are not inherently well-calibrated, temperature scaling can significantly enhance their calibration across various domains and label sets. The research highlights the importance of understanding uncertainty estimation in VLMs for their effective deployment in risk-sensitive applications.
  - **Keywords:** Vision-Language Models, Uncertainty Estimation, Calibration, Temperature Scaling, Zero-Shot Recognition, Image Recognition, Object Detection, Image Captioning, Uncertainty Calibration, Distribution Shifts, Improved Calibration Techniques, Evaluation of VLMs, ImageNet, CIFAR-10, DomainNet, CLIP, ALIGN, Visual Encoders, ViT, ConvNeXt


- [Unsupervised Parameter-free Simplicial Representation Learning with Scattering Transforms](https://icml.cc/virtual/2024/poster/32736) (Poster)
  - **Authors:** [Hiren Madhu](http://openreview.net/profile?id=~Hiren_Madhu1), [Sravanthi Gurugubelli](http://openreview.net/profile?id=~Sravanthi_Gurugubelli1), [Sundeep Prabhakar Chepuri](http://openreview.net/profile?id=~Sundeep_Prabhakar_Chepuri1)
  - **Affiliations:** Indian Institute of Science, India, Indian Institute of Science, India, Indian Institute of Science, India
  - **TL;DR:** The study introduces simplicial scattering networks (SSNs), a parameter-free model for extracting task-agnostic features from simplicial complex data, addressing challenges like high training complexity and reliance on labels. Empirical results show that SSNs outperform existing models in various tasks while maintaining computational efficiency.
  - **Keywords:** simplicial representation learning, higher-order graph data, simplicial scattering networks, scattering transforms, random walk matrices, node classification, graph classification, trajectory prediction, simplex prediction, high training complexity, dependence on task-specific labels, data sparsity, parameter-free model, topology preserving simplicial representations, robustness to perturbations, simplicial complex, simplicial neural networks (SNNs), graph neural networks (GNNs)


- [Regularized Q-learning through Robust Averaging](https://icml.cc/virtual/2024/poster/35216) (Poster)
  - **Authors:** [Peter Schmitt-Förster](http://openreview.net/profile?id=~Peter_Schmitt-F%C3%B6rster1), [Tobias Sutter](http://openreview.net/profile?id=~Tobias_Sutter1)
  - **Affiliations:** Department of Computer and Information Science, University of Konstanz, Germany, Department of Computer and Information Science, University of Konstanz, Germany
  - **TL;DR:** This paper introduces 2RA Q-learning, a new variant of Q-learning that effectively addresses estimation bias issues in existing methods. The proposed approach demonstrates convergence to the optimal policy and shows improved performance in numerical experiments compared to traditional Q-learning methods.
  - **Keywords:** Q-learning, Reinforcement Learning, 2RA Q-learning, distributionally robust estimator, Markov Decision Processes (MDPs), Estimation bias, overestimation bias, underestimation bias, Convergence to optimal policy, asymptotic mean-squared error analysis


- [Listenable Maps for Audio Classifiers](https://icml.cc/virtual/2024/poster/33268) (Oral)
  - **Authors:** [Francesco Paissan](http://openreview.net/profile?id=~Francesco_Paissan1), [Mirco Ravanelli](http://openreview.net/profile?id=~Mirco_Ravanelli1), [Cem Subakan](http://openreview.net/profile?id=~Cem_Subakan1)
  - **Affiliations:** Fondazione Bruno Kessler; Mila-Québec AI Institute, Concordia University; Mila-Québec AI Institute, Laval University
  - **TL;DR:** This paper introduces Listenable Maps for Audio Classifiers (L-MAC), a method that generates faithful and listenable interpretations for audio classifiers by utilizing binary masks to highlight relevant audio segments. The approach demonstrates improved interpretability over existing methods, confirmed by user preference studies.
  - **Keywords:** Explainable Machine Learning, Audio Classification, Listenable Maps for Audio Classifiers (L-MAC), Short-Time Fourier Transform (STFT), Inverse Short-Time Fourier Transform (ISTFT), Speech Recognition, Sound Event Recognition, Sound Generation, Interpretation of model predictions, Black-box models, Faithful and listenable interpretations, Binary masks for audio segments


- [Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining](https://icml.cc/virtual/2024/poster/33114) (Best Paper)
  - **Authors:** [Florian Tramer](http://openreview.net/profile?id=~Florian_Tram%C3%A8r1), [Gautam Kamath](http://openreview.net/profile?id=~Gautam_Kamath1), [Nicholas Carlini](http://openreview.net/profile?id=~Nicholas_Carlini1)
  - **Affiliations:** Department of Computer Science, ETH Zürich, Zürich, Switzerland, Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada; Vector Institute, Toronto, Ontario, Canada, Google DeepMind, Mountain View, USA
  - **TL;DR:** The paper critically reviews the use of large-scale public pretraining to enhance differentially private learning, questioning the privacy-preserving nature of web-scraped datasets and the appropriateness of existing benchmarks. It highlights the potential loss of privacy when relying on pretrained models and the need for careful consideration in sensitive domains.
  - **Keywords:** Differential Privacy, Transfer Learning, Machine Learning, Natural Language Processing, Memorization of training data, Privacy-sensitive information, Augmentation of differentially private learning with public data, Large public datasets, Foundation models, Differential privacy (DP), Generative models, Large language models


- [Position: Automatic Environment Shaping is the Next Frontier in RL](https://icml.cc/virtual/2024/poster/33529) (Oral)
  - **Authors:** [Younghyo Park](http://openreview.net/profile?id=~Younghyo_Park1), [Gabriel Margolis](http://openreview.net/profile?id=~Gabriel_B._Margolis1), [Pulkit Agrawal](http://openreview.net/profile?id=~Pulkit_Agrawal1)
  - **Affiliations:** Improbable AI Lab, Massachusetts Institute of Technology, Cambridge, MA, USA, Improbable AI Lab, Massachusetts Institute of Technology, Cambridge, MA, USA, Improbable AI Lab, Massachusetts Institute of Technology, Cambridge, MA, USA
  - **TL;DR:** The paper discusses the challenges of scaling reinforcement learning for diverse robotic tasks, emphasizing the need for automated environment shaping to improve data collection and policy training. It argues that current methods require substantial human effort and that overcoming these bottlenecks is essential for advancing robotics.
  - **Keywords:** reinforcement learning, robotics, environment shaping, policy optimization, sim-to-real reinforcement learning, robotic tasks, locomotion, manipulation, data collection, data sparsity, robustness and generalization of policies, automation of environment shaping procedures


- [LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning](https://icml.cc/virtual/2024/poster/33106) (Spotlight Poster)
  - **Authors:** [Hongye Jin](http://openreview.net/profile?id=~Hongye_Jin1), [Xiaotian Han](http://openreview.net/profile?id=~Xiaotian_Han1), [Jingfeng Yang](http://openreview.net/profile?id=~Jingfeng_Yang2), [Zhimeng Jiang](http://openreview.net/profile?id=~Zhimeng_Jiang1), [Zirui Liu](http://openreview.net/profile?id=~Zirui_Liu1), [Chia-Yuan Chang](http://openreview.net/profile?id=~Chia-Yuan_Chang3), [Huiyuan Chen](http://openreview.net/profile?id=~Huiyuan_Chen1), [Xia Hu](http://openreview.net/profile?id=~Xia_Hu4)
  - **Affiliations:** Texas A&M University, Texas A&M University, Amazon, Texas A&M University, Rice University, Texas A&M University, Case Western Reserve University, Rice University
  - **TL;DR:** This study introduces SelfExtend, a method to extend the context window of large language models (LLMs) without fine-tuning, by utilizing bi-level attention mechanisms. The results demonstrate that LLMs can effectively handle longer contexts, addressing inherent limitations in their performance with long input sequences.
  - **Keywords:** Long Context Handling, Large Language Models, SelfExtend, Bi-level Attention, Grouped Attention, Neighbor Attention, Natural Language Processing, Context Window Limitations, Out-of-Distribution Issues, Performance Degradation with Long Inputs, Context Window Extension without Fine-tuning


- [LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning](https://icml.cc/virtual/2024/poster/33410) (Poster)
  - **Authors:** [Hyungho Na](http://openreview.net/profile?id=~Hyungho_Na1), [IL CHUL MOON](http://openreview.net/profile?id=~Il-chul_Moon1)
  - **Affiliations:** Korea Advanced Institute of Science and Technology (KAIST), Daejeon 34141, Republic of Korea; summary.ai, Daejeon, Republic of Korea, Korea Advanced Institute of Science and Technology (KAIST), Daejeon 34141, Republic of Korea; summary.ai, Daejeon, Republic of Korea
  - **TL;DR:** This paper introduces LAtentGoal-guided Multi-Agent reinforcement learning (LAGMA), which enhances the efficiency of learning goal-reaching paths in cooperative multi-agent tasks by generating trajectories in latent space and providing intrinsic rewards. The method demonstrates improved performance in complex environments like StarCraft II and Google Research Football compared to existing approaches.
  - **Keywords:** cooperative multi-agent reinforcement learning, goal-reaching trajectories, LAtentGoal-guided Multi-Agent reinforcement learning (LAGMA), modified VQ-VAE, goal-conditioned reinforcement learning (GCRL), StarCraft II, Google Research Football, learning goal-reaching paths, sample efficiency, sparse reward settings, improved performance over state-of-the-art baselines, latent goal-guided intrinsic reward generation


- [How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?](https://icml.cc/virtual/2024/poster/34939) (Oral)
  - **Authors:** [Ryan Liu](http://openreview.net/profile?id=~Ryan_Liu1), [Theodore R Sumers](http://openreview.net/profile?id=~Theodore_Sumers1), [Ishita Dasgupta](http://openreview.net/profile?id=~Ishita_Dasgupta1), [Thomas Griffiths](http://openreview.net/profile?id=~Thomas_L._Griffiths1)
  - **Affiliations:** Department of Computer Science, Princeton University, Anthropic; work performed while at Princeton University, Google DeepMind, Department of Psychology, Princeton University
  - **TL;DR:** This study investigates how large language models navigate the trade-offs between honesty and helpfulness in conversation, revealing that reinforcement learning from human feedback enhances both qualities, while certain prompting techniques may favor helpfulness. The findings suggest that LLMs can internalize conversational values and adapt based on context.
  - **Keywords:** Large Language Models, Honesty, Helpfulness, Reinforcement Learning from Human Feedback (RLHF), Chain-of-Thought Prompting, Conversational Agents, AI Alignment, Hallucination, Trade-offs between Honesty and Helpfulness, Improved Honesty and Helpfulness in LLMs, Sensitivity to Conversational Framing, Gricean Maxims, Signaling Bandits


- [Differentially Private Post-Processing for Fair Regression](https://icml.cc/virtual/2024/poster/34381) (Poster)
  - **Authors:** [Ruicheng Xian](http://openreview.net/profile?id=~Ruicheng_Xian1), [Qiaobo Li](http://openreview.net/profile?id=~Qiaobo_Li1), [Gautam Kamath](http://openreview.net/profile?id=~Gautam_Kamath1), [Han Zhao](http://openreview.net/profile?id=~Han_Zhao1)
  - **Affiliations:** University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Waterloo; Vector Institute, University of Illinois Urbana-Champaign
  - **TL;DR:** This paper presents a differentially private post-processing algorithm for fair regression that remaps outputs to satisfy statistical parity while addressing privacy concerns. The algorithm demonstrates a trade-off between fairness and error based on the choice of histogram bins used in the estimation process.
  - **Keywords:** differentially private algorithms, fair regression, statistical parity, histogram density estimation, Laplace mechanism, Wasserstein barycenter, optimal transport, criminal justice, healthcare, finance, privacy concerns, fairness concerns, historical biases, group fairness, post-processing algorithm for fair regressors, trade-off analysis between bias and variance, Law School dataset, Communities & Crime dataset


- [Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling](https://icml.cc/virtual/2024/poster/34643) (Poster)
  - **Authors:** [Weijia Xu](http://openreview.net/profile?id=~Weijia_Xu3), [Andrzej Banburski-Fahey](http://openreview.net/profile?id=~Andrzej_Banburski1), [Nebojsa Jojic](http://openreview.net/profile?id=~Nebojsa_Jojic1)
  - **Affiliations:** Microsoft Research, Redmond, USA, Microsoft Research, Redmond, USA, Microsoft Research, Redmond, USA
  - **TL;DR:** The study introduces Reprompting, an iterative sampling algorithm that automatically learns effective Chain-of-Thought (CoT) prompts for reasoning tasks without human intervention. It significantly outperforms human-written prompts and existing optimization methods, achieving higher accuracy across various benchmarks.
  - **Keywords:** Chain-of-Thought (CoT) reasoning, automated prompt inference, Gibbs sampling, iterative sampling algorithm, Natural language processing, reasoning tasks, Multi-step reasoning, constraint propagation, prompt optimization, Improved accuracy of CoT prompts, effective CoT recipes, Big-Bench Hard (BBH), GSM8K, MATH, ChatGPT, InstructGPT


- [MultiMax: Sparse and Multi-Modal Attention Learning](https://icml.cc/virtual/2024/poster/34433) (Poster)
  - **Authors:** [Yuxuan Zhou](http://openreview.net/profile?id=~Yuxuan_Zhou2), [Mario Fritz](http://openreview.net/profile?id=~Mario_Fritz1), [Margret Keuper](http://openreview.net/profile?id=~Margret_Keuper1)
  - **Affiliations:** University of Mannheim, Germany; CISPA Helmholtz Center for Information Security, Germany, CISPA Helmholtz Center for Information Security, Germany, University of Mannheim, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany
  - **TL;DR:** The study introduces MultiMax, a piece-wise differentiable function that addresses the limitations of SoftMax by balancing sparsity and multi-modality in attention mechanisms. The findings demonstrate that MultiMax effectively suppresses irrelevant entries while maintaining multi-modality, leading to improvements in various machine learning applications.
  - **Keywords:** Sparse Attention, Multi-Modal Learning, SoftMax, MultiMax, Image Classification, Language Modeling, Machine Translation, Over-smoothing, Multi-modality vs. Sparsity Trade-off, Piece-wise Differentiable Function, Adaptive Output Distribution


- [Evaluating Model Bias Requires Characterizing its Mistakes](https://icml.cc/virtual/2024/poster/33345) (Poster)
  - **Authors:** [Isabela Albuquerque](http://openreview.net/profile?id=~Isabela_Albuquerque1), [Jessica Schrouff](http://openreview.net/profile?id=~Jessica_Schrouff1), [David Warde-Farley](http://openreview.net/profile?id=~David_Warde-Farley1), [Taylan Cemgil](http://openreview.net/profile?id=~Ali_Taylan_Cemgil2), [Sven Gowal](http://openreview.net/profile?id=~Sven_Gowal2), [Olivia Wiles](http://openreview.net/profile?id=~Olivia_Wiles1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This study introduces SKEW SIZE, a new metric for characterizing model biases by analyzing mistakes across subgroups, highlighting biases that traditional metrics overlook. The findings demonstrate its effectiveness in various model settings, emphasizing the importance of understanding model performance in the presence of spurious correlations.
  - **Keywords:** model bias, benchmarking model performance, SKEW SIZE, hypothesis testing framework, image classification, vision models, generative models, spurious correlations, unintended biases, model mistakes, new metric (SKEW SIZE), insights on instruction tuning, IMAGE NET


- [Probabilistic Subgoal Representations for Hierarchical Reinforcement Learning](https://icml.cc/virtual/2024/poster/33655) (Poster)
  - **Authors:** [Vivienne Wang](http://openreview.net/profile?id=~Vivienne_Huiling_Wang1), [Tinghuai Wang](http://openreview.net/profile?id=~Tinghuai_Wang1), [wenyan yang](http://openreview.net/profile?id=~Wenyan_Yang1), [Joni-kristian Kamarainen](http://openreview.net/profile?id=~Joni-kristian_Kamarainen1), [Joni Pajarinen](http://openreview.net/profile?id=~Joni_Pajarinen2)
  - **Affiliations:** Department of Electrical Engineering and Automation, Aalto University, Finland, Huawei Helsinki Research Center, Finland, Computing Sciences, Tampere University, Finland, Computing Sciences, Tampere University, Finland, Department of Electrical Engineering and Automation, Aalto University, Finland
  - **TL;DR:** This paper introduces a novel probabilistic subgoal representation using Gaussian Processes in goal-conditioned hierarchical reinforcement learning, addressing the limitations of deterministic mappings. The proposed method demonstrates improved performance in various environments, particularly those with stochastic elements, and shows potential for transferring low-level policies across tasks.
  - **Keywords:** Hierarchical Reinforcement Learning (HRL), Goal-Conditioned HRL, Gaussian Processes (GPs), Probabilistic Subgoal Representation, Long-term credit assignment, Stochastic uncertainties, Exploration capacity limitations, Novel learning objective for simultaneous learning of probabilistic subgoal representations and policies


- [Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models](https://icml.cc/virtual/2024/poster/34169) (Poster)
  - **Authors:** [Raviteja Vemulapalli](http://openreview.net/profile?id=~Raviteja_Vemulapalli1), [Hadi Pouransari](http://openreview.net/profile?id=~Hadi_Pouransari1), [Fartash Faghri](http://openreview.net/profile?id=~Fartash_Faghri1), [Sachin Mehta](http://openreview.net/profile?id=~Sachin_Mehta1), [Mehrdad Farajtabar](http://openreview.net/profile?id=~Mehrdad_Farajtabar1), [Mohammad Rastegari](http://openreview.net/profile?id=~Mohammad_Rastegari2), [Oncel Tuzel](http://openreview.net/profile?id=~Oncel_Tuzel2)
  - **Affiliations:** Apple, USA, Apple, USA, Apple, USA, Apple, USA, Apple, USA, Apple, USA, Apple, USA
  - **TL;DR:** This study proposes a task-oriented knowledge transfer approach to effectively train small task-specific models using knowledge from large Vision Foundation Models, demonstrating significant performance improvements and reductions in pretraining compute costs across various tasks. The findings highlight the importance of dataset selection for optimal performance in target tasks.
  - **Keywords:** Knowledge transfer, Vision Foundation Models, task-specific models, VFM distillation, retrieval-augmented knowledge transfer, Autonomous driving, medical image diagnostics, industrial automation, High inference compute cost, limited labeled training data, Improved performance on downstream tasks, reduced pretraining compute cost, ImageNet, CC3M


- [Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss](https://icml.cc/virtual/2024/poster/34635) (Spotlight Poster)
  - **Authors:** [Ingvar Ziemann](http://openreview.net/profile?id=~Ingvar_Ziemann1), [Stephen Tu](http://openreview.net/profile?id=~Stephen_Tu1), [George J. Pappas](http://openreview.net/profile?id=~George_J._Pappas1), [Nikolai Matni](http://openreview.net/profile?id=~Nikolai_Matni2)
  - **Affiliations:** University of Pennsylvania, University of Southern California, University of Pennsylvania, University of Pennsylvania
  - **TL;DR:** This study investigates statistical learning with dependent data and square loss, presenting a near mixing-free rate that mitigates sample size deflation issues. The findings indicate that the empirical risk minimizer's performance is primarily influenced by the complexity of the hypothesis class and second-order statistics, regardless of realizability assumptions.
  - **Keywords:** dependent learning, statistical learning, square loss, empirical risk minimization, weakly sub-Gaussian class, mixed tail generic chaining, forecasting applications, controls/robotics systems, sample size deflation, learning with dependent data, near mixing-free rate, sharp instance-optimal rates, β-mixing, variance proxy, martingale difference sequence


- [Image Fusion via Vision-Language Model](https://icml.cc/virtual/2024/poster/33477) (Poster)
  - **Authors:** [Zixiang Zhao](http://openreview.net/profile?id=~Zixiang_Zhao1), [Lilun Deng](http://openreview.net/profile?id=~Lilun_Deng1), [Haowen Bai](http://openreview.net/profile?id=~Haowen_Bai1), [Yukun Cui](http://openreview.net/profile?id=~Yukun_Cui2), [Zhipeng Zhang](http://openreview.net/profile?id=~Zhipeng_Zhang5), [Yulun Zhang](http://openreview.net/profile?id=~Yulun_Zhang1), [Haotong Qin](http://openreview.net/profile?id=~Haotong_Qin1), [Dongdong Chen](http://openreview.net/profile?id=~Dongdong_Chen4), [Jiangshe Zhang](http://openreview.net/profile?id=~Jiangshe_Zhang1), [Peng Wang](http://openreview.net/profile?id=~PENG_WANG15), [Luc Van Gool](http://openreview.net/profile?id=~Luc_Van_Gool1)
  - **Affiliations:** Xi’an Jiaotong University, China; ETH Zürich, Switzerland, Xi’an Jiaotong University, China, Xi’an Jiaotong University, China, Xi’an Jiaotong University, China, ETH Zürich, Switzerland; Northwestern Polytechnical University, China, MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China, ETH Zürich, Switzerland, Heriot-Watt University, United Kingdom, Xi’an Jiaotong University, China, ETH Zürich, Switzerland; Northwestern Polytechnical University, China, KU Leuven, Belgium; INSAIT, Bulgaria
  - **TL;DR:** The study introduces a novel image fusion paradigm called FILM, which utilizes textual information from images to enhance the fusion process. It demonstrates promising results across various image fusion tasks and proposes a vision-language dataset to support future research in this area.
  - **Keywords:** Image fusion, Vision-language models, Cross-attention, ChatGPT, BLIP2, Infrared-visible image fusion, Medical image fusion, Multi-exposure image fusion, Multi-focus image fusion, Integration of visual and textual information, Enhancement of feature extraction, Novel fusion paradigm (FILM), Vision-language dataset, ChatGPT, BLIP2


- [Reinforcement Learning and Regret Bounds for Admission Control](https://icml.cc/virtual/2024/poster/33905) (Poster)
  - **Authors:** [Lucas Weber](http://openreview.net/profile?id=~Lucas_Weber2), [Ana Busic](http://openreview.net/profile?id=~Ana_Busic1), [Jiamin ZHU](http://openreview.net/profile?email=jiamin.zhu%40ifpen.fr)
  - **Affiliations:** Inria and DI ENS, École Normale Supérieure, PSL University, Paris, France, Inria and DI ENS, École Normale Supérieure, PSL University, Paris, France, IFP Energies nouvelles, 1 et 4 avenue de Bois-Préau, 92852 Rueil-Malmaison, France
  - **TL;DR:** This study addresses the admission control problem in queuing systems using reinforcement learning to minimize expected total regret. The authors propose an algorithm that achieves a significant reduction in regret, particularly in finite server scenarios, while demonstrating that the dependence on buffer size vanishes in infinite server cases.
  - **Keywords:** admission control, reinforcement learning, regret minimization, UCRL2 algorithm, Markov decision process, queuing systems, communication systems, Quality of Service (QoS) management, minimizing expected total regret, class-dependent rewards and holding costs, job prioritization, upper bound on expected total regret, optimal policy for transient management, M/M/c/S queue, class-specific rates, finite buffer queue


- [Weisfeiler Leman for Euclidean Equivariant Machine Learning](https://icml.cc/virtual/2024/poster/34751) (Poster)
  - **Authors:** [Snir Hordan](http://openreview.net/profile?id=~Snir_Hordan1), [Tal Amir](http://openreview.net/profile?id=~Tal_Amir1), [Nadav Dym](http://openreview.net/profile?id=~Nadav_Dym1)
  - **Affiliations:** Faculty of Mathematics, Technion - Israel Institute of Technology, Haifa, Israel; Faculty of Computer Science, Technion - Israel Institute of Technology, Haifa, Israel, Faculty of Mathematics, Technion - Israel Institute of Technology, Haifa, Israel, Faculty of Mathematics, Technion - Israel Institute of Technology, Haifa, Israel; Faculty of Computer Science, Technion - Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This paper extends the expressive power of graph neural networks (GNNs) for point clouds by demonstrating that PPGN can simulate the 2-WL test uniformly and by developing a universal equivariant architecture, WeLNet, which achieves state-of-the-art results in N-Body dynamics and molecular conformation tasks.
  - **Keywords:** Euclidean equivariant machine learning, graph neural networks, point clouds, k-Weisfeiler-Leman (k-WL) graph isomorphism test, PPGN (Point-Cloud Graph Neural Network), Chemo-informatics, particle dynamics, computer vision, Expressive power of GNNs, limitations of existing models, Universal equivariant architecture, WeLNet architecture, state-of-the-art results on N-Body dynamics and GEOM-QM9 tasks


- [Harmony in Diversity: Merging Neural Networks with Canonical Correlation Analysis](https://icml.cc/virtual/2024/poster/33397) (Poster)
  - **Authors:** [Stefan Horoi](http://openreview.net/profile?id=~Stefan_Horoi1), [Albert Manuel Orozco Camacho](http://openreview.net/profile?id=~Albert_Manuel_Orozco_Camacho1), [Eugene Belilovsky](http://openreview.net/profile?id=~Eugene_Belilovsky1), [Guy Wolf](http://openreview.net/profile?id=~Guy_Wolf1)
  - **Affiliations:** Université de Montréal; Mila - Quebec AI Institute, Concordia University, Concordia University, Université de Montréal; Mila - Quebec AI Institute
  - **TL;DR:** This paper introduces CCA Merge, a novel algorithm based on Canonical Correlation Analysis for merging neural network models, which significantly improves performance compared to previous methods. The study demonstrates that this approach effectively aligns models trained on the same or different data splits, addressing challenges posed by high-dimensional and non-convex loss landscapes.
  - **Keywords:** model merging, ensemble learning, neural networks, Canonical Correlation Analysis (CCA), CCA Merge, machine learning, predictive performance, high-dimensional data, non-convex loss landscapes, high loss barriers, improved model alignment, better performance in model averaging


- [OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models](https://icml.cc/virtual/2024/poster/33771) (Poster)
  - **Authors:** [Ali AhmadiTeshnizi](http://openreview.net/profile?id=~Ali_AhmadiTeshnizi1), [Wenzhi Gao](http://openreview.net/profile?id=~Wenzhi_Gao1), [Madeleine Udell](http://openreview.net/profile?id=~Madeleine_Udell1)
  - **Affiliations:** Department of Management Science and Engineering, Stanford University, CA, USA; Institute for Computational and Mathematical Engineering, Stanford University, CA, USA, Institute for Computational and Mathematical Engineering, Stanford University, CA, USA, Department of Management Science and Engineering, Stanford University, CA, USA; Institute for Computational and Mathematical Engineering, Stanford University, CA, USA
  - **TL;DR:** This paper presents OptiMUS, a Large Language Model-based agent that automates the formulation and solving of optimization problems, significantly improving accessibility and efficiency in various sectors. Experiments show that OptiMUS outperforms existing methods by over 20% on easy datasets and over 30% on complex datasets.
  - **Keywords:** optimization problems, large language models, automation in optimization, mixed integer linear programming (MILP), linear programming (LP), manufacturing, distribution, healthcare, energy management, supply chain management, expertise gap in optimization modeling, challenges in formulating optimization problems, OptiMUS agent, improved optimization modeling, evaluation of generated solutions, NLP4LP (new dataset)


- [Efficient Policy Evaluation with Offline Data Informed Behavior Policy Design](https://icml.cc/virtual/2024/poster/33235) (Poster)
  - **Authors:** [Shuze Liu](http://openreview.net/profile?id=~Shuze_Liu1), [Shangtong Zhang](http://openreview.net/profile?id=~Shangtong_Zhang1)
  - **Affiliations:** Department of Computer Science, University of Virginia, Department of Computer Science, University of Virginia
  - **TL;DR:** This paper presents novel methods to enhance the data efficiency of online Monte Carlo estimators in reinforcement learning while maintaining unbiasedness. The authors propose a closed-form behavior policy that reduces variance and demonstrate its effectiveness through empirical studies across various environments.
  - **Keywords:** reinforcement learning, offline reinforcement learning, Monte Carlo estimators, closed-form behavior policy, variance reduction, data efficiency, high-fidelity simulation, reliance on online data, improved empirical performance, unbiased estimation


- [Revealing Vision-Language Integration in the Brain with Multimodal Networks](https://icml.cc/virtual/2024/poster/33050) (Poster)
  - **Authors:** [Vighnesh Subramaniam](http://openreview.net/profile?id=~Vighnesh_Subramaniam1), [Colin Conwell](http://openreview.net/profile?id=~Colin_Conwell1), [Christopher Wang](http://openreview.net/profile?id=~Christopher_Wang1), [Gabriel Kreiman](http://openreview.net/profile?id=~Gabriel_Kreiman1), [Boris Katz](http://openreview.net/profile?id=~Boris_Katz1), [Ignacio Cases](http://openreview.net/profile?id=~Ignacio_Cases2), [Andrei Barbu](http://openreview.net/profile?id=~Andrei_Barbu3)
  - **Affiliations:** MIT CSAIL; CBMM, Department of Cognitive Science, Johns Hopkins University, MIT CSAIL; CBMM, Boston Children’s Hospital, Harvard Medical School, MIT CSAIL; CBMM, MIT CSAIL; CBMM, MIT CSAIL; CBMM
  - **TL;DR:** This study investigates multimodal integration in the human brain using deep neural networks to predict SEEG recordings from subjects watching movies. The findings reveal numerous neural sites associated with multimodal integration, highlighting the superiority of CLIP-style training for predicting neural activity.
  - **Keywords:** multimodal integration, vision-language processing, deep neural networks (DNNs), convolutional networks, transformers, cross-attention, contrastive learning, neuroscience, brain activity prediction, understanding multimodal integration in the brain, limitations of unimodal models, identification of neural sites for multimodal integration, effectiveness of CLIP-style training, stereoelectroencephalography (SEEG), large-scale SEEG dataset


- [QuIP$\#$: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://icml.cc/virtual/2024/poster/34816) (Poster)
  - **Authors:** [Albert Tseng](http://openreview.net/profile?id=~Albert_Tseng1), [Jerry Chee](http://openreview.net/profile?id=~Jerry_Chee1), [Qingyao Sun](http://openreview.net/profile?id=~Qingyao_Sun1), [Volodymyr Kuleshov](http://openreview.net/profile?id=~Volodymyr_Kuleshov1), [Chris De Sa](http://openreview.net/profile?id=~Christopher_De_Sa2)
  - **Affiliations:** Department of Computer Science, Cornell University, Department of Computer Science, Cornell University, Department of Operations Research and Information Engineering, Cornell University, Department of Computer Science, Cornell University, Department of Computer Science, Cornell University
  - **TL;DR:** This paper introduces QuIP #, a weight-only post-training quantization method that significantly reduces the memory footprint of large language models while achieving state-of-the-art results in extreme compression regimes. The method employs novel techniques such as randomized Hadamard transforms and lattice codebooks, leading to improved performance and faster inference.
  - **Keywords:** Large Language Models (LLMs), Post-Training Quantization (PTQ), Randomized Hadamard Transform, Vector Quantization, Lattice Codebooks, Natural Language Processing, Scientific Modeling, Program Synthesis, Memory Footprint Reduction, Extreme Compression Ratios, State-of-the-art PTQ Method, Improved Incoherence Processing, Hardware-efficient Codebooks, E8 Lattice, Gaussian Distribution


- [Sampling-based Multi-dimensional Recalibration](https://icml.cc/virtual/2024/poster/33363) (Poster)
  - **Authors:** [Youngseog Chung](http://openreview.net/profile?id=~Youngseog_Chung1), [Ian Char](http://openreview.net/profile?id=~Ian_Char1), [Jeff Schneider](http://openreview.net/profile?id=~Jeff_Schneider1)
  - **Affiliations:** Machine Learning Department; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, None, None
  - **TL;DR:** This study proposes a recalibration method for multi-dimensional regression that accounts for the joint distribution of output dimensions, addressing the limitations of traditional calibration methods. The method is demonstrated to produce HDR calibrated samples, enhancing the quality of probabilistic forecasts in various applications.
  - **Keywords:** Calibration, Probabilistic Forecasting, Multi-dimensional Regression, Highest Density Regions (HDR), Recalibration Algorithm, Model-based Control, Physical Sciences, Plasma Dynamics, Demand Forecasting, Dependence among Output Dimensions, Calibration Assessment, HDR Calibrated Predictive Distributions, Benchmark Datasets, Real-world Dataset


- [Switching the Loss Reduces the Cost in Batch Reinforcement Learning](https://icml.cc/virtual/2024/poster/34885) (Poster)
  - **Authors:** [Alex Ayoub](http://openreview.net/profile?id=~Alex_Ayoub1), [Kaiwen Wang](http://openreview.net/profile?id=~Kaiwen_Wang1), [Vincent Liu](http://openreview.net/profile?id=~Vincent_Liu3), [Samuel Robertson](http://openreview.net/profile?email=smrobert%40ualberta.ca), [James McInerney](http://openreview.net/profile?id=~James_McInerney2), [Dawen Liang](http://openreview.net/profile?id=~Dawen_Liang1), [Nathan Kallus](http://openreview.net/profile?id=~Nathan_Kallus1), [Csaba Szepesvari](http://openreview.net/profile?id=~Csaba_Szepesvari1)
  - **Affiliations:** University of Alberta, Cornell University, University of Alberta, University of Alberta, Netflix, Inc., Netflix, Inc., Netflix, Inc., University of Alberta
  - **TL;DR:** This study introduces FQI-LOG, a method for training fitted Q-iteration using log-loss in batch reinforcement learning, demonstrating that it requires fewer samples to learn a near-optimal policy compared to traditional methods. The findings suggest that FQI-LOG achieves small-cost bounds, enhancing sample efficiency in goal-oriented tasks where optimal policies incur minimal costs.
  - **Keywords:** batch reinforcement learning, fitted Q-iteration, log-loss, FQI-LOG, small-cost bounds, goal-oriented RL tasks, sample efficiency, optimal policy cost, improved sample efficiency, computationally efficient batch RL algorithm


- [Learning Optimal Projection for Forecast Reconciliation of Hierarchical Time Series](https://icml.cc/virtual/2024/poster/34990) (Poster)
  - **Authors:** [Asterios Tsiourvas](http://openreview.net/profile?id=~Asterios_Tsiourvas1), [Wei Sun](http://openreview.net/profile?id=~Wei_Sun7), [Georgia Perakis](http://openreview.net/profile?id=~Georgia_Perakis1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1), [Yada Zhu](http://openreview.net/profile?id=~Yada_Zhu1)
  - **Affiliations:** Operations Research Center, Massachusetts Institute of Technology, USA, IBM Research, USA, Operations Research Center, Massachusetts Institute of Technology, USA, IBM Research, USA, IBM Research, USA
  - **TL;DR:** This study focuses on learning optimal oblique projections for coherent forecasting of hierarchical time series, demonstrating that this approach outperforms traditional reconciliation methods. The proposed method integrates a learnable projection layer within a neural forecaster architecture, leading to improved accuracy and coherence in forecasts across various levels of the hierarchy.
  - **Keywords:** Hierarchical time series forecasting, Forecast reconciliation, Oblique projection, Euclidean projection, Neural forecaster architecture, Retail demand forecasting, Electricity consumption forecasting, Coherency in forecasts, Aggregation constraints, Learnable projection layer, Optimal projection learning, Real-world hierarchical time series datasets


- [Hierarchical Novelty Detection via Fine-Grained Evidence Allocation](https://icml.cc/virtual/2024/poster/34333) (Poster)
  - **Authors:** [Spandan Pyakurel](http://openreview.net/profile?id=~Spandan_Pyakurel1), [Qi Yu](http://openreview.net/profile?id=~Qi_Yu1)
  - **Affiliations:** Rochester Institute of Technology, Rochester, New York, Rochester Institute of Technology, Rochester, New York
  - **TL;DR:** The study introduces Hierarchical Novelty Detection (HND), which enhances novelty detection by associating novel samples with their closest known parent class in a hierarchy. The proposed method outperforms existing techniques by providing fine-grained detection results and insights into novel samples.
  - **Keywords:** Hierarchical Novelty Detection, Novelty Detection, Fine-Grained Evidence Allocation, Loss Function, Real-World Hierarchical Datasets, Confusion between known and novel classes, Binary detection results, Evidence margin for separating known and novel classes


- [Contextual Feature Selection with Conditional Stochastic Gates](https://icml.cc/virtual/2024/poster/34597) (Poster)
  - **Authors:** [Ram Dyuthi Sristi](http://openreview.net/profile?id=~Ram_Dyuthi_Sristi1), [Ofir Lindenbaum](http://openreview.net/profile?id=~Ofir_Lindenbaum1), [Shira Lifshitz](http://openreview.net/profile?email=shiralif%40campus.technion.ac.il), [Maria Lavzin](http://openreview.net/profile?id=~Maria_Lavzin1), [Jackie Schiller](http://openreview.net/profile?id=~Jackie_Schiller1), [Gal Mishne](http://openreview.net/profile?id=~Gal_Mishne1), [Hadas Benisty](http://openreview.net/profile?id=~Hadas_Benisty1)
  - **Affiliations:** University of California San Diego, La Jolla, California, USA, Bar-Ilan University, Ramat Gan, Israel, Technion - Israel Institute of Technology, Haifa, Israel, Technion - Israel Institute of Technology, Haifa, Israel, Technion - Israel Institute of Technology, Haifa, Israel, University of California San Diego, La Jolla, California, USA, Technion - Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This study introduces a novel architecture for contextual feature selection using Conditional Stochastic Gates (c-STG), which adapts feature relevance based on context variables. The proposed method demonstrates improved feature selection capabilities and prediction accuracy across various domains.
  - **Keywords:** Contextual feature selection, Machine learning, Conditional Stochastic Gates (c-STG), Hypernetwork, Healthcare, Product recommendation systems, Feature relevance variation, High dimensionality, Interpretability, Improved feature selection capabilities, Enhanced prediction accuracy, Simulated datasets, Real-world datasets


- [A Computational Framework for Solving Wasserstein Lagrangian Flows](https://icml.cc/virtual/2024/poster/32732) (Poster)
  - **Authors:** [Kirill Neklyudov](http://openreview.net/profile?id=~Kirill_Neklyudov1), [Rob Brekelmans](http://openreview.net/profile?id=~Rob_Brekelmans1), [Alexander Tong](http://openreview.net/profile?id=~Alexander_Tong1), [Lazar Atanackovic](http://openreview.net/profile?id=~Lazar_Atanackovic1), [qiang liu](http://openreview.net/profile?id=~qiang_liu4), [Alireza Makhzani](http://openreview.net/profile?id=~Alireza_Makhzani1)
  - **Affiliations:** Université de Montréal; Mila - Quebec AI Institute, Vector Institute, Université de Montréal; Mila - Quebec AI Institute, Vector Institute; University of Toronto, University of Texas at Austin, Vector Institute; University of Toronto
  - **TL;DR:** This paper presents a deep learning framework for solving various optimal transport problems, particularly focusing on trajectory inference in single-cell RNA-sequencing data. The proposed method effectively incorporates prior knowledge and outperforms existing approaches in predicting cell dynamics.
  - **Keywords:** optimal transport, trajectory inference, deep learning, Wasserstein metrics, Lagrangian action functionals, variational problems, single-cell RNA-sequencing, population dynamics, ill-posed problem of trajectory inference, multiple dynamics yielding same marginals, novel framework for dynamics inference, improved predictions in single-cell analysis, Wasserstein-2, Wasserstein Fisher-Rao metrics


- [Autoformalizing Euclidean Geometry](https://icml.cc/virtual/2024/poster/33614) (Poster)
  - **Authors:** [Logan Murphy](http://openreview.net/profile?email=lmurphy%40cs.toronto.edu), [Kaiyu Yang](http://openreview.net/profile?id=~Kaiyu_Yang1), [Jialiang Sun](http://openreview.net/profile?id=~Jialiang_Sun2), [Zhaoyu Li](http://openreview.net/profile?id=~Zhaoyu_Li3), [Anima Anandkumar](http://openreview.net/profile?id=~Anima_Anandkumar1), [Xujie Si](http://openreview.net/profile?id=~Xujie_Si1)
  - **Affiliations:** University of Toronto, Caltech, University of Toronto, University of Toronto, Caltech, University of Toronto
  - **TL;DR:** This paper presents a neuro-symbolic framework for autoformalizing Euclidean geometry, addressing the challenge of translating informal proofs into formal theorems using large language models and automated reasoning tools. The study introduces the LeanEuclid benchmark and demonstrates the capabilities and limitations of state-of-the-art LLMs in this domain.
  - **Keywords:** autoformalization, Euclidean geometry, machine-verifiable theorems, neuro-symbolic framework, SMT solvers, large language models (LLMs), formal proofs, theorem proving, gaps in informal proofs, automatic evaluation of theorems, LeanEuclid benchmark, evaluation of autoformalized theorem statements, UniGeo dataset, Lean proof assistant, multimodal reasoning models


- [Executable Code Actions Elicit Better LLM Agents](https://icml.cc/virtual/2024/poster/33320) (Poster)
  - **Authors:** [Xingyao Wang](http://openreview.net/profile?id=~Xingyao_Wang1), [Yangyi Chen](http://openreview.net/profile?id=~Yangyi_Chen1), [Lifan Yuan](http://openreview.net/profile?id=~Lifan_Yuan1), [Yizhe Zhang](http://openreview.net/profile?id=~Yizhe_Zhang2), [Yunzhu Li](http://openreview.net/profile?id=~Yunzhu_Li1), [Hao Peng](http://openreview.net/profile?id=~Hao_Peng4), [Heng Ji](http://openreview.net/profile?id=~Heng_Ji3)
  - **Affiliations:** Department of Computer Science, University of Illinois Urbana-Champaign, Department of Computer Science, University of Illinois Urbana-Champaign, Department of Computer Science, University of Illinois Urbana-Champaign, Apple, Department of Computer Science, University of Illinois Urbana-Champaign, Department of Computer Science, University of Illinois Urbana-Champaign, Department of Computer Science, University of Illinois Urbana-Champaign
  - **TL;DR:** This study introduces CodeAct, a framework that enables Large Language Model agents to generate and execute Python code, significantly expanding their action space and improving their performance in real-world tasks. The results demonstrate a 20% higher success rate compared to traditional methods, paving the way for more effective LLM applications.
  - **Keywords:** Large Language Models, LLM agents, executable code actions, Python code execution, action modules, multi-turn interactions, Tool invocation, robot control, scientific experiments, Constrained action space, restricted flexibility, dynamic action adjustment, CodeAct framework, improved success rate, open-source LLM agent, API-Bank, CodeActInstruct


- [Time Weaver: A Conditional Time Series Generation Model](https://icml.cc/virtual/2024/poster/33847) (Spotlight Poster)
  - **Authors:** [Sai Shankar Narasimhan](http://openreview.net/profile?id=~Sai_Shankar_Narasimhan1), [Shubhankar Agarwal](http://openreview.net/profile?id=~Shubhankar_Agarwal1), [Oguzhan Akcin](http://openreview.net/profile?id=~Oguzhan_Akcin2), [Sujay Sanghavi](http://openreview.net/profile?id=~sujay_sanghavi1), [Sandeep Chinchali](http://openreview.net/profile?id=~Sandeep_P._Chinchali1)
  - **Affiliations:** Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, USA, Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, USA, Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, USA, Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, USA, Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, USA
  - **TL;DR:** The study introduces TIME WEAVER, a novel diffusion-based model for generating time series data that effectively utilizes heterogeneous contextual metadata. It outperforms existing methods, including GANs, by improving the specificity and realism of generated time series, demonstrating significant advancements in various application areas.
  - **Keywords:** time series generation, conditional generation, metadata utilization, diffusion-based model, energy demand forecasting, medical data generation, air quality monitoring, traffic data analysis, poor specificity in reproducing metadata-specific features, challenges in handling diverse metadata conditions, novel evaluation metric for conditional generation, improved time series generation performance, Generative Adversarial Networks (GANs)


- [Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel SVD and Nyström method](https://icml.cc/virtual/2024/poster/34489) (Poster)
  - **Authors:** [Qinghua Tao](http://openreview.net/profile?id=~Qinghua_Tao1), [Francesco Tonin](http://openreview.net/profile?id=~Francesco_Tonin1), [Alex Lambert](http://openreview.net/profile?id=~Alex_Lambert1), [Yingyi Chen](http://openreview.net/profile?id=~Yingyi_Chen3), [Panagiotis Patrinos](http://openreview.net/profile?id=~Panagiotis_Patrinos1), [Johan Suykens](http://openreview.net/profile?id=~Johan_Suykens1)
  - **Affiliations:** ESAT-STADIUS, KU Leuven, Belgium, LIONS, EPFL, Switzerland; ESAT-STADIUS, KU Leuven, Belgium, ESAT-STADIUS, KU Leuven, Belgium, ESAT-STADIUS, KU Leuven, Belgium, ESAT-STADIUS, KU Leuven, Belgium, ESAT-STADIUS, KU Leuven, Belgium
  - **TL;DR:** This study introduces a new asymmetric learning paradigm based on coupled covariance eigenproblems, enabling the use of infinite-dimensional feature mappings and providing empirical evaluations of the Asymmetric Kernel Singular Value Decomposition (KSVD). The findings highlight the practical utility of KSVD compared to traditional symmetrization methods in various machine learning tasks.
  - **Keywords:** Asymmetric learning, Kernel methods, Feature mappings, Asymmetric Kernel Singular Value Decomposition (KSVD), Coupled covariance eigenproblem (CCE), Asymmetric Nyström method, Machine learning, Data analysis, Infinite-dimensional feature mappings, Unbounded variational objectives, Asymmetry in data representation, New learning paradigm, Empirical evaluations of KSVD, Comparison with symmetrization methods, Singular Value Decomposition (SVD), Reproducing kernels, Gram matrix


- [Learning to Scale Logits for Temperature-Conditional GFlowNets](https://icml.cc/virtual/2024/poster/34500) (Poster)
  - **Authors:** [Minsu Kim](http://openreview.net/profile?id=~Minsu_Kim2), [Joohwan Ko](http://openreview.net/profile?id=~Joohwan_Ko2), [Taeyoung Yun](http://openreview.net/profile?id=~Taeyoung_Yun1), [Dinghuai Zhang](http://openreview.net/profile?id=~Dinghuai_Zhang1), [Ling Pan](http://openreview.net/profile?id=~Ling_Pan1), [Woo Chang Kim](http://openreview.net/profile?id=~Woo_Chang_Kim1), [Jinkyoo Park](http://openreview.net/profile?id=~Jinkyoo_Park1), [Emmanuel Bengio](http://openreview.net/profile?id=~Emmanuel_Bengio1), [Yoshua Bengio](http://openreview.net/profile?id=~Yoshua_Bengio1)
  - **Affiliations:** Korea Advanced Institute of Science and Technology; Mila – Québec AI Institute, Korea Advanced Institute of Science and Technology, Korea Advanced Institute of Science and Technology, Mila – Québec AI Institute; Université de Montréal, Hong Kong University of Science and Technology, Korea Advanced Institute of Science and Technology, Korea Advanced Institute of Science and Technology, Recursion, Mila – Québec AI Institute; Université de Montréal; CIFAR
  - **TL;DR:** This paper introduces Logit-scaling GFlowNets (Logit-GFN), a novel architecture that enhances the training of temperature-conditional GFlowNets by addressing numerical challenges associated with varying temperatures. The proposed method improves generalization in offline learning and mode discovery in online learning, validated through various biological and chemical tasks.
  - **Keywords:** GFlowNets, temperature-conditional GFlowNets, generative models, Logit-scaling, stochastic policy, compositional structures, Scientific discovery, drug discovery, biological tasks, chemical tasks, Training instability, gradient profile challenges, exploration-exploitation trade-off, Improved generalization capabilities, mode discovery capabilities


- [Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo](https://icml.cc/virtual/2024/poster/33450) (Best Paper)
  - **Authors:** [Stephen Zhao](http://openreview.net/profile?id=~Stephen_Zhao1), [Rob Brekelmans](http://openreview.net/profile?id=~Rob_Brekelmans1), [Alireza Makhzani](http://openreview.net/profile?id=~Alireza_Makhzani1), [Roger Grosse](http://openreview.net/profile?id=~Roger_Baker_Grosse1)
  - **Affiliations:** University of Toronto; Vector Institute, Vector Institute, University of Toronto; Vector Institute, University of Toronto; Vector Institute
  - **TL;DR:** This paper presents a novel approach using Twisted Sequential Monte Carlo for probabilistic inference in Large Language Models, focusing on enhancing safety and capability techniques. The proposed methods effectively sample from undesirable outputs and evaluate inference accuracy, contributing to advancements in AI safety and alignment.
  - **Keywords:** Large Language Models, Probabilistic Inference, AI Safety, Sequential Monte Carlo, Reinforcement Learning from Human Feedback (RLHF), Twist Functions, Automated Red-Teaming, Inference Evaluation, Sentiment Generation, Sampling from unnormalized distributions, Estimating normalization constants, Twisted Sequential Monte Carlo methods, Bidirectional SMC bounds, KL Divergence, Potential Function, Partition Function


- [Large Scale Dataset Distillation with Domain Shift](https://icml.cc/virtual/2024/poster/35212) (Poster)
  - **Authors:** [Noel Loo](http://openreview.net/profile?id=~Noel_Loo1), [Alaa Maalouf](http://openreview.net/profile?id=~Alaa_Maalouf1), [Ramin Hasani](http://openreview.net/profile?id=~Ramin_Hasani1), [Mathias Lechner](http://openreview.net/profile?id=~Mathias_Lechner1), [Alexander Amini](http://openreview.net/profile?id=~Alexander_Amini1), [Daniela Rus](http://openreview.net/profile?id=~Daniela_Rus1)
  - **Affiliations:** MIT CSAIL; Liquid AI, MIT CSAIL; Liquid AI, MIT CSAIL; Liquid AI, MIT CSAIL; Liquid AI, MIT CSAIL; Liquid AI, MIT CSAIL; Liquid AI
  - **TL;DR:** This study introduces Dataset Distillation with Domain Shift (D3S), a scalable algorithm that reformulates dataset distillation as a domain shift problem, achieving state-of-the-art results on large datasets like ImageNet-1K. The method provides a universal upper bound on distillation loss and demonstrates significant improvements over previous techniques.
  - **Keywords:** Dataset Distillation, Domain Shift, D3S algorithm, synthetic sample generation, Image recognition, large-scale datasets, Scaling dataset distillation, GPU memory demands, high-resolution datasets, Universal upper bound on distillation loss, efficient optimization method, Tiny-ImageNet, ImageNet-1k, ImageNet-21K


- [Saliency strikes back: How filtering out high frequencies improves white-box explanations](https://icml.cc/virtual/2024/poster/33514) (Poster)
  - **Authors:** [Sabine Muzellec](http://openreview.net/profile?id=~Sabine_Muzellec1), [Thomas FEL](http://openreview.net/profile?id=~Thomas_FEL1), [Victor Boutin](http://openreview.net/profile?id=~Victor_Boutin2), [Léo Andéol](http://openreview.net/profile?id=~L%C3%A9o_And%C3%A9ol1), [Rufin VanRullen](http://openreview.net/profile?id=~Rufin_VanRullen1), [Thomas Serre](http://openreview.net/profile?id=~Thomas_Serre1)
  - **Affiliations:** Carney Institute for Brain Science, Brown University, USA; CerCo, CNRS, France, Carney Institute for Brain Science, Brown University, USA; SNCF, France, Carney Institute for Brain Science, Brown University, USA; CerCo, CNRS, France, SNCF, France; Institute of Mathematics of Toulouse, University Paul Sabatier, France, CerCo, CNRS, France, Carney Institute for Brain Science, Brown University, USA
  - **TL;DR:** The study introduces FORGrad, a method that filters high-frequency artifacts in white-box attribution methods, significantly improving their performance and enabling them to compete with more complex black-box methods. This advancement aims to enhance the explainability of neural networks while maintaining computational efficiency.
  - **Keywords:** Explainability, Attribution methods, White-box methods, Black-box methods, FORGrad, Saliency, High-frequency artifacts, Noisy attribution maps, Enhanced performance of white-box methods, Improved explainability, eXplainable Artificial Intelligence (XAI), Artificial Neural Networks (ANNs)


- [NExT: Teaching Large Language Models to Reason about Code Execution](https://icml.cc/virtual/2024/poster/34741) (Poster)
  - **Authors:** [Ansong Ni](http://openreview.net/profile?id=~Ansong_Ni1), [Miltiadis Allamanis](http://openreview.net/profile?id=~Miltiadis_Allamanis1), [Arman Cohan](http://openreview.net/profile?id=~Arman_Cohan1), [Yinlin Deng](http://openreview.net/profile?id=~Yinlin_Deng1), [Kensen Shi](http://openreview.net/profile?id=~Kensen_Shi1), [Charles Sutton](http://openreview.net/profile?id=~Charles_Sutton1), [Pengcheng Yin](http://openreview.net/profile?id=~Pengcheng_Yin1)
  - **Affiliations:** Yale University, Google DeepMind, Yale University, University of Illinois at Urbana-Champaign, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** The study introduces NEXT, a method to enhance large language models' ability to reason about program execution by utilizing execution traces and generating natural language rationales. The approach significantly improves the fix rate of a PaLM 2 model in program repair tasks, demonstrating better rationale quality and generalization capabilities.
  - **Keywords:** Large Language Models, Program Execution Reasoning, Program Repair, Chain-of-Thought (CoT) Reasoning, Self-Training, Software Engineering, Code Debugging, Lack of semantic understanding in LLMs, Complex software engineering tasks, Improved fix rate in program repair tasks, Execution-aware rationales, MBPP, HUMAN EVAL, PaLM 2, Rubber Duck Debugging, Execution Traces


- [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://icml.cc/virtual/2024/poster/32666) (Poster)
  - **Authors:** [Xingang Guo](http://openreview.net/profile?id=~Xingang_Guo1), [Fangxu Yu](http://openreview.net/profile?id=~Fangxu_Yu1), [Huan Zhang](http://openreview.net/profile?id=~Huan_Zhang1), [Lianhui Qin](http://openreview.net/profile?id=~Lianhui_Qin1), [Bin Hu](http://openreview.net/profile?id=~Bin_Hu2)
  - **Affiliations:** University of Illinois at Urbana–Champaign, University of California, San Diego, University of Illinois at Urbana–Champaign, University of California, San Diego; Allen Institute for AI, University of Illinois at Urbana–Champaign
  - **TL;DR:** This paper introduces the COLD-Attack framework for controllable jailbreaking of large language models, addressing the challenge of generating adversarial attacks with specific control requirements. The framework demonstrates strong controllability and high success rates across various LLMs, contributing to the field of AI safety.
  - **Keywords:** Jailbreaking, Large Language Models (LLMs), AI Safety, Energy-based Constrained Decoding with Langevin Dynamics (COLD), Controllable Text Generation, Controllability of Adversarial LLM Attacks, Vulnerabilities in LLMs, COLD-Attack Framework, Automated Search of Adversarial Attacks, Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, GPT-4, Stealthiness, Contextual Coherence, Sentiment Variations


- [Uniformly Stable Algorithms for Adversarial Training and Beyond](https://icml.cc/virtual/2024/poster/33077) (Poster)
  - **Authors:** [Jiancong Xiao](http://openreview.net/profile?id=~Jiancong_Xiao1), [Jiawei Zhang](http://openreview.net/profile?id=~Jiawei_Zhang6), [Zhi-Quan Luo](http://openreview.net/profile?id=~Zhi-Quan_Luo1), [Asuman Ozdaglar](http://openreview.net/profile?id=~Asuman_E._Ozdaglar1)
  - **Affiliations:** University of Pennsylvania, PA, USA, Massachusetts Institute of Technology, MA, USA, The Chinese University of Hong Kong, Shenzhen, China, Massachusetts Institute of Technology, MA, USA
  - **TL;DR:** This study addresses the issue of robust overfitting in adversarial training by introducing a uniformly stable algorithm called Moreau envelope-A (ME-A), which effectively mitigates the problem without additional computational overhead. The findings highlight the importance of uniform stability in weakly-convex, non-smooth problems within adversarial machine learning.
  - **Keywords:** Adversarial Machine Learning, Robust Overfitting, Moreau Envelope-type algorithm, SGD (Stochastic Gradient Descent), Robust overfitting, non-smoothness of adversarial loss, Uniformly stable algorithms, ME-A (Moreau envelope-A), SVHN, CIFAR-10, CIFAR-100, Uniform stability analysis


- [Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations](https://icml.cc/virtual/2024/poster/35099) (Poster)
  - **Authors:** [Xuyang Zhong](http://openreview.net/profile?id=~Xuyang_Zhong1), [Yixiao HUANG](http://openreview.net/profile?id=~Yixiao_HUANG2), [Chen Liu](http://openreview.net/profile?id=~Chen_Liu1)
  - **Affiliations:** City University of Hong Kong, Hong Kong SAR, China, City University of Hong Kong; University of Michigan, Ann Arbor, USA, City University of Hong Kong, Hong Kong SAR, China
  - **TL;DR:** This study introduces a novel white-box attack method called sparse-PGD for generating l0bounded adversarial perturbations and demonstrates its effectiveness in evaluating model robustness and conducting adversarial training. The proposed method achieves state-of-the-art robustness against various sparse attacks compared to existing robust models.
  - **Keywords:** Adversarial Machine Learning, Robustness, Sparse-PGD, Projected Gradient Descent (PGD), Object Detection, Autonomous Driving, Sparse Adversarial Perturbations, l0norm Perturbations, Adversarial Training, Robust Models


- [PANDA: Expanded Width-Aware Message Passing Beyond Rewiring](https://icml.cc/virtual/2024/poster/34404) (Poster)
  - **Authors:** [Jeongwhan Choi](http://openreview.net/profile?id=~Jeongwhan_Choi1), [Sumin Parksumin](http://openreview.net/profile?id=~Sumin_Park2), [Hyowon Wi](http://openreview.net/profile?id=~Hyowon_Wi1), [Sung-Bae Cho](http://openreview.net/profile?id=~Sung-Bae_Cho1), [Noseong Park](http://openreview.net/profile?id=~Noseong_Park1)
  - **Affiliations:** Yonsei University, Seoul, South Korea, DNI Consulting, Seoul, South Korea, Yonsei University, Seoul, South Korea, Yonsei University, Seoul, South Korea, KAIST, Daejeon, South Korea
  - **TL;DR:** This study introduces PANDA, an expanded width-aware message passing method to address the over-squashing issue in graph neural networks, which enhances the propagation of long-range information without distorting the original graph topology. Experimental results demonstrate that this approach outperforms existing graph rewiring methods.
  - **Keywords:** Graph Neural Networks (GNN), Message Passing Neural Networks (MPNN), Expanded Width-Aware Message Passing, Signal Propagation, Over-squashing, Bottleneck Phenomenon, New message passing paradigm, Improved signal propagation


- [Harnessing Hierarchical Label Distribution Variations in Test Agnostic Long-tail Recognition](https://icml.cc/virtual/2024/poster/33489) (Poster)
  - **Authors:** [Zhiyong Yang](http://openreview.net/profile?id=~Zhiyong_Yang1), [Qianqian Xu](http://openreview.net/profile?id=~Qianqian_Xu2), [Zitai Wang](http://openreview.net/profile?id=~Zitai_Wang1), [Sicong Li](http://openreview.net/profile?id=~Sicong_Li2), [Boyu Han](http://openreview.net/profile?id=~Boyu_Han1), [Shilong Bao](http://openreview.net/profile?id=~Shilong_Bao1), [Xiaochun Cao](http://openreview.net/profile?id=~Xiaochun_Cao3), [Qingming Huang](http://openreview.net/profile?id=~Qingming_Huang1)
  - **Affiliations:** School of Computer Science and Tech., University of Chinese Academy of Sciences, Key Lab. of Intelligent Information Processing, Institute of Computing Tech., CAS; School of Computer Science and Tech., University of Chinese Academy of Sciences, Institute of Information Engineering, CAS; School of Cyber Security, University of Chinese Academy of Sciences, Institute of Information Engineering, CAS; School of Cyber Security, University of Chinese Academy of Sciences, Key Lab. of Intelligent Information Processing, Institute of Computing Tech., CAS, Institute of Information Engineering, CAS; School of Cyber Security, University of Chinese Academy of Sciences, School of Cyber Science and Tech., Shenzhen Campus of Sun Yat-sen University, School of Computer Science and Tech., University of Chinese Academy of Sciences; BDKM, University of Chinese Academy of Sciences
  - **TL;DR:** This study introduces DirMixE, a new Mixture-of-Expert strategy for test-agnostic long-tail recognition that effectively captures both global and local variations in label distributions. The proposed method enhances generalization and provides a more stable objective function, validated through comprehensive experiments across multiple benchmarks.
  - **Keywords:** test-agnostic long-tail recognition, long-tailed data, Mixture-of-Expert (MoE), Dirichlet meta-distributions, image recognition, species classification, face recognition, medical image diagnosis, social image understanding, semantic segmentation, object detection, unknown test label distributions, arbitrarily imbalanced data, local and global variations, DirMixE method, enhanced generalization, variance-based regularization


- [MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance](https://icml.cc/virtual/2024/poster/34454) (Poster)
  - **Authors:** [Yake Wei](http://openreview.net/profile?id=~Yake_Wei1), [Di Hu](http://openreview.net/profile?id=~Di_Hu1)
  - **Affiliations:** Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China
  - **TL;DR:** This study addresses the gradient conflict between multimodal and unimodal learning objectives in multimodal learning systems and proposes the MMPareto algorithm to enhance model performance and generalization. The findings suggest that integrating unimodal assistance can effectively alleviate the imbalanced utilization of modalities.
  - **Keywords:** Multimodal learning, Unimodal learning, MMPareto algorithm, Pareto integration, Imbalanced multimodal learning, Gradient conflict, Enhanced generalization, Improved model performance, Kinetics Sounds dataset, Cityscapes dataset


- [Position: AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research](https://icml.cc/virtual/2024/poster/33713) (Oral)
  - **Authors:** [Riley Simmons-Edler](http://openreview.net/profile?id=~Riley_Simmons-Edler1), [Ryan Badman](http://openreview.net/profile?id=~Ryan_Paul_Badman1), [Shayne Longpre](http://openreview.net/profile?id=~Shayne_Longpre1), [Kanaka Rajan](http://openreview.net/profile?id=~Kanaka_Rajan1)
  - **Affiliations:** Department of Neurobiology, Harvard Medical School, Boston, USA; Kempner Institute, Harvard University, Cambridge, USA, Department of Neurobiology, Harvard Medical School, Boston, USA; Kempner Institute, Harvard University, Cambridge, USA, Massachusetts Institute of Technology, Cambridge, USA, Department of Neurobiology, Harvard Medical School, Boston, USA; Kempner Institute, Harvard University, Cambridge, USA
  - **TL;DR:** The paper discusses the risks posed by the development and deployment of AI-powered Autonomous Weapons Systems (AWS) to geopolitical stability and AI research. It emphasizes the need for transparency and caution in military AI development to mitigate potential negative impacts on global stability.
  - **Keywords:** autonomous weapons systems (AWS), geopolitical stability, military technology, machine learning (ML), military applications, warfare, risks of geopolitical instability, ethical issues in military AI, escalation of conflicts, regulatory suggestions for AWS, awareness of risks in military technology, lethal autonomous weapon systems (LAWS), AI-powered arms race


- [Evaluating Quantized Large Language Models](https://icml.cc/virtual/2024/poster/34632) (Poster)
  - **Authors:** [Shiyao Li](http://openreview.net/profile?id=~Shiyao_Li2), [Xuefei Ning](http://openreview.net/profile?id=~Xuefei_Ning1), [Luning Wang](http://openreview.net/profile?id=~Luning_Wang2), [Tengxuan Liu](http://openreview.net/profile?id=~Tengxuan_Liu1), [Xiangsheng Shi](http://openreview.net/profile?id=~Xiangsheng_Shi1), [Shengen Yan](http://openreview.net/profile?id=~Shengen_Yan1), [Guohao Dai](http://openreview.net/profile?id=~Guohao_Dai4), [Huazhong Yang](http://openreview.net/profile?id=~Huazhong_Yang2), [Yu Wang](http://openreview.net/profile?id=~Yu_Wang3)
  - **Affiliations:** Department of Electronic Engineering, Tsinghua University, Beijing, China; None, Department of Electronic Engineering, Tsinghua University, Beijing, China; Infinigence AI, Department of Electronic Engineering, Tsinghua University, Beijing, China; None, Department of Electronic Engineering, Tsinghua University, Beijing, China; Infinigence AI, Department of Electronic Engineering, Tsinghua University, Beijing, China; None, Infinigence AI; Shanghai Jiaotong University, Shanghai, China, Infinigence AI; Shanghai Jiaotong University, Shanghai, China, Department of Electronic Engineering, Tsinghua University, Beijing, China; None, Department of Electronic Engineering, Tsinghua University, Beijing, China; None
  - **TL;DR:** This paper evaluates the impact of post-training quantization on large language models, focusing on memory consumption and computational efficiency across various tasks. The findings provide insights into the effectiveness of different quantization methods and their implications for model performance.
  - **Keywords:** Post-training quantization, Large Language Models (LLMs), Weight-only Quantization, Weight-Activation Quantization, KV Cache Quantization, Natural Language Processing (NLP), dialogue systems, Memory consumption, computational overhead, efficiency in deployment, Evaluation of quantization methods, recommendations for quantization techniques, General Matrix-Vector Multiply (GEMV), General Matrix Multiply (GEMM), Key-Value Cache (KV Cache)


- [SelfVC: Voice Conversion With Iterative Refinement using Self Transformations](https://icml.cc/virtual/2024/poster/34904) (Poster)
  - **Authors:** [Paarth Neekhara](http://openreview.net/profile?id=~Paarth_Neekhara1), [Shehzeen Hussain](http://openreview.net/profile?id=~Shehzeen_Samarah_Hussain1), [Rafael Valle](http://openreview.net/profile?id=~Rafael_Valle1), [Boris Ginsburg](http://openreview.net/profile?id=~Boris_Ginsburg1), [Rishabh Ranjan](http://openreview.net/profile?id=~Rishabh_Ranjan5), [Shlomo Dubnov](http://openreview.net/profile?id=~Shlomo_Dubnov1), [Farinaz Koushanfar](http://openreview.net/profile?id=~Farinaz_Koushanfar1), [Julian McAuley](http://openreview.net/profile?id=~Julian_McAuley1)
  - **Affiliations:** NVIDIA, NVIDIA, NVIDIA, NVIDIA, UC San Diego, UC San Diego, UC San Diego, UC San Diego
  - **TL;DR:** The study introduces SelfVC, a novel training strategy for voice conversion that leverages self-synthesized examples to enhance model performance without explicit disentanglement of speech attributes. The results demonstrate significant improvements in speaker similarity and overall audio quality in zero-shot voice conversion tasks.
  - **Keywords:** voice conversion, self-supervised learning, self-synthesized examples, predictive submodules, speech synthesis, speaker verification, information loss in disentangled representations, state-of-the-art results in zero-shot voice conversion, improved speaker similarity, disentangled representations, prosodic information


- [Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization](https://icml.cc/virtual/2024/poster/33398) (Poster)
  - **Authors:** [Yihan Du](http://openreview.net/profile?id=~Yihan_Du2), [Anna Winnicki](http://openreview.net/profile?id=~Anna_Winnicki1), [Gal Dalal](http://openreview.net/profile?id=~Gal_Dalal2), [Shie Mannor](http://openreview.net/profile?id=~Shie_Mannor2), [R Srikant](http://openreview.net/profile?id=~R._Srikant1)
  - **Affiliations:** University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, NVIDIA Research, Technion, University of Illinois Urbana-Champaign
  - **TL;DR:** This study investigates a policy optimization algorithm for Reinforcement Learning from Human Feedback (RLHF) that does not assume prior knowledge of the reward function, demonstrating that a small amount of human feedback can lead to effective learning. The findings provide theoretical insights into the efficiency of query usage in RLHF, highlighting the potential of policy-based approaches.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Policy Optimization, Policy Cover-Policy Gradient (PC-PG), trajectory-based comparison feedback, Robotics, Large Language Models (LLMs), Limited theoretical justification for RLHF, challenges in reward function design, Performance bounds for PO-RLHF, query complexity insights


- [Model-Based Minimum Bayes Risk Decoding for Text Generation](https://icml.cc/virtual/2024/poster/33005) (Poster)
  - **Authors:** [Yuu Jinnai](http://openreview.net/profile?id=~Yuu_Jinnai1), [Tetsuro Morimura](http://openreview.net/profile?id=~Tetsuro_Morimura1), [Ukyo Honda](http://openreview.net/profile?id=~Ukyo_Honda1), [Kaito Ariu](http://openreview.net/profile?id=~Kaito_Ariu1), [Kenshi Abe](http://openreview.net/profile?id=~Kenshi_Abe1)
  - **Affiliations:** CyberAgent, Tokyo, Japan, CyberAgent, Tokyo, Japan, CyberAgent, Tokyo, Japan, CyberAgent, Tokyo, Japan, CyberAgent, Tokyo, Japan
  - **TL;DR:** This study introduces model-based Minimum Bayes Risk (MBMBR) decoding as a more effective alternative to traditional beam search in text generation tasks. The findings demonstrate that MBMBR outperforms existing methods by utilizing model probabilities directly, leading to improved performance across various applications.
  - **Keywords:** Minimum Bayes Risk (MBR), text generation, model-based MBR (MBMBR), Monte Carlo estimator, machine translation, text summarization, image captioning, degeneration problems of beam search, estimation error in expected utility, improved decoding strategy, model-based estimate, Kullback-Leibler (KL) divergence


- [Recurrent Distance Filtering for Graph Representation Learning](https://icml.cc/virtual/2024/poster/34968) (Poster)
  - **Authors:** [Yuhui Ding](http://openreview.net/profile?id=~Yuhui_Ding1), [Antonio Orvieto](http://openreview.net/profile?id=~Antonio_Orvieto3), [Bobby He](http://openreview.net/profile?id=~Bobby_He1), [Thomas Hofmann](http://openreview.net/profile?id=~Thomas_Hofmann1)
  - **Affiliations:** Department of Computer Science, ETH Zürich, ELLIS Institute Tübingen; MPI-IS, Tübingen AI Center, Department of Computer Science, ETH Zürich, Department of Computer Science, ETH Zürich
  - **TL;DR:** This paper proposes a new architecture for graph representation learning that effectively aggregates information from distant nodes using a linear RNN, addressing the limitations of traditional graph neural networks and outperforming state-of-the-art graph transformers with reduced computational cost.
  - **Keywords:** Graph representation learning, Graph neural networks, Long-range modeling, Message passing neural networks (MPNNs), Linear RNN, Social networks, Recommender systems, Molecular data, Difficulty in harnessing information from distant nodes, Over-squashing, New architecture for graph representation, Improved performance over graph transformers


- [Causal Discovery with Fewer Conditional Independence Tests](https://icml.cc/virtual/2024/poster/34449) (Poster)
  - **Authors:** [Kirankumar Shiragur](http://openreview.net/profile?id=~Kirankumar_Shiragur1), [Jiaqi Zhang](http://openreview.net/profile?id=~Jiaqi_Zhang2), [Caroline Uhler](http://openreview.net/profile?id=~Caroline_Uhler1)
  - **Affiliations:** Eric and Wendy Schmidt Center, Broad Institute, Laboratory for Information & Decision Systems, Massachusetts Institute of Technology, Laboratory for Information & Decision Systems, Massachusetts Institute of Technology
  - **TL;DR:** This study presents an efficient algorithm for causal discovery that reduces the number of conditional independence tests required, allowing for the recovery of a coarser representation of the causal graph with a polynomial number of tests. The findings suggest that it is possible to identify the true causal graph under certain conditions using observational data and interventions.
  - **Keywords:** Causal discovery, causal relationships, Conditional independence tests, Causal Consistent Partition Graph (CCPG), Biology, economics, sociology, Exponential number of conditional independence tests, limitations in causal structure learning, Efficient algorithm for recovering causal graph, polynomial number of tests, Directed acyclic graphs (DAGs), causal graph


- [Exploiting Human-AI Dependence for Learning to Defer](https://icml.cc/virtual/2024/poster/33674) (Poster)
  - **Authors:** [Zixi Wei](http://openreview.net/profile?id=~Zixi_Wei1), [Yuzhou Cao](http://openreview.net/profile?id=~Yuzhou_Cao1), [Lei Feng](http://openreview.net/profile?id=~Lei_Feng1)
  - **Affiliations:** College of Computer Science, Chongqing University, China, School of Computer Science and Engineering, Nanyang Technological University, Singapore, Information Systems Technology Design Pillar, Singapore University of Technology and Design, Singapore
  - **TL;DR:** This paper introduces a new formulation of Bayes optimality, termed dependent Bayes optimality, to enhance the learning to defer (L2D) framework, allowing models to better decide when to defer to human experts. The proposed method demonstrates superior performance in minimizing critical mispredictions across various risk-critical domains.
  - **Keywords:** Learning to defer (L2D), Human-AI collaboration, Dependent Bayes optimality, Consistent surrogate loss, Medical diagnosis, Criminal justice, Autonomous driving, Critical mispredictions, Risk-minimization, 0-1-deferral risk, Novel consistent surrogate loss, Improved decision-making framework, Synthetic datasets, Real-world datasets


- [Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion](https://icml.cc/virtual/2024/poster/33184) (Poster)
  - **Authors:** [Hila Manor](http://openreview.net/profile?id=~Hila_Manor1), [Tomer Michaeli](http://openreview.net/profile?id=~Tomer_Michaeli1)
  - **Affiliations:** Faculty of Electrical and Computer Engineering, Technion – Israel Institute of Technology, Haifa, Israel, Faculty of Electrical and Computer Engineering, Technion – Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This paper presents two novel zero-shot editing techniques for audio signals using pre-trained diffusion models: ZEro-shot Text-based Audio (ZETA) editing and ZEro-shot UnSupervized (ZEUS) editing. The methods allow for significant audio manipulation, including instrument control and melody improvisation, while maintaining high perceptual quality.
  - **Keywords:** zero-shot audio editing, generative models, audio signal manipulation, DDPM inversion, text-guided editing, unsupervised editing, audio domain, music signal processing, ZETA editing, ZEUS editing, Denoising Diffusion Models (DDMs), latent noise vectors


- [Towards General Algorithm Discovery for Combinatorial Optimization: Learning Symbolic Branching Policy from Bipartite Graph](https://icml.cc/virtual/2024/poster/33946) (Poster)
  - **Authors:** [Yufei Kuang](http://openreview.net/profile?id=~Yufei_Kuang1), [Jie Wang](http://openreview.net/profile?id=~Jie_Wang1), [Yuyan Zhou](http://openreview.net/profile?id=~Yuyan_Zhou1), [Xijun Li](http://openreview.net/profile?id=~Xijun_Li1), [Fangzhou Zhu](http://openreview.net/profile?id=~Fangzhou_Zhu1), [Jianye Hao](http://openreview.net/profile?id=~Jianye_HAO1), [Feng Wu](http://openreview.net/profile?id=~Feng_Wu1)
  - **Affiliations:** CAS Key Laboratory of Technology in GIPAS, University of Science and Technology of China; MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS, University of Science and Technology of China; MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS, University of Science and Technology of China; MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China, CAS Key Laboratory of Technology in GIPAS, University of Science and Technology of China; MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China; Noah’s Ark Lab, Huawei Technologies, Noah’s Ark Lab, Huawei Technologies, Noah’s Ark Lab, Huawei Technologies; College of Intelligence and Computing, Tianjin University, CAS Key Laboratory of Technology in GIPAS, University of Science and Technology of China; MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, University of Science and Technology of China
  - **TL;DR:** This study introduces GS4CO, a graph-based framework for discovering interpretable branching policies in combinatorial optimization solvers, addressing the limitations of black-box machine learning models. The proposed method outperforms existing human-designed and learning-based approaches, marking a significant advancement in algorithm discovery for CO problems.
  - **Keywords:** Combinatorial Optimization, Machine Learning, Graph Neural Networks, Transformer, Exact CO Solvers, Variable Selection, NP-hard Problems, Black-box Nature of ML Models, Interpretable Branching Policies, Algorithm Discovery, Bipartite Graph Representation, Mixed-Integer Linear Programming (MILP)


- [Gradient Compressed Sensing: A Query-Efficient Gradient Estimator for High-Dimensional Zeroth-Order Optimization](https://icml.cc/virtual/2024/poster/32788) (Poster)
  - **Authors:** [Ruizhong Qiu](http://openreview.net/profile?id=~Ruizhong_Qiu1), [Hanghang Tong](http://openreview.net/profile?id=~Hanghang_Tong3)
  - **Affiliations:** Department of Computer Science, University of Illinois Urbana–Champaign, USA, Department of Computer Science, University of Illinois Urbana–Champaign, USA
  - **TL;DR:** This paper introduces Gradient Compressed Sensing (GraCe), a novel query-efficient gradient estimator for high-dimensional zeroth-order optimization that significantly reduces query complexity while maintaining convergence rates. The proposed method outperforms existing techniques in empirical benchmarks, demonstrating its effectiveness in optimizing functions with sparse gradients.
  - **Keywords:** zeroth-order optimization, high-dimensional optimization, Gradient Compressed Sensing (GraCe), Indyk–Price–Woodruff (IPW) algorithm, high-dimensional data, query complexity, gradient sparsity, query-efficient gradient estimator, improved optimization algorithms


- [Fundamental Limitations of Alignment in Large Language Models](https://icml.cc/virtual/2024/poster/34338) (Poster)
  - **Authors:** [Yotam Wolf](http://openreview.net/profile?id=~Yotam_Wolf1), [Noam Wies](http://openreview.net/profile?id=~Noam_Wies1), [Oshri Avnery](http://openreview.net/profile?id=~Oshri_Avnery1), [Yoav Levine](http://openreview.net/profile?id=~Yoav_Levine1), [Amnon Shashua](http://openreview.net/profile?id=~Amnon_Shashua1)
  - **Affiliations:** Department of Computer Science, Hebrew University of Jerusalem, Israel, Department of Computer Science, Hebrew University of Jerusalem, Israel, Department of Computer Science, Hebrew University of Jerusalem, Israel, AI21 Labs, Israel, Department of Computer Science, Hebrew University of Jerusalem, Israel; AI21 Labs, Israel
  - **TL;DR:** This paper investigates the fundamental limitations of aligning large language models (LLMs) to ensure they behave safely and usefully for human users. It demonstrates that any alignment process that does not completely eliminate undesired behaviors is vulnerable to adversarial prompting, highlighting the urgent need for improved AI safety mechanisms.
  - **Keywords:** AI Alignment, Large Language Models, Reinforcement Learning from Human Feedback (RLHF), Behavior Expectation Bounds (BEB), Natural Language Processing, AI Safety, Undesired behaviors in language models, adversarial prompting attacks, Limitations of alignment processes, need for reliable AI safety mechanisms, ChatGPT jailbreaks, harmful behaviors


- [Flexible Residual Binarization for Image Super-Resolution](https://icml.cc/virtual/2024/poster/32619) (Poster)
  - **Authors:** [Yulun Zhang](http://openreview.net/profile?id=~Yulun_Zhang1), [Haotong Qin](http://openreview.net/profile?id=~Haotong_Qin1), [Zixiang Zhao](http://openreview.net/profile?id=~Zixiang_Zhao1), [Xianglong Liu](http://openreview.net/profile?id=~Xianglong_Liu3), [Martin Danelljan](http://openreview.net/profile?id=~Martin_Danelljan4), [Fisher Yu](http://openreview.net/profile?id=~Fisher_Yu2)
  - **Affiliations:** MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China, ETH Zürich, Switzerland, ETH Zürich, Switzerland, Beihang University, China, ETH Zürich, Switzerland, ETH Zürich, Switzerland
  - **TL;DR:** This study introduces a flexible residual binarization method for image super-resolution that addresses information loss and representation distortion caused by traditional binarization techniques. The proposed methods, FRBC and FRBT, demonstrate superior performance in both quantitative and visual assessments compared to existing binary approaches.
  - **Keywords:** image super-resolution, binarization, flexible residual binarization (FRB), second-order residual binarization (SRB), Distillation-guided Binarization Training (DBT), image processing, neural network compression, information loss due to binarization, representation content distortion, performance drop in binary neural networks, superior performance of FRBC and FRBT, alignment of contents across different bit widths, Urban100, binary neural networks (BNNs), convolutional networks, Transformer-based networks


- [Minimum-Norm Interpolation Under Covariate Shift](https://icml.cc/virtual/2024/poster/33714) (Poster)
  - **Authors:** [Neil Mallinar](http://openreview.net/profile?id=~Neil_Rohit_Mallinar1), [Austin Zane](http://openreview.net/profile?id=~Austin_Zane1), [Spencer Frei](http://openreview.net/profile?id=~Spencer_Frei1), [Bin Yu](http://openreview.net/profile?id=~Bin_Yu5)
  - **Affiliations:** Department of Computer Science, University of California San Diego, CA, USA, Department of Statistics, University of California Berkeley, CA, USA, Department of Statistics, University of California Davis, CA, USA, Department of Statistics, University of California Berkeley, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California Berkeley, CA, USA
  - **TL;DR:** This study investigates the performance of high-dimensional linear models under transfer learning, specifically focusing on benign overfitting and its implications. The authors establish non-asymptotic excess risk bounds for minimum-norm interpolators and propose a taxonomy of covariate shifts, supported by empirical studies.
  - **Keywords:** Transfer learning, benign overfitting, high-dimensional linear regression, Minimum-norm interpolator (MNI), ridge regression, Noisy training labels, generalization behavior, covariate shift, Non-asymptotic excess risk bounds, taxonomy of covariate shifts, Real image data, Overparameterization, out-of-distribution (OOD) datasets


- [What is the Long-Run Distribution of Stochastic Gradient Descent? A Large Deviations Analysis](https://icml.cc/virtual/2024/poster/32770) (Poster)
  - **Authors:** [Waïss Azizian](http://openreview.net/profile?id=~Wa%C3%AFss_Azizian1), [Franck Iutzeler](http://openreview.net/profile?id=~Franck_Iutzeler1), [Jérôme Malick](http://openreview.net/profile?id=~Jerome_Malick1), [Panayotis Mertikopoulos](http://openreview.net/profile?id=~Panayotis_Mertikopoulos1)
  - **Affiliations:** Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LJK, 38000 Grenoble, France, Institut de Mathématiques de Toulouse, Université de Toulouse, CNRS, UPS, 31062 Toulouse, France, Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France, Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France
  - **TL;DR:** This study investigates the long-run distribution of stochastic gradient descent (SGD) in non-convex optimization problems, revealing that SGD visits critical regions exponentially more often than non-critical ones. The findings suggest that the distribution of SGD iterates resembles the Boltzmann–Gibbs distribution, with implications for understanding the behavior of SGD in complex loss landscapes.
  - **Keywords:** Stochastic Gradient Descent, Long-Run Distribution, Non-Convex Optimization, Large Deviations Theory, Randomly Perturbed Dynamical Systems, Machine Learning, Deep Learning, Critical Points, Saddle Points, Local Minima, Boltzmann–Gibbs Distribution, Energy Levels, Step-Size Implications


- [Provably Efficient Long-Horizon Exploration in Monte Carlo Tree Search through State Occupancy Regularization](https://icml.cc/virtual/2024/poster/33952) (Poster)
  - **Authors:** [Liam Schramm](http://openreview.net/profile?id=~Liam_Schramm2), [Abdeslam Boularias](http://openreview.net/profile?id=~Abdeslam_Boularias1)
  - **Affiliations:** Department of Computer Science, Rutgers University, New Brunswick, USA, Department of Computer Science, Rutgers University, New Brunswick, USA
  - **TL;DR:** This study introduces Volume-MCTS, a novel algorithm that enhances long-horizon exploration in Monte Carlo Tree Search by utilizing state occupancy measure regularization. The results demonstrate that Volume-MCTS outperforms existing methods like AlphaZero in various robot navigation tasks, providing strong exploration guarantees.
  - **Keywords:** Monte Carlo Tree Search, Long-Horizon Exploration, Robotics, Volume-MCTS, State Occupancy Measure Regularization, Policy Optimization, Robot Navigation, Autonomous Driving, Long-Horizon Exploration Challenges, Exploration Efficiency, New MCTS Algorithm, Exploration Guarantees, Non-Asymptotic High-Probability Bounds, Count-Based Exploration, Sampling-Based Motion Planning, Voronoi Bias


- [Collage: Light-Weight Low-Precision Strategy for LLM Training](https://icml.cc/virtual/2024/poster/34271) (Poster)
  - **Authors:** [Tao Yu](http://openreview.net/profile?id=~Tao_Yu1), [Gaurav Gupta](http://openreview.net/profile?id=~Gaurav_Gupta2), [KARTHICK GOPALSWAMY](http://openreview.net/profile?id=~Karthick_Gopalswamy1), [Amith Mamidala](http://openreview.net/profile?id=~Amith_R_Mamidala1), [Hao Zhou](http://openreview.net/profile?id=~Hao_Zhou12), [Jeffrey Huynh](http://openreview.net/profile?id=~Jeffrey_Huynh1), [Youngsuk Park](http://openreview.net/profile?id=~Youngsuk_Park1), [Ron Diamant](http://openreview.net/profile?email=diamant%40amazon.com), [Anoop Deoras](http://openreview.net/profile?id=~Anoop_Deoras1), [Luke Huan](http://openreview.net/profile?id=~Luke_Huan2)
  - **Affiliations:** Cornell University, Ithaca, NY, AWS AI Labs, Santa Clara, CA; AWS AI Research and Education, Santa Clara, CA, AWS Annapurna Labs, Cupertino, CA, AWS Annapurna Labs, Cupertino, CA, AWS SageMaker, Santa Clara, CA, AWS Annapurna Labs, Cupertino, CA, AWS AI Research and Education, Santa Clara, CA, AWS Annapurna Labs, Cupertino, CA, AWS AI Labs, Santa Clara, CA, AWS AI Labs, Santa Clara, CA
  - **TL;DR:** The study introduces COLLAGE, a low-precision training strategy for large language models that effectively compensates for numerical errors, achieving similar or better performance compared to traditional mixed-precision methods while significantly reducing memory usage and training time.
  - **Keywords:** Low-precision representation, Large Language Models (LLMs), Multi-component float representation, Mixed-precision training, Generative language modeling, Image generation, Speech generation, High compute cost, Limited hardware memory, Numerical inaccuracies, COLLAGE method, Improved training performance, Memory efficiency, PyTorch, MCTensor library, 16-bit floating points, 32-bit floating points, 8-bit precision


- [Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable Term Frequency](https://icml.cc/virtual/2024/poster/32866) (Poster)
  - **Authors:** [Hyeongjin Kim](http://openreview.net/profile?id=~HyeongJin_Kim1), [Sangwon Kim](http://openreview.net/profile?id=~Sangwon_Kim1), [Dasom Ahn](http://openreview.net/profile?id=~Dasom_Ahn1), [Jong Taek Lee](http://openreview.net/profile?id=~Jong_Taek_Lee1), [Byoung Chul Ko](http://openreview.net/profile?id=~Byoung_Chul_Ko1)
  - **Affiliations:** Department of Computer Engineering, Keimyung University, Daegu, South Korea, Electronics and Telecommunications Research Institute (ETRI), Daegu 42994, South Korea, Department of Computer Engineering, Keimyung University, Daegu, South Korea, Department of Computer Engineering, Kyungpook National University, Daegu, South Korea, Department of Computer Engineering, Keimyung University, Daegu, South Korea
  - **TL;DR:** This study proposes a novel approach called CooK that enhances scene graph generation by incorporating co-occurrence knowledge and learnable term frequency-inverse document frequency (TF-IDF) to address the long-tail problem. The proposed method demonstrates a performance improvement of up to 3.8% over existing state-of-the-art models in the SGG subtask.
  - **Keywords:** Scene Graph Generation, Image Understanding, Message-Passing Neural Networks (MPNN), Co-occurrence Knowledge, TF-IDF, Image Retrieval, Image Captioning, Visual Question Answering, Action Recognition, Co-occurrence of Objects, Long-tail Problem, Performance Improvement in SGG, Generalization Ability, SGG Benchmark Dataset


- [Feel-Good Thompson Sampling for Contextual Dueling Bandits](https://icml.cc/virtual/2024/poster/33221) (Poster)
  - **Authors:** [Xuheng Li](http://openreview.net/profile?id=~Xuheng_Li1), [Heyang Zhao](http://openreview.net/profile?id=~Heyang_Zhao1), [Quanquan Gu](http://openreview.net/profile?id=~Quanquan_Gu1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, CA 90095, USA, Department of Computer Science, University of California, Los Angeles, CA 90095, USA, Department of Computer Science, University of California, Los Angeles, CA 90095, USA
  - **TL;DR:** This paper introduces a Thompson sampling algorithm called FGTS.CDB for linear contextual dueling bandits, which incorporates a novel Feel-Good exploration term to enhance decision-making. The algorithm achieves nearly minimax-optimal regret and outperforms existing methods in evaluations on synthetic data.
  - **Keywords:** Contextual Dueling Bandits, Reinforcement Learning from Human Feedback (RLHF), Thompson Sampling, Feel-Good Thomson Sampling (FGTS), Upper Confidence Bound (UCB), Large Language Models (LLMs), Preference Learning, Preference comparison, Contextual decision-making, Minimax-optimal regret, New exploration term for dueling bandits, Plackett-Luce model, Bradley-Terry-Luce model, Linear contextual bandits


- [Predicting Dose-Response Curves with Deep Neural Networks](https://icml.cc/virtual/2024/poster/34250) (Poster)
  - **Authors:** [Pedro A. Campana](http://openreview.net/profile?id=~Pedro_Alonso_Campana1), [Paul Prasse](http://openreview.net/profile?id=~Paul_Prasse1), [Tobias Scheffer](http://openreview.net/profile?id=~Tobias_Scheffer1)
  - **Affiliations:** Department of Computer Science, University of Potsdam, Potsdam, Germany, Department of Computer Science, University of Potsdam, Potsdam, Germany, Department of Computer Science, University of Potsdam, Potsdam, Germany
  - **TL;DR:** This study presents a deep neural network model that predicts dose-response curves more accurately than traditional Hill-equation models, addressing the limitations of existing methods in drug discovery and personalized medicine. The model demonstrates superior performance in interpolating and extrapolating drug effects on various cell types, potentially improving therapeutic recommendations.
  - **Keywords:** Dose-response curves, Drug discovery, Personalized medicine, Deep neural networks, Embedding models, Drug performance evaluation, In vitro drug testing, Model inadequacy of Hill equation, Zero-shot predictions for unseen drugs or cell lines, Neural model for estimating dose-response curves, Improved interpolation and extrapolation of drug effects, Hill equation, IC50, Biphasic compounds


- [Causal Effect Identification in LiNGAM Models with Latent Confounders](https://icml.cc/virtual/2024/poster/34702) (Poster)
  - **Authors:** [Daniele Tramontano](http://openreview.net/profile?id=~Daniele_Tramontano1), [Yaroslav Kivva](http://openreview.net/profile?id=~Yaroslav_Kivva1), [Saber Salehkaleybar](http://openreview.net/profile?id=~Saber_Salehkaleybar1), [Mathias Drton](http://openreview.net/profile?id=~Mathias_Drton2), [Negar Kiyavash](http://openreview.net/profile?id=~Negar_Kiyavash1)
  - **Affiliations:** Technical University of Munich, Munich, Germany; Munich Center for Machine Learning, Munich, Germany, Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, Leiden Institute of Advanced Computer Science, Leiden University, Netherlands, Technical University of Munich, Munich, Germany; Munich Center for Machine Learning, Munich, Germany, Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland
  - **TL;DR:** This study investigates the identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables, providing graphical criteria and efficient algorithms for estimating these effects. The findings highlight the effectiveness of the proposed methods in various application areas, including medicine and policy evaluation.
  - **Keywords:** Causal Effect Identification, LiNGAM Models, Latent Confounders, Linear Non-Gaussian Acyclic Models (LiNGAM), Reconstruction Independent Component Analysis (RICA), Medicine, Policy Evaluation, Fair Decision-Making, Finance, Neuroscience, Economics, Epidemiology, Identifiability of causal effects, Observational data challenges, Graphical characterization of causal effects, Efficient algorithms for causal effect estimation, Directed Acyclic Graph (DAG), Structural Equation Models (SEM)


- [Transformers, parallel computation, and logarithmic depth](https://icml.cc/virtual/2024/poster/34096) (Spotlight Poster)
  - **Authors:** [Clayton Sanford](http://openreview.net/profile?id=~Clayton_Sanford1), [Daniel Hsu](http://openreview.net/profile?id=~Daniel_Hsu1), [Matus Telgarsky](http://openreview.net/profile?id=~Matus_Telgarsky1)
  - **Affiliations:** Department of Computer Science, Columbia University, New York, NY, USA, Department of Computer Science, Columbia University, New York, NY, USA, Courant Institute, New York University, New York, NY, USA
  - **TL;DR:** This study establishes a formal connection between transformers and Massively Parallel Computation, demonstrating that logarithmic depth is sufficient for transformers to solve computational tasks that are challenging for other neural architectures. The findings highlight parallelism as a key distinguishing feature of transformers.
  - **Keywords:** Transformers, Parallel Computation, Self-attention, Massively Parallel Computation (MPC), Basic computational tasks, limitations of neural sequence models, Logarithmic-depth transformers, simulation of MPC protocols


- [First-Order Manifold Data Augmentation for Regression Learning](https://icml.cc/virtual/2024/poster/33419) (Poster)
  - **Authors:** [Ilya Kaufman](http://openreview.net/profile?id=~Ilya_Kaufman1), [Omri Azencot](http://openreview.net/profile?id=~Omri_Azencot1)
  - **Affiliations:** Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel, Department of Computer Science, Ben-Gurion University of the Negev, Beer-Sheva, Israel
  - **TL;DR:** This study introduces FOMA, a domain-independent data augmentation method for regression learning, which samples new examples from the tangent planes of the training distribution. The method enhances generalization in various neural architectures and outperforms existing mixup-based approaches.
  - **Keywords:** Data Augmentation, Regression Learning, FOMA (First-Order Manifold Data Augmentation), mixup, Overfitting, Generalization, Domain-independent augmentation, Improved generalization of neural architectures, Domain-dependent methods, Domain-independent methods, Neural networks


- [On the Convergence of Projected Bures-Wasserstein Gradient Descent under Euclidean Strong Convexity](https://icml.cc/virtual/2024/poster/33923) (Poster)
  - **Authors:** [Junyi FAN](http://openreview.net/profile?id=~Junyi_FAN1), [Yuxuan Han](http://openreview.net/profile?id=~Yuxuan_Han1), [Zijian Liu](http://openreview.net/profile?id=~Zijian_Liu1), [Jian-Feng Cai](http://openreview.net/profile?id=~Jian-Feng_Cai4), [Yang Wang](http://openreview.net/profile?id=~Yang_Wang25), [Zhengyuan Zhou](http://openreview.net/profile?id=~Zhengyuan_Zhou2)
  - **Affiliations:** Department of Mathematics, Hong Kong University of Science and Technology, Department of Mathematics, Hong Kong University of Science and Technology, Stern School of Business, New York University, Department of Mathematics, Hong Kong University of Science and Technology, Department of Industrial Engineering and Decision Analytics, Hong Kong University of Science and Technology, Stern School of Business, New York University; Arena Technologies
  - **TL;DR:** This paper presents a general convergence rate guarantee for Bures-Wasserstein gradient descent under the assumption of Euclidean strong convexity, along with a closed-form solution for projections onto BW distance-constrained sets. The findings demonstrate significant improvements in computational efficiency and convergence speed in practical applications.
  - **Keywords:** Bures-Wasserstein gradient descent, constrained optimization, Riemannian gradient descent, projection onto BW distance-constrained sets, Gaussian barycenter, matrix recovery, variational inference, distributionally robust optimization, Convergence analysis, Euclidean strong convexity, constrained settings, General convergence rate guarantee, closed-form solution for projections, Wasserstein geometry, positive-definite matrices, Riemannian manifold


- [Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models](https://icml.cc/virtual/2024/poster/33920) (Poster)
  - **Authors:** [Tanmay Gautam](http://openreview.net/profile?id=~Tanmay_Gautam1), [Youngsuk Park](http://openreview.net/profile?id=~Youngsuk_Park1), [Hao Zhou](http://openreview.net/profile?id=~Hao_Zhou12), [Parameswaran Raman](http://openreview.net/profile?id=~Parameswaran_Raman1), [Wooseok Ha](http://openreview.net/profile?id=~Wooseok_Ha1)
  - **Affiliations:** University of California, Berkeley, USA, Amazon AI Research & Education, Santa Clara, USA, Amazon AI Labs, Santa Clara, USA, Amazon AI Research & Education, Santa Clara, USA, Amazon AI Labs, Santa Clara, USA
  - **TL;DR:** This study introduces MeZO-SVRG, a variance-reduced zeroth-order optimization method for fine-tuning language models, which significantly improves test accuracy and reduces memory requirements compared to existing methods. The approach eliminates the need for task-specific prompts and demonstrates efficiency across various large language models.
  - **Keywords:** Fine-tuning language models, Memory-efficient optimization, Zeroth-order optimization, Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG), ZO-SGD, Natural language processing (NLP), High memory requirements for backpropagation, Challenges in large-scale settings, Improved test accuracies, Reduced computation time, Decreased memory requirements, Large language models, Autoregressive models, Masked models


- [Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks](https://icml.cc/virtual/2024/poster/34241) (Poster)
  - **Authors:** [Atli Kosson](http://openreview.net/profile?id=~Atli_Kosson1), [Bettina Messmer](http://openreview.net/profile?id=~Bettina_Messmer1), [Martin Jaggi](http://openreview.net/profile?id=~Martin_Jaggi1)
  - **Affiliations:** EPFL, Switzerland, EPFL, Switzerland, EPFL, Switzerland
  - **TL;DR:** This study explores the impact of weight decay on the update behavior of neurons in deep neural networks, introducing the concept of rotational equilibrium. It reveals that balanced rotation enhances the effectiveness of various optimization methods and reduces the need for learning rate warmup.
  - **Keywords:** weight decay, deep neural networks, rotational equilibrium, Adam, Lion, SGD with momentum, Weight Standardization, AdamW, ℓ2-regularization, optimization dynamics, effective learning rate, balanced rotation, update behavior of neurons


- [Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions](https://icml.cc/virtual/2024/poster/33868) (Poster)
  - **Authors:** [Jingtan Wang](http://openreview.net/profile?id=~Jingtan_Wang1), [Xiaoqiang Lin](http://openreview.net/profile?id=~Xiaoqiang_Lin1), [Rui Qiao](http://openreview.net/profile?id=~Rui_Qiao3), [Chuan-Sheng Foo](http://openreview.net/profile?id=~Chuan-Sheng_Foo1), [Bryan Kian Hsiang Low](http://openreview.net/profile?id=~Bryan_Kian_Hsiang_Low1)
  - **Affiliations:** Department of Computer Science, National University of Singapore; Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR), Department of Computer Science, National University of Singapore, Department of Computer Science, National University of Singapore, Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR), Department of Computer Science, National University of Singapore
  - **TL;DR:** This paper introduces FreeShap, an efficient fine-tuning-free approximation of the Shapley value for instance attribution, addressing the robustness of instance scores in large language models. The findings demonstrate that FreeShap outperforms existing methods in various data-centric applications, enhancing explainability in foundational models.
  - **Keywords:** Explainability, Instance Attribution, Large Language Models, Shapley Value, Neural Tangent Kernel, Fine-tuning-free Approximation, Data Removal, Data Selection, Wrong Label Detection, Model Debugging, Robustness of Instance Scores, Dataset Resampling, FreeShap, Improved Instance Attribution Methods


- [Tilt and Average : Geometric Adjustment of the Last Layer for Recalibration](https://icml.cc/virtual/2024/poster/34949) (Poster)
  - **Authors:** [Gyusang Cho](http://openreview.net/profile?id=~Gyusang_Cho1), [Chan-Hyun Youn](http://openreview.net/profile?id=~Chan-Hyun_Youn1)
  - **Affiliations:** Department of Electrical Engineering, KAIST, Daejeon, Republic of Korea, Department of Electrical Engineering, KAIST, Daejeon, Republic of Korea
  - **TL;DR:** This paper presents a novel recalibration algorithm called Tilt and Average (TNA) that modifies the weights of the last layer of neural networks to improve calibration performance. The method leverages the geometry of the feature space and demonstrates superior results compared to traditional calibration-map approaches.
  - **Keywords:** model calibration, neural networks, Tilt and Average (TNA), calibration maps, medical usage, autonomous driving, satellite applications, overconfident predictions, miscalibration, uncertainty quantification, improved calibration performance, geometric adjustment of weights


- [Local Feature Selection without Label or Feature Leakage for Interpretable Machine Learning Predictions](https://icml.cc/virtual/2024/poster/34223) (Poster)
  - **Authors:** [Harrie Oosterhuis](http://openreview.net/profile?id=~Harrie_Oosterhuis2), [Lijun Lyu](http://openreview.net/profile?id=~Lijun_Lyu1), [Avishek Anand](http://openreview.net/profile?id=~Avishek_Anand1)
  - **Affiliations:** Radboud University, Nijmegen, The Netherlands, TU Delft, Delft, The Netherlands, TU Delft, Delft, The Netherlands
  - **TL;DR:** This study addresses the issue of misleading explanations in local feature selection for interpretable machine learning by formalizing label and feature leakage. The authors propose a new method, SUWR, which effectively mitigates these issues while maintaining high predictive performance and feature-selection sparsity.
  - **Keywords:** Local feature selection, Interpretable machine learning, SUWR (local feature selection method), Label leakage, Feature leakage, Misleading explanations, Overfitting, Instance-specific explanations, High feature-selection sparsity, Feature attributions, Feature masks


- [Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/34417) (Spotlight Poster)
  - **Authors:** [Dake Zhang](http://openreview.net/profile?id=~Dake_Zhang1), [Boxiang Lyu](http://openreview.net/profile?id=~Boxiang_Lyu1), [Shuang Qiu](http://openreview.net/profile?id=~Shuang_Qiu2), [Mladen Kolar](http://openreview.net/profile?id=~mladen_kolar1), [Tong Zhang](http://openreview.net/profile?id=~Tong_Zhang2)
  - **Affiliations:** University of Chicago, IL, USA, University of Chicago, IL, USA, Hong Kong University of Science and Technology, Hong Kong, China, University of Southern California, CA, USA, University of Illinois Urbana-Champaign, IL, USA
  - **TL;DR:** This study introduces two sample-efficient algorithms for risk-sensitive offline reinforcement learning, focusing on the entropic risk measure and linear Markov Decision Processes. The findings highlight the potential for deriving near-optimal policies from pre-collected datasets, addressing significant challenges in risk-sensitive decision-making.
  - **Keywords:** risk-sensitive reinforcement learning, decision-making under uncertainty, entropic risk measure, linear Markov Decision Process (MDP), risk-sensitive pessimistic value iteration algorithm, finance, optimal control, managing uncertainty, minimizing adverse outcomes, offline reinforcement learning, provably sample-efficient algorithms, risk-sensitive offline RL algorithms


- [Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics](https://icml.cc/virtual/2024/poster/33658) (Poster)
  - **Authors:** [Manuel Brenner](http://openreview.net/profile?id=~Manuel_Brenner1), [Florian Hess](http://openreview.net/profile?id=~Florian_Hess1), [Georgia Koppe](http://openreview.net/profile?id=~Georgia_Koppe1), [Daniel Durstewitz](http://openreview.net/profile?id=~Daniel_Durstewitz1)
  - **Affiliations:** Dept. of Theoretical Neuroscience, Central Institute of Mental Health (CIMH), Medical Faculty Mannheim, Heidelberg University, Germany; Faculty of Physics and Astronomy, Heidelberg University, Germany, Dept. of Theoretical Neuroscience, Central Institute of Mental Health (CIMH), Medical Faculty Mannheim, Heidelberg University, Germany; Faculty of Physics and Astronomy, Heidelberg University, Germany, Interdisciplinary Center for Scientific Computing, Heidelberg University, Germany; Hector Institute for AI in Psychiatry, CIMH, Dept. of Theoretical Neuroscience, Central Institute of Mental Health (CIMH), Medical Faculty Mannheim, Heidelberg University, Germany; Interdisciplinary Center for Scientific Computing, Heidelberg University, Germany
  - **TL;DR:** This study presents a novel algorithmic framework for integrating multimodal data to reconstruct nonlinear dynamical systems, utilizing a multimodal variational autoencoder. The framework effectively combines various data sources, including discrete random variables, to generate accurate trajectories that reflect the underlying system dynamics.
  - **Keywords:** multimodal data integration, nonlinear dynamical systems, deep learning for dynamical systems reconstruction, multimodal variational autoencoder, recurrent neural networks (RNNs), neuroscience, climate science, medical domain, reconstruction from discrete random variables, integration of multiple data modalities, generative modeling of complex dynamics, optimal reconstruction from various information sources


- [Coarse-To-Fine Tensor Trains for Compact Visual Representations](https://icml.cc/virtual/2024/poster/33220) (Poster)
  - **Authors:** [Sebastian Loeschcke](http://openreview.net/profile?id=~Sebastian_Bugge_Loeschcke1), [Dan Wang](http://openreview.net/profile?id=~Dan_Wang3), [Christian Leth-Espensen](http://openreview.net/profile?id=~Christian_Munklinde_Leth-Espensen1), [Serge Belongie](http://openreview.net/profile?id=~Serge_Belongie1), [Michael Kastoryano](http://openreview.net/profile?id=~Michael_Kastoryano1), [Sagie Benaim](http://openreview.net/profile?id=~Sagie_Benaim1)
  - **Affiliations:** University of Copenhagen; IT University of Copenhagen, University of Copenhagen, Aarhus University, University of Copenhagen, University of Copenhagen, Hebrew University of Jerusalem
  - **TL;DR:** This paper introduces the Prolongation Upsampling Tensor Train (PuTT), a novel method for learning compact and high-quality tensor train representations in a coarse-to-fine manner. The proposed method demonstrates improved performance in compression, denoising, and image completion tasks compared to existing tensor-based methods.
  - **Keywords:** compact visual representations, tensor networks, optimization, tensor train representation, Prolongation Upsampling Tensor Train (PuTT), novel view synthesis, 3D reconstruction, image fitting, 3D fitting, optimization challenges, local minima, noisy and incomplete data, improved performance in compression, denoising, and image completion, tensor decomposition, low-rank tensor, tensor trains


- [How Smooth Is Attention?](https://icml.cc/virtual/2024/poster/33690) (Poster)
  - **Authors:** [Valérie Castin](http://openreview.net/profile?id=~Val%C3%A9rie_Castin1), [Pierre Ablin](http://openreview.net/profile?id=~Pierre_Ablin2), [Gabriel Peyré](http://openreview.net/profile?id=~Gabriel_Peyr%C3%A92)
  - **Affiliations:** Ecole Normale Supérieure PSL, Paris, France; CNRS, None, Apple, Paris, France, Ecole Normale Supérieure PSL, Paris, France; CNRS
  - **TL;DR:** This study investigates the Lipschitz properties of self-attention in Transformers, revealing that the Lipschitz constant is bounded by √n for inputs of length n, with implications for robustness and expressive power. The findings are supported by experiments on pretrained and randomly initialized models like BERT and GPT-2.
  - **Keywords:** self-attention, masked self-attention, Transformers, Lipschitz properties, local Lipschitz constant, natural language processing, computer vision, robustness, expressive power, adversarial robustness, bounds on Lipschitz constant, mean-field framework, BERT, GPT-2, Transformers, multi-head attention


- [KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning](https://icml.cc/virtual/2024/poster/34564) (Poster)
  - **Authors:** [Junnan Liu](http://openreview.net/profile?id=~Junnan_Liu1), [Qianren Mao](http://openreview.net/profile?id=~Qianren_Mao4), [Weifeng Jiang](http://openreview.net/profile?id=~Weifeng_Jiang2), [Jianxin Li](http://openreview.net/profile?id=~Jianxin_Li3)
  - **Affiliations:** Zhongguancun Laboratory, Beijing, P.R. China; SCSE, Beihang University, Beijing, P.R. China, Zhongguancun Laboratory, Beijing, P.R. China, SCSE, Nanyang Technological University, Singapore, Zhongguancun Laboratory, Beijing, P.R. China; SCSE, Beihang University, Beijing, P.R. China
  - **TL;DR:** This paper proposes KNOW FORMER, a novel transformer-based method for knowledge graph reasoning that addresses limitations of path-based methods by utilizing a message-passing perspective. Experimental results show that KNOW FORMER outperforms existing baseline methods in both transductive and inductive settings.
  - **Keywords:** Knowledge Graph Reasoning, Transformers, Message-Passing Neural Networks, Attention Computation, Structure-Aware Modules, High Incompleteness in Knowledge Graphs, Missing Paths, Information Over-Squashing, KNOW FORMER Method, Efficient Attention Computation, Knowledge Graphs (KGs), Knowledge Graph Embedding (KGE), Pretrained Language Models (PLMs)


- [Weakly-Supervised Residual Evidential Learning for Multi-Instance Uncertainty Estimation](https://icml.cc/virtual/2024/poster/33575) (Poster)
  - **Authors:** [Pei Liu](http://openreview.net/profile?id=~Pei_Liu5), [Luping Ji](http://openreview.net/profile?id=~Luping_Ji1)
  - **Affiliations:** School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China
  - **TL;DR:** This paper addresses the challenge of uncertainty estimation in weakly-supervised multiple instance learning by proposing a new method called Multi-Instance Residual Evidential Learning (MIREL). The results demonstrate that MIREL significantly enhances the performance of existing models in uncertainty estimation tasks, particularly at the instance level.
  - **Keywords:** Uncertainty Estimation, Weakly-Supervised Learning, Multiple Instance Learning, Multi-Instance Residual Operator, Bayesian Perspective, Histopathology Diagnosis, Video Anomaly Detection, Video Analysis, Lack of Sufficiently-Labeled Data, Overconfidence in Predictions, Multi-Instance Residual Evidential Learning (MIREL), Improved Predictive Distribution Modeling


- [Fundamental Limits of Distributed Covariance Matrix Estimation Under Communication Constraints](https://icml.cc/virtual/2024/poster/33626) (Poster)
  - **Authors:** [Mohammad Reza Rahmani](http://openreview.net/profile?id=~Mohammad_Reza_Rahmani1), [Mohammad Hossein Yassaee](http://openreview.net/profile?id=~Mohammad_Hossein_Yassaee1), [Mohammad Ali Maddah Ali](http://openreview.net/profile?id=~Mohammad_Ali_Maddah-Ali2), [Mohammad Reza Aref](http://openreview.net/profile?email=aref%40sharif.edu)
  - **Affiliations:** Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran, Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; Sharif Center for Information Systems and Data Science, Sharif Institute for Convergence Science & Technology, Tehran, Iran, Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA, Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran
  - **TL;DR:** This study investigates the estimation of high-dimensional covariance matrices in a distributed setting where agents can only communicate limited bits to a central server. It establishes a lower bound on estimation error and presents a near-optimal algorithm that balances communication cost, sample size, and accuracy.
  - **Keywords:** Distributed covariance matrix estimation, communication constraints, Sample covariance estimator, lower bound analysis, Financial mathematics, statistics, machine learning, High-dimensional data, communication cost, estimation accuracy, Near-optimal algorithm for covariance matrix estimation


- [On the Nonlinearity of Layer Normalization](https://icml.cc/virtual/2024/poster/35163) (Poster)
  - **Authors:** [Yunhao Ni](http://openreview.net/profile?id=~Yunhao_Ni1), [Yuxin Guo](http://openreview.net/profile?id=~Yuxin_Guo5), [Junlong Jia](http://openreview.net/profile?id=~Junlong_Jia1), [Lei Huang](http://openreview.net/profile?id=~Lei_Huang1)
  - **Affiliations:** SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China, SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China, SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China, SKLCCSE, Institute of Artificial Intelligence, Beihang University, Beijing, China
  - **TL;DR:** This paper investigates the nonlinearity and representation capacity of Layer Normalization (LN) through a theoretical framework, demonstrating that networks composed of linear and LN transformations (LN-Net) can effectively classify data. The study also explores methods to amplify LN's nonlinearity, particularly through group-based LN, with empirical support for its effectiveness in neural architecture design.
  - **Keywords:** Layer Normalization, Nonlinearity, Representation Capacity, LN-Net, Group based LN (LN-G), Natural Language Processing (NLP), Computer Vision (CV), Train-inference inconsistency, Theoretical understanding of LN, Amplification of nonlinearity, VC dimension analysis, Transformer, Batch Normalization (BN)


- [Implicit Representations for Constrained Image Segmentation](https://icml.cc/virtual/2024/poster/34423) (Poster)
  - **Authors:** [Jan Philipp Schneider](http://openreview.net/profile?id=~Jan_Philipp_Schneider1), [Mishal Fatima](http://openreview.net/profile?id=~Mishal_Fatima1), [Jovita Lukasik](http://openreview.net/profile?id=~Jovita_Lukasik1), [Andreas Kolb](http://openreview.net/profile?id=~Andreas_Kolb1), [Margret Keuper](http://openreview.net/profile?id=~Margret_Keuper1), [Michael Moeller](http://openreview.net/profile?id=~Michael_Moeller1)
  - **Affiliations:** University of Siegen, University of Mannheim, University of Siegen, University of Siegen, University of Mannheim; Max-Planck-Institute for Informatics, Saarland Informatics Campus, University of Siegen
  - **TL;DR:** This study explores the use of implicit representations in image segmentation to enforce geometric constraints such as convexity and path-connectedness. The findings demonstrate that these representations can effectively address challenges like data scarcity and occlusions, leading to improved segmentation outcomes.
  - **Keywords:** Implicit representations, Image segmentation, Geometric constraints, Neural networks, Parametric functions, Image processing, Scene representation, Data scarcity, Object occlusion, Enforcing geometric properties, Provable constraints, Improved segmentation methods, Convexity, Path-connectedness, Star-shape, Periodicity, Symmetry


- [Sliding Down the Stairs: How Correlated Latent Variables Accelerate Learning with Neural Networks](https://icml.cc/virtual/2024/poster/34799) (Poster)
  - **Authors:** [Lorenzo Bardone](http://openreview.net/profile?id=~Lorenzo_Bardone1), [Sebastian Goldt](http://openreview.net/profile?id=~Sebastian_Goldt1)
  - **Affiliations:** International School of Advanced Studies, Trieste, Italy, International School of Advanced Studies, Trieste, Italy
  - **TL;DR:** This study investigates how neural networks efficiently learn from higher-order input correlations despite the computational challenges associated with extracting information from higher-order cumulants. The authors demonstrate that correlations between latent variables can accelerate learning, providing insights into hierarchical learning mechanisms in neural networks.
  - **Keywords:** Neural networks, Higher-order input cumulants, Stochastic gradient descent (SGD), Tensor PCA, Online SGD, Image classification, Computational difficulty in extracting higher-order correlations, Sample complexity in high-dimensional inputs, Mechanism for hierarchical learning, Efficient learning from higher-order cumulants, CIFAR10


- [InferCept: Efficient Intercept Support for Augmented Large Language Model Inference](https://icml.cc/virtual/2024/poster/32755) (Poster)
  - **Authors:** [Reyna Abhyankar](http://openreview.net/profile?id=~Reyna_Abhyankar1), [Zijian He](http://openreview.net/profile?id=~Zijian_He5), [Vikranth Srivatsa](http://openreview.net/profile?id=~Vikranth_Srivatsa1), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang2), [Yiying Zhang](http://openreview.net/profile?id=~Yiying_Zhang2)
  - **Affiliations:** University of California, San Diego, La Jolla, United States, University of California, San Diego, La Jolla, United States, University of California, San Diego, La Jolla, United States, University of California, San Diego, La Jolla, United States, University of California, San Diego, La Jolla, United States
  - **TL;DR:** This paper introduces I NFER CEPT, a novel inference framework designed to enhance the efficiency of augmented large language models by minimizing GPU resource waste during external interactions. The framework significantly improves serving throughput and reduces unnecessary recomputation, allowing for more requests to be processed per second.
  - **Keywords:** Large Language Models, Augmented LLMs, Inference Systems, Efficient Interception, KV Cache Management, Natural Language Understanding, Content Generation, Recomputational Waste, Interception Handling, Context Management, I NFER CEPT Framework, Improved Throughput, GPU Resource Optimization


- [Sign Rank Limitations for Inner Product Graph Decoders](https://icml.cc/virtual/2024/poster/34285) (Poster)
  - **Authors:** [Su Hyeong Lee](http://openreview.net/profile?id=~Su_Hyeong_Lee1), [QINGQI ZHANG](http://openreview.net/profile?email=qingqi%40uchicago.edu), [Risi Kondor](http://openreview.net/profile?id=~Risi_Kondor1)
  - **Affiliations:** Department of Statistics, University of Chicago; Department of Computer Science, University of Chicago, Department of Statistics, University of Chicago; Department of Computer Science, University of Chicago, Department of Statistics, University of Chicago; Department of Computer Science, University of Chicago
  - **TL;DR:** This paper investigates the limitations of inner product-based decoders in graph reconstruction tasks and provides a theoretical framework to understand these limitations. It proposes modifications to enhance the representation capacity of these decoders, allowing for more effective graph data analysis.
  - **Keywords:** Inner product-based decoders, graph reconstruction, latent embeddings, Inner product algorithms, graph neural networks (GNNs), adversarial training, Graph data analysis, biochemistry, molecular drug design, Limitations in representation capacity, low dimensionality in latent representations, Deterministic bounds on latent feature dimensions, decoding architecture modifications, Sign rank, low-rank graphs


- [Scalable Safe Policy Improvement for Factored Multi-Agent MDPs](https://icml.cc/virtual/2024/poster/34073) (Poster)
  - **Authors:** [Federico Bianchi](http://openreview.net/profile?id=~Federico_Bianchi2), [Edoardo Zorzi](http://openreview.net/profile?id=~Edoardo_Zorzi1), [Alberto Castellini](http://openreview.net/profile?id=~Alberto_Castellini1), [Thiago Simão](http://openreview.net/profile?id=~Thiago_D._Sim%C3%A3o1), [Matthijs T. J. Spaan](http://openreview.net/profile?id=~Matthijs_T._J._Spaan1), [Alessandro Farinelli](http://openreview.net/profile?id=~Alessandro_Farinelli1)
  - **Affiliations:** Department of Computer Science, University of Verona, Verona, Italy, Department of Computer Science, University of Verona, Verona, Italy, Department of Computer Science, University of Verona, Verona, Italy, Department of Software Science, Eindhoven University of Technology, Eindhoven, Netherlands, Department of Intelligent Systems, Delft University of Technology, Delft, Netherlands, Department of Computer Science, University of Verona, Verona, Italy
  - **TL;DR:** This study presents a scalable approach for Safe Policy Improvement in multi-agent domains, addressing challenges posed by large state and action spaces. The proposed method enhances sample efficiency and reliability, demonstrating effectiveness in large-scale applications such as SysAdmin and UAV delivery scenarios.
  - **Keywords:** Safe Policy Improvement, Multi-Agent Reinforcement Learning (MARL), Monte Carlo Tree Search, Baseline Bootstrapping, Max-Plus (Variable Elimination), Multi-agent systems, Robotics, Environmental monitoring, Large state and action spaces, Safety issues in learning, Reliability challenges in offline settings, Factorized SPI approach, Improved policy generation


- [Self-Consistency Training for Density-Functional-Theory Hamiltonian Prediction](https://icml.cc/virtual/2024/poster/33896) (Poster)
  - **Authors:** [He Zhang](http://openreview.net/profile?id=~He_Zhang1), [Chang Liu](http://openreview.net/profile?id=~Chang_Liu10), [wang](http://openreview.net/profile?id=~Zun_Wang2), [Xinran Wei](http://openreview.net/profile?id=~Xinran_Wei1), [Siyuan Liu](http://openreview.net/profile?id=~Siyuan_Liu3), [Nanning Zheng](http://openreview.net/profile?id=~Nanning_Zheng1), [Bin Shao](http://openreview.net/profile?id=~Bin_Shao1), [Tie-Yan Liu](http://openreview.net/profile?id=~Tie-Yan_Liu1)
  - **Affiliations:** National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Microsoft Research AI for Science, Microsoft Research AI for Science, Microsoft Research AI for Science, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Microsoft Research AI for Science, Microsoft Research AI for Science
  - **TL;DR:** This study introduces self-consistency training for predicting the mean-field Hamiltonian matrix in density functional theory, addressing the challenges of data scarcity and inefficiency in generating labeled data. The proposed method enhances generalization and efficiency, making Hamiltonian prediction more applicable in molecular science.
  - **Keywords:** Hamiltonian prediction, density functional theory, machine learning, Self-consistency training, neural networks, Molecular science, drug design, protein engineering, material discovery, Data scarcity, insufficient labeled data, Improved generalization, efficient training method, Mean-field Hamiltonian matrix, Fock matrix, DFT initialization


- [Improving Context Understanding in Multimodal Large Language Models via Multimodal Composition Learning](https://icml.cc/virtual/2024/poster/34189) (Poster)
  - **Authors:** [Wei Li](http://openreview.net/profile?id=~Wei_Li55), [Hehe Fan](http://openreview.net/profile?id=~Hehe_Fan1), [Yongkang Wong](http://openreview.net/profile?id=~Yongkang_Wong1), [Yi Yang](http://openreview.net/profile?id=~Yi_Yang4), [Mohan Kankanhalli](http://openreview.net/profile?id=~Mohan_Kankanhalli1)
  - **Affiliations:** ReLER, CCAI, School of Computer Science and Technology, Zhejiang University, China, ReLER, CCAI, School of Computer Science and Technology, Zhejiang University, China, School of Computing, National University of Singapore, Singapore, ReLER, CCAI, School of Computer Science and Technology, Zhejiang University, China, School of Computing, National University of Singapore, Singapore
  - **TL;DR:** This study introduces Multimodal Composition Learning (MCL) to enhance the understanding of complex multimodal contexts in Multimodal Large Language Models (MLLMs). The proposed method significantly improves the accuracy of image retrieval and text generation tasks by fostering better alignment between visual and language inputs.
  - **Keywords:** Multimodal Large Language Models, visual understanding, Multimodal Composition Learning (MCL), Multimodal-Context Captioning (MC-Cap), Multimodal-Context Retrieval (MC-Ret), image-text retrieval, visual question answering, visual storytelling, complex multimodal scenarios, data intensiveness, insufficient cross-modal interaction, improved mapping between vision and language modalities, enhanced retrieval accuracy, frozen Large Language Models (LLMs), vision-language mapping


- [On the sample complexity of conditional independence testing with Von Mises estimator with application to causal discovery](https://icml.cc/virtual/2024/poster/33085) (Poster)
  - **Authors:** [Fateme Jamshidi](http://openreview.net/profile?id=~Fateme_Jamshidi1), [Luca Ganassali](http://openreview.net/profile?email=luca.ganassali%40universite-paris-saclay.fr), [Negar Kiyavash](http://openreview.net/profile?id=~Negar_Kiyavash1)
  - **Affiliations:** College of Management of Technology, EPFL, Lausanne, Switzerland, College of Management of Technology, EPFL, Lausanne, Switzerland, College of Management of Technology, EPFL, Lausanne, Switzerland
  - **TL;DR:** This study introduces the VM-CI test for conditional independence testing using the nonparametric Von Mises estimator, achieving optimal parametric rates and providing the first sample complexity guarantees for causal discovery in non-linear and non-Gaussian contexts. The results demonstrate that VM-CI outperforms existing CI tests in terms of time and sample complexity, enhancing structure learning performance.
  - **Keywords:** Conditional independence testing, Causal discovery, Von Mises estimator, Kernel density estimator, Sample complexity, Non-linear models, Non-Gaussian continuous variables, VM-CI test, Exponential concentration inequality, Constraint-based causal discovery algorithms


- [When is Transfer Learning Possible?](https://icml.cc/virtual/2024/poster/34791) (Poster)
  - **Authors:** [My Phan](http://openreview.net/profile?id=~My_Phan1), [Kianté Brantley](http://openreview.net/profile?id=~Kiant%C3%A9_Brantley2), [Stephanie Milani](http://openreview.net/profile?id=~Stephanie_Milani1), [Soroush Mehri](http://openreview.net/profile?id=~Soroush_Mehri1), [Gokul Swamy](http://openreview.net/profile?id=~Gokul_Swamy1), [Geoff Gordon](http://openreview.net/profile?id=~Geoffrey_J._Gordon1)
  - **Affiliations:** None, Cornell University, Carnegie Mellon University, Elementera AI, Carnegie Mellon University, Carnegie Mellon University
  - **TL;DR:** This paper presents a unified framework for transfer learning that challenges existing assumptions about when transfer is possible, demonstrating that sparse changes across environments are neither necessary nor sufficient for successful transfer. The authors derive a procedure to propagate constraints in structural causal models, leading to more efficient learning across different environments.
  - **Keywords:** transfer learning, structural causal models, supervised learning, reinforcement learning, imitation learning, transfer of learned information, sparse changes across environments, freezing layers in networks, novel formalism for transfer learning, meta-algorithm for transfer, Independent Causal Mechanisms (ICM), local constraints, global constraints, covariate shift, representation learning, few-shot learning, zero-shot learning


- [LoCoCo: Dropping In Convolutions for Long Context Compression](https://icml.cc/virtual/2024/poster/34202) (Poster)
  - **Authors:** [Ruisi Cai](http://openreview.net/profile?id=~Ruisi_Cai1), [Yuandong Tian](http://openreview.net/profile?id=~Yuandong_Tian1), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1), [Beidi Chen](http://openreview.net/profile?id=~Beidi_Chen1)
  - **Affiliations:** University of Texas at Austin, Meta AI (FAIR), University of Texas at Austin, Meta AI (FAIR); Carnegie Mellon University
  - **TL;DR:** This paper introduces LoCoCo, a method for compressing long context sequences in Large Language Models (LLMs) using a fixed-size Key-Value cache, which enhances efficiency during inference and fine-tuning. The method achieves significant context compression while maintaining performance, allowing for extended context lengths without architectural changes.
  - **Keywords:** Long Context Compression, Large Language Models, Convolutional Kernels, Key-Value Cache, Natural Language Processing, Memory Hurdle, Context Length Limitations, Context Compression Rate, Performance Improvement, KV Cache, Attention Modeling


- [Generalization Analysis for Multi-Label Learning](https://icml.cc/virtual/2024/poster/33891) (Poster)
  - **Authors:** [Yi-Fan Zhang](http://openreview.net/profile?id=~Yifan_Zhang13), [Min-Ling Zhang](http://openreview.net/profile?id=~Min-Ling_Zhang2)
  - **Affiliations:** School of Cyber Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China
  - **TL;DR:** This study addresses the theoretical analysis of generalization in multi-label learning, proposing novel vector-contraction inequalities to derive generalization bounds with reduced dependency on the number of labels. The findings highlight the importance of label correlations and provide insights into the generalization ability of multi-label models.
  - **Keywords:** Multi-label learning, Generalization analysis, Vector-contraction inequalities, Lipschitz continuity, Text categorization, Multimedia content annotation, Bioinformatics, Dependency on the number of labels, Label correlations, Generalization performance, Generalization bounds, Macro-Averaged AUC, Class-imbalance


- [GenCO: Generating Diverse Designs with Combinatorial Constraints](https://icml.cc/virtual/2024/poster/34616) (Poster)
  - **Authors:** [Aaron Ferber](http://openreview.net/profile?id=~Aaron_M_Ferber1), [Arman Zharmagambetov](http://openreview.net/profile?id=~Arman_Zharmagambetov1), [Taoan Huang](http://openreview.net/profile?id=~Taoan_Huang2), [Bistra Dilkina](http://openreview.net/profile?id=~Bistra_Dilkina2), [Yuandong Tian](http://openreview.net/profile?id=~Yuandong_Tian1)
  - **Affiliations:** Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Department of Computer Science, Cornell University, Ithaca, NY, USA, AI at Meta (FAIR), Menlo Park, CA, USA, Department of Computer Science, University of Southern California, Los Angeles, CA, USA, Department of Computer Science, University of Southern California, Los Angeles, CA, USA, AI at Meta (FAIR), Menlo Park, CA, USA
  - **TL;DR:** The study introduces GenCO, a generative framework that ensures the generation of diverse designs while satisfying hard combinatorial constraints. It demonstrates effectiveness across various applications, yielding high-quality solutions that adhere to user-specified properties.
  - **Keywords:** Generative design, combinatorial constraints, deep generative models, GAN (Generative Adversarial Networks), VAE (Variational Autoencoder), differentiable combinatorial solvers, Industrial design, material science, computer graphics, game level generation, photonic device design, Constraint satisfaction, feasibility of generated objects, optimization of nonlinear objectives, GenCO framework, diverse and high-quality solutions, combinatorial loss components, Combinatorial properties, Wasserstein distance, Lagrangian reformulation


- [Lightweight Image Super-Resolution via Flexible Meta Pruning](https://icml.cc/virtual/2024/poster/34094) (Poster)
  - **Authors:** [Yulun Zhang](http://openreview.net/profile?id=~Yulun_Zhang1), [Kai Zhang](http://openreview.net/profile?id=~Kai_Zhang8), [Luc Van Gool](http://openreview.net/profile?id=~Luc_Van_Gool1), [Martin Danelljan](http://openreview.net/profile?id=~Martin_Danelljan4), [Fisher Yu](http://openreview.net/profile?id=~Fisher_Yu2)
  - **Affiliations:** MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China, School of Intelligence Science and Technology, Nanjing University, China, ETH Zürich, Switzerland; KU Leuven, Belgium; INSAIT, Bulgaria, ETH Zürich, Switzerland, ETH Zürich, Switzerland
  - **TL;DR:** This study introduces Flexible Meta Pruning (FMP) for lightweight image super-resolution, addressing network redundancy by simultaneously pruning network channels and weights. The proposed method demonstrates superior performance in image reconstruction compared to existing lightweight SR models.
  - **Keywords:** lightweight image super-resolution, model compression, flexible meta pruning, structured pruning, unstructured pruning, hypernetwork, image super-resolution, network redundancy, computational resource limitations, flexible pruning methods, optimization techniques, Urban100


- [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://icml.cc/virtual/2024/poster/34590) (Poster)
  - **Authors:** [Zechun Liu](http://openreview.net/profile?id=~Zechun_Liu1), [Changsheng Zhao](http://openreview.net/profile?id=~Changsheng_Zhao2), [Forrest Iandola](http://openreview.net/profile?id=~Forrest_Iandola1), [Chen Lai](http://openreview.net/profile?id=~Chen_Lai1), [Yuandong Tian](http://openreview.net/profile?id=~Yuandong_Tian1), [Igor Fedorov](http://openreview.net/profile?id=~Igor_Fedorov1), [Yunyang Xiong](http://openreview.net/profile?id=~Yunyang_Xiong2), [Ernie Chang](http://openreview.net/profile?id=~Ernie_Chang4), [Yangyang Shi](http://openreview.net/profile?id=~Yangyang_Shi1), [Raghuraman Krishnamoorthi](http://openreview.net/profile?id=~Raghuraman_Krishnamoorthi1), [Liangzhen Lai](http://openreview.net/profile?id=~Liangzhen_Lai3), [Vikas Chandra](http://openreview.net/profile?id=~Vikas_Chandra2)
  - **Affiliations:** Meta, Meta, Meta, Meta, Meta, Meta, Meta, Meta, Meta, Meta, Meta, Meta
  - **TL;DR:** This paper presents MobileLLM, a family of efficient large language models with fewer than a billion parameters optimized for mobile devices, achieving significant accuracy improvements over existing models. The study highlights the importance of model architecture and introduces a block-wise weight-sharing method that enhances performance without increasing model size.
  - **Keywords:** Efficient large language models, Mobile deployment, Deep and thin architectures, Embedding sharing, Grouped-query attention mechanisms, Block-wise weight-sharing, On-device use cases, Chat benchmarks, Cloud costs, Latency concerns, Model quality determination, MobileLLM, MobileLLM-LS, Accuracy enhancements, Large language models (LLMs), Sub-billion parameter models


- [Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits](https://icml.cc/virtual/2024/poster/34022) (Poster)
  - **Authors:** [Kyoungseok Jang](http://openreview.net/profile?id=~Kyoungseok_Jang1), [Chicheng Zhang](http://openreview.net/profile?id=~Chicheng_Zhang1), [Kwang-Sung Jun](http://openreview.net/profile?id=~Kwang-Sung_Jun1)
  - **Affiliations:** Dipartimento di Informatica, Università degli Studi di Milano, Milan, MI, Italy, Department of Computer Science, University of Arizona, Tucson, AZ, United States, Department of Computer Science, University of Arizona, Tucson, AZ, United States
  - **TL;DR:** This study introduces a novel low-rank matrix estimation method called LowPopArt and an experimental design criterion that enhances the efficiency of low-rank matrix bandits. The proposed methods yield tighter recovery guarantees and improved regret bounds compared to previous approaches.
  - **Keywords:** low-rank matrix estimation, low-rank matrix bandits, LowPopArt, nuclear norm penalized least squares, drug discovery, online advertising, low-rank structure, exploration-exploitation tradeoff, improved regret upper bounds, novel experimental design criterion, low-rank trace regression, σ-subgaussian noise


- [TravelPlanner: A Benchmark for Real-World Planning with Language Agents](https://icml.cc/virtual/2024/poster/33227) (Spotlight Poster)
  - **Authors:** [Jian Xie](http://openreview.net/profile?id=~Jian_Xie3), [Kai Zhang](http://openreview.net/profile?id=~Kai_Zhang10), [Jiangjie Chen](http://openreview.net/profile?id=~Jiangjie_Chen1), [Tinghui Zhu](http://openreview.net/profile?id=~Tinghui_Zhu1), [Renze Lou](http://openreview.net/profile?id=~Renze_Lou1), [Yuandong Tian](http://openreview.net/profile?id=~Yuandong_Tian1), [Yanghua Xiao](http://openreview.net/profile?id=~Yanghua_Xiao1), [Yu Su](http://openreview.net/profile?id=~Yu_Su2)
  - **Affiliations:** School of Computer Science, Fudan University, The Ohio State University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, The Pennsylvania State University, Meta AI, School of Computer Science, Fudan University, The Ohio State University
  - **TL;DR:** The study introduces TravelPlanner, a benchmark for evaluating language agents in complex travel planning scenarios. Current language agents, including GPT-4, struggle with these tasks, achieving only a 0.6% success rate, highlighting the challenges in real-world planning.
  - **Keywords:** travel planning, language agents, real-world planning, large language models (LLMs), tool use, reasoning, AI planning, language processing, complex planning tasks, multi-constraint management, TravelPlanner benchmark, evaluation of language agents, nearly four million data records, 1225 planning intents


- [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://icml.cc/virtual/2024/poster/33390) (Oral)
  - **Authors:** [Jiawei Zhao](http://openreview.net/profile?id=~Jiawei_Zhao2), [Zhenyu Zhang](http://openreview.net/profile?id=~Zhenyu_Zhang4), [Beidi Chen](http://openreview.net/profile?id=~Beidi_Chen1), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1), [Anima Anandkumar](http://openreview.net/profile?id=~Anima_Anandkumar1), [Yuandong Tian](http://openreview.net/profile?id=~Yuandong_Tian1)
  - **Affiliations:** California Institute of Technology, University of Texas at Austin, Meta AI; Carnegie Mellon University, University of Texas at Austin, None, Meta AI
  - **TL;DR:** This paper introduces Gradient Low-Rank Projection (GaLore), a memory-efficient training strategy for Large Language Models (LLMs) that significantly reduces memory usage while allowing full-parameter learning. The method enables pre-training of a 7B model on consumer GPUs without the need for model parallelism or checkpointing, achieving up to 82.5% reduction in optimizer memory.
  - **Keywords:** Large Language Models, Memory Efficiency, Gradient Low-Rank Projection (GaLore), Low-Rank Adaptation (LoRA), Adam optimizer, Adafactor, Pre-training, Fine-tuning, Conversational AI, Language Translation, Memory challenges in training LLMs, High memory requirements for optimizer states, Memory usage reduction, Feasibility of training on consumer GPUs, LLaMA, C4 dataset, GLUE tasks, NVIDIA RTX 4090


- [Dense Reward for Free in Reinforcement Learning from Human Feedback](https://icml.cc/virtual/2024/poster/33476) (Poster)
  - **Authors:** [Alexander Chan](http://openreview.net/profile?id=~Alex_James_Chan1), [Hao Sun](http://openreview.net/profile?id=~Hao_Sun1), [Samuel Holt](http://openreview.net/profile?id=~Samuel_Holt1), [M van der Schaar](http://openreview.net/profile?id=~Mihaela_van_der_Schaar2)
  - **Affiliations:** University of Cambridge, Cambridge, UK, University of Cambridge, Cambridge, UK, University of Cambridge, Cambridge, UK, University of Cambridge, Cambridge, UK
  - **TL;DR:** This study presents a method to enhance reinforcement learning from human feedback by redistributing rewards based on attention weights from a reward model, leading to more efficient and stable training of large language models. The approach theoretically aligns with potential-based reward shaping and demonstrates improved learning rates and better local optima in practice.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Large Language Models (LLMs), Proximal Policy Optimization (PPO), potential-based reward shaping, Natural Language Processing, instruction following, dialogue systems, Sparse reward, credit assignment problem, training stability, Densified reward signal, improved training efficiency, accelerated learning, Attention map, transformer architecture


- [ReLU Network with Width $d+\mathcal{O}(1)$ Can Achieve Optimal Approximation Rate](https://icml.cc/virtual/2024/poster/33098) (Poster)
  - **Authors:** [Chenghao Liu](http://openreview.net/profile?id=~Chenghao_LIU4), [Minghua Chen](http://openreview.net/profile?id=~Minghua_Chen1)
  - **Affiliations:** School of Data Science, City University of Hong Kong, School of Data Science, City University of Hong Kong
  - **TL;DR:** This study demonstrates that ReLU networks with a width of d + 1 can achieve the optimal approximation rate for continuous functions in the domain [0,1]^d under Lp norm. The findings extend to narrow feed-forward networks with various activations, enhancing the understanding of universal approximation in narrow networks.
  - **Keywords:** universal function approximation, narrow neural networks, ReLU networks, approximation rate, optimal approximation rate, minimal parameter count, optimal relation between parameters and approximation error, Lp norm, uniform norm


- [PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition](https://icml.cc/virtual/2024/poster/32869) (Poster)
  - **Authors:** [Ziyang Zhang](http://openreview.net/profile?id=~Ziyang_Zhang9), [Qizhen Zhang](http://openreview.net/profile?id=~Qizhen_Zhang1), [Jakob Foerster](http://openreview.net/profile?id=~Jakob_Nicolaus_Foerster1)
  - **Affiliations:** University of Oxford, FLAIR, University of Oxford, FLAIR, University of Oxford
  - **TL;DR:** This study introduces PARDEN, a novel method for detecting jailbreaks in large language models by asking the model to repeat its outputs, which significantly reduces false positive rates while maintaining high true positive rates. The findings suggest that using the model itself as a safeguard can effectively mitigate security risks associated with harmful outputs.
  - **Keywords:** AI Safety, Large Language Models, Jailbreaks, Natural Language Processing, Security risks, Exploitation, Auto-regressive trap, PARDEN, Jailbreak detection, Llama 2, Claude 2, True Positive Rate, False Positive Rate


- [Training-Free Long-Context Scaling of Large Language Models](https://icml.cc/virtual/2024/poster/34420) (Poster)
  - **Authors:** [Chenxin An](http://openreview.net/profile?id=~Chenxin_An1), [Fei Huang](http://openreview.net/profile?id=~Fei_Huang3), [Jun Zhang](http://openreview.net/profile?id=~Jun_Zhang27), [Shansan Gong](http://openreview.net/profile?id=~Shansan_Gong1), [Xipeng Qiu](http://openreview.net/profile?id=~Xipeng_Qiu1), [Chang Zhou](http://openreview.net/profile?id=~Chang_Zhou2), [Lingpeng Kong](http://openreview.net/profile?id=~Lingpeng_Kong1)
  - **Affiliations:** The University of Hong Kong; Alibaba Group, Alibaba Group, None, The University of Hong Kong, Fudan University, Alibaba Group, The University of Hong Kong
  - **TL;DR:** This paper introduces Dual Chunk Attention (DCA), a training-free method that allows Large Language Models to effectively process over 100k tokens, achieving performance comparable to finetuned models. The approach addresses the limitations of traditional long-context processing by capturing both intra- and inter-chunk positional information.
  - **Keywords:** Large Language Models, long-context processing, Dual Chunk Attention (DCA), Flash Attention, Interactive chatbots, dialogue history management, PDF analysis, Limitations of pretraining length, challenges in long-context information processing, Training-free context scaling, performance comparison with finetuned models, LLAMA 270B, gpt-3.5-16k, Rotary Positional Encodings (RoPE), Position Interpolation (PI), NTK-Aware RoPE (NTK)


- [MD tree: a model-diagnostic tree grown on loss landscape](https://icml.cc/virtual/2024/poster/32861) (Poster)
  - **Authors:** [Yefan Zhou](http://openreview.net/profile?id=~Yefan_Zhou1), [Jianlong Chen](http://openreview.net/profile?id=~Jianlong_Chen1), [Qinxue Cao](http://openreview.net/profile?id=~Qinxue_Cao1), [Konstantin Schürholt](http://openreview.net/profile?id=~Konstantin_Sch%C3%BCrholt1), [Yaoqing Yang](http://openreview.net/profile?id=~Yaoqing_Yang1)
  - **Affiliations:** Department of Computer Science, Dartmouth College, NH, USA, Zhejiang University, Zhejiang, China, University of Illinois Urbana-Champaign, IL, USA, University of St. Gallen, SUI, Department of Computer Science, Dartmouth College, NH, USA
  - **TL;DR:** This study presents a novel method called MD tree for diagnosing the underperformance of pre-trained neural networks by analyzing the loss landscape, achieving an accuracy of 87.7% in identifying failure sources. The method outperforms traditional validation-based approaches, providing actionable insights without requiring detailed training configurations.
  - **Keywords:** model diagnosis, neural networks, classification problem, MD tree, loss landscape analysis, few-shot dataset transfer, scale transfer, underperformance of pre-trained neural networks, model failure sources, diagnosis method based on loss landscape metrics, improved accuracy in model diagnosis


- [Minimum Norm Interpolation Meets The Local Theory of Banach Spaces](https://icml.cc/virtual/2024/poster/34513) (Poster)
  - **Authors:** [Gil Kur](http://openreview.net/profile?id=~Gil_Kur2), [Pedro Abdalla](http://openreview.net/profile?id=~Pedro_Abdalla1), [Pierre Bizeul](http://openreview.net/profile?id=~Pierre_Bizeul1), [Fanny Yang](http://openreview.net/profile?id=~Fanny_Yang1)
  - **Affiliations:** ETH Zürich, ETH Zürich, Technion - Israel Institute of Technology, ETH Zürich
  - **TL;DR:** This paper establishes a framework connecting minimum-norm interpolators to high-dimensional geometry concepts, demonstrating that under certain conditions, the bias of these solutions is bounded by Gaussian complexity. It also provides a reverse Efron-Stein lower bound for expected conditional variance, highlighting the implications for ℓp-linear regression in high-dimensional settings.
  - **Keywords:** minimum-norm interpolation, double descent phenomenon, benign overfitting, ℓp-spaces, linear regression, Gaussian complexity, high-dimensional data analysis, neural networks, model overfitting, generalization bounds, bias in high-dimensional settings, bounds on expected conditional variance, sharp bounds for ℓp-linear regression, Banach spaces, cotype 2, 2-uniform convexity


- [Characterizing ResNet's Universal Approximation Capability](https://icml.cc/virtual/2024/poster/32643) (Poster)
  - **Authors:** [Chenghao Liu](http://openreview.net/profile?id=~Chenghao_LIU4), [Enming Liang](http://openreview.net/profile?id=~Enming_Liang1), [Minghua Chen](http://openreview.net/profile?id=~Minghua_Chen1)
  - **Affiliations:** School of Data Science, City University of Hong Kong, School of Data Science, City University of Hong Kong, School of Data Science, City University of Hong Kong
  - **TL;DR:** This paper investigates the universal approximation capability of ResNet, demonstrating that it can approximate various function classes with a significantly reduced number of tunable weights compared to classical ReLU networks. The findings provide theoretical justification for ResNet's practical performance in deep learning applications.
  - **Keywords:** ResNet, Universal Approximation, Deep Neural Networks, Gradient vanishing/exploding, Function approximation, Approximation bounds, Tunable weights, Lipschitz continuous functions, Lebesgue-integrable functions, ReLU networks


- [Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback](https://icml.cc/virtual/2024/poster/32763) (Poster)
  - **Authors:** [Vincent Conitzer](http://openreview.net/profile?id=~Vincent_Conitzer2), [Rachel Freedman](http://openreview.net/profile?id=~Rachel_Freedman1), [Jobstq Heitzig](http://openreview.net/profile?id=~Jobst_Heitzig1), [Wesley H. Holliday](http://openreview.net/profile?id=~Wesley_H._Holliday1), [Bob Jacobs](http://openreview.net/profile?id=~Bob_M._Jacobs1), [Nathan Lambert](http://openreview.net/profile?id=~Nathan_Lambert1), [Milan Mosse](http://openreview.net/profile?email=milan.mosse%40gmail.com), [Eric Pacuit](http://openreview.net/profile?email=epacuit%40umd.edu), [Stuart Russell](http://openreview.net/profile?id=~Stuart_Russell1), [Hailey Schoelkopf](http://openreview.net/profile?id=~Hailey_Schoelkopf1), [Emanuel Tewolde](http://openreview.net/profile?id=~Emanuel_Tewolde1), [William Zwicker](http://openreview.net/profile?email=zwickerw%40union.edu)
  - **Affiliations:** Foundations of Cooperative AI Lab, Computer Science Department, Carnegie Mellon University, Pittsburgh, USA; Institute for Ethics in AI, University of Oxford, Oxford, United Kingdom, Center for Human-Compatible AI, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, USA, Potsdam Institute for Climate Impact Research, Potsdam, Brandenburg, Germany, Department of Philosophy, University of California, Berkeley, USA, Department of Philosophy and Moral Sciences, Ghent University, Ghent, Belgium, Allen Institute for AI, Berkeley, California, USA, Department of Philosophy, University of California, Berkeley, USA, Department of Philosophy, University of Maryland, College Park, USA, Center for Human-Compatible AI, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, USA, EleutherAI, None, Department of Mathematics, Union College, Schenectady, USA; Murat Sertel Center for Advanced Economic Studies, Istanbul Bilgi University, Istanbul, Turkey
  - **TL;DR:** The paper argues that social choice theory can effectively address challenges in AI alignment by integrating diverse human feedback. It highlights the limitations of current methods like reinforcement learning from human feedback and proposes a framework for aggregating collective preferences.
  - **Keywords:** AI alignment, human feedback, social choice theory, Reinforcement learning from human feedback (RLHF), constitutional AI (CAI), reinforcement learning from AI feedback (RLAIF), Large language models (LLMs), AI ethics, AI safety, Diverging human input, aggregation of preferences, political bias, unrepresentative data


- [Adaptive Online Experimental Design for Causal Discovery](https://icml.cc/virtual/2024/poster/33130) (Spotlight Poster)
  - **Authors:** [Muhammad Qasim Elahi](http://openreview.net/profile?id=~Muhammad_Qasim_Elahi1), [Lai Wei](http://openreview.net/profile?id=~Lai_Wei5), [Murat Kocaoglu](http://openreview.net/profile?id=~Murat_Kocaoglu1), [Mahsa Ghasemi](http://openreview.net/profile?id=~Mahsa_Ghasemi1)
  - **Affiliations:** School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA, Life Sciences Institute, University of Michigan, Ann Arbor, Michigan, USA, School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA, School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana, USA
  - **TL;DR:** This study presents a novel track-and-stop causal discovery algorithm that efficiently learns causal graphs using a limited number of interventional samples. The proposed method outperforms existing approaches, achieving higher accuracy with significantly fewer samples.
  - **Keywords:** Causal discovery, causal graphs, interventional data efficiency, Track-and-stop causal discovery algorithm, graph separating system, allocation matching, Limited interventional samples, causal relationships, observational data inadequacy, Higher accuracy in causal graph learning, structural hamming distance (SHD) measurement, Directed acyclic graph (DAG), Markov equivalence class (MEC), conditional independence tests


- [Breaking the Barrier: Enhanced Utility and Robustness in Smoothed DRL Agents](https://icml.cc/virtual/2024/poster/33878) (Poster)
  - **Authors:** [Chung-En Sun](http://openreview.net/profile?id=~Chung-En_Sun1), [Sicun Gao](http://openreview.net/profile?id=~Sicun_Gao1), [Lily Weng](http://openreview.net/profile?id=~Tsui-Wei_Weng1)
  - **Affiliations:** UC San Diego, UC San Diego, UC San Diego
  - **TL;DR:** This study introduces S-DQN and S-PPO, innovative algorithms that significantly enhance the clean rewards and robustness of smoothed deep reinforcement learning agents, outperforming existing methods. The findings highlight the need for improved strategies in training robust DRL agents to address vulnerabilities to adversarial attacks.
  - **Keywords:** Deep Reinforcement Learning (DRL), Robustness, Adversarial Perturbations, Randomized Smoothing, S-DQN, S-PPO, Smoothed Attack, Game Environments, Safety-Critical Tasks, Vulnerability to Adversarial Attacks, Low Clean Rewards, Weak Robustness, Enhanced Clean Rewards, Empirical Robustness, Robustness Guarantee, Atari Games, Continuous Control Tasks


- [Position: Video as the New Language for Real-World Decision Making](https://icml.cc/virtual/2024/poster/34577) (Poster)
  - **Authors:** [Sherry Yang](http://openreview.net/profile?id=~Sherry_Yang1), [Jacob C Walker](http://openreview.net/profile?id=~Jacob_C_Walker1), [Jack Parker-Holder](http://openreview.net/profile?id=~Jack_Parker-Holder1), [Yilun Du](http://openreview.net/profile?id=~Yilun_Du1), [Jake Bruce](http://openreview.net/profile?id=~Jake_Bruce1), [Andre Barreto](http://openreview.net/profile?id=~Andre_Barreto1), [Pieter Abbeel](http://openreview.net/profile?id=~Pieter_Abbeel2), [Dale Schuurmans](http://openreview.net/profile?id=~Dale_Schuurmans1)
  - **Affiliations:** Google DeepMind; UC Berkeley, Google DeepMind, Google DeepMind, MIT, Google DeepMind, Google DeepMind, UC Berkeley, Google DeepMind
  - **TL;DR:** This paper argues that video generation can serve as a powerful tool for real-world decision making, similar to the role of language models in the digital realm. It highlights the potential of video data to capture complex physical interactions and proposes leveraging video generation for applications in robotics, self-driving, and scientific research.
  - **Keywords:** video generation, language modeling, real-world decision making, self-supervised learning, next token prediction, in-context learning, planning, reinforcement learning, robotics, self-driving, science, limitations of current video generation, challenges in video generation, advanced capabilities in video generation, potential for autonomous agents, large language models (LLMs), unified representation, generative modeling


- [SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models](https://icml.cc/virtual/2024/poster/33621) (Poster)
  - **Authors:** [Xiaoxuan Wang](http://openreview.net/profile?id=~Xiaoxuan_Wang2), [ziniu hu](http://openreview.net/profile?id=~Ziniu_Hu1), [Pan Lu](http://openreview.net/profile?id=~Pan_Lu2), [Yanqiao Zhu](http://openreview.net/profile?id=~Yanqiao_Zhu1), [Jieyu Zhang](http://openreview.net/profile?id=~Jieyu_Zhang1), [Satyen Subramaniam](http://openreview.net/profile?id=~Satyen_Subramaniam1), [Arjun Loomba](http://openreview.net/profile?id=~Arjun_R_Loomba1), [Shichang Zhang](http://openreview.net/profile?id=~Shichang_Zhang2), [Yizhou Sun](http://openreview.net/profile?id=~Yizhou_Sun1), [Wei Wang](http://openreview.net/profile?id=~Wei_Wang13)
  - **Affiliations:** University of California, Los Angeles, Los Angeles, CA, USA, California Institute of Technology, Pasadena, CA, USA, University of California, Los Angeles, Los Angeles, CA, USA, University of California, Los Angeles, Los Angeles, CA, USA, University of Washington, Seattle, WA, USA, University of California, Los Angeles, Los Angeles, CA, USA, University of California, Los Angeles, Los Angeles, CA, USA, University of California, Los Angeles, Los Angeles, CA, USA, University of California, Los Angeles, Los Angeles, CA, USA, University of California, Los Angeles, Los Angeles, CA, USA
  - **TL;DR:** The study introduces SCIBENCH, a benchmark suite designed to evaluate the scientific problem-solving abilities of Large Language Models (LLMs) using collegiate-level problems. The findings indicate that current LLMs perform inadequately, achieving a maximum score of only 43.22%, highlighting the need for improved reasoning capabilities in LLMs.
  - **Keywords:** Large Language Models, Scientific Problem Solving, Benchmarking, Prompting Strategies, Chain-of-Thought (CoT), Mathematics, Chemistry, Physics, Limitations of Current Benchmarks, Complex Scientific Problems, SCIBENCH Benchmark Suite, Error Categorization in Problem-Solving, SCIBENCH Dataset


- [Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning](https://icml.cc/virtual/2024/poster/34177) (Poster)
  - **Authors:** [Hongming Zhang](http://openreview.net/profile?id=~Hongming_Zhang3), [Tongzheng Ren](http://openreview.net/profile?id=~Tongzheng_Ren1), [Chenjun Xiao](http://openreview.net/profile?id=~Chenjun_Xiao1), [Dale Schuurmans](http://openreview.net/profile?id=~Dale_Schuurmans1), [Bo Dai](http://openreview.net/profile?id=~Bo_Dai1)
  - **Affiliations:** University of Alberta, UT Austin, The Chinese University of Hong Kong, Shenzhen, Google DeepMind, Georgia Tech
  - **TL;DR:** This paper presents a representation-based approach to efficiently plan and learn in partially observable reinforcement learning settings, addressing the challenges posed by non-Markovian dependencies. The proposed algorithm demonstrates superior performance compared to state-of-the-art methods across various benchmarks, enhancing the practical applicability of reinforcement learning.
  - **Keywords:** Partially Observable Reinforcement Learning, POMDPs, Representation-based approach, Algorithmic approach, Chatbot learning, Video game control, Partial observability, Non-Markovian dependence, Computational challenges, Statistical challenges, Statistical efficiency of proposed algorithm, State-of-the-art performance, Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP)


- [Aligning Transformers with Weisfeiler-Leman](https://icml.cc/virtual/2024/poster/35028) (Poster)
  - **Authors:** [Luis Müller](http://openreview.net/profile?id=~Luis_M%C3%BCller1), [Christopher Morris](http://openreview.net/profile?id=~Christopher_Morris1)
  - **Affiliations:** Department of Computer Science, RWTH Aachen University, Germany, Department of Computer Science, RWTH Aachen University, Germany
  - **TL;DR:** This study advances the alignment of transformers with the k-dimensional Weisfeiler–Leman hierarchy, demonstrating improved expressivity and practical feasibility. The proposed transformers show competitive performance on large-scale datasets and strong results when fine-tuned on smaller molecular datasets.
  - **Keywords:** Graph Neural Networks, Transformers, Weisfeiler–Leman Hierarchy, k-dimensional Weisfeiler–Leman (k-WL), Graph Transformers, Positional Encodings, Graph Learning, Molecular Datasets, Limited Expressivity, Runtime and Memory Complexity, Stronger Expressivity Results, Theoretical Framework for Positional Encodings, PCQM4Mv2


- [Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models](https://icml.cc/virtual/2024/poster/33325) (Poster)
  - **Authors:** [Dennis Wu](http://openreview.net/profile?id=~Dennis_Wu1), [Jerry Yao-Chieh Hu](http://openreview.net/profile?id=~Jerry_Yao-Chieh_Hu1), [Teng-Yun Hsiao](http://openreview.net/profile?id=~Teng-Yun_Hsiao1), [Han Liu](http://openreview.net/profile?id=~Han_Liu4)
  - **Affiliations:** Department of Computer Science, University of Northwestern, Evanston, USA, Department of Computer Science, University of Northwestern, Evanston, USA, Department of Physics, National Taiwan University, Taipei, Taiwan, Department of Computer Science, University of Northwestern, Evanston, USA; Department of Statistics and Data Science, University of Northwestern, Evanston, USA
  - **TL;DR:** This study introduces U-Hop, a two-stage memory retrieval dynamic for modern Hopfield models that enhances memory capacity by minimizing memory confusion through a learnable kernel transformation. Empirical results demonstrate that U-Hop significantly outperforms existing models in associative memory retrieval and deep learning tasks.
  - **Keywords:** memory retrieval, modern Hopfield models, memory capacity, two-stage optimization, learnable feature map, kernel space transformation, separation loss, associative memory retrieval, deep learning tasks, memory confusion, metastable states, enhanced memory capacity, improved retrieval dynamics, MNIST, Hopfield energy function, kernelized energy


- [Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning](https://icml.cc/virtual/2024/poster/33383) (Spotlight Poster)
  - **Authors:** [Michael Matthews](http://openreview.net/profile?id=~Michael_Matthews4), [Michael Beukman](http://openreview.net/profile?id=~Michael_Beukman1), [Benjamin Ellis](http://openreview.net/profile?id=~Benjamin_Ellis1), [Mikayel Samvelyan](http://openreview.net/profile?id=~Mikayel_Samvelyan1), [Matthew T Jackson](http://openreview.net/profile?id=~Matthew_Thomas_Jackson1), [Samuel Coward](http://openreview.net/profile?id=~Samuel_Coward1), [Jakob Foerster](http://openreview.net/profile?id=~Jakob_Nicolaus_Foerster1)
  - **Affiliations:** University of Oxford, University of Oxford, University of Oxford, University College London, University of Oxford, University of Oxford, University of Oxford
  - **TL;DR:** The study introduces Craftax, a fast benchmark for open-ended reinforcement learning, addressing the limitations of existing benchmarks that are either too slow or not complex enough. It demonstrates that current exploration methods struggle with the challenges posed by Craftax, paving the way for more effective research in complex environments.
  - **Keywords:** Reinforcement Learning, Open-Ended Learning, PPO (Proximal Policy Optimization), JAX, Slow runtime of benchmarks, Complexity in open-ended environments, Craftax-Classic, Craftax benchmark


- [Agent Instructs Large Language Models to be General Zero-Shot Reasoners](https://icml.cc/virtual/2024/poster/32631) (Poster)
  - **Authors:** [Nicholas Crispino](http://openreview.net/profile?id=~Nicholas_Crispino1), [Kyle Montgomery](http://openreview.net/profile?id=~Kyle_Montgomery1), [Fankun Zeng](http://openreview.net/profile?id=~Fankun_Zeng1), [Dawn Song](http://openreview.net/profile?id=~Dawn_Song1), [Chenguang Wang](http://openreview.net/profile?id=~Chenguang_Wang1)
  - **Affiliations:** Washington University in St. Louis, MO, USA, Washington University in St. Louis, MO, USA, Washington University in St. Louis, MO, USA, UC Berkeley, CA, USA, Washington University in St. Louis, MO, USA; UC Berkeley, CA, USA
  - **TL;DR:** This paper presents a method to enhance the zero-shot reasoning capabilities of large language models by using an autonomous agent to generate task-specific instructions. The approach significantly improves performance across various tasks, achieving state-of-the-art results on multiple datasets.
  - **Keywords:** zero-shot reasoning, large language models, agent-instructed reasoning, language understanding tasks, reasoning ability of LLMs on general tasks, state-of-the-art zero-shot performance, task-specific instructions


- [Theoretical insights for diffusion guidance: A case study for Gaussian mixture models](https://icml.cc/virtual/2024/poster/34260) (Poster)
  - **Authors:** [Yuchen Wu](http://openreview.net/profile?id=~Yuchen_Wu1), [Minshuo Chen](http://openreview.net/profile?id=~Minshuo_Chen1), [Zihao Li](http://openreview.net/profile?id=~Zihao_Li3), [Mengdi Wang](http://openreview.net/profile?id=~Mengdi_Wang1), [Yuting Wei](http://openreview.net/profile?id=~Yuting_Wei1)
  - **Affiliations:** Department of Statistics and Data Science, the Wharton School, University of Pennsylvania, Department of Electrical and Computer Engineering, Princeton University, Department of Electrical and Computer Engineering, Princeton University, Department of Electrical and Computer Engineering, Princeton University, Department of Statistics and Data Science, the Wharton School, University of Pennsylvania
  - **TL;DR:** This paper provides a theoretical analysis of how guidance in diffusion models influences sample generation, demonstrating that it enhances classification confidence while reducing diversity in generated samples. The findings highlight the trade-off between alignment to task-specific information and the diversity of outputs in generative modeling.
  - **Keywords:** Diffusion models, Gaussian mixture models, guidance in generative models, Score function, classifier guidance, SDE (Stochastic Differential Equations), ODE (Ordinary Differential Equations), Fokker-Planck equation, Text-to-image synthesis, generative modeling, Alignment to task-specific information, diversity of generated samples, classification confidence, Theoretical insights on guidance effects, reduction in differential entropy


- [Position: Will we run out of data? Limits of LLM scaling based on human-generated data](https://icml.cc/virtual/2024/poster/33903) (Poster)
  - **Authors:** [Pablo Villalobos](http://openreview.net/profile?id=~Pablo_Villalobos1), [Anson Ho](http://openreview.net/profile?id=~Anson_Ho1), [Jaime Sevilla](http://openreview.net/profile?email=jaime%40epochai.org), [Tamay Besiroglu](http://openreview.net/profile?email=tamay%40epochai.org), [Lennart Heim](http://openreview.net/profile?id=~Lennart_Heim1), [Marius Hobbhahn](http://openreview.net/profile?id=~Marius_Hobbhahn1)
  - **Affiliations:** Epoch; University of Aberdeen, Epoch, Epoch; MIT CSAIL, Epoch; Centre for the Governance of AI, Epoch; University of Tübingen, Epoch
  - **TL;DR:** The study investigates the limitations of scaling large language models (LLMs) due to the finite availability of public human-generated text data, predicting that the stock will be fully utilized between 2026 and 2032. It suggests that future progress may rely on synthetic data generation, transfer learning, and data efficiency improvements to overcome these constraints.
  - **Keywords:** LLM scaling, human-generated text data, Neural scaling laws, synthetic data generation, transfer learning, Language modeling, Data availability constraints, overtraining, Predictions on data stock utilization, strategies for continued progress, RefinedWeb, C4, RedPajama, Large Language Models (LLMs)


- [Learning Linear Block Error Correction Codes](https://icml.cc/virtual/2024/poster/34334) (Poster)
  - **Authors:** [Yoni Choukroun](http://openreview.net/profile?id=~Yoni_Choukroun1), [Lior Wolf](http://openreview.net/profile?id=~Lior_Wolf1)
  - **Affiliations:** The Blavatnik School of Computer Science, Tel Aviv University, The Blavatnik School of Computer Science, Tel Aviv University
  - **TL;DR:** This study presents a novel approach to optimize binary linear block codes alongside a neural decoder through a unified and differentiable training framework. The results indicate that the proposed decoder and codes outperform existing methods, enhancing performance in both neural and traditional decoding techniques.
  - **Keywords:** error correction codes, linear block codes, neural decoding, unified encoder-decoder training, differentiable training, Transformer model, communication technology, data transmission over noisy channels, design of optimal codes, efficient decoding, finite-length codes, improved performance of codes, enhanced decoding techniques, Galois field, Polar codes, Low-Density Parity-Check (LDPC) codes, Belief Propagation


- [Individual Fairness in Graph Decomposition](https://icml.cc/virtual/2024/poster/34841) (Spotlight Poster)
  - **Authors:** [Kamesh Munagala](http://openreview.net/profile?id=~Kamesh_Munagala2), [Govind S. Sankar](http://openreview.net/profile?id=~Govind_S._Sankar1)
  - **Affiliations:** Computer Science Department, Duke University, Durham NC 27708-0129, USA, Computer Science Department, Duke University, Durham NC 27708-0129, USA
  - **TL;DR:** This paper investigates the concept of individual fairness in low diameter decomposition (LDD) for planar graphs, emphasizing the need for equitable separation probabilities among pairs of nodes at comparable distances. The authors present new algorithms that balance individual fairness with connectivity and optimality in clustering, demonstrating their effectiveness in real-world applications like congressional redistricting.
  - **Keywords:** individual fairness, low diameter decomposition, clustering, randomized algorithms, metric embeddings, congressional redistricting, community cohesion, separation of nodes, fairness in clustering, novel algorithms, trade-offs in clustering properties, low diameter decomposition (LDD), pairwise envy-freeness


- [Joint Composite Latent Space Bayesian Optimization](https://icml.cc/virtual/2024/poster/32740) (Poster)
  - **Authors:** [Natalie Maus](http://openreview.net/profile?id=~Natalie_Maus1), [Zhiyuan Jerry Lin](http://openreview.net/profile?id=~Zhiyuan_Jerry_Lin1), [Maximilian Balandat](http://openreview.net/profile?id=~Maximilian_Balandat1), [Eytan Bakshy](http://openreview.net/profile?id=~Eytan_Bakshy1)
  - **Affiliations:** Department of Computer and Information Science, University of Pennsylvania, Meta, Meta, Meta
  - **TL;DR:** This paper introduces Joint Composite Latent Space Bayesian Optimization (JoCo), a novel framework that enhances Bayesian Optimization by jointly training models to compress high-dimensional input and output spaces. JoCo significantly outperforms existing methods in high-dimensional optimization tasks across various applications.
  - **Keywords:** Bayesian Optimization, composite functions, sample-efficient optimization, probabilistic models, Gaussian Process, neural network encoders, generative AI, molecular design, robotics, aerodynamic design, high-dimensional outputs, expensive-to-evaluate functions, optimization challenges, Joint Composite Latent Space Bayesian Optimization (JoCo), improved BO performance


- [Spectral Phase Transition and Optimal PCA in Block-Structured Spiked Models](https://icml.cc/virtual/2024/poster/33887) (Poster)
  - **Authors:** [Pierre Mergny](http://openreview.net/profile?id=~Pierre_Mergny1), [Justin Ko](http://openreview.net/profile?id=~Justin_Ko2), [FLORENT KRZAKALA](http://openreview.net/profile?id=~Florent_Krzakala1)
  - **Affiliations:** IdePHICS laboratory, École Fédérale Polytechnique de Lausanne, Switzerland, Department of Statistics and Actuarial Science, University of Waterloo, Canada; UMPA, ENS Lyon, France, IdePHICS laboratory, École Fédérale Polytechnique de Lausanne, Switzerland
  - **TL;DR:** This study extends the spiked Wigner model to account for structured noise, providing an optimal spectral method for inferring low-dimensional signals from high-dimensional observations. The findings reveal a precise threshold for the emergence of outliers and positive overlaps, establishing the method's optimality in the context of inhomogeneous matrices.
  - **Keywords:** inhomogeneous spiked Wigner model, structured noise, random matrix theory, spectral method, iterative methods, optimal reconstruction method, community detection, deep Boltzmann machines, degree-corrected stochastic block model, inferring low-dimensional signal from high-dimensional observation, noise level impact, extension of BBP phase transition criterion, optimal threshold for spectral method, spiked random matrix models, Hadamard product, block-constant matrix


- [Bivariate Causal Discovery using Bayesian Model Selection](https://icml.cc/virtual/2024/poster/32850) (Poster)
  - **Authors:** [Anish Dhir](http://openreview.net/profile?id=~Anish_Dhir1), [Samuel Power](http://openreview.net/profile?id=~Samuel_Power1), [Mark van der Wilk](http://openreview.net/profile?id=~Mark_van_der_Wilk1)
  - **Affiliations:** Imperial College London, University of Bristol, University of Oxford
  - **TL;DR:** This study presents a Bayesian framework for identifying causal direction within a Markov equivalence class, addressing the limitations of traditional methods that rely on strong assumptions. The proposed approach outperforms existing methods across various benchmark datasets by allowing for more flexible modeling of causal structures.
  - **Keywords:** Causal discovery, Bayesian model selection, Bayesian non-parametric model, Gaussian process latent variable model (GPLVM), Causal direction identifiability, Markov equivalence class, model misspecification, Differentiation between Markov equivalent causal structures, improved identification of causal direction, Benchmark datasets, Independent causal mechanisms (ICM), additive noise model (ANM)


- [BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback](https://icml.cc/virtual/2024/poster/33101) (Poster)
  - **Authors:** [Gaurav Pandey](http://openreview.net/profile?id=~Gaurav_Pandey2), [Yatin Nandwani](http://openreview.net/profile?id=~Yatin_Nandwani1), [Tahira Naseem](http://openreview.net/profile?id=~Tahira_Naseem1), [Mayank Mishra](http://openreview.net/profile?id=~Mayank_Mishra1), [Guangxuan Xu](http://openreview.net/profile?id=~Guangxuan_Xu1), [Dinesh Raghu](http://openreview.net/profile?id=~Dinesh_Raghu1), [Sachindra Joshi](http://openreview.net/profile?id=~Sachindra_Joshi1), [Asim Munawar](http://openreview.net/profile?id=~Asim_Munawar2), [Ramón Astudillo](http://openreview.net/profile?id=~Ram%C3%B3n_Fernandez_Astudillo1)
  - **Affiliations:** IBM Research AI, IBM Research AI, IBM Research AI, IBM Research AI, IBM Research AI, IBM Research AI, IBM Research AI, IBM Research AI, IBM Research AI
  - **TL;DR:** The study proposes BRAI N, a Bayesian Reward-conditioned Amortized Inference method, to improve language model alignment through distribution matching techniques in reinforcement learning from human feedback. It addresses high variance in gradient estimates and significantly outperforms existing methods in summarization and Antropic HH tasks.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Language Model Alignment, Distribution Matching, Generation with Distributional Control (GDC), Distributional Policy Gradient (DPG), Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO), Natural Language Generation, Conversational Agents, High variance of gradient estimates, Challenges in sampling from target energy-based models (EBM), Self-normalized baseline to reduce variance, Reward-conditioned posterior, Large Language Models (LLMs), Supervised Fine-Tuned (SFT) model, Reward Model (RM), Energy-based model (EBM)


- [Local vs. Global Interpretability: A Computational Complexity Perspective](https://icml.cc/virtual/2024/poster/32776) (Spotlight Poster)
  - **Authors:** [Shahaf Bassan](http://openreview.net/profile?id=~Shahaf_Bassan1), [Guy Amir](http://openreview.net/profile?id=~Guy_Amir1), [Guy Katz](http://openreview.net/profile?id=~Guy_Katz1)
  - **Affiliations:** The Hebrew University of Jerusalem, Jerusalem, Israel, The Hebrew University of Jerusalem, Jerusalem, Israel, The Hebrew University of Jerusalem, Jerusalem, Israel
  - **TL;DR:** This study proposes a framework using computational complexity theory to analyze local and global interpretability in machine learning models. It reveals that the complexity of computing explanations varies significantly across different model types, with linear models being harder to interpret globally than locally, while the opposite is true for neural networks and decision trees.
  - **Keywords:** Interpretability, Machine Learning (ML), Computational complexity theory, Lack of mathematical rigor in interpretability, complexity of computing explanations, Framework for evaluating local and global interpretability, insights into computational complexity of explanations, Local interpretability, global interpretability, linear models, decision trees, neural networks


- [ACPO: A Policy Optimization Algorithm for Average MDPs with Constraints](https://icml.cc/virtual/2024/poster/33535) (Poster)
  - **Authors:** [Akhil Agnihotri](http://openreview.net/profile?id=~Akhil_Agnihotri1), [Rahul Jain](http://openreview.net/profile?id=~Rahul_Jain1), [Haipeng Luo](http://openreview.net/profile?id=~Haipeng_Luo1)
  - **Affiliations:** University of Southern California, Los Angeles, CA, USA, University of Southern California, Los Angeles, CA, USA; Google DeepMind, University of Southern California, Los Angeles, CA, USA
  - **TL;DR:** This paper introduces the Average-Constrained Policy Optimization (ACPO) algorithm for reinforcement learning in average-constrained Markov Decision Processes (ACMDPs), addressing the limitations of existing algorithms designed for discounted settings. The authors demonstrate its superior performance through extensive experiments in various challenging environments, highlighting its effectiveness in ensuring long-term safety and optimality.
  - **Keywords:** Reinforcement Learning, Constrained MDPs, Average Criterion, Average-Constrained Policy Optimization (ACPO), Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Constrained Policy Optimization (CPO), Mobile Robot Safety, Generative AI, Robotic Control, Constrained MDPs, Average-CMDPs, Policy Optimization Challenges, New policy optimization algorithm, Theoretical guarantees on performance, OpenAI Gym


- [Parameter-Dependent Competitive Analysis for Online Capacitated Coverage Maximization through Boostings and Attenuations](https://icml.cc/virtual/2024/poster/34215) (Poster)
  - **Authors:** [Pan Xu](http://openreview.net/profile?id=~Pan_Xu2)
  - **Affiliations:** Department of Computer Science, New Jersey Institute of Technology, Newark, USA
  - **TL;DR:** This paper presents a model for Online Capacitated Coverage Maximization, focusing on the dynamic arrival of online agents and the coverage valuation of offline agents. It introduces two matching policies and conducts a competitive analysis to characterize their performance based on key parameters.
  - **Keywords:** Online Capacitated Coverage Maximization, Online Matching Markets, Matching policies, Linear programming, Boostings, Attenuations, Crowdsourcing, Internet advertising, Task-worker assignment, Dynamic arrival of online agents, Coverage valuation, Finite budgets, Operational constraints, Competitive analysis, Competitive ratio characterization


- [Fault Tolerant ML: Efficient Meta-Aggregation and Synchronous Training](https://icml.cc/virtual/2024/poster/34443) (Poster)
  - **Authors:** [Tehila Dahan](http://openreview.net/profile?id=~Tehila_Dahan1), [Kfir Levy](http://openreview.net/profile?id=~Kfir_Yehuda_Levy1)
  - **Affiliations:** Department of Data and Decision Sciences, Technion, Haifa, Israel, Department of Electrical and Computer Engineering, Technion, Haifa, Israel
  - **TL;DR:** This paper presents an efficient meta-aggregator, the Centered Trimmed Meta Aggregator (CTMA), for Byzantine-robust training in distributed machine learning systems, addressing the challenges of resilience against faulty updates. The proposed methods enhance performance while simplifying the tuning process and reducing hyperparameter reliance, supported by theoretical insights and empirical evidence.
  - **Keywords:** Byzantine-robust training, distributed machine learning, Centered Trimmed Meta Aggregator (CTMA), double-momentum strategy, robust aggregation rules, Byzantine failures, resilience against incorrect updates, faults and errors in distributed systems, Efficient meta-aggregator, theoretical and practical advantages for Byzantine-robust training, simplified tuning process, Byzantine model, stochastic convex optimization (SCO), robust aggregation rules


- [Retrieval-Augmented Score Distillation for Text-to-3D Generation](https://icml.cc/virtual/2024/poster/35124) (Poster)
  - **Authors:** [Junyoung Seo](http://openreview.net/profile?id=~Junyoung_Seo1), [Susung Hong](http://openreview.net/profile?id=~Susung_Hong1), [Wooseok Jang](http://openreview.net/profile?id=~Wooseok_Jang2), [Inès Hyeonsu Kim](http://openreview.net/profile?id=~In%C3%A8s_Hyeonsu_Kim1), [Min-Seop Kwak](http://openreview.net/profile?id=~Min-Seop_Kwak1), [Doyup Lee](http://openreview.net/profile?id=~Doyup_Lee1), [Seungryong Kim](http://openreview.net/profile?id=~Seungryong_Kim1)
  - **Affiliations:** Korea University, Seoul, Korea, Korea University, Seoul, Korea, Korea University, Seoul, Korea, Korea University, Seoul, Korea, Korea University, Seoul, Korea, Runway, New York, USA, Korea University, Seoul, Korea
  - **TL;DR:** This study introduces ReDream, a retrieval-augmented framework for text-to-3D generation that enhances geometric consistency and fidelity by leveraging semantically relevant 3D assets. The approach addresses the limitations of existing methods by incorporating geometric priors without the need for extensive fine-tuning of 2D diffusion models.
  - **Keywords:** Text-to-3D generation, 3D geometry consistency, Score Distillation Sampling (SDS), 2D diffusion models, Neural Radiance Field (NeRF), 3D content creation, novel view synthesis, Insufficient quality and diversity of 3D datasets, geometric inconsistencies, artifacts in generated scenes, Retrieval-augmented framework (ReDream), quality enhancement in text-to-3D generation, Objaverse


- [A Study of First-Order Methods with a Deterministic Relative-Error Gradient Oracle](https://icml.cc/virtual/2024/poster/34143) (Poster)
  - **Authors:** [Nadav Hallak](http://openreview.net/profile?id=~Nadav_Hallak1), [Kfir Levy](http://openreview.net/profile?id=~Kfir_Yehuda_Levy1)
  - **Affiliations:** The Faculty of Data and Decision Sciences, The Technion, Haifa, Israel, Viterby Faculty of Electrical and Computer Engineering, The Technion, Haifa, Israel
  - **TL;DR:** This study investigates the theoretical guarantees of classical projected and conditional gradient methods for constrained optimization problems using biased relative-error gradient oracles. The findings establish convergence guarantees and demonstrate the invariance of the conditional gradient method to relative-error in specific scenarios.
  - **Keywords:** constrained optimization, first-order methods, projected gradient, conditional gradient, relative-error gradient oracle, distributed optimization, derivative-free optimization, optimization with biased relative-error gradients, convergence guarantees, performance analysis of optimization methods, Coordinate-Wise Erroneous Oracle (CWEO), Erroneous Oracle (EO)


- [In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization](https://icml.cc/virtual/2024/poster/33892) (Poster)
  - **Authors:** [Herilalaina Rakotoarison](http://openreview.net/profile?id=~Herilalaina_Rakotoarison1), [Steven Adriaensen](http://openreview.net/profile?id=~Steven_Adriaensen1), [Neeratyoy Mallik](http://openreview.net/profile?id=~Neeratyoy_Mallik1), [Samir Garibov](http://openreview.net/profile?id=~Samir_Garibov1), [Edward Bergman](http://openreview.net/profile?id=~Eddie_Bergman1), [Frank Hutter](http://openreview.net/profile?id=~Frank_Hutter1)
  - **Affiliations:** Machine Learning Lab, University of Freiburg, Germany; ELLIS Institute Tübingen, Machine Learning Lab, University of Freiburg, Germany; ELLIS Institute Tübingen, Machine Learning Lab, University of Freiburg, Germany; ELLIS Institute Tübingen, Machine Learning Lab, University of Freiburg, Germany; ELLIS Institute Tübingen, Machine Learning Lab, University of Freiburg, Germany; ELLIS Institute Tübingen, Machine Learning Lab, University of Freiburg, Germany; ELLIS Institute Tübingen
  - **TL;DR:** This paper introduces FT-PFN, a novel surrogate for Freeze-thaw Bayesian Optimization that leverages transformers for efficient Bayesian learning curve extrapolation. The proposed method demonstrates significant improvements in prediction accuracy and speed, achieving state-of-the-art performance in hyperparameter optimization benchmarks.
  - **Keywords:** Hyperparameter Optimization, Bayesian Optimization, Deep Learning, Freeze-thaw Bayesian Optimization, Prior-data Fitted Network (PFN), Transformers, Multi-fidelity methods, Computational costs, Resource allocation, Model training inefficiencies, FT-PFN, In-context freeze-thaw BO (ifBO), State-of-the-art performance


- [Efficient Value Iteration for s-rectangular Robust Markov Decision Processes](https://icml.cc/virtual/2024/poster/34401) (Poster)
  - **Authors:** [Navdeep Kumar](http://openreview.net/profile?id=~Navdeep_Kumar1), [Kaixin Wang](http://openreview.net/profile?id=~Kaixin_Wang1), [Kfir Levy](http://openreview.net/profile?id=~Kfir_Yehuda_Levy1), [Shie Mannor](http://openreview.net/profile?id=~Shie_Mannor2)
  - **Affiliations:** Technion, Haifa, Israel, Technion, Haifa, Israel, Technion, Haifa, Israel, Technion, Haifa, Israel; Nvidia, Haifa, Israel
  - **TL;DR:** This study presents efficient value iteration methods for s-rectangular robust Markov decision processes, addressing the complexities introduced by interdependent uncertainties across actions. The findings reveal optimal policies that favor a limited set of actions based on their advantage functions, highlighting a connection between policy robustness and value function variance.
  - **Keywords:** robust Markov decision processes, uncertainty in MDPs, robust Bellman operators, value iteration methods, model uncertainty, NP-hard problems, faster time complexities, optimal policies, threshold behavior, s-rectangular robust MDPs, sa-rectangular robust MDPs


- [Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers](https://icml.cc/virtual/2024/poster/34184) (Poster)
  - **Authors:** [Ron Dorfman](http://openreview.net/profile?id=~Ron_Dorfman2), [Naseem Yehya](http://openreview.net/profile?id=~Naseem_Amin_Yehya1), [Kfir Levy](http://openreview.net/profile?id=~Kfir_Yehuda_Levy1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Technion, Haifa, Israel, Department of Electrical and Computer Engineering, Technion, Haifa, Israel, Department of Electrical and Computer Engineering, Technion, Haifa, Israel
  - **TL;DR:** This paper introduces DynaBRO, a method for Byzantine-robust learning that adapts to dynamic Byzantine worker behaviors, capable of withstanding a sublinear number of identity changes. The method achieves nearly optimal convergence rates while utilizing a multi-level Monte Carlo gradient estimation technique and an adaptive learning rate.
  - **Keywords:** Byzantine-robust learning, distributed machine learning, DynaBRO, multi-level Monte Carlo (MLMC) gradient estimation, Dynamic Byzantine behaviors, intermittent malfunctions, targeted attacks, Robust aggregation of worker updates, adaptive learning rate, Byzantine faults, fault-tolerant systems


- [Private and Federated Stochastic Convex Optimization: Efficient Strategies for Centralized Systems](https://icml.cc/virtual/2024/poster/32908) (Poster)
  - **Authors:** [Roie Reshef](http://openreview.net/profile?id=~Roie_Reshef1), [Kfir Levy](http://openreview.net/profile?id=~Kfir_Yehuda_Levy1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Technion, Haifa, Israel, Department of Electrical and Computer Engineering, Technion, Haifa, Israel
  - **TL;DR:** This paper presents methods for ensuring Differential Privacy in Federated Learning within centralized systems, addressing both trusted and untrusted server scenarios. The proposed approach achieves optimal convergence rates while maintaining linear computational complexity, enhancing the practicality of privacy-preserving techniques in machine learning.
  - **Keywords:** Federated Learning, Privacy Preservation, Differential Privacy, Stochastic Convex Optimization, Centralized Systems, Machine Learning, Privacy Risks, Confidentiality of Model Updates, Optimal Convergence Rates, Linear Computational Complexity


- [A Touch, Vision, and Language Dataset for Multimodal Alignment](https://icml.cc/virtual/2024/poster/32873) (Oral)
  - **Authors:** [Letian Fu](http://openreview.net/profile?id=~Letian_Fu1), [Gaurav Datta](http://openreview.net/profile?id=~Gaurav_Datta1), [Huang Huang](http://openreview.net/profile?id=~Huang_Huang1), [William Panitch](http://openreview.net/profile?id=~William_Chung-Ho_Panitch1), [Jaimyn Drake](http://openreview.net/profile?email=jaimyndrake%40berkeley.edu), [Joseph Ortiz](http://openreview.net/profile?id=~Joseph_Ortiz2), [Mustafa Mukadam](http://openreview.net/profile?id=~Mustafa_Mukadam1), [Mike Lambeta](http://openreview.net/profile?id=~Mike_Lambeta1), [Roberto Calandra](http://openreview.net/profile?id=~Roberto_Calandra1), [Ken Goldberg](http://openreview.net/profile?id=~Ken_Goldberg1)
  - **Affiliations:** UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, Meta AI, Meta AI, Meta AI, TU Dresden, UC Berkeley
  - **TL;DR:** This study introduces a novel dataset of 44K vision-touch pairs to enhance multimodal alignment by incorporating touch into generative language models. The proposed touch-vision-language model (TVL) significantly improves classification accuracy and understanding of tactile-vision relationships compared to existing models.
  - **Keywords:** multimodal alignment, tactile perception, vision-language integration, vision-language-aligned tactile encoder, touch-vision-language model (TVL), robotic applications, contact-rich manipulation tasks, data scarcity, alignment of tactile readings with visual observations and language descriptions, improved tactile-vision-language alignment, new touch-vision understanding benchmark, 44K vision-touch pairs dataset, GPT-4V


- [Stochastic Q-learning for Large Discrete Action Spaces](https://icml.cc/virtual/2024/poster/34466) (Poster)
  - **Authors:** [Fares Fourati](http://openreview.net/profile?id=~Fares_Fourati1), [Vaneet Aggarwal](http://openreview.net/profile?id=~Vaneet_Aggarwal1), [Mohamed-Slim Alouini](http://openreview.net/profile?id=~Mohamed-Slim_Alouini1)
  - **Affiliations:** Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, KSA, School of Industrial Engineering, Purdue University, West Lafayette, IN 47907, USA, Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, KSA
  - **TL;DR:** This paper presents stochastic value-based reinforcement learning methods that optimize decision-making in environments with large discrete action spaces by considering a variable stochastic set of actions. The proposed methods, including Stochastic Q-learning, demonstrate improved computational efficiency and outperform baseline methods in various control problems.
  - **Keywords:** Reinforcement Learning, Decision-Making, Value-Based Approaches, Stochastic Q-learning, StochDQN, StochDDQN, Combinatorial Optimization, Natural Language Processing, Communications and Networking, Recommendation Systems, Transportation, Robotics, Large Discrete Action Spaces, Computational Efficiency, Stochastic Value-Based RL Methods, Theoretical Convergence


- [SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning](https://icml.cc/virtual/2024/poster/34198) (Poster)
  - **Authors:** [Chaoqun Du](http://openreview.net/profile?id=~Chaoqun_Du1), [Yizeng Han](http://openreview.net/profile?id=~Yizeng_Han1), [Gao Huang](http://openreview.net/profile?id=~Gao_Huang1)
  - **Affiliations:** Department of Automation, BNRist, Tsinghua University, Beijing, China, Department of Automation, BNRist, Tsinghua University, Beijing, China, Department of Automation, BNRist, Tsinghua University, Beijing, China
  - **TL;DR:** This study introduces SimPro, a novel framework for realistic long-tailed semi-supervised learning that does not assume predefined class distributions for unlabeled data. The framework enhances pseudo-label quality and demonstrates state-of-the-art performance across various benchmarks.
  - **Keywords:** Semi-supervised learning, Long-tailed learning, Expectation-Maximization (EM) algorithm, Bayes classifier, Imbalance in labeled data, Inconsistent class distribution of unlabeled data, SimPro framework, Pseudo-label enhancement, Realistic Long-tailed semi-supervised learning (ReaLTSSL)


- [Promoting External and Internal Equities Under Ex-Ante/Ex-Post Metrics in Online Resource Allocation](https://icml.cc/virtual/2024/poster/35152) (Spotlight Poster)
  - **Authors:** [Karthik Abinav Sankararaman](http://openreview.net/profile?id=~Karthik_Abinav_Sankararaman1), [Aravind Srinivasan](http://openreview.net/profile?id=~Aravind_Srinivasan1), [Pan Xu](http://openreview.net/profile?id=~Pan_Xu2)
  - **Affiliations:** Meta AI, Menlo Park, USA, Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, New Jersey Institute of Technology, Newark, USA
  - **TL;DR:** This paper presents two models for equitable resource allocation in online settings, focusing on external and internal equity promotion. It proposes optimal allocation policies under different equity metrics and highlights the importance of fairness in distributing limited resources among diverse requesters.
  - **Keywords:** equitable resource allocation, external equity promotion, internal equity promotion, linear programming (LP), resource allocation in non-profit settings, refugee resettlement, public housing allocation, emergency aid distribution, food donation allocation, fairness in resource distribution, heterogeneity in demands and demographics, optimal allocation policies, equity metrics (ex-ante, ex-post)


- [Federated Combinatorial Multi-Agent Multi-Armed Bandits](https://icml.cc/virtual/2024/poster/33199) (Poster)
  - **Authors:** [Fares Fourati](http://openreview.net/profile?id=~Fares_Fourati1), [Mohamed-Slim Alouini](http://openreview.net/profile?id=~Mohamed-Slim_Alouini1), [Vaneet Aggarwal](http://openreview.net/profile?id=~Vaneet_Aggarwal1)
  - **Affiliations:** Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, KSA, Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, KSA, School of Industrial Engineering, Purdue University, West Lafayette, IN 47907, USA
  - **TL;DR:** This paper presents a federated learning framework for online combinatorial optimization using multi-agent multi-armed bandits, achieving efficient communication and sublinear regret bounds. The proposed method effectively addresses challenges in bandit feedback and demonstrates practical applications in areas like recommender systems and data summarization.
  - **Keywords:** Federated Learning, Combinatorial Optimization, Multi-Agent Systems, Multi-Armed Bandits, (α−ϵ)-approximation algorithms, Recommender Systems, Revenue Maximization, Influence Maximization, Data Summarization, Bandit Feedback, Noisy Rewards, Communication Efficiency, Online Multi-Agent Algorithm, Regret Bounds, Communication-Efficient Framework, Full-Bandit Feedback, Semi-Bandit Feedback


- [Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems](https://icml.cc/virtual/2024/poster/34445) (Poster)
  - **Authors:** [David T. Hoffmann](http://openreview.net/profile?id=~David_T_Hoffmann1), [Simon Schrodi](http://openreview.net/profile?id=~Simon_Schrodi1), [Jelena Bratulić](http://openreview.net/profile?id=~Jelena_Bratuli%C4%871), [Nadine Behrmann](http://openreview.net/profile?id=~Nadine_Behrmann1), [Volker Fischer](http://openreview.net/profile?id=~Volker_Fischer1), [Thomas Brox](http://openreview.net/profile?id=~Thomas_Brox1)
  - **Affiliations:** University of Freiburg; Bosch Center for AI, University of Freiburg, University of Freiburg, Amazon (work done while at Bosch), Bosch Center for AI, University of Freiburg
  - **TL;DR:** This study investigates the rapid improvements in training loss of transformers when faced with multi-step decision tasks, identifying a phenomenon termed "Eureka-moments" where the model suddenly learns complex concepts. The findings suggest that these abrupt transitions are linked to the multi-step nature of tasks and propose methods to enhance training efficiency and robustness.
  - **Keywords:** transformers, multi-step decision tasks, training loss, Softmax function, self-attention, language modeling, in-context learning (ICL), learning intermediate tasks, saturation of training loss, abrupt improvements, optimization methods, training improvements, robustness to hyper-parameters, Eureka-moments


- [Graph Neural Network Explanations are Fragile](https://icml.cc/virtual/2024/poster/32998) (Poster)
  - **Authors:** [Jiate Li](http://openreview.net/profile?id=~Jiate_Li1), [Meng Pang](http://openreview.net/profile?id=~Meng_Pang1), [Yun Dong](http://openreview.net/profile?id=~Yun_Dong1), [Jinyuan Jia](http://openreview.net/profile?id=~Jinyuan_Jia2), [Binghui Wang](http://openreview.net/profile?id=~Binghui_Wang2)
  - **Affiliations:** Nanchang University, China, Nanchang University, China; Illinois Institute of Technology, USA, Milwaukee School of Engineering, USA, The Pennsylvania State University, USA, Illinois Institute of Technology, USA; Nanchang University, China
  - **TL;DR:** This study investigates the fragility of explainable Graph Neural Networks (GNNs) under adversarial attacks, demonstrating that slight perturbations in graph structure can lead to accurate predictions while drastically altering the explanations provided by GNN explainers. The findings highlight significant security implications for real-world applications of GNNs.
  - **Keywords:** Explainable Graph Neural Networks, Adversarial Attacks, GNN explainers, perturbation-based explainers, loss-based methods, deduction-based methods, Molecular property prediction, disease diagnosis, drug analysis, fake news spreader prediction, Robustness of GNN explainers, adversarial manipulation of graph structure, Evaluation of GNN explainers under adversarial conditions, GNN (Graph Neural Network), explanatory subgraph


- [Bayesian Optimization of Function Networks with Partial Evaluations](https://icml.cc/virtual/2024/poster/32902) (Poster)
  - **Authors:** [Poompol Buathong](http://openreview.net/profile?id=~Poompol_Buathong1), [Jiayue Wan](http://openreview.net/profile?id=~Jiayue_Wan1), [Raul Astudillo](http://openreview.net/profile?id=~Raul_Astudillo1), [Samuel Daulton](http://openreview.net/profile?id=~Sam_Daulton1), [Maximilian Balandat](http://openreview.net/profile?id=~Maximilian_Balandat1), [Peter Frazier](http://openreview.net/profile?id=~Peter_I._Frazier1)
  - **Affiliations:** Center for Applied Mathematics, Cornell University, School of Operations Research and Information Engineering, Cornell University, Department of Computing and Mathematical Sciences, Caltech, Meta, Meta, School of Operations Research and Information Engineering, Cornell University
  - **TL;DR:** This study introduces a novel knowledge gradient acquisition function for Bayesian optimization of function networks that allows for cost-aware evaluations of nodes, significantly improving performance by reducing query costs. The proposed method outperforms existing approaches across various synthetic and real-world problems.
  - **Keywords:** Bayesian optimization, function networks, cost-aware evaluation, knowledge gradient acquisition function, manufacturing, hyperparameter tuning, materials design, vaccine manufacturing, pharmaceutical product development, expensive evaluations, partial evaluations, optimization of function networks, improved performance of BO, cost-aware optimization methods


- [Generalization Error of Graph Neural Networks in the Mean-field Regime](https://icml.cc/virtual/2024/poster/34840) (Poster)
  - **Authors:** [Gholamali Aminian](http://openreview.net/profile?id=~Gholamali_Aminian1), [Yixuan He](http://openreview.net/profile?id=~Yixuan_He2), [Gesine Reinert](http://openreview.net/profile?id=~Gesine_Reinert1), [Lukasz Szpruch](http://openreview.net/profile?id=~Lukasz_Szpruch1), [Samuel Cohen](http://openreview.net/profile?id=~Samuel_N._Cohen1)
  - **Affiliations:** The Alan Turing Institute, London, United Kingdom, Department of Statistics, University of Oxford, Oxford, United Kingdom, The Alan Turing Institute, London, United Kingdom; Department of Statistics, University of Oxford, Oxford, United Kingdom, School of Mathematics, University of Edinburgh; Mathematical Institute, University of Oxford, Oxford, United Kingdom, The Alan Turing Institute, London, United Kingdom; Mathematical Institute, University of Oxford, Oxford, United Kingdom
  - **TL;DR:** This study establishes a theoretical framework for assessing the generalization error of graph neural networks in the over-parameterized regime, deriving upper bounds with a convergence rate of O(1/n). The findings enhance understanding of GNN performance on unseen data, particularly in graph classification tasks.
  - **Keywords:** Generalization error, Graph Neural Networks (GNNs), Over-parameterization, Graph Convolutional Networks, Message Passing Graph Neural Networks, Mean-field regime, Functional derivatives, Rademacher complexity, Graph classification, Social networks, Recommendation systems, Computer vision, Generalization performance, Over-parameterized regime, Model performance on unseen data, Upper bounds on generalization error, Convergence rate of O(1/n)


- [BLO-SAM: Bi-level Optimization Based Finetuning of the Segment Anything Model for Overfitting-Preventing Semantic Segmentation](https://icml.cc/virtual/2024/poster/32990) (Poster)
  - **Authors:** [Li Zhang](http://openreview.net/profile?id=~Li_Zhang21), [Youwei Liang](http://openreview.net/profile?id=~Youwei_Liang1), [Ruiyi Zhang](http://openreview.net/profile?id=~Ruiyi_Zhang4), [Amirhosein Javadi](http://openreview.net/profile?id=~Amirhosein_Javadi1), [Pengtao Xie](http://openreview.net/profile?id=~Pengtao_Xie3)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA
  - **TL;DR:** The study introduces BLO-SAM, a bi-level optimization approach for finetuning the Segment Anything Model (SAM) to enhance automatic image segmentation and reduce overfitting, particularly in medical imaging contexts. The results indicate that BLO-SAM outperforms existing state-of-the-art methods in semantic segmentation tasks.
  - **Keywords:** semantic segmentation, foundation models, bi-level optimization (BLO), finetuning, medical imaging, general semantic segmentation tasks, overfitting, distribution discrepancy, manual prompts requirement, BLO-SAM, automatic image segmentation, learnable prompt embedding, 11 million image-mask pairs, Segment Anything Model (SAM)


- [Self-cognitive Denoising in the Presence of Multiple Noisy Label Sources](https://icml.cc/virtual/2024/poster/34692) (Poster)
  - **Authors:** [Yi-Xuan Sun](http://openreview.net/profile?id=~Yi-Xuan_Sun1), [Ya-Lin Zhang](http://openreview.net/profile?id=~Ya-Lin_Zhang1), [BIN HAN](http://openreview.net/profile?id=~BIN_HAN2), [Longfei Li](http://openreview.net/profile?id=~Longfei_Li1), [JUN ZHOU](http://openreview.net/profile?id=~JUN_ZHOU6)
  - **Affiliations:** Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China, Ant Group, Hangzhou, China
  - **TL;DR:** This paper presents a novel approach called Self-cognitive Denoising for Multiple noisy label sources (SDM) that leverages the self-cognition ability of neural networks to effectively learn from multiple noisy label sources. The proposed method demonstrates superior performance in denoising during training and optimizing computational efficiency across various datasets.
  - **Keywords:** Noisy label sources, Self-cognition in neural networks, Weakly supervised learning, Self-cognitive Denoising (SDM), Selective distillation module, Binary classification, Learning from multiple noisy label sources, Acquiring ground-truth labels, Enhanced denoising during training, Improved computational efficiency, Various datasets


- [Explorations of Self-Repair in Language Models](https://icml.cc/virtual/2024/poster/34973) (Poster)
  - **Authors:** [Cody Rushing](http://openreview.net/profile?id=~Cody_Rushing1), [Neel Nanda](http://openreview.net/profile?id=~Neel_Nanda1)
  - **Affiliations:** University of Texas at Austin, None
  - **TL;DR:** This study investigates the phenomenon of self-repair in large language models, demonstrating that while components can compensate for the ablation of attention heads, this self-repair is imperfect and varies across different prompts. The findings have significant implications for interpretability efforts in understanding model behavior.
  - **Keywords:** Self-repair, Interpretability in Language Models, Ablation, Attention Heads, Large Language Models, Imperfect self-repair, Noisy compensation mechanisms, Evidence for Iterative Inference hypothesis, Mechanisms of self-repair, LayerNorm, Anti-Erasure


- [On The Complexity of First-Order Methods in Stochastic Bilevel Optimization](https://icml.cc/virtual/2024/poster/34164) (Poster)
  - **Authors:** [Jeongyeol Kwon](http://openreview.net/profile?id=~Jeongyeol_Kwon1), [Dohyun Kwon](http://openreview.net/profile?id=~Dohyun_Kwon1), [Hanbaek Lyu](http://openreview.net/profile?id=~Hanbaek_Lyu1)
  - **Affiliations:** Wisconsin Institute for Discovery, Wisconsin, USA, Department of Mathematics, University of Seoul, Republic of Korea; Center for AI and Natural Sciences, Korea Institute for Advanced Study, Republic of Korea, Wisconsin Institute for Discovery, Wisconsin, USA
  - **TL;DR:** This study investigates the complexity of finding stationary points in stochastic bilevel optimization, proposing a first-order method that utilizes a y∗-aware oracle to achieve improved complexity bounds. The findings indicate that existing methods can be enhanced, providing both upper and lower bounds for the complexity of first-order methods in this context.
  - **Keywords:** Bilevel optimization, Stationary points, First-order methods, y∗-aware oracle, Meta-learning, Hyper-parameter optimization, Model selection, Adversarial networks, Game theory, Reinforcement learning, Finding stationary points, Complexity of optimization, O(ϵ−6), O(ϵ−4) complexity results, Upper and lower bounds for first-order methods, Strongly convex, Gradient estimators, Implicit function theorem


- [Regularizing with Pseudo-Negatives for Continual Self-Supervised Learning](https://icml.cc/virtual/2024/poster/34797) (Poster)
  - **Authors:** [Sungmin Cha](http://openreview.net/profile?id=~Sungmin_Cha1), [Kyunghyun Cho](http://openreview.net/profile?id=~Kyunghyun_Cho1), [Taesup Moon](http://openreview.net/profile?id=~Taesup_Moon1)
  - **Affiliations:** New York University; None, New York University; Genentech, ASRI / INMC / IPAI / AIIS, Seoul National University
  - **TL;DR:** This paper introduces a novel Pseudo-Negative Regularization (PNR) framework for effective continual self-supervised learning (CSSL), which utilizes pseudo-negatives to ensure that new representations do not contradict previous learnings. The proposed method achieves state-of-the-art performance by balancing the trade-off between plasticity and stability in representation learning.
  - **Keywords:** Continual Self-Supervised Learning (CSSL), Self-Supervised Learning (SSL), Pseudo-Negative Regularization (PNR), InfoNCE-based contrastive learning, Representation learning, Balancing plasticity and stability, preventing forgetting, State-of-the-art performance in representation learning, Model-based augmentation


- [Prospective Side Information for Latent MDPs](https://icml.cc/virtual/2024/poster/33059) (Spotlight Poster)
  - **Authors:** [Jeongyeol Kwon](http://openreview.net/profile?id=~Jeongyeol_Kwon1), [Yonathan Efroni](http://openreview.net/profile?id=~Yonathan_Efroni2), [Shie Mannor](http://openreview.net/profile?id=~Shie_Mannor2), [Constantine Caramanis](http://openreview.net/profile?id=~Constantine_Caramanis1)
  - **Affiliations:** Wisconsin Institute for Discovery, Wisconsin, USA, Meta AI, New York, USA, Electrical Engineering, Technion, Haifa, Israel; NVIDIA, Electrical and Computer Engineering, University of Texas at Austin, Texas, USA
  - **TL;DR:** This study introduces a new class of decision problems called LMDPs with prospective side information, where agents receive weakly revealing information about latent contexts. The authors demonstrate that this setting allows for efficient learning, achieving exponential improvements in sample complexity compared to traditional LMDPs.
  - **Keywords:** Latent MDPs, decision-making problems, contextual side information, Reinforcement learning algorithms, sample efficient algorithms, Interactive decision-making, healthcare systems, recommender systems, Unobserved latent context, sample complexity, Ω(K2/3)-regret, Exponential improvement in sample complexity, matching upper bounds, POMDP, LMDP, prospective side information


- [Autoencoding Conditional Neural Processes for Representation Learning](https://icml.cc/virtual/2024/poster/33799) (Poster)
  - **Authors:** [Victor Prokhorov](http://openreview.net/profile?id=~Victor_Prokhorov1), [Ivan Titov](http://openreview.net/profile?id=~Ivan_Titov1), [Siddharth N](http://openreview.net/profile?id=~Siddharth_N1)
  - **Affiliations:** School of Informatics, University of Edinburgh; None, School of Informatics, University of Edinburgh; ILLC, University of Amsterdam, School of Informatics, University of Edinburgh; The Alan Turing Institute
  - **TL;DR:** This study introduces the Partial Pixel Space Variational Autoencoder (PPS-VAE) to enhance the selection of context pixels for Conditional Neural Processes (CNPs) in image completion tasks. The findings indicate that strategically chosen pixels improve CNP fitting and provide meaningful insights into image representation.
  - **Keywords:** Conditional Neural Processes, Representation Learning, Partial Pixel Space Variational Autoencoder (PPS-VAE), Amortised Variational Framework, Contextual Image Completion, Selection of context pixels, Fitting better CNPs, Improved fitting of CNPs, Characterization of image information


- [Explaining Graph Neural Networks via Structure-aware Interaction Index](https://icml.cc/virtual/2024/poster/35104) (Poster)
  - **Authors:** [Ngoc Bui](http://openreview.net/profile?id=~Ngoc_Bui1), [Trung Hieu Nguyen](http://openreview.net/profile?id=~Hieu_Trung_Nguyen2), [Viet Anh Nguyen](http://openreview.net/profile?id=~Viet_Anh_Nguyen2), [ZHITAO YING](http://openreview.net/profile?id=~Zhitao_Ying1)
  - **Affiliations:** Yale University, VinAI Research, The Chinese University of Hong Kong, Yale University
  - **TL;DR:** This paper introduces the Myerson-Taylor interaction index to enhance the explainability of Graph Neural Networks (GNNs) by considering graph structure in node value attribution. The proposed Myerson-Taylor Structure-Aware Graph Explainer (MAGE) outperforms existing methods in providing meaningful subgraph explanations for model predictions.
  - **Keywords:** Explainability, Graph Neural Networks, Myerson-Taylor interaction index, Shapley value, Natural language processing, image recognition, point cloud analysis, AI for science, Lack of explainability in GNNs, challenges in attributing node importance, Myerson-Taylor Structure-Aware Graph Explainer (MAGE), superior subgraph explanations


- [AlphaFold Meets Flow Matching for Generating Protein Ensembles](https://icml.cc/virtual/2024/poster/32938) (Oral)
  - **Authors:** [Bowen Jing](http://openreview.net/profile?id=~Bowen_Jing1), [Bonnie Berger](http://openreview.net/profile?id=~Bonnie_Berger1), [Tommi Jaakkola](http://openreview.net/profile?id=~Tommi_S._Jaakkola1)
  - **Affiliations:** CSAIL, Massachusetts Institute of Technology; Department of Mathematics, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology; Department of Mathematics, Massachusetts Institute of Technology, CSAIL, Massachusetts Institute of Technology
  - **TL;DR:** This study introduces a flow-based generative modeling approach that repurposes AlphaFold and ESMFold to sample protein conformational landscapes, addressing the limitations of single-state modeling. The proposed models, Alpha FLOW and ESM FLOW, demonstrate improved precision and diversity in capturing protein ensembles compared to traditional methods.
  - **Keywords:** protein structure, structural ensembles, generative modeling, flow matching, AlphaFold, ESMFold, structural biology, molecular dynamics, conformational heterogeneity, limitations of single-state modeling, Alpha FLOW, ESM FLOW, improved sampling of protein conformations, PDB (Protein Data Bank), molecular dynamics ensembles


- [Neural NeRF Compression](https://icml.cc/virtual/2024/poster/34937) (Poster)
  - **Authors:** [Tuan Pham](http://openreview.net/profile?id=~Tuan_Pham4), [Stephan Mandt](http://openreview.net/profile?id=~Stephan_Mandt1)
  - **Affiliations:** Department of Computer Science, University of California Irvine, Department of Computer Science, University of California Irvine
  - **TL;DR:** This paper presents a novel method for efficiently compressing grid-based Neural Radiance Fields (NeRFs) to address significant storage overhead, utilizing non-linear transform coding and an importance-weighted rate-distortion objective. Experimental results demonstrate that the proposed method outperforms existing approaches in compression efficacy and reconstruction quality.
  - **Keywords:** Neural Radiance Fields, 3D scene modeling, compression, Non-linear transform coding, neural compression, importance-weighted rate-distortion objective, sparse entropy model, Virtual Reality (VR), Augmented Reality (AR), Storage overhead, efficiency concerns in training and rendering, Efficient compression methods for grid-based NeRF models, improved reconstruction quality, Feature grids, voxel pruning, vector quantization


- [Generative Conditional Distributions by Neural (Entropic) Optimal Transport](https://icml.cc/virtual/2024/poster/34524) (Poster)
  - **Authors:** [Bao Nguyen](http://openreview.net/profile?id=~Bao_Nguyen2), [Binh Nguyen](http://openreview.net/profile?id=~Binh_Nguyen2), [Trung Hieu Nguyen](http://openreview.net/profile?id=~Hieu_Trung_Nguyen2), [Viet Anh Nguyen](http://openreview.net/profile?id=~Viet_Anh_Nguyen2)
  - **Affiliations:** The Chinese University of Hong Kong; VinUniversity, Department of Mathematics, National University of Singapore, VinAI Research, VinAI Research
  - **TL;DR:** This study introduces a novel neural entropic optimal transport method for learning generative models of conditional distributions, particularly effective in scenarios with limited sample sizes. The proposed method demonstrates improved performance over existing techniques in real-world datasets, addressing challenges such as data scarcity and model overfitting.
  - **Keywords:** Conditional distributions, Generative models, Optimal transport, Neural networks, Minimax training, Inverse cumulative distribution functions, Conditional Kantorovich potential, Healthcare, Economics, Data scarcity, High-dimensional covariate space, Model overfitting, Novel neural entropic optimal transport method, Regularization techniques, Lalonde-Dehjia-Wahba (LDW) dataset


- [Federated Continual Learning via Prompt-based Dual Knowledge Transfer](https://icml.cc/virtual/2024/poster/34325) (Poster)
  - **Authors:** [Hongming Piao](http://openreview.net/profile?id=~Hongming_Piao1), [Yichen WU](http://openreview.net/profile?id=~Yichen_Wu2), [Dapeng Wu](http://openreview.net/profile?id=~Dapeng_Wu1), [Ying WEI](http://openreview.net/profile?id=~Ying_Wei1)
  - **Affiliations:** City University of Hong Kong, City University of Hong Kong, City University of Hong Kong, Nanyang Technological University
  - **TL;DR:** This paper introduces a novel algorithm called Powder for Federated Continual Learning, which enhances knowledge transfer across tasks while addressing privacy concerns and communication overhead. Experimental results show significant improvements in communication costs and knowledge transfer effectiveness compared to existing methods.
  - **Keywords:** Federated Learning, Continual Learning, Knowledge Transfer, Prompt-based Knowledge Transfer, Dual Knowledge Transfer, Catastrophic Forgetting, Privacy Protection, Communication Overhead, Powder Algorithm, Knowledge Transfer Enhancement, Non-IID Distribution, Memory Buffer, Rehearsal-based Methods, Rehearsal-free Methods


- [An Interpretable Evaluation of Entropy-based Novelty of Generative Models](https://icml.cc/virtual/2024/poster/32747) (Poster)
  - **Authors:** [Jingwei Zhang](http://openreview.net/profile?id=~Jingwei_Zhang9), [Cheuk Ting Li](http://openreview.net/profile?id=~Cheuk_Ting_Li1), [Farzan Farnia](http://openreview.net/profile?id=~Farzan_Farnia1)
  - **Affiliations:** Department of Computer Science and Engineering, The Chinese University of Hong Kong, Department of Information Engineering, The Chinese University of Hong Kong, Department of Computer Science and Engineering, The Chinese University of Hong Kong
  - **TL;DR:** This study introduces the Kernel-based Entropic Novelty (KEN) score to evaluate the novelty of generative models compared to reference models, focusing on multi-modal distributions. The proposed framework effectively detects novel modes and compares generative models using numerical results from synthetic and real image datasets.
  - **Keywords:** generative models, novelty assessment, multi-modal distributions, Kernel-based Entropic Novelty (KEN) score, spectral approach, differential clustering, image datasets, text datasets, novelty evaluation, comparison of generative models, KEN score, detection of novel modes, variational autoencoders (VAEs), generative adversarial networks (GANs), denoising diffusion models


- [xT: Nested Tokenization for Larger Context in Large Images](https://icml.cc/virtual/2024/poster/32754) (Poster)
  - **Authors:** [Ritwik Gupta](http://openreview.net/profile?id=~Ritwik_Gupta1), [Shufan Li](http://openreview.net/profile?id=~Shufan_Li1), [Tyler Zhu](http://openreview.net/profile?id=~Tyler_Zhu2), [Jitendra Malik](http://openreview.net/profile?id=~Jitendra_Malik2), [Trevor Darrell](http://openreview.net/profile?id=~Trevor_Darrell2), [Karttikeya Mangalam](http://openreview.net/profile?id=~Karttikeya_Mangalam1)
  - **Affiliations:** University of California, Berkeley, University of California, Los Angeles, University of California, Berkeley; Princeton University, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley
  - **TL;DR:** The study introduces xT, a framework for vision transformers that effectively integrates global context with local details to model large images without memory constraints. The method demonstrates significant improvements in accuracy and F1 scores on challenging classification and segmentation tasks involving images as large as 29,000 x 29,000 pixels.
  - **Keywords:** computer vision, large images, global context, vision transformers, token hierarchies, two-stage architecture, satellite imagery, image classification, context-dependent segmentation, downsampling, cropping, loss of information, memory limitations, improved accuracy, increased F1 score, end-to-end modeling, benchmark datasets


- [Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes](https://icml.cc/virtual/2024/poster/34952) (Poster)
  - **Authors:** [Jaehyeong Jo](http://openreview.net/profile?id=~Jaehyeong_Jo1), [Sung Ju Hwang](http://openreview.net/profile?id=~Sung_Ju_Hwang1)
  - **Affiliations:** Korea Advanced Institute of Science and Technology (KAIST), Korea Advanced Institute of Science and Technology (KAIST); DeepAuto.ai
  - **TL;DR:** This study introduces the Riemannian Diffusion Mixture, a novel framework for generative modeling on Riemannian manifolds that overcomes limitations of existing methods by avoiding expensive divergence computations and heat kernel approximations. The proposed method demonstrates superior performance across various manifolds with significantly fewer in-training simulation steps.
  - **Keywords:** Riemannian manifolds, generative modeling, diffusion processes, Riemannian Diffusion Mixture, bridge processes, diffusion process construction, Earth and climate science, protein structures, robotic movements, 3D computer graphics, Expensive divergence computation, heat kernel approximations, scalability to high dimensions, Scalable training objective, superior performance on diverse manifolds, reduced in-training simulation steps


- [GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding](https://icml.cc/virtual/2024/poster/33157) (Poster)
  - **Authors:** [Cunxiao Du](http://openreview.net/profile?id=~Cunxiao_Du3), [Jing Jiang](http://openreview.net/profile?id=~Jing_Jiang1), [Xu Yuanchen](http://openreview.net/profile?id=~Xu_Yuanchen1), [Jiawei Wu](http://openreview.net/profile?id=~Jiawei_Wu8), [Sicheng Yu](http://openreview.net/profile?id=~Sicheng_Yu2), [Yongqi Li](http://openreview.net/profile?id=~Yongqi_Li1), [Shenggui Li](http://openreview.net/profile?id=~Shenggui_Li1), [Kai Xu](http://openreview.net/profile?id=~Kai_Xu1), [Liqiang Nie](http://openreview.net/profile?id=~Liqiang_Nie2), [Zhaopeng Tu](http://openreview.net/profile?id=~Zhaopeng_Tu1), [Yang You](http://openreview.net/profile?id=~Yang_You1)
  - **Affiliations:** Singapore Management University, Singapore Management University, National University of Singapore, National University of Singapore, Singapore Management University, The Hong Kong Polytechnic University, HPC-AI Tech., HPC-AI Tech., Harbin Institute of Technology (Shenzhen), Tencent AI Lab, National University of Singapore
  - **TL;DR:** This study introduces GLIDE and CAPE, two modifications to speculative decoding that enhance the decoding speed of large language models by utilizing efficient draft models and confidence scores for token selection. Experimental results show that these methods can significantly reduce decoding latency, achieving up to 2.61x acceleration in certain models.
  - **Keywords:** Speculative Decoding, Large Language Models (LLMs), Draft Model Architecture, Proposal Expansion Method, Machine Translation, Real-Time Response Systems, High Latency in LLMs, Token Prediction Difficulty, GLIDE and CAPE methods, Decoding Speed Improvement, KV Cache, Acceptance Rate, Confidence Scores


- [Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond](https://icml.cc/virtual/2024/poster/33054) (Poster)
  - **Authors:** [Xutong Liu](http://openreview.net/profile?id=~Xutong_Liu1), [Siwei Wang](http://openreview.net/profile?id=~Siwei_Wang2), [Jinhang Zuo](http://openreview.net/profile?id=~Jinhang_Zuo1), [Han Zhong](http://openreview.net/profile?id=~Han_Zhong1), [Xuchuang Wang](http://openreview.net/profile?id=~Xuchuang_Wang1), [Zhiyong Wang](http://openreview.net/profile?id=~Zhiyong_Wang9), [Shuai Li](http://openreview.net/profile?id=~Shuai_Li3), [Mohammad Hajiesmaili](http://openreview.net/profile?id=~Mohammad_Hajiesmaili1), [John C.S. Lui](http://openreview.net/profile?id=~John_C.S._Lui2), [Wei Chen](http://openreview.net/profile?id=~Wei_Chen10)
  - **Affiliations:** The Chinese University of Hong Kong, Hong Kong SAR, China, Microsoft Research, Beijing, China, University of Massachusetts Amherst, Massachusetts, United States; California Institute of Technology, California, United States, Peking University, Beijing, China, University of Massachusetts Amherst, Massachusetts, United States, The Chinese University of Hong Kong, Hong Kong SAR, China, Shanghai Jiao Tong University, Shanghai, China, University of Massachusetts Amherst, Massachusetts, United States, The Chinese University of Hong Kong, Hong Kong SAR, China, Microsoft Research, Beijing, China
  - **TL;DR:** This paper introduces a novel framework for combinatorial multi-armed bandits with multivariant and probabilistically triggering arms, enhancing modeling capabilities and achieving improved regret bounds. It establishes a connection between episodic reinforcement learning and combinatorial multi-armed bandits, potentially fostering further research interactions in these areas.
  - **Keywords:** Combinatorial multi-armed bandits, Multivariant random variables, Episodic reinforcement learning, CUCB-MT algorithm, 1-norm multivariant smoothness condition, Goods distribution, Online advertising, Network optimization, Healthcare systems, Expected regret minimization, Exploration vs. exploitation in decision-making, Improved regret bounds, New framework for episodic RL, Combinatorial actions, Super arm, Semi-bandit feedback, Probabilistically triggered arms, Triggering probability modulated smoothness


- [SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation](https://icml.cc/virtual/2024/poster/32934) (Poster)
  - **Authors:** [Junjie Zhang](http://openreview.net/profile?id=~Junjie_Zhang6), [Chenjia Bai](http://openreview.net/profile?id=~Chenjia_Bai2), [Haoran He](http://openreview.net/profile?id=~Haoran_He1), [Zhigang Wang](http://openreview.net/profile?id=~Zhigang_Wang3), [Bin Zhao](http://openreview.net/profile?id=~Bin_Zhao7), [Xiu Li](http://openreview.net/profile?id=~Xiu_Li1), [Xuelong Li](http://openreview.net/profile?id=~Xuelong_Li2)
  - **Affiliations:** Tsinghua Shenzhen International Graduate School, Tsinghua University; None, Shanghai Artificial Intelligence Laboratory, Hong Kong University of Science and Technology, Shanghai Artificial Intelligence Laboratory, Shanghai Artificial Intelligence Laboratory, Tsinghua Shenzhen International Graduate School, Tsinghua University; None, Shanghai Artificial Intelligence Laboratory; Institute of Artificial Intelligence (TeleAI), China Telecom, P. R. China
  - **TL;DR:** This paper presents SAM-E, a novel architecture for robot manipulation that utilizes a vision-foundation model for improved scene understanding and sequence imitation, addressing challenges in long-horizon reasoning. Experimental results show that SAM-E significantly enhances execution efficiency and generalization capabilities in few-shot adaptations to new tasks.
  - **Keywords:** Robot manipulation, Imitation learning, Scene understanding, Vision-foundation model, Sequence imitation, Multi-channel heatmap, 3D manipulation, Instruction-following tasks, Scene understanding challenges, Action prediction, Generalization in unseen tasks, Long-horizon reasoning, SAM-E architecture, Enhanced execution efficiency, Improved generalization in few-shot adaptation, Segment Anything (SAM), Embodied datasets


- [When Will Gradient Regularization Be Harmful?](https://icml.cc/virtual/2024/poster/34951) (Poster)
  - **Authors:** [Yang Zhao](http://openreview.net/profile?id=~Yang_Zhao11), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang37), [Xiuyuan Hu](http://openreview.net/profile?id=~Xiuyuan_Hu1)
  - **Affiliations:** Department of Electronic Engineering, Tsinghua University, Department of Electronic Engineering, Tsinghua University, None
  - **TL;DR:** This study investigates the adverse effects of Gradient Regularization (GR) in adaptive optimization scenarios, particularly during learning rate warmup, revealing that it can lead to performance degeneration. The authors propose three GR warmup strategies that mitigate these issues, resulting in improved model performance on the Cifar10 dataset.
  - **Keywords:** Gradient Regularization, Deep Neural Networks, Adaptive Optimization, Learning Rate Warmup, Gradient Norm Penalty, Performance Degeneration, Instability in Gradient Statistics, GR Warmup Strategies, Improved Model Performance, Cifar10, Vision Transformer, Sharpness-Aware Minimization (SAM)


- [Improving Group Robustness on Spurious Correlation Requires Preciser Group Inference](https://icml.cc/virtual/2024/poster/34320) (Poster)
  - **Authors:** [Yujin Han](http://openreview.net/profile?id=~Yujin_Han1), [Difan Zou](http://openreview.net/profile?id=~Difan_Zou1)
  - **Affiliations:** Department of Computer Science, The University of Hong Kong, Department of Computer Science & Institute of Data Science, The University of Hong Kong
  - **TL;DR:** This study introduces GIC, a method for accurately inferring group labels to enhance worst-group performance in machine learning models affected by spurious correlations. Empirical results demonstrate that GIC significantly improves accuracy across various datasets, addressing the challenges posed by spurious features.
  - **Keywords:** Group robustness, Spurious correlation, Empirical risk minimization (ERM), Group distributional robust optimization (GroupDRO), GIC (Group Inference Classifier), Medical AI, Facial recognition, Sentiment analysis, Spurious correlations, Worst-group performance, Improved worst-group accuracy, Precise group inference, Semantic consistency


- [Sign is Not a Remedy: Multiset-to-Multiset Message Passing for Learning on Heterophilic Graphs](https://icml.cc/virtual/2024/poster/33563) (Poster)
  - **Authors:** [Langzhang Liang](http://openreview.net/profile?id=~Langzhang_Liang1), [Sunwoo Kim](http://openreview.net/profile?id=~Sunwoo_Kim4), [Kijung Shin](http://openreview.net/profile?id=~Kijung_Shin2), [Zenglin Xu](http://openreview.net/profile?id=~Zenglin_Xu2), [Shirui Pan](http://openreview.net/profile?id=~Shirui_Pan1), [Yuan Qi](http://openreview.net/profile?id=~Yuan_Qi2)
  - **Affiliations:** Harbin Institute of Technology, Shenzhen, Shenzhen, China, Korea Advanced Institute of Science and Technology, Seoul, South Korea, Korea Advanced Institute of Science and Technology, Seoul, South Korea, Fudan University, Shanghai, China; Shanghai Academy of Artificial Intelligence for Science, Shanghai, China, Pengcheng Laboratory, Shenzhen, China; Griffith University, Gold Coast, Australia, Fudan University, Shanghai, China; Shanghai Academy of Artificial Intelligence for Science, Shanghai, China
  - **TL;DR:** This study addresses the limitations of Signed Message Passing (SMP) in Graph Neural Networks (GNNs) for heterophilic graphs by proposing a new message-passing function called Multiset to Multiset GNN (M2M-GNN), which effectively mitigates issues like oversmoothing and undesirable representation updates. The findings demonstrate that M2M-GNN outperforms SMP, providing superior performance in learning on heterophilic graphs.
  - **Keywords:** Graph Neural Networks (GNNs), Heterophilic Graphs, Signed Message Passing (SMP), Multiset to Multiset GNN (M2M-GNN), Social Networks, Biomedical Networks, Oversmoothing, Representation Update for Multi-hop Neighbors, Novel message-passing function (M2M-GNN), Theoretical analysis of SMP limitations


- [BayOTIDE: Bayesian Online Multivariate Time Series Imputation with Functional Decomposition](https://icml.cc/virtual/2024/poster/33693) (Spotlight Poster)
  - **Authors:** [Shikai Fang](http://openreview.net/profile?id=~Shikai_Fang2), [Qingsong Wen](http://openreview.net/profile?id=~Qingsong_Wen2), [Yingtao Luo](http://openreview.net/profile?id=~Yingtao_Luo1), [Shandian Zhe](http://openreview.net/profile?id=~Shandian_Zhe1), [Liang Sun](http://openreview.net/profile?id=~Liang_Sun2)
  - **Affiliations:** University of Utah, USA, DAMO Academy, Alibaba Group; Squirrel AI, USA, Carnegie Mellon University, USA, University of Utah, USA, DAMO Academy, Alibaba Group
  - **TL;DR:** The study introduces BayOTIDE, a Bayesian online method for imputing missing values in multivariate time series data, addressing challenges such as irregular sampling and local horizon limitations. The method utilizes Gaussian Processes and offers computational efficiency, uncertainty quantification, and interpretability for real-world applications.
  - **Keywords:** time series imputation, multivariate time series, Bayesian methods, Gaussian Processes, state-space models, stochastic differential equations, traffic management, energy management, missing values, noise, irregular sampling patterns, local horizon limitations, BayOTIDE method, online inference algorithm, uncertainty quantification, interpretability


- [Block Acceleration Without Momentum: On Optimal Stepsizes of Block Gradient Descent for Least-Squares](https://icml.cc/virtual/2024/poster/33357) (Spotlight Poster)
  - **Authors:** [Liangzu Peng](http://openreview.net/profile?id=~Liangzu_Peng2), [Wotao Yin](http://openreview.net/profile?id=~Wotao_Yin1)
  - **Affiliations:** Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA 19104, USA, Decision Intelligence Lab, DAMO Academy, Alibaba Group US, Bellevue, WA 98004, USA
  - **TL;DR:** This study investigates block gradient descent (BGD) for least-squares optimization, demonstrating that optimal stepsizes can lead to convergence rates twice as fast as traditional gradient descent with momentum. The findings suggest that tuning stepsizes alone can effectively accelerate BGD without the need for additional momentum.
  - **Keywords:** Block coordinate descent, big data optimization, Block gradient descent (BGD), gradient descent (GD), optimal stepsizes, Least-squares optimization, generalized alternating projection, Convergence rates, empirical superiority of BGD over GD, Asymptotic convergence rates, optimal stepsizes for BGD, Lipschitz smoothness constant, Polyak’s momentum, spectral radius


- [ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models](https://icml.cc/virtual/2024/poster/34112) (Poster)
  - **Authors:** [Rohan Wadhawan](http://openreview.net/profile?id=~Rohan_Wadhawan1), [Hritik Bansal](http://openreview.net/profile?id=~Hritik_Bansal2), [Kai-Wei Chang](http://openreview.net/profile?id=~Kai-Wei_Chang1), [Nanyun Peng](http://openreview.net/profile?id=~Nanyun_Peng1)
  - **Affiliations:** Department of Computer Science, University of California Los Angeles, USA, Department of Computer Science, University of California Los Angeles, USA, Department of Computer Science, University of California Los Angeles, USA, Department of Computer Science, University of California Los Angeles, USA
  - **TL;DR:** This study introduces CONTEXTUAL, a dataset designed for evaluating context-sensitive text-rich visual reasoning in large multimodal models. The findings reveal a significant performance gap between the best-performing model, GPT-4V, and human evaluators, highlighting challenges in interpreting time-related data and infographics.
  - **Keywords:** context-sensitive reasoning, multimodal models, visual reasoning, text-rich images, lack of datasets for benchmarking, performance gap in model responses, introduction of CONTEXTUAL dataset, human performance baseline, CONTEXTUAL, Large Multimodal Models, GPT-4V, Gemini-Pro-Vision, LLaV A-Next


- [Toward Adaptive Reasoning in Large Language Models with Thought Rollback](https://icml.cc/virtual/2024/poster/33669) (Poster)
  - **Authors:** [Sijia Chen](http://openreview.net/profile?id=~Sijia_Chen2), [Baochun Li](http://openreview.net/profile?id=~Baochun_Li1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Toronto, Toronto, Ontario, Canada, Department of Electrical and Computer Engineering, University of Toronto, Toronto, Ontario, Canada
  - **TL;DR:** This paper introduces Thought Rollback (TR), a new reasoning framework for large language models that allows adaptive thought structure adjustments to improve problem-solving under conditions of hallucinations. Experiments show that GPT-4 with TR achieves a 9% higher solving rate on the MATH dataset compared to existing methods.
  - **Keywords:** Adaptive reasoning, Large language models, Thought Rollback (TR), error analysis, Mathematical problem-solving, multi-task reasoning, Hallucinations, error propagation, Improved problem-solving rate, interaction cost efficiency, MATH dataset


- [Effective Federated Graph Matching](https://icml.cc/virtual/2024/poster/32950) (Poster)
  - **Authors:** [Yang Zhou](http://openreview.net/profile?id=~Yang_Zhou4), [Zijie Zhang](http://openreview.net/profile?id=~Zijie_Zhang1), [Zeru Zhang](http://openreview.net/profile?id=~Zeru_Zhang1), [Lingjuan Lyu](http://openreview.net/profile?id=~Lingjuan_Lyu1), [Wei-Shinn Ku](http://openreview.net/profile?id=~Wei-Shinn_Ku1)
  - **Affiliations:** Auburn University, USA, Auburn University, USA, Auburn University, USA, Sony AI, Japan, Auburn University, USA
  - **TL;DR:** This paper presents an unsupervised federated graph matching algorithm (UFGM) that infers matched node pairs across different graphs while ensuring privacy. The proposed method utilizes graphlet theory and trust region optimization to address the challenges of unsupervised graph matching in federated learning settings.
  - **Keywords:** Federated Graph Learning, Graph Matching, Unsupervised Federated Graph Matching, Graphlet Theory, Trust Region Optimization, User Account Linking, Financial Crime Detection, Privacy Concerns, Unsupervised Graph Matching Dilemma, Pseudo Matched Node Pairs, Approximate Graphlet Enumeration, Trust Region Algorithm, SWIFT Transfer Data, Graphlet Features, Quasi-Newton Conditions


- [Iterated Denoising Energy Matching for Sampling from Boltzmann Densities](https://icml.cc/virtual/2024/poster/33422) (Poster)
  - **Authors:** [Tara Akhound-Sadegh](http://openreview.net/profile?id=~Tara_Akhound-Sadegh1), [Jarrid Rector-Brooks](http://openreview.net/profile?id=~Jarrid_Rector-Brooks2), [Joey Bose](http://openreview.net/profile?id=~Joey_Bose1), [Sarthak Mittal](http://openreview.net/profile?id=~Sarthak_Mittal1), [Pablo Lemos](http://openreview.net/profile?id=~Pablo_Lemos1), [Chenghao Liu](http://openreview.net/profile?id=~Cheng-Hao_Liu1), [Marcin Sendera](http://openreview.net/profile?id=~Marcin_Sendera1), [Siamak Ravanbakhsh](http://openreview.net/profile?id=~Siamak_Ravanbakhsh1), [Gauthier Gidel](http://openreview.net/profile?id=~Gauthier_Gidel1), [Yoshua Bengio](http://openreview.net/profile?id=~Yoshua_Bengio1), [Nikolay Malkin](http://openreview.net/profile?id=~Nikolay_Malkin1), [Alexander Tong](http://openreview.net/profile?id=~Alexander_Tong1)
  - **Affiliations:** Mila – Québec AI Institute; McGill University; Dreamfold, Mila – Québec AI Institute; Dreamfold; Université de Montréal, Mila – Québec AI Institute; Dreamfold; University of Oxford, Mila – Québec AI Institute; Université de Montréal, Mila – Québec AI Institute; Dreamfold; Ciela Institute; Center for Computational Astrophysics, Flatiron Institute, Mila – Québec AI Institute; McGill University; Dreamfold, Mila – Québec AI Institute; Université de Montréal; Jagiellonian University, Mila – Québec AI Institute; McGill University, Mila – Québec AI Institute; Université de Montréal, Mila – Québec AI Institute; Université de Montréal, Mila – Québec AI Institute; Université de Montréal, Mila – Québec AI Institute; Dreamfold; Université de Montréal
  - **TL;DR:** This paper introduces Iterated Denoising Energy Matching (iDEM), an innovative algorithm for efficiently sampling from unnormalized probability distributions, particularly in many-body systems. The method demonstrates state-of-the-art performance and significantly faster training times, enabling effective exploration of complex energy landscapes.
  - **Keywords:** Sampling from unnormalized probability distributions, Many-body systems, Iterated Denoising Energy Matching (iDEM), Stochastic score matching, Diffusion-based sampler, Equilibrium samples, Molecular systems, N-body particle systems, Efficiently generating statistically independent samples, Challenges in sampling with little to no initial samples, State-of-the-art performance, Faster training methods, Amortized sampler, Boltzmann-type distribution, Energy function, Mode mixing behavior


- [From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning](https://icml.cc/virtual/2024/poster/33570) (Poster)
  - **Authors:** [Wei Chen](http://openreview.net/profile?id=~Wei_Chen59), [Zhen Huang](http://openreview.net/profile?id=~Zhen_Huang4), [Liang Xie](http://openreview.net/profile?id=~Liang_Xie3), [Binbin Lin](http://openreview.net/profile?id=~Binbin_Lin3), [Houqiang Li](http://openreview.net/profile?id=~Houqiang_Li1), [Le Lu](http://openreview.net/profile?id=~Le_Lu3), [Xinmei Tian](http://openreview.net/profile?id=~Xinmei_Tian1), [CAI DENG](http://openreview.net/profile?id=~Deng_Cai4), [Yonggang Zhang](http://openreview.net/profile?id=~Yonggang_Zhang1), [Wenxiao Wang](http://openreview.net/profile?id=~Wenxiao_Wang2), [Xu Shen](http://openreview.net/profile?id=~Xu_Shen1), [Jieping Ye](http://openreview.net/profile?id=~Jieping_Ye4)
  - **Affiliations:** State Key Lab of CAD&CG, Zhejiang University; Alibaba Cloud, Alibaba Cloud, Zhejiang University of Technology, School of Software Technology, Zhejiang University; Fullong Inc., University of Science and Technology of China, Alibaba Cloud, University of Science and Technology of China, State Key Lab of CAD&CG, Zhejiang University, Hong Kong Baptist University, School of Software Technology, Zhejiang University, Alibaba Cloud, Alibaba Cloud
  - **TL;DR:** This study introduces a novel method called supervised pinpoint tuning (SPT) to address the sycophancy issue in large language models, which leads to inaccurate responses when challenged. The results demonstrate that SPT effectively mitigates sycophancy without degrading the general capabilities of the models.
  - **Keywords:** Large Language Models, Sycophancy, Supervised Fine-Tuning (SFT), Supervised Pinpoint Tuning (SPT), Reinforcement Learning from Human Feedback (RLHF), AI Assistants, Natural Language Processing, Sycophancy in AI responses, Inaccurate information admission, Mitigation of sycophancy, Improved targeted ability of LLMs, SycophancyEval datasets, Mistral Instruct, Llama-2 Chat


- [Easing Concept Bleeding in Diffusion via Entity Localization and Anchoring](https://icml.cc/virtual/2024/poster/34222) (Poster)
  - **Authors:** [Jiewei Zhang](http://openreview.net/profile?id=~Jiewei_Zhang1), [Song Guo](http://openreview.net/profile?id=~Song_Guo5), [Peiran Dong](http://openreview.net/profile?id=~Peiran_Dong1), [Jie ZHANG](http://openreview.net/profile?id=~Jie_ZHANG18), [Ziming Liu](http://openreview.net/profile?id=~Ziming_Liu1), [Yue Yu](http://openreview.net/profile?id=~Yue_Yu8), [Xiao-Ming Wu](http://openreview.net/profile?id=~Xiao-Ming_Wu1)
  - **Affiliations:** The Hong Kong Polytechnic University; Peng Cheng Laboratory, The Hong Kong University of Science and Technology, The Hong Kong Polytechnic University, The Hong Kong Polytechnic University, The Hong Kong Polytechnic University, Peng Cheng Laboratory, The Hong Kong Polytechnic University
  - **TL;DR:** This study addresses the issue of concept bleeding in text-to-image diffusion models by proposing a method called Entity Localization and Anchoring (ELA), which enhances the accuracy of object generation in complex scenes. The findings demonstrate that ELA effectively reduces the merging of distinct entities and improves the fidelity of generated images to user prompts.
  - **Keywords:** Concept bleeding, text-to-image synthesis, diffusion models, Entity Localization and Anchoring (ELA), cross-attention maps, Image generation, computer vision, Concept bleeding, missing entities, overlapping objects, Improved object generation accuracy, tailored loss function


- [Optimal Hessian/Jacobian-Free Nonconvex-PL Bilevel Optimization](https://icml.cc/virtual/2024/poster/33493) (Oral)
  - **Authors:** [Feihu Huang](http://openreview.net/profile?id=~Feihu_Huang1)
  - **Affiliations:** College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, China
  - **TL;DR:** This paper presents an efficient Hessian/Jacobian-free method (HJF-BiO) for solving nonconvex-PL bilevel optimization problems, achieving optimal convergence rates and gradient complexities. The proposed method addresses the high computational challenges associated with existing approaches, demonstrating its effectiveness through numerical experiments.
  - **Keywords:** bilevel optimization, nonconvex optimization, machine learning, Hessian-free methods, Jacobian-free methods, Polyak-Łojasiewicz condition, hyper-parameter learning, meta learning, reinforcement learning, high convergence complexity, high computation complexity, nonconvex lower-level problems, HJF-BiO method, optimal convergence rate, optimal gradient complexity


- [Differentiable Model Scaling using Differentiable Topk](https://icml.cc/virtual/2024/poster/33641) (Poster)
  - **Authors:** [Kai Liu](http://openreview.net/profile?id=~Kai_Liu16), [Ruohui Wang](http://openreview.net/profile?id=~Ruohui_Wang3), [Jianfei Gao](http://openreview.net/profile?id=~Jianfei_Gao1), [Kai Chen](http://openreview.net/profile?id=~Kai_Chen4)
  - **Affiliations:** Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China
  - **TL;DR:** This study presents Differentiable Model Scaling (DMS) to efficiently optimize the width and depth of neural networks, outperforming existing Neural Architecture Search methods. DMS demonstrates significant improvements in accuracy across various tasks, including image classification and object detection, while requiring minimal computational resources for searching.
  - **Keywords:** Differentiable Model Scaling, Neural Architecture Search, scaling networks, Differentiable Topk, gradient descent, structural hyperparameters, Image classification, object detection, language modeling, Low search efficiency, tuning structural hyperparameters, Improved network structures, enhanced accuracy, optimized search efficiency, ImageNet, COCO, Llama-7B, CNNs (Convolutional Neural Networks), Transformers, EfficientNet, Yolo-v8-n


- [Improving Neural Logic Machines via Failure Reflection](https://icml.cc/virtual/2024/poster/34379) (Poster)
  - **Authors:** [Zhiming Li](http://openreview.net/profile?id=~Zhiming_Li1), [Yushi Cao](http://openreview.net/profile?id=~Yushi_Cao1), [Yan Zheng](http://openreview.net/profile?id=~YAN_ZHENG1), [Xu Liu](http://openreview.net/profile?id=~Xu_Liu9), [Bozhi Wu](http://openreview.net/profile?id=~Bozhi_Wu1), [Tianlin Li](http://openreview.net/profile?id=~Tianlin_Li2), [Xiufeng Xu](http://openreview.net/profile?id=~Xiufeng_Xu1), [Junzhe Jiang](http://openreview.net/profile?id=~Junzhe_Jiang2), [Yon Shin Teo](http://openreview.net/profile?id=~Yon_Shin_Teo1), [Shang-Wei Lin](http://openreview.net/profile?id=~Shang-Wei_Lin1), [Yang Liu](http://openreview.net/profile?id=~Yang_Liu36)
  - **Affiliations:** Nanyang Technological University, Singapore, Nanyang Technological University, Singapore, Tianjin University, Tianjin, China, National University of Singapore, Singapore, Nanyang Technological University, Singapore, Nanyang Technological University, Singapore, Nanyang Technological University, Singapore, Hong Kong Polytechnic University, Hong Kong, Continental Automotive Singapore Pte. Ltd., Singapore, Nanyang Technological University, Singapore, Nanyang Technological University, Singapore
  - **TL;DR:** This study introduces the Failure Reflection Guided Regularizer (FRGR) to enhance the training of Neural Logic Machines (NLMs) by dynamically identifying and penalizing repeated mistakes, leading to improved performance and efficiency in reasoning tasks. Experimental results demonstrate FRGR's effectiveness in achieving better generalization and faster convergence.
  - **Keywords:** Neural-symbolic AI, Reasoning, Artificial General Intelligence, Neural Logic Machines (NLMs), Failure Reflection Guided Regularizer (FRGR), Relational reasoning, Decision-making tasks, Sub-optimal performance, Repeated mistakes during training, Ill-performed local optimum, Improved performance, Generalization, Training efficiency, Data efficiency, Horn clauses, Gradient-based optimization


- [A Nearly Optimal Single Loop Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness](https://icml.cc/virtual/2024/poster/35081) (Poster)
  - **Authors:** [Xiaochuan Gong](http://openreview.net/profile?id=~Xiaochuan_Gong1), [Jie Hao](http://openreview.net/profile?id=~Jie_Hao3), [Mingrui Liu](http://openreview.net/profile?id=~Mingrui_Liu2)
  - **Affiliations:** Department of Computer Science, George Mason University, USA, Department of Computer Science, George Mason University, USA, Department of Computer Science, George Mason University, USA
  - **TL;DR:** This paper presents a novel Single Loop bIlevel oPtimizer (SLIP) for stochastic bilevel optimization, addressing the challenges posed by unbounded smoothness in the upper-level function. The proposed algorithm demonstrates significant improvements in efficiency and performance compared to existing methods, achieving convergence to ϵ-stationary points with nearly optimal complexity.
  - **Keywords:** stochastic bilevel optimization, meta-learning, stochastic gradient descent, normalized stochastic gradient descent with momentum, text classification, machine learning, unbounded smoothness, nonconvex upper-level function, strongly convex lower-level function, Single Loop bIlevel oPtimizer (SLIP), ϵ-stationary point convergence, recurrent neural networks (RNNs), long-short-term memory networks (LSTMs), transformers


- [Energy-Efficient Gaussian Processes Using Low-Precision Arithmetic](https://icml.cc/virtual/2024/poster/32793) (Poster)
  - **Authors:** [Nicolas Alder](http://openreview.net/profile?id=~Nicolas_Alder1), [Ralf Herbrich](http://openreview.net/profile?id=~Ralf_Herbrich1)
  - **Affiliations:** Hasso Plattner Institute, Potsdam, Germany, Hasso Plattner Institute, Potsdam, Germany
  - **TL;DR:** This study proposes using low-precision arithmetic in Gaussian process regression to significantly reduce energy consumption while maintaining model performance. The findings indicate that a well-conditioned kernel matrix can lead to up to 89.01% reduction in energy use for the majority of arithmetic operations.
  - **Keywords:** energy-efficient AI, Gaussian process regression, low-precision floating-point representations, Cholesky decomposition, conjugate gradients, power consumption, energy efficiency, power reduction, efficiency gains, Gaussian processes, kernel matrix


- [Optimal Acceleration for Minimax and Fixed-Point Problems is Not Unique](https://icml.cc/virtual/2024/poster/33727) (Spotlight Poster)
  - **Authors:** [TaeHo Yoon](http://openreview.net/profile?id=~TaeHo_Yoon1), [Jaeyeon Kim](http://openreview.net/profile?id=~Jaeyeon_Kim2), [Jaewook Suh](http://openreview.net/profile?id=~Jaewook_J._Suh1), [Ernest Ryu](http://openreview.net/profile?id=~Ernest_K._Ryu1)
  - **Affiliations:** Department of Mathematical Sciences, Seoul National University, Department of Mathematical Sciences, Seoul National University, Department of Mathematical Sciences, Seoul National University, Department of Mathematics, University of California, Los Angeles
  - **TL;DR:** This study reveals that the optimal acceleration mechanism for minimax optimization and fixed-point problems is not unique, introducing new algorithms that achieve the same worst-case convergence rates as existing methods. The findings suggest a broader perspective on acceleration mechanisms, paving the way for future research in this area.
  - **Keywords:** minimax optimization, fixed-point problems, accelerated algorithms, anchoring mechanism, H-duality, optimal acceleration mechanisms, convergence rates, new algorithms, dual algorithms, family of acceleration methods


- [Privacy Profiles for Private Selection](https://icml.cc/virtual/2024/poster/33382) (Poster)
  - **Authors:** [Antti Koskela](http://openreview.net/profile?id=~Antti_Koskela1), [Rachel Redberg](http://openreview.net/profile?id=~Rachel_Emily_Redberg1), [Yu-Xiang Wang](http://openreview.net/profile?id=~Yu-Xiang_Wang1)
  - **Affiliations:** Nokia Bell Labs, Northeastern University, UC San Diego
  - **TL;DR:** This paper presents an improved privacy analysis for private hyperparameter tuning algorithms in machine learning, focusing on bounding privacy profiles of mechanisms like Report Noisy Max and Private Tuning. The findings indicate that the proposed approach enhances privacy accounting and offers significant benefits in practical applications.
  - **Keywords:** Differential Privacy, Private Selection Mechanisms, Hyperparameter Tuning, Report Noisy Max, Sparse Vector, Renyi Differential Privacy (RDP), Private Query Release, Voting, Machine Learning, Privacy Cost in Machine Learning, Hyperparameter Evaluation, Improved Privacy Analysis, Bounding Privacy Profiles, (ϵ, δ)-DP, f-DPs, Lipschitz-like Stability


- [LoRA Training in the NTK Regime has No Spurious Local Minima](https://icml.cc/virtual/2024/poster/32931) (Oral)
  - **Authors:** [Uijeong Jang](http://openreview.net/profile?id=~Uijeong_Jang1), [Jason Lee](http://openreview.net/profile?id=~Jason_D._Lee1), [Ernest Ryu](http://openreview.net/profile?id=~Ernest_K._Ryu1)
  - **Affiliations:** Department of Mathematical Sciences, Seoul National University, Department of Electrical and Computer Engineering, Princeton University, Department of Mathematics, University of California, Los Angeles
  - **TL;DR:** This study theoretically analyzes low-rank adaptation (LoRA) fine-tuning in the neural tangent kernel (NTK) regime, demonstrating that using LoRA with a sufficient rank eliminates spurious local minima and allows for effective learning of low-rank solutions that generalize well.
  - **Keywords:** Low-rank adaptation, Parameter-efficient fine-tuning, Large language models, Neural tangent kernel (NTK), Stochastic gradient descent, Spurious local minima, Efficient learning, Low-rank solutions, Generalization of low-rank solutions, LoRA (Low-rank adaptation), PEFT (Parameter-Efficient Fine-Tuning)


- [Quantum Algorithms and Lower Bounds for Finite-Sum Optimization](https://icml.cc/virtual/2024/poster/34684) (Poster)
  - **Authors:** [Yexin Zhang](http://openreview.net/profile?id=~Yexin_Zhang2), [Chenyi Zhang](http://openreview.net/profile?id=~Chenyi_Zhang2), [Cong Fang](http://openreview.net/profile?id=~Cong_Fang1), [Liwei Wang](http://openreview.net/profile?id=~Liwei_Wang1), [Tongyang Li](http://openreview.net/profile?id=~Tongyang_Li1)
  - **Affiliations:** School of Electronics Engineering and Computer Science, Peking University, China, Computer Science Department, Stanford University, USA, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Institute for Artificial Intelligence, Peking University, National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Institute for Artificial Intelligence, Peking University, Center on Frontiers of Computing Studies, Peking University; School of Computer Science, Peking University, China
  - **TL;DR:** This paper explores the application of quantum computing to finite-sum optimization problems, presenting a quantum algorithm that improves upon classical bounds and establishing quantum lower bounds for these problems. The findings suggest significant advancements in solving optimization challenges in machine learning and related fields.
  - **Keywords:** finite-sum optimization, quantum computing, quantum algorithms, stochastic gradient descent (SGD), variance reduction methods, machine learning, statistics, operations research, optimization of convex functions, finding approximate minima, improved quantum algorithm complexity, quantum lower bounds, ℓ-smooth convex functions, µ-strongly convex proximal function


- [Less is More: on the Over-Globalizing Problem in Graph Transformers](https://icml.cc/virtual/2024/poster/32833) (Oral)
  - **Authors:** [Yujie Xing](http://openreview.net/profile?id=~Yujie_Xing2), [Xiao Wang](http://openreview.net/profile?id=~Xiao_Wang2), [Yibo Li](http://openreview.net/profile?id=~Yibo_Li2), [Hai Huang](http://openreview.net/profile?id=~Hai_Huang9), [Chuan Shi](http://openreview.net/profile?id=~Chuan_Shi1)
  - **Affiliations:** School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China, School of Software, Beihang University, Beijing, China, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China
  - **TL;DR:** This study investigates the over-globalizing problem in Graph Transformers, revealing that the global attention mechanism may weaken the influence of nearby nodes. The authors propose a novel model, CoBFormer, which balances the extraction of information from both distant and nearby nodes while enhancing generalization through collaborative training.
  - **Keywords:** Graph Transformers, Global Attention Mechanism, Bi-Level Global Graph Transformer, Collaborative Training, Graph-structured data, Molecular property prediction, Over-globalizing problem, Over-smoothing, Over-squashing, CoBFormer, Improved generalization ability, Graph Neural Networks (GNNs), Attention Mechanism


- [Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models](https://icml.cc/virtual/2024/poster/32905) (Oral)
  - **Authors:** [Mina Dalirrooyfard](http://openreview.net/profile?id=~Mina_Dalirrooyfard1), [Konstantin Makarychev](http://openreview.net/profile?id=~Konstantin_Makarychev1), [Slobodan Mitrovic](http://openreview.net/profile?id=~Slobodan_Mitrovic1)
  - **Affiliations:** Morgan Stanley, Department of Computer Science, Northwestern University, Evanston, USA, Department of Computer Science, UC Davis, Davis, USA
  - **TL;DR:** This paper presents a novel approach to the correlation clustering problem, focusing on dynamic, parallel, and local computation models. The authors introduce a fully dynamic algorithm that operates in expected amortized constant time, improving upon existing methods while matching the approximation guarantee of the PIVOT algorithm.
  - **Keywords:** Correlation Clustering, Dynamic Graphs, Data Mining, Unsupervised Learning, PIVOT algorithm, Approximation algorithms, Community detection, Product categorization, Minimizing disagreement in clustering, Handling massive and dynamic graphs, Fully dynamic algorithm, Improved runtime complexities


- [Learning Modality Knowledge Alignment for Cross-Modality Transfer](https://icml.cc/virtual/2024/poster/33203) (Poster)
  - **Authors:** [Wenxuan Ma](http://openreview.net/profile?id=~Wenxuan_Ma2), [Shuang Li](http://openreview.net/profile?id=~Shuang_Li6), [Lincan Cai](http://openreview.net/profile?id=~Lincan_Cai1), [Jingxuan Kang](http://openreview.net/profile?id=~Jingxuan_Kang1)
  - **Affiliations:** Beijing Institute of Technology, Beijing, China, Beijing Institute of Technology, Beijing, China, Beijing Institute of Technology, Beijing, China, University of Illinois Urbana-Champaign, USA
  - **TL;DR:** This study investigates the challenges of cross-modality transfer, particularly the impact of modality gaps on knowledge reuse. It introduces a meta-learning approach called Modality Knowledge Alignment (MoNA) to enhance the effectiveness of knowledge transfer from pretrained models across different modalities.
  - **Keywords:** Cross-modality transfer, Knowledge alignment, Meta-learning, Conditional distribution, Modality gap, Knowledge misalignment, Data scarcity, Modality Knowledge Alignment (MoNA), Improved knowledge reuse


- [Stochastic Localization via Iterative Posterior Sampling](https://icml.cc/virtual/2024/poster/35113) (Spotlight Poster)
  - **Authors:** [Louis Grenioux](http://openreview.net/profile?id=~Louis_Grenioux1), [Maxence Noble](http://openreview.net/profile?id=~Maxence_Noble1), [Marylou Gabrié](http://openreview.net/profile?id=~Marylou_Gabri%C3%A91), [Alain Oliviero Durmus](http://openreview.net/profile?id=~Alain_Oliviero_Durmus1)
  - **Affiliations:** CMAP, CNRS, École polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France, CMAP, CNRS, École polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France, CMAP, CNRS, École polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France, CMAP, CNRS, École polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France
  - **TL;DR:** This paper introduces Stochastic Localization via Iterative Posterior Sampling (SLIPS), a methodology for sampling from unnormalized target densities using a stochastic localization framework. The approach leverages Markov chain Monte Carlo techniques and demonstrates effectiveness on various multi-modal distribution benchmarks.
  - **Keywords:** Stochastic localization, Sampling from unnormalized target density, Markov chain Monte Carlo (MCMC), Score-based learning, Denoising schedules, Bayesian statistics, Statistical mechanics, Generative modeling, High-dimensional sampling, Complex distributions, Stochastic Localization via Iterative Posterior Sampling (SLIPS), Approximate sampling methodology


- [Differentially Private Decentralized Learning with Random Walks](https://icml.cc/virtual/2024/poster/33273) (Poster)
  - **Authors:** [Edwige Cyffers](http://openreview.net/profile?id=~Edwige_Cyffers1), [Aurélien Bellet](http://openreview.net/profile?id=~Aur%C3%A9lien_Bellet1), [Jalaj Upadhyay](http://openreview.net/profile?id=~Jalaj_Upadhyay1)
  - **Affiliations:** Université de Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France, Inria, Univ Montpellier, Montpellier, France, Rutgers University
  - **TL;DR:** This study characterizes the privacy guarantees of decentralized learning using random walk algorithms, demonstrating that they provide better privacy protections than gossip algorithms for closely connected nodes. The findings highlight the importance of communication topology in ensuring data security in federated learning contexts.
  - **Keywords:** Federated Learning, Decentralized Learning, Privacy, Random Walk Algorithms, Pairwise Network Differential Privacy, Social Networks, Peer-to-Peer Communications, Privacy Attack Surface, Data Reconstruction Attacks, Privacy Guarantees, Empirical Evaluation, Differential Privacy, Local Differential Privacy


- [Towards Resource-friendly, Extensible and Stable Incomplete Multi-view Clustering](https://icml.cc/virtual/2024/poster/34180) (Spotlight Poster)
  - **Authors:** [Shengju Yu](http://openreview.net/profile?id=~Shengju_Yu1), [Dong Zhibin](http://openreview.net/profile?id=~Zhibin_Dong1), [Siwei Wang](http://openreview.net/profile?id=~Siwei_Wang4), [Xinhang Wan](http://openreview.net/profile?id=~Xinhang_Wan1), [Yue Liu](http://openreview.net/profile?id=~Yue_Liu10), [Weixuan Liang](http://openreview.net/profile?id=~Weixuan_Liang1), [Pei Zhang](http://openreview.net/profile?id=~Pei_Zhang9), [Wenxuan Tu](http://openreview.net/profile?id=~Wenxuan_Tu1), [Xinwang Liu](http://openreview.net/profile?id=~Xinwang_Liu1)
  - **Affiliations:** School of Computer, National University of Defense Technology, Changsha, Hunan, China, School of Computer, National University of Defense Technology, Changsha, Hunan, China, Intelligent Game and Decision Lab, Beijing, China, School of Computer, National University of Defense Technology, Changsha, Hunan, China, School of Computer, National University of Defense Technology, Changsha, Hunan, China, School of Computer, National University of Defense Technology, Changsha, Hunan, China, School of Computer, National University of Defense Technology, Changsha, Hunan, China, School of Computer, National University of Defense Technology, Changsha, Hunan, China, School of Computer, National University of Defense Technology, Changsha, Hunan, China
  - **TL;DR:** The study presents ToRES, a resource-friendly and effective method for incomplete multi-view clustering (IMVC) that addresses issues of high computational costs, hyper-parameter complexity, and result variance. The proposed method demonstrates advantages over 20 state-of-the-art algorithms, particularly in scenarios with a higher ratio of incomplete data.
  - **Keywords:** Incomplete Multi-view Clustering (IMVC), Multi-view Data (MVD), Prototype-sample affinity, Self-expression affinity, Cross-view prototypes, Intense time and/or space overheads, Intractable hyper-parameters, Non-zero variance results, Incomplete data, ToRES (new IMVC scheme), Optimization of discrete cluster indicators, State-of-the-art (SOTA) algorithms


- [MH-pFLID: Model Heterogeneous personalized Federated Learning via Injection and Distillation for Medical Data Analysis](https://icml.cc/virtual/2024/poster/34360) (Poster)
  - **Authors:** [Luyuan Xie](http://openreview.net/profile?id=~Luyuan_Xie1), [Manqing Lin](http://openreview.net/profile?id=~Manqing_Lin1), [Tianyu Luan](http://openreview.net/profile?id=~Tianyu_Luan1), [Cong Li](http://openreview.net/profile?id=~Cong_Li5), [Yuejian Fang](http://openreview.net/profile?id=~Yuejian_Fang1), [Qingni Shen](http://openreview.net/profile?id=~Qingni_Shen1), [Zhonghai Wu](http://openreview.net/profile?id=~Zhonghai_Wu1)
  - **Affiliations:** School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China; PKU-OCTA Laboratory for Blockchain and Privacy Computing, Peking University, Beijing, China, School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China; PKU-OCTA Laboratory for Blockchain and Privacy Computing, Peking University, Beijing, China, State University of New York at Buffalo, NY, United States, School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China; PKU-OCTA Laboratory for Blockchain and Privacy Computing, Peking University, Beijing, China, School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China; PKU-OCTA Laboratory for Blockchain and Privacy Computing, Peking University, Beijing, China, School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China; PKU-OCTA Laboratory for Blockchain and Privacy Computing, Peking University, Beijing, China, School of Software and Microelectronics, Peking University, Beijing, China; National Engineering Research Center for Software Engineering, Peking University, Beijing, China; PKU-OCTA Laboratory for Blockchain and Privacy Computing, Peking University, Beijing, China
  - **TL;DR:** This paper introduces a novel federated learning framework called MH-pFLID, which addresses challenges in aggregating information from heterogeneous clients without requiring public datasets. The proposed method demonstrates superior performance in various medical tasks, highlighting its efficiency and generalizability.
  - **Keywords:** Federated Learning, Medical Data Analysis, Knowledge Distillation, Model Injection, Image Classification, Image Segmentation, Time-Series Classification, System Heterogeneity, Non-IID Data, Privacy Concerns, Model Heterogeneous Personalized Federated Learning (MH-pFLID), Lightweight Messenger Model


- [ESM All-Atom: Multi-Scale Protein Language Model for Unified Molecular Modeling](https://icml.cc/virtual/2024/poster/35119) (Poster)
  - **Authors:** [Kangjie Zheng](http://openreview.net/profile?id=~Kangjie_Zheng1), [Siyu Long](http://openreview.net/profile?id=~Siyu_Long1), [Tianyu Lu](http://openreview.net/profile?id=~Tianyu_Lu2), [Junwei Yang](http://openreview.net/profile?id=~Junwei_Yang2), [Xinyu Dai](http://openreview.net/profile?id=~Xinyu_Dai1), [Ming Zhang](http://openreview.net/profile?id=~Ming_Zhang5), [Zaiqing Nie](http://openreview.net/profile?id=~Zaiqing_Nie2), [Wei-Ying Ma](http://openreview.net/profile?id=~Wei-Ying_Ma2), [Hao Zhou](http://openreview.net/profile?id=~Hao_Zhou5)
  - **Affiliations:** School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, Beijing 100871, China, School of Artificial Intelligence, National Key Laboratory for Novel Software Technology, Nanjing University, Department of Computer Science, Tsinghua University, School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, Beijing 100871, China, School of Artificial Intelligence, National Key Laboratory for Novel Software Technology, Nanjing University, School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, Beijing 100871, China, Institute for AI Industry Research (AIR), Tsinghua University; PharMolix Inc., Institute for AI Industry Research (AIR), Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University
  - **TL;DR:** This paper introduces ESM-AA, a novel protein language model that integrates atom-scale and residue-scale molecular modeling to enhance the understanding and application of protein interactions with small molecules. Experimental results show that ESM-AA outperforms previous methods in protein-molecule tasks, demonstrating its effectiveness in capturing molecular knowledge while retaining protein understanding.
  - **Keywords:** protein language models, molecular modeling, multi-scale code-switch protein sequences, multi-scale position encoding, protein engineering, drug discovery, enzyme engineering, limitations of residue-scale modeling, need for atom-scale information, ESM-AA model, unified molecular modeling


- [Mol-AE: Auto-Encoder Based Molecular Representation Learning With 3D Cloze Test Objective](https://icml.cc/virtual/2024/poster/33340) (Poster)
  - **Authors:** [Junwei Yang](http://openreview.net/profile?id=~Junwei_Yang2), [Kangjie Zheng](http://openreview.net/profile?id=~Kangjie_Zheng1), [Siyu Long](http://openreview.net/profile?id=~Siyu_Long1), [Zaiqing Nie](http://openreview.net/profile?id=~Zaiqing_Nie2), [Ming Zhang](http://openreview.net/profile?id=~Ming_Zhang5), [Xinyu Dai](http://openreview.net/profile?id=~Xinyu_Dai1), [Wei-Ying Ma](http://openreview.net/profile?id=~Wei-Ying_Ma2), [Hao Zhou](http://openreview.net/profile?id=~Hao_Zhou5)
  - **Affiliations:** School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, School of Artificial Intelligence, National Key Laboratory for Novel Software Technology, Nanjing University, Institute for AI Industry Research (AIR), Tsinghua University; PharMolix Inc., School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University, School of Artificial Intelligence, National Key Laboratory for Novel Software Technology, Nanjing University, Institute for AI Industry Research (AIR), Tsinghua University, Institute for AI Industry Research (AIR), Tsinghua University
  - **TL;DR:** The study introduces MOL-AE, an auto-encoder model for 3D molecular representation learning that addresses issues with existing encoder-only models and coordinate denoising objectives. The proposed 3D Cloze Test objective enhances the model's ability to learn spatial relationships among atoms, resulting in significant performance improvements over state-of-the-art methods.
  - **Keywords:** 3D molecular representation learning, molecular understanding, Auto-encoder model, coordinate denoising, positional encoding, Drug discovery, molecular property prediction, reaction prediction, Inconsistency between pre-training and downstream objectives, disrupted atomic identifiers, MOL-AE model, 3D Cloze Test objective, performance gain in molecular modeling


- [Decouple then Classify: A Dynamic Multi-view Labeling Strategy with Shared and Specific Information](https://icml.cc/virtual/2024/poster/33857) (Poster)
  - **Authors:** [Xinhang Wan](http://openreview.net/profile?id=~Xinhang_Wan1), [Jiyuan Liu](http://openreview.net/profile?id=~Jiyuan_Liu1), [Xinwang Liu](http://openreview.net/profile?id=~Xinwang_Liu1), [Yi Wen](http://openreview.net/profile?id=~Yi_Wen1), [Hao Yu](http://openreview.net/profile?id=~Hao_Yu13), [Siwei Wang](http://openreview.net/profile?id=~Siwei_Wang4), [Shengju Yu](http://openreview.net/profile?id=~Shengju_Yu1), [Tianjiao Wan](http://openreview.net/profile?id=~Tianjiao_Wan1), [Jun Wang](http://openreview.net/profile?email=wang_jun%40nudt.edu.cn), [En Zhu](http://openreview.net/profile?id=~En_Zhu1)
  - **Affiliations:** College of Computer, National University of Defense Technology, Changsha, China, College of Systems Engineering, National University of Defense Technology, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China, Intelligent Game and Decision Lab, Academy of Military Sciences, Beijing, China, College of Computer, National University of Defense Technology, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China
  - **TL;DR:** This study introduces a Dynamic Multi-view Labeling Strategy that effectively utilizes shared and specific information to improve sample labeling in semi-supervised learning. The proposed method demonstrates promising performance through adaptive updates of classifiers, addressing the challenges of labeling costs and classification confidence in multi-view data settings.
  - **Keywords:** semi-supervised learning, multi-view data labeling, Dynamic Multi-view Labeling Strategy, classifiers, recommendation systems, labeling cost, classification confidence, data sparsity, adaptive classifier updates, improved labeling strategy, shared and specific information (SSI), semi-supervised multi-view (SSMV)


- [Adaptive Robust Learning using Latent Bernoulli Variables](https://icml.cc/virtual/2024/poster/32797) (Poster)
  - **Authors:** [Aleksandr Karakulev](http://openreview.net/profile?id=~Aleksandr_Karakulev1), [Dave Zachariah](http://openreview.net/profile?id=~Dave_Zachariah1), [Prashant Singh](http://openreview.net/profile?id=~Prashant_Singh1)
  - **Affiliations:** Uppsala University, Sweden; Science for Life Laboratory, Sweden, Uppsala University, Sweden, Uppsala University, Sweden; Science for Life Laboratory, Sweden
  - **TL;DR:** This paper presents an adaptive robust learning method that identifies corrupted and non-corrupted samples using latent Bernoulli variables, improving prediction accuracy in the presence of noise. The approach is scalable, parameter-free, and applicable to various machine learning tasks, demonstrating superior performance over traditional methods.
  - **Keywords:** robust learning, corrupted training sets, adaptive learning, latent Bernoulli variables, variational inference, Expectation-Maximization, online learning, deep learning, data corruption, noise adaptation, automatic corruption level inference, parameter-free learning, HAR dataset, Huber contamination model


- [Discovering Multiple Solutions from a Single Task in Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/33323) (Poster)
  - **Authors:** [Takayuki Osa](http://openreview.net/profile?id=~Takayuki_Osa1), [Tatsuya Harada](http://openreview.net/profile?id=~Tatsuya_Harada1)
  - **Affiliations:** The University of Tokyo, RIKEN Center for Advanced Intelligence Project, The University of Tokyo, RIKEN Center for Advanced Intelligence Project
  - **TL;DR:** This study proposes algorithms for discovering multiple solutions in offline reinforcement learning by unsupervised learning of latent skill representations. The experimental results indicate that the proposed method effectively learns qualitatively and quantitatively distinct solutions, facilitating few-shot adaptation to new environments.
  - **Keywords:** offline reinforcement learning, discovering diverse solutions, latent-conditioned policy, coordinate ascent approach, expectation-maximization (EM) algorithm, locomotion tasks, few-shot adaptation, learning multiple solutions, learning latent skill space, algorithms for learning multiple solutions, unsupervised learning of latent skill representations, D4RL framework, MuJoCo


- [Confidence Aware Inverse Constrained Reinforcement Learning](https://icml.cc/virtual/2024/poster/34920) (Poster)
  - **Authors:** [Sriram Ganapathi Subramanian](http://openreview.net/profile?id=~Sriram_Ganapathi_Subramanian1), [Guiliang Liu](http://openreview.net/profile?id=~Guiliang_Liu1), [Mohammed Elmahgiubi](http://openreview.net/profile?id=~Mohammed_Elmahgiubi1), [Kasra Rezaee](http://openreview.net/profile?id=~Kasra_Rezaee1), [Pascal Poupart](http://openreview.net/profile?id=~Pascal_Poupart2)
  - **Affiliations:** Vector Institute for Artificial Intelligence, Toronto, Canada, School of Data Science, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China, Huawei Technologies Canada, Huawei Technologies Canada, Cheriton School of Computer Science, University of Waterloo, Canada
  - **TL;DR:** This paper presents a confidence-aware method for Inverse Constrained Reinforcement Learning (CA-ICRL) that allows practitioners to learn constraints from expert demonstrations with a specified level of confidence. The method enables users to determine if they have sufficient expert trajectories to achieve the desired confidence and performance in policy learning.
  - **Keywords:** Inverse Constraint Reinforcement Learning, Reinforcement Learning, Confidence-aware ICRL, Constraint functions, Autonomous driving, Robotics, Learning underlying constraints, Estimating confidence in constraints, New algorithm for ICRL, Confidence estimation in constraints


- [Bayesian Exploration Networks](https://icml.cc/virtual/2024/poster/34156) (Poster)
  - **Authors:** [Mattie Fellows](http://openreview.net/profile?id=~Mattie_Fellows1), [Brandon Kaplowitz](http://openreview.net/profile?id=~Brandon_Gary_Kaplowitz1), [Christian Schroeder de Witt](http://openreview.net/profile?id=~Christian_Schroeder_de_Witt1), [Shimon Whiteson](http://openreview.net/profile?id=~Shimon_Whiteson1)
  - **Affiliations:** Department of Engineering Science, University of Oxford, Oxford, United Kingdom, Department of Economics, New York University, New York, United States of America, Department of Computer Science, University of Oxford, Oxford, United Kingdom, Department of Computer Science, University of Oxford, Oxford, United Kingdom
  - **TL;DR:** This paper introduces a novel Bayesian model-free formulation, the Bayesian exploration network (BEN), which addresses the exploration/exploitation dilemma in reinforcement learning and demonstrates that existing model-free approaches can yield Bayes-optimal policies. Empirical results show that BEN can learn true Bayes-optimal policies in scenarios where traditional methods fail.
  - **Keywords:** Bayesian reinforcement learning, model-free approaches, Bayesian exploration network, normalising flows, density estimation, variational inference, Bellman operator, Exploration/exploitation dilemma, intractability in learning Bayes-optimal policies, Bayes-optimal policies, theoretical analysis of model-free approaches, Markov decision process (MDP), aleatoric uncertainty, epistemic uncertainty


- [Flextron: Many-in-One Flexible Large Language Model](https://icml.cc/virtual/2024/poster/34793) (Oral)
  - **Authors:** [Ruisi Cai](http://openreview.net/profile?id=~Ruisi_Cai1), [Saurav Muralidharan](http://openreview.net/profile?id=~Saurav_Muralidharan1), [Greg Heinrich](http://openreview.net/profile?id=~Greg_Heinrich1), [Hongxu Yin](http://openreview.net/profile?id=~Hongxu_Yin2), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1), [Jan Kautz](http://openreview.net/profile?id=~Jan_Kautz1), [Pavlo Molchanov](http://openreview.net/profile?id=~Pavlo_Molchanov1)
  - **Affiliations:** NVIDIA; The University of Texas at Austin, NVIDIA, NVIDIA, NVIDIA, The University of Texas at Austin, NVIDIA, NVIDIA
  - **TL;DR:** The paper introduces FLEXTRON, a flexible architecture for large language models that optimizes performance and efficiency without additional fine-tuning. It demonstrates superior adaptability and resource efficiency compared to existing models, achieving significant performance improvements with minimal token consumption during pretraining.
  - **Keywords:** Large Language Models, Model Optimization, Nested Elastic Structure, Input-Adaptive Routing, Sample-Efficient Training, Natural Language Processing, Resource Intensity, Customization for Limited Resources, FLEXTRON Framework, Superior Performance over Variants, GPT-3, Llama-2


- [MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](https://icml.cc/virtual/2024/poster/33545) (Oral)
  - **Authors:** [Dongping Chen](http://openreview.net/profile?id=~Dongping_Chen1), [Ruoxi Chen](http://openreview.net/profile?id=~Ruoxi_Chen1), [Shilin Zhang](http://openreview.net/profile?id=~Shilin_Zhang2), [Yaochen Wang](http://openreview.net/profile?id=~Yaochen_Wang1), [Yinuo Liu](http://openreview.net/profile?id=~Yinuo_Liu1), [Huichi Zhou](http://openreview.net/profile?id=~Huichi_Zhou1), [Qihui Zhang](http://openreview.net/profile?id=~Qihui_Zhang1), [Yao Wan](http://openreview.net/profile?id=~Yao_Wan2), [Pan Zhou](http://openreview.net/profile?id=~Pan_Zhou5), [Lichao Sun](http://openreview.net/profile?id=~Lichao_Sun1)
  - **Affiliations:** Huazhong University of Science and Technology, Zhejiang University of Technology, Huazhong University of Science and Technology, Huazhong University of Science and Technology, Huazhong University of Science and Technology, Huazhong University of Science and Technology, Huazhong University of Science and Technology, Huazhong University of Science and Technology, Huazhong University of Science and Technology; LAIR Lab, Lehigh University, LAIR Lab, Lehigh University
  - **TL;DR:** This study introduces the MLLM-as-a-Judge benchmark to evaluate the effectiveness of Multimodal Large Language Models (MLLMs) in assisting judges across various tasks. The findings indicate that while MLLMs show human-like discernment in pair comparisons, they significantly diverge from human preferences in scoring evaluations and batch rankings, highlighting the need for further improvements in their judgment capabilities.
  - **Keywords:** Multimodal Large Language Models, Evaluation of MLLMs, LLM-as-a-Judge, Scoring Evaluation, Pair Comparison, Batch Ranking, Artificial General Intelligence, Multimodal Applications, Assessment challenges of MLLMs, Human preference alignment, MLLM-as-a-Judge benchmark, Insights into judgment capacities of LLMs, 14 datasets, 4,414 image-instruction pairs, GPT-4V, LLaVA, Chain-of-Thought (CoT), Hallucinatory responses, Biases in judgment


- [Solving Poisson Equations using Neural Walk-on-Spheres](https://icml.cc/virtual/2024/poster/33557) (Poster)
  - **Authors:** [Hong Chul Nam](http://openreview.net/profile?id=~Hong_Chul_Nam1), [Julius Berner](http://openreview.net/profile?id=~Julius_Berner1), [Anima Anandkumar](http://openreview.net/profile?id=~Anima_Anandkumar1)
  - **Affiliations:** ETH Zurich, Caltech, Caltech
  - **TL;DR:** This paper introduces Neural Walk-on-Spheres (NWoS), a novel method for efficiently solving high-dimensional Poisson equations using deep learning techniques. The method demonstrates significant improvements in accuracy, speed, and computational efficiency compared to existing approaches like PINNs and the Deep Ritz method.
  - **Keywords:** neural PDE solver, high-dimensional Poisson equations, deep learning, Neural Walk-on-Spheres (NWoS), physics-informed neural networks (PINNs), Deep Ritz method, stochastic differential equations, geometry processing, theoretical physics, molecular dynamics, high-dimensional data, computational costs, curse of dimensionality, novel losses for neural networks, efficient solution methods


- [Attack-free Evaluating and Enhancing Adversarial Robustness on Categorical Data](https://icml.cc/virtual/2024/poster/34854) (Poster)
  - **Authors:** [Yujun Zhou](http://openreview.net/profile?id=~Yujun_Zhou1), [Yufei Han](http://openreview.net/profile?id=~Yufei_Han1), [Haomin Zhuang](http://openreview.net/profile?id=~Haomin_Zhuang1), [Hongyan Bao](http://openreview.net/profile?id=~Hongyan_Bao1), [Xiangliang Zhang](http://openreview.net/profile?id=~Xiangliang_Zhang1)
  - **Affiliations:** Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA, INRIA, France, Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia, Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA
  - **TL;DR:** This study focuses on evaluating and enhancing the adversarial robustness of classification models on categorical data using an attack-free approach. The proposed Integrated Gradient-Smoothed Gradient (IGSG) metric and its associated regularization method significantly improve adversarial accuracy compared to existing robust training methods.
  - **Keywords:** Adversarial robustness, Categorical data, Integrated Gradient-Smoothed Gradient (IGSG), Regularization, Safety-critical applications, Classification, Adversarial perturbations, Sensitivity of categorical attributes, Robust overfitting, IGSG-based regularization, Enhanced adversarial accuracy


- [ELF: Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis](https://icml.cc/virtual/2024/poster/33936) (Poster)
  - **Authors:** [Jungil Kong](http://openreview.net/profile?id=~Jungil_Kong3), [Junmo Lee](http://openreview.net/profile?id=~Junmo_Lee1), [Jeongmin Kim](http://openreview.net/profile?id=~Jeongmin_Kim2), [Beomjeong Kim](http://openreview.net/profile?id=~Beomjeong_Kim1), [JIHOON PARK](http://openreview.net/profile?id=~JIHOON_PARK2), [Dohee Kong](http://openreview.net/profile?id=~Dohee_Kong1), [Changheon Lee](http://openreview.net/profile?id=~Changheon_Lee1), [Sangjin Kim](http://openreview.net/profile?id=~Sangjin_Kim1)
  - **Affiliations:** SK Telecom, Jung-gu, Seoul, Republic of Korea, None, None, None, None, None, None, None
  - **TL;DR:** This study presents a novel method for modeling numerous speakers in speech synthesis without requiring additional training on target speaker datasets. The proposed method significantly improves similarity scores and demonstrates the ability to generate new artificial speakers while effectively reconstructing original speech characteristics.
  - **Keywords:** speaker modeling, speech synthesis, feature learning, speaker embedding, zero-shot speech synthesis, speech synthesis, multi-speaker models, limitations of training processes, modeling numerous speakers, overfitting, underfitting, higher similarity mean opinion score (SMOS), generation of new artificial speakers, reconstruction of original speaker's speech


- [Bridging Environments and Language with Rendering Functions and Vision-Language Models](https://icml.cc/virtual/2024/poster/33722) (Poster)
  - **Authors:** [Théo Cachet](http://openreview.net/profile?id=~Theo_Cachet1), [Christopher Dance](http://openreview.net/profile?id=~Christopher_R_Dance1), [Olivier Sigaud](http://openreview.net/profile?id=~Olivier_Sigaud1)
  - **Affiliations:** NA VER LABS Europe, Meylan; Institute of Intelligent Systems and Robotics, Sorbonne University, Paris, NA VER LABS Europe, Meylan, Institute of Intelligent Systems and Robotics, Sorbonne University, Paris
  - **TL;DR:** This paper presents a novel approach to building language-conditioned agents (LCAs) by first identifying environment configurations that yield high scores from vision-language models (VLMs) for specified tasks, followed by using a goal-conditioned reinforcement learning policy to reach those configurations. The proposed method demonstrates superior performance in zero-shot generalization compared to multi-task reinforcement learning baselines, without requiring textual task descriptions during training.
  - **Keywords:** language-conditioned agents (LCAs), vision-language models (VLMs), reinforcement learning (RL), multi-task reinforcement learning (MTRL), goal-conditioned reinforcement learning (GCRL), distilled models, data scarcity, training policy for new tasks, generalization to new tasks, novel decomposition of LCA problem, high VLM score configurations, zero-shot generalization


- [FedLMT: Tackling System Heterogeneity of Federated Learning via Low-Rank Model Training with Theoretical Guarantees](https://icml.cc/virtual/2024/poster/33672) (Poster)
  - **Authors:** [Jiahao Liu](http://openreview.net/profile?id=~Jiahao_Liu8), [Yipeng Zhou](http://openreview.net/profile?id=~Yipeng_Zhou1), [Di Wu](http://openreview.net/profile?id=~Di_Wu21), [Miao Hu](http://openreview.net/profile?id=~Miao_Hu2), [Mohsen Guizani](http://openreview.net/profile?email=mguizani%40ieee.org), [Quan Sheng](http://openreview.net/profile?id=~Quan_Z._Sheng1)
  - **Affiliations:** School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China, School of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, Australia, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE, School of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, Australia
  - **TL;DR:** This study introduces the FedLMT framework to address system heterogeneity in federated learning by utilizing a homogeneous low-rank model, which reduces resource consumption and avoids performance degradation from heterogeneous model aggregation. The proposed methods demonstrate improved efficiency and robustness in federated learning settings compared to existing approaches.
  - **Keywords:** Federated Learning, System Heterogeneity, Low-Rank Model Training, Model Pruning, Knowledge Distillation, Resource Capability Heterogeneity, Performance Degradation in Global Models, FedLMT Framework, pFedLMT Extension, Convergence Guarantees


- [Towards the Theory of Unsupervised Federated Learning: Non-asymptotic Analysis of Federated EM Algorithms](https://icml.cc/virtual/2024/poster/33251) (Poster)
  - **Authors:** [Ye Tian](http://openreview.net/profile?id=~Ye_Tian9), [Haolei Weng](http://openreview.net/profile?email=wenghaol%40msu.edu), [Yang Feng](http://openreview.net/profile?id=~Yang_Feng5)
  - **Affiliations:** Department of Statistics, Columbia University, New York, USA, Department of Statistics and Probability, Michigan State University, East Lansing, USA, Department of Biostatistics, School of Global Public Health, New York University, New York, USA
  - **TL;DR:** This paper introduces a federated gradient EM algorithm (FedGrEM) for unsupervised learning of mixture models, addressing task heterogeneity and adversarial attacks. The authors present a finite-sample theory that demonstrates FedGrEM's superiority over existing unsupervised federated learning benchmarks.
  - **Keywords:** Unsupervised Federated Learning, Federated Learning, Federated EM algorithms, Federated gradient EM algorithm (FedGrEM), Task heterogeneity, Adversarial attacks, Mixture models, Finite-sample theory, Estimation error characterization, Mixture models, Statistical models


- [Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis](https://icml.cc/virtual/2024/poster/34765) (Poster)
  - **Authors:** [Shirin Shoushtari](http://openreview.net/profile?id=~Shirin_Shoushtari1), [JIAMING LIU](http://openreview.net/profile?id=~Jiaming_Liu3), [Edward Chandler](http://openreview.net/profile?id=~Edward_P._Chandler1), [Salman Asif](http://openreview.net/profile?id=~M._Salman_Asif1), [Ulugbek Kamilov](http://openreview.net/profile?id=~Ulugbek_S._Kamilov1)
  - **Affiliations:** Washington University in St. Louis, St. Louis, MO, USA, Washington University in St. Louis, St. Louis, MO, USA, Washington University in St. Louis, St. Louis, MO, USA, University of California, Riverside, CA, USA, Washington University in St. Louis, St. Louis, MO, USA
  - **TL;DR:** This paper investigates the impact of prior distribution mismatch in Plug-and-Play ADMM methods for imaging inverse problems, providing theoretical error bounds and demonstrating that performance gaps can be significantly reduced with minimal training samples from the desired distribution. The findings suggest that PnP-ADMM is relatively robust to prior distribution mismatch.
  - **Keywords:** Plug-and-Play (PnP) priors, distribution mismatch, domain adaptation, Alternating Direction Method of Multipliers (ADMM), deep denoisers, Imaging inverse problems, image super-resolution, Prior distribution mismatch, performance gap due to mismatched denoisers, Error bounds for PnP-ADMM, domain adaptation strategies


- [SAPG: Split and Aggregate Policy Gradients](https://icml.cc/virtual/2024/poster/35012) (Oral)
  - **Authors:** [Jayesh Singla](http://openreview.net/profile?id=~Jayesh_Singla1), [Ananye Agarwal](http://openreview.net/profile?id=~Ananye_Agarwal1), [Deepak Pathak](http://openreview.net/profile?id=~Deepak_Pathak1)
  - **Affiliations:** Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University
  - **TL;DR:** This paper introduces SAPG, a new on-policy reinforcement learning algorithm that effectively utilizes large-scale environments by splitting them into chunks and aggregating data through importance sampling. The proposed method significantly outperforms traditional approaches like PPO in various challenging environments.
  - **Keywords:** on-policy reinforcement learning, policy gradients, SAPG (Split and Aggregate Policy Gradients), PPO (Proximal Policy Optimization), sim2real robotic applications, gaming (StarCraft), sample inefficiency, performance saturation in RL methods, new on-policy RL algorithm, improved performance in challenging environments, IsaacGym, PhysX, Mujoco-3.0


- [Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling](https://icml.cc/virtual/2024/poster/34082) (Poster)
  - **Authors:** [Brooks(Ruijia) Niu](http://openreview.net/profile?id=~Ruijia_Niu1), [Dongxia Wu](http://openreview.net/profile?id=~Dongxia_Wu1), [Kai Kim](http://openreview.net/profile?id=~Kai_Kim1), [Yian Ma](http://openreview.net/profile?id=~Yian_Ma1), [Duncan Watson-Parris](http://openreview.net/profile?id=~Duncan_Watson-Parris1), [Rose Yu](http://openreview.net/profile?id=~Rose_Yu1)
  - **Affiliations:** Department of Computer Science and Engineering, University of California San Diego, La Jolla, California, USA, Department of Computer Science and Engineering, University of California San Diego, La Jolla, California, USA, Department of Computer Science and Engineering, University of California San Diego, La Jolla, California, USA, Halıcıoğlu Data Science Institute, University of California San Diego, La Jolla, California, USA, Halıcıoğlu Data Science Institute, University of California San Diego, La Jolla, California, USA; Scripps Institution of Oceanography, University of California San Diego, La Jolla, California, USA, Department of Computer Science and Engineering, University of California San Diego, La Jolla, California, USA
  - **TL;DR:** This study introduces Multi-fidelity Residual Neural Processes (MFRNP) to enhance surrogate modeling by effectively combining data from multiple fidelity levels, addressing scalability and inference performance issues. The proposed method significantly outperforms existing approaches in learning partial differential equations and real-world climate modeling tasks.
  - **Keywords:** Multi-fidelity surrogate modeling, Deep learning, Neural Processes, Multi-fidelity Residual Neural Processes (MFRNP), Climate modeling, Computational modeling, Scalability issues, Inference performance, Limited domain coverage, Improved inference performance, Accurate surrogate modeling, Gaussian Processes, Partial differential equations


- [Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts](https://icml.cc/virtual/2024/poster/33894) (Poster)
  - **Authors:** [Zhi-Yi Chin](http://openreview.net/profile?id=~Zhi-Yi_Chin1), [Chieh Ming Jiang](http://openreview.net/profile?id=~Chieh_Ming_Jiang1), [Ching-Chun Huang](http://openreview.net/profile?id=~Ching-Chun_Huang1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1), [Wei-Chen Chiu](http://openreview.net/profile?id=~Wei-Chen_Chiu3)
  - **Affiliations:** Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan, IBM Research, NY 10598, USA, Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan
  - **TL;DR:** This study introduces Prompting4Debugging (P4D), a tool designed to identify problematic prompts in text-to-image diffusion models, revealing vulnerabilities in existing safety mechanisms. The findings indicate that many prompts previously deemed "safe" can be manipulated to bypass these safety measures, highlighting the need for comprehensive testing in AI safety evaluations.
  - **Keywords:** text-to-image generation, generative AI, safety mechanisms, diffusion models, Prompting4Debugging (P4D), content generation, AI safety, misuse of generative technology, reliability of safety mechanisms, bypassing safety filters, uncovering vulnerabilities in safety mechanisms, testing safety mechanisms, NSFW (not safe for work), concept removal, negative prompt


- [DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning](https://icml.cc/virtual/2024/poster/35129) (Poster)
  - **Authors:** [Jianxiong Li](http://openreview.net/profile?id=~Jianxiong_Li1), [Jinliang Zheng](http://openreview.net/profile?id=~Jinliang_Zheng1), [Yinan Zheng](http://openreview.net/profile?id=~Yinan_Zheng1), [Liyuan Mao](http://openreview.net/profile?id=~Liyuan_Mao2), [Xiao Hu](http://openreview.net/profile?id=~Xiao_Hu7), [Sijie Cheng](http://openreview.net/profile?id=~Sijie_Cheng1), [Haoyi Niu](http://openreview.net/profile?id=~Haoyi_Niu1), [Jihao Liu](http://openreview.net/profile?id=~Jihao_Liu4), [Yu Liu](http://openreview.net/profile?id=~Yu_Liu2), [Jingjing Liu](http://openreview.net/profile?id=~Jingjing_Liu2), [Ya-Qin Zhang](http://openreview.net/profile?id=~Ya-Qin_Zhang1), [Xianyuan Zhan](http://openreview.net/profile?id=~Xianyuan_Zhan1)
  - **Affiliations:** AIR, Tsinghua University, AIR, Tsinghua University; SenseTime Research, AIR, Tsinghua University, AIR, Tsinghua University; Shanghai Jiaotong University, AIR, Tsinghua University, AIR, Tsinghua University, AIR, Tsinghua University, SenseTime Research; CUHK MMLab, SenseTime Research, AIR, Tsinghua University, AIR, Tsinghua University, AIR, Tsinghua University; Shanghai AI Lab
  - **TL;DR:** This paper introduces DecisionNCE, a unified framework for multimodal representation learning in autonomous robots that effectively aligns visual trajectories with language instructions. The framework enhances task progression extraction and temporal consistency, demonstrating its efficacy in diverse downstream policy learning tasks.
  - **Keywords:** Multimodal pretraining, representation learning, autonomous robots, Bradley-Terry model, InfoNCE-style objective, implicit time contrastive learning, Autonomous robotics, decision-making tasks, Data scarcity, sub-optimal solutions in existing methods, DecisionNCE framework, unified representation and reward learning


- [Fine-grained Local Sensitivity Analysis of Standard Dot-Product Self-Attention](https://icml.cc/virtual/2024/poster/34593) (Poster)
  - **Authors:** [Aaron Havens](http://openreview.net/profile?id=~Aaron_J_Havens1), [Alexandre Araujo](http://openreview.net/profile?id=~Alexandre_Araujo3), [Huan Zhang](http://openreview.net/profile?id=~Huan_Zhang1), [Bin Hu](http://openreview.net/profile?id=~Bin_Hu2)
  - **Affiliations:** ECE & CSL, University of Illinois Urbana-Champaign, ECE, New York University, ECE & CSL, University of Illinois Urbana-Champaign, ECE & CSL, University of Illinois Urbana-Champaign
  - **TL;DR:** This paper conducts a fine-grained local sensitivity analysis of the standard dot-product self-attention mechanism, revealing how input perturbations affect attention outputs and leading to new certified robustness results for vision transformers. The findings provide insights into controlling local sensitivity, which is crucial for the effective application of self-attention in deep learning models.
  - **Keywords:** self-attention, robustness, sensitivity analysis, dot-product self-attention, Local Fine-grained Attention Sensitivity (LoFAST), vision transformers, machine learning, non-Lipschitz sensitivity, input perturbations, certified robustness, analytical bounds for local sensitivity, insights on controlling sensitivity, CIFAR-10, SVHN, Transformer, attention weight matrices


- [Scalable Multiple Kernel Clustering: Learning Clustering Structure from Expectation](https://icml.cc/virtual/2024/poster/33226) (Poster)
  - **Authors:** [Weixuan Liang](http://openreview.net/profile?id=~Weixuan_Liang1), [En Zhu](http://openreview.net/profile?id=~En_Zhu1), [Shengju Yu](http://openreview.net/profile?id=~Shengju_Yu1), [Huiying Xu](http://openreview.net/profile?email=xhy%40zjnu.edu.cn), [Xinzhong Zhu](http://openreview.net/profile?id=~Xinzhong_Zhu1), [Xinwang Liu](http://openreview.net/profile?id=~Xinwang_Liu1)
  - **Affiliations:** College of Computer, National University of Defense Technology, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China, College of Computer, National University of Defense Technology, Changsha, China, School of Computer Science and Technology, Zhejiang Normal University, Jinhua, China, School of Computer Science and Technology, Zhejiang Normal University, Jinhua, China, College of Computer, National University of Defense Technology, Changsha, China
  - **TL;DR:** This paper presents a novel multiple kernel clustering framework that learns from the expectation of kernel matrices, demonstrating that the empirical kernel matrix can closely approximate a rank-k matrix under certain conditions. The proposed method effectively minimizes the distance between base kernels and their expected forms, enabling improved clustering performance on large-scale datasets.
  - **Keywords:** Multiple Kernel Clustering, Clustering Structure, Gaussian Kernel, Rank-k Matrix, Anchor-based Method, Large-scale Datasets, Deviation of Empirical Kernel Matrix from Expectation, Clustering Performance, Novel Multiple Kernel Clustering Framework, Approximation Guarantee, Isotropic Gaussian Distribution, Frobenius Norm


- [Learning to Compile Programs to Neural Networks](https://icml.cc/virtual/2024/poster/32957) (Poster)
  - **Authors:** [Logan Weber](http://openreview.net/profile?id=~Logan_Weber1), [Jesse Michel](http://openreview.net/profile?id=~Jesse_Michel1), [Alex Renda](http://openreview.net/profile?id=~Alex_Renda2), [Michael Carbin](http://openreview.net/profile?id=~Michael_Carbin1)
  - **Affiliations:** MIT CSAIL, Cambridge, MA, MIT CSAIL, Cambridge, MA, MIT CSAIL, Cambridge, MA, MIT CSAIL, Cambridge, MA
  - **TL;DR:** This paper introduces a novel technique called neural surrogate compilation, which generates neural surrogates directly from program text, improving data efficiency and reducing training epochs compared to traditional methods. The approach addresses limitations of existing universal neural surrogates by separating the generation and execution processes.
  - **Keywords:** neural surrogates, program behavior modeling, hypernetworks, universal neural surrogates, signal processing, robotics, computer architecture design, limited accuracy, resource consumption, data efficiency, neural surrogate compilation, improved training efficiency, dataset of C programs


- [Gambling-Based Confidence Sequences for Bounded Random Vectors](https://icml.cc/virtual/2024/poster/33151) (Spotlight Poster)
  - **Authors:** [Jongha (Jon) Ryu](http://openreview.net/profile?id=~Jongha_Jon_Ryu1), [Gregory Wornell](http://openreview.net/profile?id=~Gregory_W._Wornell1)
  - **Affiliations:** Department of EECS, MIT, Cambridge, Massachusetts, USA, Department of EECS, MIT, Cambridge, Massachusetts, USA
  - **TL;DR:** This paper introduces a new gambling-based framework for constructing confidence sequences for bounded multivariate stochastic processes, demonstrating that the resulting sequences are tighter than existing methods, particularly in the context of sampling without replacement for categorical data. The findings suggest significant implications for sequential decision-making applications, such as A/B testing and election audits.
  - **Keywords:** Confidence sequences, Stochastic processes, Gambling framework, Mixture portfolio, Universal portfolio, A/B testing, Election audits, Tight confidence sequences, Sampling without replacement, New methods for constructing confidence sequences, Empirical tightness of confidence sequences, Bounded multivariate stochastic processes, Categorical data


- [MoMo: Momentum Models for Adaptive Learning Rates](https://icml.cc/virtual/2024/poster/33842) (Poster)
  - **Authors:** [Fabian Schaipp](http://openreview.net/profile?id=~Fabian_Schaipp1), [Ruben Ohana](http://openreview.net/profile?id=~Ruben_Ohana1), [Michael Eickenberg](http://openreview.net/profile?id=~Michael_Eickenberg5), [Aaron Defazio](http://openreview.net/profile?id=~Aaron_Defazio1), [Robert Gower](http://openreview.net/profile?id=~Robert_M._Gower1)
  - **Affiliations:** Department of Mathematics, Technical University of Munich, Munich, Flatiron Institute, CCM, New York, Flatiron Institute, CCM, New York, Meta AI, Fundamental AI Research (FAIR) team, New York, Flatiron Institute, CCM, New York
  - **TL;DR:** This paper introduces MoMo, a new adaptive learning rate method based on momentum models, which reduces the need for extensive hyperparameter tuning in training machine learning models. The proposed methods, MoMo and MoMo-Adam, demonstrate improved robustness and convergence rates across various tasks and datasets.
  - **Keywords:** adaptive learning rates, momentum methods, MoMo, MoMo-Adam, SGD-M, Adam, image classification, recommender systems, translation tasks, learning-rate tuning, computational cost, hyperparameter tuning, new adaptive learning rates, convergence rate improvement, MNIST, CIFAR, ImageNet, Criteo, IWSLT14, PyTorch, Optax, Polyak-type adaptive learning rates, model-based stochastic optimization


- [Operator SVD with Neural Networks via Nested Low-Rank Approximation](https://icml.cc/virtual/2024/poster/33002) (Poster)
  - **Authors:** [Jongha (Jon) Ryu](http://openreview.net/profile?id=~Jongha_Jon_Ryu1), [Xiangxiang Xu](http://openreview.net/profile?id=~Xiangxiang_Xu1), [Hasan Sabri Melihcan Erol](http://openreview.net/profile?id=~Hasan_Sabri_Melihcan_Erol1), [Yuheng Bu](http://openreview.net/profile?id=~Yuheng_Bu1), [Lizhong Zheng](http://openreview.net/profile?id=~Lizhong_Zheng1), [Gregory Wornell](http://openreview.net/profile?id=~Gregory_W._Wornell1)
  - **Affiliations:** Department of EECS, MIT, Cambridge, Massachusetts, USA, Department of EECS, MIT, Cambridge, Massachusetts, USA, Department of EECS, MIT, Cambridge, Massachusetts, USA, Department of ECE, University of Florida, Gainesville, Florida, USA, Department of EECS, MIT, Cambridge, Massachusetts, USA, Department of EECS, MIT, Cambridge, Massachusetts, USA
  - **TL;DR:** This paper presents a novel optimization framework for computing eigenvalue decomposition using neural networks, addressing high-dimensional problems efficiently. The proposed method leverages low-rank approximations and promotes orthogonality in learned functions, demonstrating effectiveness in computational physics and machine learning applications.
  - **Keywords:** eigenvalue decomposition, singular value decomposition, neural networks, low-rank approximation, optimization framework, gradient-based optimization, computational physics, machine learning, high-dimensional eigenvalue problems, memory and computational complexity, new optimization framework, learning top singular values and functions


- [Mean-field Underdamped Langevin Dynamics and its Spacetime Discretization](https://icml.cc/virtual/2024/poster/35003) (Poster)
  - **Authors:** [Qiang Fu](http://openreview.net/profile?id=~Qiang_Fu11), [Ashia Wilson](http://openreview.net/profile?id=~Ashia_Camage_Wilson1)
  - **Affiliations:** Department of Computer Science, Yale University, New Haven, CT, USA, Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA, USA
  - **TL;DR:** This paper introduces a novel N-particle underdamped Langevin algorithm for optimizing non-linear functionals over probability measures, demonstrating fast mixing and global convergence properties. The method is particularly effective for applications in training mean-field neural networks and minimizing discrepancies in probability measures.
  - **Keywords:** mean-field underdamped Langevin dynamics, optimization of non-linear functionals, N-particle underdamped Langevin algorithm, spacetime discretization, training mean-field neural networks, maximum mean discrepancy minimization, kernel Stein discrepancy minimization, entropy regularized mean-field optimization, convergence properties, fast mixing guarantee, global convergence in total variation distance, mean-field Langevin dynamics (MLD), entropy regularized mean-field optimization (EMO), maximum mean discrepancy (MMD), kernel Stein discrepancy (KSD)


- [Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers](https://icml.cc/virtual/2024/poster/34293) (Poster)
  - **Authors:** [Brian Chen](http://openreview.net/profile?id=~Brian_K_Chen1), [Tianyang Hu](http://openreview.net/profile?id=~Tianyang_Hu1), [Hui Jin](http://openreview.net/profile?id=~Hui_Jin1), [Hwee Lee](http://openreview.net/profile?id=~Hwee_Kuan_Lee1), [Kenji Kawaguchi](http://openreview.net/profile?id=~Kenji_Kawaguchi1)
  - **Affiliations:** National University of Singapore; Bioinformatics Institute, Agency for Science, Technology and Research (A*STAR), Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, National University of Singapore; Bioinformatics Institute, Agency for Science, Technology and Research (A*STAR); Nanyang Technological University; Singapore Eye Research Institute; Singapore International Research Laboratory on Artificial Intelligence; Singapore Institute for Clinical Sciences, National University of Singapore
  - **TL;DR:** This study presents a method for permanently incorporating In-Context Learning (ICL) into the weights of linearized transformer networks through the addition of bias terms, demonstrating that this approach allows for exact conversion of ICL tokens. The findings suggest that even approximate conversions in regular transformers can still enhance model performance by providing valuable context.
  - **Keywords:** In-Context Learning (ICL), Large Language Models (LLMs), Linearized transformer networks, bias terms, Temporary incorporation of contextual information, limitations of ICL, Exact conversion of ICL tokens, algorithm (ICLCA), GPT-2


- [DoRA: Weight-Decomposed Low-Rank Adaptation](https://icml.cc/virtual/2024/poster/35052) (Oral)
  - **Authors:** [Shih-Yang Liu](http://openreview.net/profile?id=~Shih-yang_Liu1), [Chien-Yi Wang](http://openreview.net/profile?id=~Chien-Yi_Wang1), [Hongxu Yin](http://openreview.net/profile?id=~Hongxu_Yin2), [Pavlo Molchanov](http://openreview.net/profile?id=~Pavlo_Molchanov1), [Yu-Chiang Wang](http://openreview.net/profile?id=~Yu-Chiang_Frank_Wang2), [Kwang-Ting Cheng](http://openreview.net/profile?id=~Kwang-Ting_Cheng1), [Min-Hung Chen](http://openreview.net/profile?id=~Min-Hung_Chen2)
  - **Affiliations:** NVIDIA; HKUST, NVIDIA, NVIDIA, NVIDIA, NVIDIA, HKUST, NVIDIA
  - **TL;DR:** This study introduces Weight-Decomposed Low-Rank Adaptation (DoRA), a novel method that enhances the learning capacity and training stability of LoRA by decomposing pre-trained weights into magnitude and direction components. DoRA outperforms LoRA in fine-tuning various models on downstream tasks without incurring additional inference costs.
  - **Keywords:** Parameter-efficient fine-tuning, Low-Rank Adaptation, LoRA, Weight Decomposition, Weight Normalization, Natural Language Processing, Multi-modal tasks, Commonsense reasoning, Visual instruction tuning, Image/video-text understanding, Accuracy gap between LoRA and full fine-tuning, High computational cost of full fine-tuning, Weight-Decomposed Low-Rank Adaptation (DoRA), Enhanced learning capacity and training stability, LLaMA, LLaVA, VL-BART


- [Stereographic Spherical Sliced Wasserstein Distances](https://icml.cc/virtual/2024/poster/32783) (Spotlight Poster)
  - **Authors:** [Huy Tran](http://openreview.net/profile?id=~Huy_Tran3), [Yikun Bai](http://openreview.net/profile?id=~Yikun_Bai2), [Abihith Kothapalli](http://openreview.net/profile?id=~Abihith_Kothapalli1), [Ashkan Shahbazi](http://openreview.net/profile?id=~Ashkan_Shahbazi1), [XINRAN LIU](http://openreview.net/profile?id=~Xinran_Liu2), [Rocio Diaz Martin](http://openreview.net/profile?id=~Rocio_P_Diaz_Martin1), [Soheil Kolouri](http://openreview.net/profile?id=~Soheil_Kolouri1)
  - **Affiliations:** Department of Computer Science, Vanderbilt University, Nashville, TN, Department of Computer Science, Vanderbilt University, Nashville, TN, Department of Computer Science, Vanderbilt University, Nashville, TN, Department of Computer Science, Vanderbilt University, Nashville, TN, Department of Computer Science, Vanderbilt University, Nashville, TN, Department of Mathematics, Vanderbilt University, Nashville, TN, Department of Computer Science, Vanderbilt University, Nashville, TN; Department of Mathematics, Vanderbilt University, Nashville, TN
  - **TL;DR:** This paper introduces the Stereographic Spherical Sliced Wasserstein (S3W) distance for efficiently comparing spherical probability distributions using stereographic projection and the generalized Radon transform. The proposed method addresses computational challenges and evaluates performance against existing baselines, demonstrating its effectiveness in various applications.
  - **Keywords:** Spherical probability distributions, Optimal transport, Wasserstein distance, Stereographic projection, Generalized Radon transform, Sliced Wasserstein distances, Geology, Medical imaging, Computer vision, Deep representation learning, Computational cost of optimal transport, Distance distortion, Stereographic Spherical Sliced Wasserstein (S3W) distance, Performance evaluation of metrics


- [Mitigating Oversmoothing Through Reverse Process of GNNs for Heterophilic Graphs](https://icml.cc/virtual/2024/poster/34053) (Poster)
  - **Authors:** [MoonJeong Park](http://openreview.net/profile?id=~MoonJeong_Park1), [Jaeseung Heo](http://openreview.net/profile?id=~Jaeseung_Heo1), [Dongwoo Kim](http://openreview.net/profile?id=~Dongwoo_Kim1)
  - **Affiliations:** Graduate School of Artificial Intelligence, Pohang University of Science and Technology (POSTECH), Pohang, Republic of Korea, Graduate School of Artificial Intelligence, Pohang University of Science and Technology (POSTECH), Pohang, Republic of Korea, Graduate School of Artificial Intelligence, Pohang University of Science and Technology (POSTECH), Pohang, Republic of Korea; Computer Science and Engineering, Pohang University of Science and Technology (POSTECH), Pohang, Republic of Korea
  - **TL;DR:** This study proposes a reverse process for message passing in Graph Neural Networks (GNNs) to mitigate over-smoothing and enhance the distinguishability of node representations in heterophilic graphs. Experimental results demonstrate significant improvements in prediction performance, particularly in scenarios where adjacent nodes have different labels.
  - **Keywords:** Graph Neural Networks (GNNs), Heterophilic Graphs, Message Passing, Reverse Process, Over-smoothing, Distinguishable Node Representations, Improved Prediction Performance, Mitigation of Over-smoothing, Minesweeper Dataset


- [Offline Actor-Critic Reinforcement Learning Scales to Large Models](https://icml.cc/virtual/2024/poster/32858) (Oral)
  - **Authors:** [Jost Tobias Springenberg](http://openreview.net/profile?id=~Jost_Tobias_Springenberg1), [Abbas Abdolmaleki](http://openreview.net/profile?id=~Abbas_Abdolmaleki3), [Jingwei Zhang](http://openreview.net/profile?id=~Jingwei_Zhang2), [Oliver M Groth](http://openreview.net/profile?id=~Oliver_Groth1), [Michael Bloesch](http://openreview.net/profile?id=~Michael_Bloesch1), [Thomas Lampe](http://openreview.net/profile?id=~Thomas_Lampe1), [Philemon Brakel](http://openreview.net/profile?id=~Philemon_Brakel1), [Sarah Bechtle](http://openreview.net/profile?id=~Sarah_Maria_Elisabeth_Bechtle1), [Steven Kapturowski](http://openreview.net/profile?id=~Steven_Kapturowski1), [Roland Hafner](http://openreview.net/profile?id=~Roland_Hafner1), [Nicolas Heess](http://openreview.net/profile?id=~Nicolas_Heess1), [Martin Riedmiller](http://openreview.net/profile?id=~Martin_Riedmiller1)
  - **Affiliations:** Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind, Google Deepmind
  - **TL;DR:** This study demonstrates that offline actor-critic reinforcement learning can effectively scale to large models like transformers, outperforming supervised behavioral cloning in multi-task training on diverse datasets. The findings suggest that offline RL can leverage sub-optimal data to learn robust multi-task policies, addressing the challenge of data scarcity in real-world applications.
  - **Keywords:** offline reinforcement learning, actor-critic methods, scaling models, Perceiver-based actor-critic model, KL-regularized RL objective, offline actor-critic algorithms, continuous control tasks, robotics, data scarcity, need for sub-optimal data, high-quality expert data requirement, multi-task policies, scaling laws in offline RL, efficient training methods, large datasets, 132 continuous control tasks, transformers, behavioral cloning, Q-learning


- [From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers](https://icml.cc/virtual/2024/poster/34896) (Poster)
  - **Authors:** [Muhammed Emrullah Ildiz](http://openreview.net/profile?id=~Muhammed_Emrullah_Ildiz1), [Yixiao HUANG](http://openreview.net/profile?id=~Yixiao_HUANG2), [Yingcong Li](http://openreview.net/profile?id=~Yingcong_Li1), [Ankit Singh Rawat](http://openreview.net/profile?id=~Ankit_Singh_Rawat1), [Samet Oymak](http://openreview.net/profile?id=~Samet_Oymak2)
  - **Affiliations:** University of Michigan, Ann Arbor, USA, University of Michigan, Ann Arbor, USA, University of Michigan, Ann Arbor, USA, Google Research, NYC, USA, University of Michigan, Ann Arbor, USA
  - **TL;DR:** This study establishes a formal link between self-attention mechanisms in language models and Markov models, introducing Context-Conditioned Markov Chains (CCMC) to analyze the generative process. The findings reveal that self-attention can lead to repetitive text generation due to a winner-takes-all phenomenon in token sampling.
  - **Keywords:** self-attention, Markov models, generative transformers, language modeling, Context-Conditioned Markov Chains (CCMC), maximum likelihood estimation, learning from prompts, repetitive text generation, optimization landscape, consistent estimation of latent models, convex optimization of self-attention weights, transformer architecture, large language models (LLMs)


- [Hypergraph-enhanced Dual Semi-supervised Graph Classification](https://icml.cc/virtual/2024/poster/34252) (Poster)
  - **Authors:** [Wei Ju](http://openreview.net/profile?id=~Wei_Ju1), [Zhengyang Mao](http://openreview.net/profile?id=~Zhengyang_Mao1), [Siyu Yi](http://openreview.net/profile?id=~Siyu_Yi1), [Yifang Qin](http://openreview.net/profile?id=~Yifang_Qin1), [Yiyang Gu](http://openreview.net/profile?id=~Yiyang_Gu1), [Zhiping Xiao](http://openreview.net/profile?id=~Zhiping_Xiao1), [Yifan Wang](http://openreview.net/profile?id=~Yifan_Wang19), [Xiao Luo](http://openreview.net/profile?id=~Xiao_Luo3), [Ming Zhang](http://openreview.net/profile?id=~Ming_Zhang5)
  - **Affiliations:** School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, China, School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, China, School of Statistics and Data Science, Nankai University, China, School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, China, School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, China, Department of Computer Science, University of California, Los Angeles, USA, School of Information Technology & Management, University of International Business and Economics, China, Department of Computer Science, University of California, Los Angeles, USA, School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University-Anker Embodied AI Lab, Peking University, China
  - **TL;DR:** This study proposes the HEAL framework for semi-supervised graph classification, addressing the challenges of limited labeled graphs and high annotation costs by leveraging hypergraph and line graph structures. The method effectively captures higher-order relationships among nodes and demonstrates improved performance on real-world graph datasets compared to existing approaches.
  - **Keywords:** semi-supervised graph classification, graph neural networks (GNNs), hypergraph structure learning, line graph, relational consistency learning, bioinformatics, chemoinformatics, social network analysis, limited labeled graphs, high cost of annotation, data sparsity, HEAL framework, effective graph representations


- [Meta-Learners for Partially-Identified Treatment Effects Across Multiple Environments](https://icml.cc/virtual/2024/poster/32926) (Poster)
  - **Authors:** [Jonas Schweisthal](http://openreview.net/profile?id=~Jonas_Schweisthal1), [Dennis Frauen](http://openreview.net/profile?id=~Dennis_Frauen1), [M van der Schaar](http://openreview.net/profile?id=~Mihaela_van_der_Schaar2), [Stefan Feuerriegel](http://openreview.net/profile?id=~Stefan_Feuerriegel1)
  - **Affiliations:** LMU Munich, Germany; Munich Center for Machine Learning (MCML), Germany, LMU Munich, Germany; Munich Center for Machine Learning (MCML), Germany, University of Cambridge, UK, LMU Munich, Germany; Munich Center for Machine Learning (MCML), Germany
  - **TL;DR:** This paper presents a framework for estimating conditional average treatment effects (CATE) from observational data across multiple environments, addressing challenges related to causal assumption violations. The authors propose meta-learners that leverage treatment assignment mechanisms to provide bounds for CATE estimation, demonstrating their effectiveness through various experiments.
  - **Keywords:** Conditional Average Treatment Effect (CATE), Partial Identification, Observational Data, Meta-learners, Instrumental Variable (IV) methods, Personalized Medicine, Economics, Marketing, Violations of causal assumptions, Overlap, Unconfoundedness, Bounds for partial identification, Model-agnostic learners


- [Uncertainty for Active Learning on Graphs](https://icml.cc/virtual/2024/poster/34735) (Poster)
  - **Authors:** [Dominik Fuchsgruber](http://openreview.net/profile?id=~Dominik_Fuchsgruber1), [Tom Wollschläger](http://openreview.net/profile?id=~Tom_Wollschl%C3%A4ger1), [Bertrand Charpentier](http://openreview.net/profile?id=~Bertrand_Charpentier2), [Antonio Oroz](http://openreview.net/profile?id=~Antonio_Oroz1), [Stephan Günnemann](http://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1)
  - **Affiliations:** School of Computation, Information and Technology, Technical University of Munich, Germany; Munich Data Science Institute, Germany, School of Computation, Information and Technology, Technical University of Munich, Germany; Munich Data Science Institute, Germany, School of Computation, Information and Technology, Technical University of Munich, Germany, School of Computation, Information and Technology, Technical University of Munich, Germany, School of Computation, Information and Technology, Technical University of Munich, Germany; Munich Data Science Institute, Germany
  - **TL;DR:** This study investigates the effectiveness of Uncertainty Sampling for node classification in graphs, revealing significant performance gaps compared to other Active Learning strategies. The authors develop ground-truth Bayesian uncertainty estimates that enhance query strategies, demonstrating improved data efficiency in labeling.
  - **Keywords:** Active Learning, Uncertainty Sampling, Node Classification, Bayesian uncertainty estimates, uncertainty estimators, Graph data, machine learning, Data efficiency, uncertainty in labeling, aleatoric and epistemic uncertainty, Ground-truth uncertainty measures, improved query strategies, Synthetic data, real datasets


- [Decomposing and Editing Predictions by Modeling Model Computation](https://icml.cc/virtual/2024/poster/32949) (Poster)
  - **Authors:** [Harshay Shah](http://openreview.net/profile?id=~Harshay_Shah1), [Andrew Ilyas](http://openreview.net/profile?id=~Andrew_Ilyas1), [Aleksander Madry](http://openreview.net/profile?id=~Aleksander_Madry1)
  - **Affiliations:** MIT, MIT, MIT
  - **TL;DR:** This paper introduces a framework called component modeling to understand how individual model components influence predictions and presents the COAR algorithm for estimating component attributions. The findings enable effective model editing by allowing targeted interventions to modify model behavior.
  - **Keywords:** model interpretability, component modeling, machine learning, component attribution, counterfactual estimator, COAR algorithm, black box nature of machine learning models, understanding model behavior, effective model editing, targeted interventions, convolution filters, attention heads, knowledge neurons, induction heads


- [PGODE: Towards High-quality System Dynamics Modeling](https://icml.cc/virtual/2024/poster/33285) (Poster)
  - **Authors:** [Xiao Luo](http://openreview.net/profile?id=~Xiao_Luo3), [Yiyang Gu](http://openreview.net/profile?id=~Yiyang_Gu1), [Huiyu Jiang](http://openreview.net/profile?id=~Huiyu_Jiang1), [Hang Zhou](http://openreview.net/profile?id=~Hang_Zhou13), [Jinsheng Huang](http://openreview.net/profile?id=~Jinsheng_Huang1), [Wei Ju](http://openreview.net/profile?id=~Wei_Ju1), [Zhiping Xiao](http://openreview.net/profile?id=~Zhiping_Xiao1), [Ming Zhang](http://openreview.net/profile?id=~Ming_Zhang5), [Yizhou Sun](http://openreview.net/profile?id=~Yizhou_Sun1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, USA, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Department of Statistics and Applied Probability, University of California, Santa Barbara, USA, Department of Statistics, University of California, Davis, USA, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Department of Computer Science, University of California, Los Angeles, USA, National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University, Department of Computer Science, University of California, Los Angeles, USA
  - **TL;DR:** This study introduces Prototypical Graph ODE (PGODE) for modeling multi-agent dynamical systems, addressing challenges in predicting interacting dynamics under complex scenarios. The proposed method enhances generalization capabilities and model expressivity, outperforming existing approaches in both in-distribution and out-of-distribution settings.
  - **Keywords:** multi-agent dynamical systems, interacting dynamics, Prototypical Graph ODE (PGODE), graph neural networks (GNNs), representation disentanglement, variational inference, fluid mechanics, autonomous driving, molecular dynamics, out-of-distribution shift, complicated governing rules, long-term dynamics, enhanced model expressivity, generalization capability under system changes, geometric graphs, message passing mechanism


- [Structure-based drug design by denoising voxel grids](https://icml.cc/virtual/2024/poster/34357) (Poster)
  - **Authors:** [Pedro O. Pinheiro](http://openreview.net/profile?id=~Pedro_O._Pinheiro1), [Arian Jamasb](http://openreview.net/profile?id=~Arian_Rokkum_Jamasb1), [Omar Mahmood](http://openreview.net/profile?id=~Omar_Mahmood1), [Vishnu Sresht](http://openreview.net/profile?id=~Vishnu_Sresht1), [Saeed Saremi](http://openreview.net/profile?id=~Saeed_Saremi1)
  - **Affiliations:** Prescient Design, Genentech, Prescient Design, Genentech, Prescient Design, Genentech, Prescient Design, Genentech, Prescient Design, Genentech
  - **TL;DR:** The study introduces V oxBind, a novel score-based generative model for creating 3D molecules conditioned on protein structures, which outperforms existing methods in terms of training simplicity, sampling speed, and the quality of generated molecules. The model demonstrates enhanced diversity, reduced steric clashes, and improved binding affinity to protein pockets.
  - **Keywords:** Structure-based drug design (SBDD), generative modeling, Score-based generative model, 3D voxel-denoising network, underdamped Langevin MCMC, Drug design, molecular generation, High-dimensional data, inefficiency of random search in chemical space, V oxBind model, improved diversity and binding affinity of generated molecules, Voxel grids, atomic density grids, E(3) equivariant diffusion models


- [Online Linear Regression in Dynamic Environments via Discounting](https://icml.cc/virtual/2024/poster/34798) (Poster)
  - **Authors:** [Andrew Jacobsen](http://openreview.net/profile?id=~Andrew_Jacobsen1), [Ashok Cutkosky](http://openreview.net/profile?id=~Ashok_Cutkosky1)
  - **Affiliations:** Department of Computing Science, University of Alberta, Edmonton, Canada, Department of Electrical and Computer Engineering, Boston University, Boston, Massachusetts
  - **TL;DR:** This paper develops algorithms for online linear regression that achieve optimal static and dynamic regret guarantees without prior knowledge of the data stream. The proposed methods utilize a discounted variant of the Vovk-Azoury-Warmuth forecaster, demonstrating strong performance in dynamic environments.
  - **Keywords:** online linear regression, dynamic environments, Vovk-Azoury-Warmuth forecaster, discounted algorithms, dynamic regret, prediction error, data-generating distribution changes, optimal static and dynamic regret guarantees, adaptive algorithms, improper online regression, regret


- [Towards Realistic Model Selection for Semi-supervised Learning](https://icml.cc/virtual/2024/poster/33901) (Poster)
  - **Authors:** [Muyang Li](http://openreview.net/profile?id=~Muyang_Li3), [Xiaobo Xia](http://openreview.net/profile?id=~Xiaobo_Xia1), [Runze Wu](http://openreview.net/profile?id=~Runze_Wu1), [Fengming Huang](http://openreview.net/profile?id=~Fengming_Huang2), [Jun Yu](http://openreview.net/profile?id=~Jun_Yu3), [Bo Han](http://openreview.net/profile?id=~Bo_Han1), [Tongliang Liu](http://openreview.net/profile?id=~Tongliang_Liu1)
  - **Affiliations:** Sydney AI Center, The University of Sydney, Sydney AI Center, The University of Sydney, FUXI AI Lab, NetEase, Sydney AI Center, The University of Sydney, University of Science and Technology of China, Hong Kong Baptist University, Sydney AI Center, The University of Sydney
  - **TL;DR:** This paper addresses the challenges of model selection in semi-supervised learning (SSL) due to limited labeled data and proposes a novel method called Spectral-normalized Labeled-margin Minimization (SLAM) that effectively estimates generalization performance without relying on a validation set. The proposed method demonstrates significant improvements over existing approaches, validated through comprehensive experiments.
  - **Keywords:** Semi-supervised Learning (SSL), Model Selection, Spectral-normalized Labeled-margin Minimization (SLAM), Scarcity of labeled data, Generalization error, Ineffective model selection, Improved model selection method, Theoretical and empirical validation of SLAM


- [Initial Guessing Bias: How Untrained Networks Favor Some Classes](https://icml.cc/virtual/2024/poster/33939) (Poster)
  - **Authors:** [Emanuele Francazi](http://openreview.net/profile?id=~Emanuele_Francazi1), [Aurelien Lucchi](http://openreview.net/profile?id=~Aurelien_Lucchi1), [Marco Baity-Jesi](http://openreview.net/profile?id=~Marco_Baity-Jesi1)
  - **Affiliations:** Physics Department, EPFL, Switzerland; SIAM Department, Eawag, Switzerland, Department of Mathematics and Computer Science, University of Basel, Switzerland, SIAM Department, Eawag, Switzerland
  - **TL;DR:** This study introduces the concept of Initial Guessing Bias (IGB), demonstrating how the structure of deep neural networks can lead to biased predictions even before training begins. The findings highlight the importance of model design choices in shaping initial predictive biases, with implications for architecture selection and data preprocessing.
  - **Keywords:** Initial Guessing Bias, deep neural networks, bias in neural networks, classification problems, biasing effects, model predictions, initial predictions, identification of Initial Guessing Bias (IGB), influence of model design on predictions, activation functions, max-pooling layers, network depth


- [On Statistical Learning Theory for Distributional Inputs](https://icml.cc/virtual/2024/poster/34308) (Poster)
  - **Authors:** [Christian Fiedler](http://openreview.net/profile?id=~Christian_Fiedler1), [Pierre-François Massiani](http://openreview.net/profile?id=~Pierre-Fran%C3%A7ois_Massiani2), [Friedrich Solowjow](http://openreview.net/profile?id=~Friedrich_Solowjow1), [Sebastian Trimpe](http://openreview.net/profile?id=~Sebastian_Trimpe1)
  - **Affiliations:** Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, Aachen, Germany, Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, Aachen, Germany, Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, Aachen, Germany, Institute for Data Science in Mechanical Engineering (DSME), RWTH Aachen University, Aachen, Germany
  - **TL;DR:** This paper addresses the theoretical aspects of kernel-based statistical learning with distributional inputs, proving oracle inequalities and generalization results. The findings enhance the understanding of distributional learning and open avenues for future research in this area.
  - **Keywords:** statistical learning, distributional inputs, distributional regression, kernel machines, regularized least-squares, kernel mean embeddings (KME), kernel ridge regression (KRR), Hilbertian embeddings, medical diagnostics, causal inference, gaps in theory, prediction from distributional inputs, oracle inequalities, generalization results, algorithmic stability, sliced Wasserstein distances


- [DFlow: A Generative Model Combining Denoising AutoEncoder and Normalizing Flow for High Fidelity Waveform Generation](https://icml.cc/virtual/2024/poster/34776) (Poster)
  - **Authors:** [Chenfeng Miao](http://openreview.net/profile?id=~Chenfeng_Miao1), [Qingying Zhu](http://openreview.net/profile?id=~Qingying_Zhu1), [Chen Minchuan](http://openreview.net/profile?id=~Minchuan_Chen1), [Wei Hu](http://openreview.net/profile?id=~Wei_Hu15), [Zijian Li](http://openreview.net/profile?id=~Zijian_Li8), [Shaojun Wang](http://openreview.net/profile?id=~Shaojun_Wang1), [Jing Xiao](http://openreview.net/profile?id=~Jing_Xiao3)
  - **Affiliations:** Ping An Technology, Shanghai, China, Ping An Technology, Shanghai, China, Ping An Technology, Shanghai, China, Ping An Technology, Shanghai, China, Georgia Institute of Technology, GA, US, Ping An Technology, Shanghai, China, Ping An Technology, Shanghai, China
  - **TL;DR:** This study introduces DFlow, a generative framework that combines Normalizing Flow and Denoising AutoEncoder for high-fidelity waveform generation, achieving superior performance and synthesis speed compared to existing models. The model demonstrates strong generalization capabilities and competitive performance against leading vocoders.
  - **Keywords:** Generative Models, Waveform Generation, Normalizing Flow (NF), Denoising AutoEncoder (DAE), Audio Generation, Vocoder, Robustness to Input Variations, Initial Errors during Inference, DFlow Model, High-Fidelity Waveform Generation, Fast Synthesis Speed, Deep Generative Models (DGMs), Generative Adversarial Networks (GAN), Autoregressive Models (AR)


- [On the Unexpected Effectiveness of Reinforcement Learning for Sequential Recommendation](https://icml.cc/virtual/2024/poster/33344) (Poster)
  - **Authors:** [Álvaro Labarca Silva](http://openreview.net/profile?id=~%C3%81lvaro_Labarca_Silva1), [Denis Parra](http://openreview.net/profile?id=~Denis_Parra1), [Rodrigo A Toro Icarte](http://openreview.net/profile?id=~Rodrigo_Toro_Icarte1)
  - **Affiliations:** Department of Computer Science, Pontificia Universidad Católica de Chile, Santiago, Chile; National Center for Artificial Intelligence (CENIA), Santiago, Chile, Department of Computer Science, Pontificia Universidad Católica de Chile, Santiago, Chile; Instituto Milenio en Ingeniería e Inteligencia Artificial para la Salud (iHealth), Santiago, Chile; Instituto Milenio Fundamentos de los Datos (IMFD), Santiago, Chile, Department of Computer Science, Pontificia Universidad Católica de Chile, Santiago, Chile; National Center for Artificial Intelligence (CENIA), Santiago, Chile
  - **TL;DR:** This study investigates the effectiveness of Reinforcement Learning (RL) in sequential recommendation systems, particularly for the Next-item Prediction (NIP) task. The findings reveal that using RL as an auxiliary loss can enhance performance by promoting the learning of user interaction embeddings, achieving results comparable to traditional RL methods.
  - **Keywords:** Reinforcement Learning, Sequential Recommendation, Next-item Prediction, Deep Reinforcement Learning, Q-learning, REINFORCE, E-commerce, Video, News, Advertisement, Session-based Recommendation, Short-term vs. long-term preferences, Cold-start problem, Performance gains through auxiliary loss, Learning embeddings, Multi-armed bandits, Session-based recommendation systems, Proxy learning objectives


- [Asymptotics of feature learning in two-layer networks after one gradient-step](https://icml.cc/virtual/2024/poster/34572) (Spotlight Poster)
  - **Authors:** [Hugo Cui](http://openreview.net/profile?id=~Hugo_Cui1), [Luca Pesce](http://openreview.net/profile?id=~Luca_Pesce1), [Yatin Dandi](http://openreview.net/profile?id=~Yatin_Dandi1), [FLORENT KRZAKALA](http://openreview.net/profile?id=~Florent_Krzakala1), [Yue Lu](http://openreview.net/profile?id=~Yue_Lu1), [Lenka Zdeborova](http://openreview.net/profile?id=~Lenka_Zdeborova1), [Bruno Loureiro](http://openreview.net/profile?id=~Bruno_Loureiro1)
  - **Affiliations:** Statistical Physics Of Computation lab., École Polytechnique Fédérale de Lausanne (EPFL), 1015 Lausanne, Switzerland, Information Learning & Physics lab., École Polytechnique Fédérale de Lausanne (EPFL), 1015 Lausanne, Switzerland, Statistical Physics Of Computation lab., École Polytechnique Fédérale de Lausanne (EPFL); Information Learning & Physics lab., École Polytechnique Fédérale de Lausanne (EPFL), 1015 Lausanne, Switzerland, Information Learning & Physics lab., École Polytechnique Fédérale de Lausanne (EPFL), 1015 Lausanne, Switzerland, Harvard University, School of Engineering and Applied Sciences, Statistical Physics Of Computation lab., École Polytechnique Fédérale de Lausanne (EPFL), 1015 Lausanne, Switzerland, Département d’Informatique, École Normale Supérieure (ENS) - PSL & CNRS, F-75230 Paris cedex 05, France
  - **TL;DR:** This study investigates how two-layer neural networks learn features from data after a single gradient descent step, modeling the trained network with a spiked Random Features (sRF) model. The findings provide an exact asymptotic description of the generalization error in high-dimensional settings, illustrating the advantages of feature learning over traditional kernel methods.
  - **Keywords:** two-layer neural networks, feature learning, kernel methods, spiked Random Features (sRF), gradient descent, generalization error, high-dimensional limit, adapting to data, asymptotic characterization of test error, learning curves, Gaussian universality, Neural Tangent Kernel (NTK), lazy regime


- [SPABA: A Single-Loop and Probabilistic Stochastic Bilevel Algorithm Achieving Optimal Sample Complexity](https://icml.cc/virtual/2024/poster/35144) (Poster)
  - **Authors:** [Tianshu Chu](http://openreview.net/profile?id=~Tianshu_Chu4), [Dachuan Xu](http://openreview.net/profile?id=~Dachuan_Xu1), [Wei Yao](http://openreview.net/profile?id=~Wei_Yao3), [Jin Zhang](http://openreview.net/profile?id=~Jin_Zhang8)
  - **Affiliations:** Institute of Operations Research and Information Engineering, Beijing University of Technology, Beijing 100124, P.R. China, Institute of Operations Research and Information Engineering, Beijing University of Technology, Beijing 100124, P.R. China, National Center for Applied Mathematics Shenzhen; Southern University of Science and Technology, Shenzhen, China, Department of Mathematics, Southern University of Science and Technology, Shenzhen, China
  - **TL;DR:** This paper presents SPABA, a stochastic bilevel optimization algorithm that achieves optimal sample complexity, demonstrating that the complexity bounds for bilevel optimization align with those of single-level optimization. The authors also propose additional algorithms that improve upon existing sample complexity results, supported by numerical experiments.
  - **Keywords:** Bilevel optimization, Stochastic optimization, SPABA, PAGE method, Implicit differentiation, Machine learning, Hyperparameter optimization, Meta-learning, Neural architecture search, Nested optimization problems, Complexity analysis, Optimal sample complexity, Single-loop stochastic bilevel algorithms


- [Differentially Private Synthetic Data via Foundation Model APIs 2: Text](https://icml.cc/virtual/2024/poster/34291) (Spotlight Poster)
  - **Authors:** [Chulin Xie](http://openreview.net/profile?id=~Chulin_Xie1), [Zinan Lin](http://openreview.net/profile?id=~Zinan_Lin1), [Arturs Backurs](http://openreview.net/profile?id=~Arturs_Backurs1), [Sivakanth Gopi](http://openreview.net/profile?id=~Sivakanth_Gopi1), [Da Yu](http://openreview.net/profile?id=~Da_Yu1), [Huseyin Inan](http://openreview.net/profile?id=~Huseyin_A_Inan1), [Harsha Nori](http://openreview.net/profile?id=~Harsha_Nori1), [Haotian Jiang](http://openreview.net/profile?id=~Haotian_Jiang2), [Huishuai Zhang](http://openreview.net/profile?id=~Huishuai_Zhang3), [Yin Tat Lee](http://openreview.net/profile?id=~Yin_Tat_Lee1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19), [Sergey Yekhanin](http://openreview.net/profile?id=~Sergey_Yekhanin1)
  - **Affiliations:** University of Illinois Urbana-Champaign, Microsoft Research, Microsoft Research, Microsoft Research, Sun Yat-sen University, Microsoft Research, Microsoft Research, Microsoft Research, Microsoft Research, Microsoft Research, University of Chicago; University of Illinois Urbana-Champaign, Microsoft Research
  - **TL;DR:** This study introduces the AUG-PE algorithm, which generates differentially private synthetic text using API access to large language models without requiring model training. The results show that AUG-PE produces high-quality synthetic text comparable to state-of-the-art methods, highlighting a scalable solution for privacy-preserving applications in natural language processing.
  - **Keywords:** Differential Privacy, Synthetic Data, Large Language Models, API access, Private Evolution (PE) algorithm, DP-SGD, Natural Language Processing, Privacy-preserving applications, Privacy concerns, Data sharing limitations, AUG-PE algorithm, High-quality DP synthetic text, Differential Privacy (DP), Large Language Models (LLMs)


- [Non-parametric Online Change Point Detection on Riemannian Manifolds](https://icml.cc/virtual/2024/poster/33912) (Poster)
  - **Authors:** [Xiuheng Wang](http://openreview.net/profile?id=~Xiuheng_Wang1), [Ricardo Borsoi](http://openreview.net/profile?id=~Ricardo_Augusto_Borsoi1), [Cédric Richard](http://openreview.net/profile?id=~C%C3%A9dric_Richard1)
  - **Affiliations:** Université Côte d’Azur, CNRS, OCA, France, Université de Lorraine, CNRS, CRAN, Vandoeuvre-lès-Nancy, France, Université Côte d’Azur, CNRS, OCA, France
  - **TL;DR:** This paper presents a non-parametric algorithm for online change point detection in data streams that reside on Riemannian manifolds, addressing the limitations of existing methods in Euclidean spaces. The proposed method utilizes the generalized Karcher mean and stochastic Riemannian optimization, demonstrating effective performance through theoretical analysis and experimental results.
  - **Keywords:** Change point detection, Non-parametric methods, Riemannian manifolds, Generalized Karcher mean, Stochastic Riemannian optimization, Riemannian stochastic gradient descent (R-SGD), Time series analysis, Remote sensing, Climatology, Financial data analysis, Online change point detection, Non-Euclidean data analysis, New non-parametric algorithm for CPD, Theoretical bounds on detection and false alarm rates


- [On the Second-Order Convergence of Biased Policy Gradient Algorithms](https://icml.cc/virtual/2024/poster/34044) (Poster)
  - **Authors:** [Siqiao Mu](http://openreview.net/profile?id=~Siqiao_Mu1), [Diego Klabjan](http://openreview.net/profile?id=~Diego_Klabjan1)
  - **Affiliations:** Department of Engineering Sciences and Applied Mathematics, Northwestern University, Evanston, IL, Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL
  - **TL;DR:** This paper analyzes the second-order convergence of biased policy gradient algorithms in reinforcement learning, highlighting the challenges posed by bias in gradient estimation and the need for novel techniques. The authors provide a second-order analysis of both vanilla and actor-critic methods, establishing convergence results for TD(0) on Markov chains.
  - **Keywords:** reinforcement learning, policy gradient methods, nonconvex optimization, biased policy gradient algorithms, Monte-Carlo sampling, actor-critic methods, TD(0) learning, saddle points, second-order stationary points, bias in gradient estimation, second-order analysis of biased policy gradient methods, convergence of TD(0) on Markov chains


- [Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization](https://icml.cc/virtual/2024/poster/34023) (Oral)
  - **Authors:** [Yang Jin](http://openreview.net/profile?id=~Yang_Jin1), [Zhicheng Sun](http://openreview.net/profile?id=~Zhicheng_Sun1), [Kun Xu](http://openreview.net/profile?id=~Kun_Xu4), [Kun Xu](http://openreview.net/profile?id=~Kun_Xu6), [Liwei Chen](http://openreview.net/profile?id=~Liwei_Chen3), [Hao Jiang](http://openreview.net/profile?id=~Hao_Jiang10), [Quzhe Huang](http://openreview.net/profile?id=~Quzhe_Huang1), [Chengru Song](http://openreview.net/profile?id=~Chengru_Song1), [Yuliang Liu](http://openreview.net/profile?id=~Yuliang_Liu3), [Di ZHANG](http://openreview.net/profile?id=~Di_ZHANG3), [Yang Song](http://openreview.net/profile?id=~Yang_Song6), [Kun Gai](http://openreview.net/profile?id=~Kun_Gai1), [Yadong Mu](http://openreview.net/profile?id=~Yadong_MU1)
  - **Affiliations:** Peking University, China, Peking University, China, Kuaishou Technology, China, Kuaishou Technology, China, Kuaishou Technology, China, Peking University, China, Peking University, China, Kuaishou Technology, China, Kuaishou Technology, China, Kuaishou Technology, China, Kuaishou Technology, China, Kuaishou Technology, China, Peking University, China
  - **TL;DR:** This paper presents Video-LaVIT, a framework for unified video-language pre-training that efficiently tokenizes videos into keyframes and motion vectors, addressing the challenges of spatiotemporal dynamics. The framework demonstrates competitive performance across multiple benchmarks in image and video understanding and generation.
  - **Keywords:** Video-language pre-training, Multimodal Large Language Models, Tokenization, Video decomposition, Keyframes, Temporal motions, Video understanding, Video generation, Image understanding, Image generation, Spatiotemporal dynamics, Temporal redundancy, Encoding challenges, Unified generative pre-training, Efficient tokenization, Large Language Models (LLMs), Motion vectors


- [On Online Experimentation without Device Identifiers](https://icml.cc/virtual/2024/poster/33162) (Poster)
  - **Authors:** [Shiv Shankar](http://openreview.net/profile?id=~Shiv_Shankar2), [Ritwik Sinha](http://openreview.net/profile?id=~Ritwik_Sinha1), [Madalina Fiterau](http://openreview.net/profile?id=~Madalina_Fiterau3)
  - **Affiliations:** College of Information and Computer Sciences, University of Massachusetts, USA, Adobe Research, USA, College of Information and Computer Sciences, University of Massachusetts, USA; Adobe Research, USA
  - **TL;DR:** This paper addresses the challenges of estimating user preferences in online experimentation without device identifiers, proposing a new variational method called HIFIVE to estimate global average treatment effects. The results demonstrate that HIFIVE outperforms standard estimators by reducing bias and enhancing robustness in the face of network uncertainty.
  - **Keywords:** Online experimentation, Human feedback, A/B testing, Variational method, Global average treatment effects (GATE), Data-driven decision-making, Marketing, Identity fragmentation, Lack of identifiable information across devices, Causal inference challenges, HIFIVE estimator, Lower bias, Greater robustness to network uncertainty, SUTVA (Stable Unit Treatment Value Assumption)


- [Transferable Facial Privacy Protection against Blind Face Restoration via Domain-Consistent Adversarial Obfuscation](https://icml.cc/virtual/2024/poster/32891) (Poster)
  - **Authors:** [Kui Zhang](http://openreview.net/profile?id=~Kui_Zhang2), [Hang Zhou](http://openreview.net/profile?id=~Hang_Zhou5), [Jie Zhang](http://openreview.net/profile?id=~Jie_Zhang11), [Wenbo Zhou](http://openreview.net/profile?id=~Wenbo_Zhou1), [Weiming Zhang](http://openreview.net/profile?id=~Weiming_Zhang2), [Nenghai Yu](http://openreview.net/profile?id=~Nenghai_Yu1)
  - **Affiliations:** University of Science and Technology of China, Simon Fraser University, Nanyang Technological University, University of Science and Technology of China, University of Science and Technology of China, University of Science and Technology of China
  - **TL;DR:** This study addresses the vulnerability of traditional facial obfuscation methods against blind face restoration (BFR) techniques, proposing a transferable adversarial obfuscation method to enhance privacy protection. The results demonstrate the effectiveness and transferability of the proposed method across various BFR models.
  - **Keywords:** Facial privacy protection, Blind face restoration, Adversarial obfuscation, Domain-consistent adversarial method, Facial recognition, Privacy protection, Privacy breach of obfuscated faces, Vulnerability of traditional obfuscation methods, Transferable adversarial obfuscation method, Enhanced generalization of obfuscated images, Blind face restoration (BFR), Obfuscation


- [Do Efficient Transformers Really Save Computation?](https://icml.cc/virtual/2024/poster/32716) (Poster)
  - **Authors:** [Kai Yang](http://openreview.net/profile?id=~Kai_Yang12), [Jan Ackermann](http://openreview.net/profile?id=~Jan_Ackermann1), [Zhenyu He](http://openreview.net/profile?id=~Zhenyu_He3), [Guhao Feng](http://openreview.net/profile?id=~Guhao_Feng1), [Bohang Zhang](http://openreview.net/profile?id=~Bohang_Zhang1), [Yunzhen Feng](http://openreview.net/profile?id=~Yunzhen_Feng1), [Qiwei Ye](http://openreview.net/profile?id=~Qiwei_Ye1), [Di He](http://openreview.net/profile?id=~Di_He1), [Liwei Wang](http://openreview.net/profile?id=~Liwei_Wang1)
  - **Affiliations:** School of EECS, Peking University, ETH Zürich, National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University, School of EECS, Peking University, National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Center for Machine Learning Research, Peking University, New York University, Beijing Academy of Artificial Intelligence, National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Center for Machine Learning Research, Peking University, National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University; Center for Machine Learning Research, Peking University
  - **TL;DR:** This paper investigates the capabilities and limitations of efficient Transformers, specifically the Sparse and Linear Transformers, in solving reasoning tasks modeled as Dynamic Programming problems. The findings reveal that while these models can be expressive, they require a model size that scales with the problem size, although they can outperform standard Transformers in certain DP problems.
  - **Keywords:** Efficient Transformers, Large Language Models, Sparse Transformer, Linear Transformer, Dynamic Programming, Natural Language Processing, Computer Vision, Speech, Computational efficiency, Quadratic complexity of self-attention, Understanding capabilities and limitations of efficient Transformers, Transformer, Chain-of-Thought prompts, DP tasks


- [Emergent Equivariance in Deep Ensembles](https://icml.cc/virtual/2024/poster/33027) (Oral)
  - **Authors:** [Jan Gerken](http://openreview.net/profile?id=~Jan_E_Gerken1), [Pan Kessel](http://openreview.net/profile?id=~Pan_Kessel1)
  - **Affiliations:** Department of Mathematical Sciences, Chalmers University of Technology and the University of Gothenburg, SE-412 96 Gothenburg, Sweden, Prescient Design, Genentech Roche, Basel, Switzerland
  - **TL;DR:** This study demonstrates that deep ensembles achieve emergent equivariance for all inputs and training times through full data augmentation, even off-manifold and at initialization. The findings suggest that deep ensembles can effectively enforce equivariance without requiring individual ensemble members to have an equivariant architecture.
  - **Keywords:** deep ensembles, equivariance, data augmentation, neural tangent kernel (NTK), medical domain, cancer cell detection, protein folding, uncertainty estimation, off-manifold predictions, emergent equivariance, Monte-Carlo estimate


- [Online Learning and Information Exponents: The Importance of Batch size & Time/Complexity Tradeoffs](https://icml.cc/virtual/2024/poster/33737) (Poster)
  - **Authors:** [Luca Arnaboldi](http://openreview.net/profile?id=~Luca_Arnaboldi2), [Yatin Dandi](http://openreview.net/profile?id=~Yatin_Dandi1), [FLORENT KRZAKALA](http://openreview.net/profile?id=~Florent_Krzakala1), [Bruno Loureiro](http://openreview.net/profile?id=~Bruno_Loureiro1), [Luca Pesce](http://openreview.net/profile?id=~Luca_Pesce1), [Ludovic Stephan](http://openreview.net/profile?id=~Ludovic_Stephan2)
  - **Affiliations:** Information Learning and Physics Laboratory, École Polytechnique Fédérale de Lausanne (EPFL); None, Statistical Physics Of Computation Laboratory, École Polytechnique Fédérale de Lausanne (EPFL); None, Information Learning and Physics Laboratory, École Polytechnique Fédérale de Lausanne (EPFL); None, Département d’Informatique, École Normale Supérieure, Paris, France; None, Information Learning and Physics Laboratory, École Polytechnique Fédérale de Lausanne (EPFL); None, Information Learning and Physics Laboratory, École Polytechnique Fédérale de Lausanne (EPFL); None
  - **TL;DR:** This study investigates the effect of batch size on the training time of two-layer neural networks using one-pass stochastic gradient descent, revealing that optimal batch sizes can minimize training time without altering sample complexity. The authors propose a new training protocol, Correlation loss SGD, to address limitations of traditional SGD methods.
  - **Keywords:** online learning, stochastic gradient descent (SGD), two-layer neural networks, one-pass SGD, correlation loss SGD, ordinary differential equations (ODEs), multi-index target functions, isotropic covariates, batch size impact, time complexity, sample complexity, optimal batch size characterization, training time minimization, information exponents, mean-field analysis


- [Taylor Videos for Action Recognition](https://icml.cc/virtual/2024/poster/33582) (Poster)
  - **Authors:** [Lei Wang](http://openreview.net/profile?id=~Lei_Wang20), [Xiuyuan Yuan](http://openreview.net/profile?id=~Xiuyuan_Yuan1), [Tom Gedeon](http://openreview.net/profile?id=~Tom_Gedeon1), [Liang Zheng](http://openreview.net/profile?id=~Liang_Zheng4)
  - **Affiliations:** School of Computing, Australian National University, Canberra, Australia, School of Computing, Australian National University, Canberra, Australia, School of Electrical Engineering, Computing and Mathematical Sciences, Curtin University, Perth, Australia, School of Computing, Australian National University, Canberra, Australia
  - **TL;DR:** This study introduces the Taylor video format for action recognition, which effectively extracts dominant motions from video frames while minimizing noise and static objects. The proposed method demonstrates competitive accuracy compared to traditional RGB videos and optical flow, with further improvements when combined with these formats.
  - **Keywords:** action recognition, motion extraction, Taylor series, implicit motion-extraction function, higher-order differences, video analysis, skeleton-based action recognition, extracting motions from videos, noise in video data, static object removal, Taylor video format, competitive action recognition accuracy, improved recognition with fused inputs


- [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://icml.cc/virtual/2024/poster/33475) (Poster)
  - **Authors:** [Mantas Mazeika](http://openreview.net/profile?id=~Mantas_Mazeika3), [Long Phan](http://openreview.net/profile?id=~Long_Phan1), [Xuwang Yin](http://openreview.net/profile?id=~Xuwang_Yin2), [Andy Zou](http://openreview.net/profile?id=~Andy_Zou1), [Zifan Wang](http://openreview.net/profile?id=~Zifan_Wang1), [Norman Mu](http://openreview.net/profile?id=~Norman_Mu1), [Elham Sakhaee](http://openreview.net/profile?id=~Elham_Sakhaee3), [Nathaniel Li](http://openreview.net/profile?id=~Nathaniel_Li1), [Steven Basart](http://openreview.net/profile?id=~Steven_Basart1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19), [David Forsyth](http://openreview.net/profile?id=~David_Forsyth1), [Dan Hendrycks](http://openreview.net/profile?id=~Dan_Hendrycks1)
  - **Affiliations:** University of Illinois Urbana-Champaign, Center for AI Safety, Center for AI Safety, Carnegie Mellon University, Center for AI Safety, UC Berkeley, Microsoft, UC Berkeley, Center for AI Safety, University of Chicago, University of Illinois Urbana-Champaign, Center for AI Safety
  - **TL;DR:** The study introduces HarmBench, a standardized evaluation framework for automated red teaming aimed at enhancing the robustness of large language models against malicious use. It reveals that no current attack or defense is uniformly effective and emphasizes the importance of large-scale evaluations in AI safety.
  - **Keywords:** Automated red teaming, AI safety, Large language models, Adversarial training, Evaluation framework, Malicious use of AI, Vulnerabilities in AI systems, HarmBench framework, Insights on attacks and defenses, Large language models (LLMs), Red teaming


- [The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents](https://icml.cc/virtual/2024/poster/33361) (Poster)
  - **Authors:** [Yatin Dandi](http://openreview.net/profile?id=~Yatin_Dandi1), [Emanuele Troiani](http://openreview.net/profile?id=~Emanuele_Troiani2), [Luca Arnaboldi](http://openreview.net/profile?id=~Luca_Arnaboldi2), [Luca Pesce](http://openreview.net/profile?id=~Luca_Pesce1), [Lenka Zdeborova](http://openreview.net/profile?id=~Lenka_Zdeborova1), [FLORENT KRZAKALA](http://openreview.net/profile?id=~Florent_Krzakala1)
  - **Affiliations:** Statistical Physics Of Computation Laboratory, École Polytechnique Fédérale de Lausanne (EPFL); Information Learning and Physics Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Statistical Physics Of Computation Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Information Learning and Physics Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Information Learning and Physics Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Statistical Physics Of Computation Laboratory, École Polytechnique Fédérale de Lausanne (EPFL), Information Learning and Physics Laboratory, École Polytechnique Fédérale de Lausanne (EPFL)
  - **TL;DR:** This study explores the training dynamics of two-layer neural networks using multi-pass gradient descent, demonstrating that it can learn a broader class of functions than previously thought, overcoming limitations imposed by information and leap exponents. The findings suggest that even minimal repetition of data batches can lead to effective learning in finite time.
  - **Keywords:** two-layer neural networks, gradient descent, multi-pass training, multi-pass gradient descent, Dynamical Mean-Field Theory (DMFT), learnability of functions, limitations of single-pass gradient descent, information exponent, leap exponent, efficient learning of functions, overcoming limitations of gradient flow, multi-index target functions, staircase property


- [Finite Time Logarithmic Regret Bounds for Self-Tuning Regulation](https://icml.cc/virtual/2024/poster/32865) (Poster)
  - **Authors:** [Rahul Singh](http://openreview.net/profile?id=~Rahul_Singh5), [Akshay Mete](http://openreview.net/profile?id=~Akshay_Mete1), [Avik Kar](http://openreview.net/profile?id=~Avik_Kar1), [P. R. Kumar](http://openreview.net/profile?id=~Panganamala_Kumar1)
  - **Affiliations:** Indian Institute of Science, Bengaluru, Karnataka, India, Texas A&M University, College Station, TX, USA, Indian Institute of Science, Bengaluru, Karnataka, India, Texas A&M University, College Station, TX, USA
  - **TL;DR:** This study establishes finite-time logarithmic regret bounds for the self-tuning regulation problem using a modified certainty equivalence algorithm called PIECE, which improves performance in controlling transient behavior. The findings highlight the algorithm's effectiveness in achieving lower regret in the presence of bounded and sub-Gaussian noise.
  - **Keywords:** self-tuning regulation, control systems, PIECE algorithm, certainty equivalence algorithm, engineering applications, paper production, ore crushing, adaptive autopiloting, minimizing variance, transient behavior, finite time regret, logarithmic regret bounds, improved performance, bounded noise, sub-Gaussian noise, linear systems


- [A Unified View of FANOVA: A Comprehensive Bayesian Framework for Component Selection and Estimation](https://icml.cc/virtual/2024/poster/33553) (Poster)
  - **Authors:** [Yosra MARNISSI](http://openreview.net/profile?id=~yosra_marnissi1), [Maxime Leiber](http://openreview.net/profile?id=~Maxime_Leiber1)
  - **Affiliations:** Safran Tech, Digital Sciences & Technologies Department, Châteaufort, France; INRIA, DI/ENS, PSL Research University, France, Safran Tech, Digital Sciences & Technologies Department, Châteaufort, France; INRIA, DI/ENS, PSL Research University, France
  - **TL;DR:** This paper introduces a comprehensive Bayesian framework for FANOVA models that enhances flexibility and interpretability while addressing data sparsity and uncertainty quantification. The proposed model unifies deterministic and Bayesian approaches, paving the way for novel developments in the field.
  - **Keywords:** Bayesian framework, Functional Analysis of Variance (FANOVA), uncertainty quantification, Component Selection and Smoothing Operator (COSSO), Gaussian process (GP), Dirichlet mixing model, Data sparsity, model interpretability, estimation challenges, Novel model developments, unification of deterministic and Bayesian methods


- [Sequence Compression Speeds Up Credit Assignment in Reinforcement Learning](https://icml.cc/virtual/2024/poster/34190) (Poster)
  - **Authors:** [Aditya A. Ramesh](http://openreview.net/profile?id=~Aditya_Ramesh2), [Kenny Young](http://openreview.net/profile?id=~Kenny_John_Young1), [Louis Kirsch](http://openreview.net/profile?id=~Louis_Kirsch1), [Jürgen Schmidhuber](http://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1)
  - **Affiliations:** The Swiss AI Lab IDSIA, USI & SUPSI, University of Alberta and the Alberta Machine Intelligence Institute, The Swiss AI Lab IDSIA, USI & SUPSI, The Swiss AI Lab IDSIA, USI & SUPSI; AI Initiative, King Abdullah University of Science and Technology
  - **TL;DR:** This paper introduces Chunked-TD, a novel approach to credit assignment in reinforcement learning that utilizes predicted transition probabilities to improve performance while mitigating the challenges of delayed and stochastic outcomes. The proposed method demonstrates faster problem-solving capabilities compared to conventional TD(λ) methods.
  - **Keywords:** reinforcement learning, credit assignment, Monte Carlo (MC) returns, Temporal Difference (TD) learning, TD(λ), Chunked-TD, delayed outcomes, stochastic outcomes, bias-variance tradeoff, algorithms for online implementation, improved performance over conventional TD(λ)


- [Provably Robust DPO: Aligning Language Models with Noisy Feedback](https://icml.cc/virtual/2024/poster/32659) (Poster)
  - **Authors:** [Sayak Ray Chowdhury](http://openreview.net/profile?id=~Sayak_Ray_Chowdhury1), [Anush Kini](http://openreview.net/profile?email=t-anushkini%40microsoft.com), [Nagarajan Natarajan](http://openreview.net/profile?id=~Nagarajan_Natarajan1)
  - **Affiliations:** Microsoft Research, India, Microsoft Research, India, Microsoft Research, India
  - **TL;DR:** This study introduces a robust framework for optimizing language models using noisy preference feedback, specifically enhancing the Direct Preference Optimization (DPO) method. The proposed robust DPO (rDPO) demonstrates improved resilience to noise in preference labels compared to traditional DPO and other heuristics, as evidenced by experiments on real-world datasets.
  - **Keywords:** Language Model Alignment, Human Feedback, Direct Preference Optimization (DPO), Reinforcement Learning from Human Feedback (RLHF), Bradley-Terry-Luce (BTL) model, Noisy Preference Data, Ambiguous Preferences, Robust DPO (rDPO), Novel Loss Function, IMDb Sentiment Generation, Anthropic’s Helpful-Harmless Dataset, Large Language Models (LLMs), Supervised Fine Tuning (SFT), Proximal Policy Optimization (PPO)


- [GPTSwarm: Language Agents as Optimizable Graphs](https://icml.cc/virtual/2024/poster/32826) (Oral)
  - **Authors:** [Mingchen Zhuge](http://openreview.net/profile?id=~Mingchen_Zhuge2), [Wenyi Wang](http://openreview.net/profile?id=~Wenyi_Wang1), [Louis Kirsch](http://openreview.net/profile?id=~Louis_Kirsch1), [Francesco Faccio](http://openreview.net/profile?id=~Francesco_Faccio1), [Dmitrii Khizbullin](http://openreview.net/profile?id=~Dmitrii_Khizbullin2), [Jürgen Schmidhuber](http://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1)
  - **Affiliations:** AI Initiative, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia, AI Initiative, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia, The Swiss AI Lab IDSIA, USI, SUPSI, Lugano, Switzerland, AI Initiative, King Abdullah University of Science and Technology (KAUST); The Swiss AI Lab IDSIA, USI, SUPSI, Lugano, Switzerland, AI Initiative, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia, AI Initiative, King Abdullah University of Science and Technology (KAUST); The Swiss AI Lab IDSIA, USI, SUPSI, Lugano, Switzerland
  - **TL;DR:** This paper presents a unified framework for LLM-based agents modeled as computational graphs, enabling efficient development and optimization of these agents through node and edge optimization techniques. The findings suggest that this approach can enhance collaboration among agents and improve their overall performance in problem-solving tasks.
  - **Keywords:** Large Language Models, Language Agents, Computational Graphs, Node-level LLM prompts, Edge optimization, Graph optimization, Autonomous problem solvers, Multi-agent frameworks, Disparate code bases, Human engineering for prompting schemes, Automatic graph optimizers, Improved agent orchestration, Society of Mind (SOM), Chain of Thought (COT), Tree of Thought (TOT), Graph of Thought (GOT)


- [Path-Guided Particle-based Sampling](https://icml.cc/virtual/2024/poster/34323) (Poster)
  - **Authors:** [Mingzhou Fan](http://openreview.net/profile?id=~Mingzhou_Fan1), [Ruida Zhou](http://openreview.net/profile?id=~Ruida_Zhou1), [Chao Tian](http://openreview.net/profile?id=~Chao_Tian2), [Xiaoning Qian](http://openreview.net/profile?id=~Xiaoning_Qian2)
  - **Affiliations:** Department of Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA, Department of Electrical and Computer Engineering, University of California, Los Angeles, CA, USA, Department of Electrical & Computer Engineering, Texas A&M University, College Station, Texas, USA, Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computational Science Initiative, Brookhaven National Laboratory, Upton, NY, USA
  - **TL;DR:** This study introduces a path-guided particle-based sampling (PGPS) method that utilizes a Log-weighted Shrinkage density path to enhance Bayesian inference accuracy and calibration. The proposed method outperforms existing techniques like SVGD and Langevin dynamics in both synthetic and real-world tasks.
  - **Keywords:** Bayesian inference, particle-based sampling, Stein variational gradient descent (SVGD), Log-weighted Shrinkage (LwS), ordinary differential equation (ODE), Fokker-Planck equation, Bayesian learning tasks, synthetic and real-world applications, Intractable partition function, mode missing, slow mixing, Path-guided particle-based sampling (PGPS), improved Bayesian inference accuracy, better calibration ability, Kullback-Leibler (KL) divergence, Wasserstein distance, Bayesian Neural Networks (BNNs)


- [Logistic Variational Bayes Revisited](https://icml.cc/virtual/2024/poster/35075) (Poster)
  - **Authors:** [Michael Komodromos](http://openreview.net/profile?id=~Michael_Komodromos1), [Marina Evangelou](http://openreview.net/profile?id=~Marina_Evangelou1), [Sarah Filippi](http://openreview.net/profile?id=~Sarah_Lucie_Filippi1)
  - **Affiliations:** Department of Mathematics, Imperial College London, United Kingdom, Department of Mathematics, Imperial College London, United Kingdom, Department of Mathematics, Imperial College London, United Kingdom
  - **TL;DR:** This paper introduces a new bound for the expectation of the softplus function, enhancing variational logistic regression and Gaussian process classification. The proposed method achieves state-of-the-art performance while being significantly faster than traditional Monte Carlo methods.
  - **Keywords:** Variational Inference, Bayesian Inference, Logistic Regression, Variational Logistic Regression, Gaussian Process Classification, Kullback-Leibler Divergence, Evidence Lower Bound, Bayesian Optimization, Reinforcement Learning, Multi-Instance Learning, Intractability of Evidence Lower Bound, Poor Approximations to True Posterior, New Bound for Expectation of Softplus Function, State-of-the-Art Variational Posterior


- [Decentralized Convex Finite-Sum Optimization with Better Dependence on Condition Numbers](https://icml.cc/virtual/2024/poster/34299) (Poster)
  - **Authors:** [Yuxing Liu](http://openreview.net/profile?id=~Yuxing_Liu1), [Lesi Chen](http://openreview.net/profile?id=~Lesi_Chen1), [Luo Luo](http://openreview.net/profile?id=~Luo_Luo1)
  - **Affiliations:** School of Data Science, Fudan University, Shanghai, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China, School of Data Science, Fudan University, Shanghai, China; Shanghai Key Laboratory for Contemporary Applied Mathematics, Shanghai, China
  - **TL;DR:** This paper presents a stochastic variance reduced first-order method for decentralized convex optimization that improves computation time bounds by considering the global condition number. The proposed method demonstrates efficiency in local oracle calls and communication costs, validated through numerical experiments.
  - **Keywords:** Decentralized optimization, Convex optimization, Stochastic variance reduced first-order method, First-order decentralized optimization methods, Machine learning, Large-scale data training, Strongly convex functions, Local incremental first-order oracle, Improved computation time bounds, Reduced local oracle calls


- [Cross-domain Open-world Discovery](https://icml.cc/virtual/2024/poster/33849) (Poster)
  - **Authors:** [Shuo Wen](http://openreview.net/profile?id=~Shuo_Wen2), [Maria Brbic](http://openreview.net/profile?id=~Maria_Brbic1)
  - **Affiliations:** EPFL, Switzerland, EPFL, Switzerland
  - **TL;DR:** This study presents CROW, a prototype-based method for cross-domain open-world discovery that effectively assigns samples to known classes and identifies novel classes under domain shifts. Experimental results show that CROW achieves an 8% average performance improvement across various settings compared to existing methods.
  - **Keywords:** Cross-domain learning, Open-world discovery, Prototype-based approach, Cluster-then-match strategy, Image classification, Categorical shifts, Distribution shifts, Novel class discovery, CROW method, Performance improvement, Benchmark datasets


- [Balanced Resonate-and-Fire Neurons](https://icml.cc/virtual/2024/poster/33537) (Poster)
  - **Authors:** [Saya Higuchi](http://openreview.net/profile?id=~Saya_Higuchi1), [Sebastian Kairat](http://openreview.net/profile?email=sebastian.kairat%40student.uni-tuebingen.de), [Sander Bohte](http://openreview.net/profile?id=~Sander_Bohte1), [Sebastian Otte](http://openreview.net/profile?id=~Sebastian_Otte1)
  - **Affiliations:** Adaptive AI Lab, Institute of Robotics and Cognitive Systems, University of Lübeck, Germany, Adaptive AI Lab, Institute of Robotics and Cognitive Systems, University of Lübeck, Germany, Machine Learning Group, Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands, Adaptive AI Lab, Institute of Robotics and Cognitive Systems, University of Lübeck, Germany; Machine Learning Group, Centrum Wiskunde & Informatica (CWI), Amsterdam, The Netherlands
  - **TL;DR:** This study introduces the balanced resonate-and-fire (BRF) neuron, which overcomes limitations of traditional RF neurons, demonstrating enhanced performance in recurrent spiking neural networks (RSNNs) for sequence learning tasks. The BRF-RSNNs achieve higher task performance with fewer spikes and parameters, along with faster and more stable training convergence.
  - **Keywords:** spiking neural networks, resonate-and-fire neurons, sequence learning, balanced RF neuron, recurrent spiking neural networks (RSNNs), backpropagation through time (BPTT), edge computing, large-scale neural network architectures, effective learning limitations, computational efficiency, improved task performance, reduced spike generation, faster training convergence, Intel Loihi 2, short-time Fourier transform (STFT), biological plausibility, membrane dynamics


- [A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models](https://icml.cc/virtual/2024/poster/34144) (Poster)
  - **Authors:** [Taehong Moon](http://openreview.net/profile?id=~Taehong_Moon1), [Moonseok Choi](http://openreview.net/profile?id=~Moonseok_Choi1), [EungGu Yun](http://openreview.net/profile?id=~EungGu_Yun1), [Jongmin Yoon](http://openreview.net/profile?id=~Jongmin_Yoon1), [Gayoung Lee](http://openreview.net/profile?id=~Gayoung_Lee1), [Jaewoong Cho](http://openreview.net/profile?id=~Jaewoong_Cho1), [Juho Lee](http://openreview.net/profile?id=~Juho_Lee2)
  - **Affiliations:** KRAFTON, Graduate School of AI, KAIST, Independent researcher, Graduate School of AI, KAIST, Naver AI Lab, South Korea, KRAFTON, Graduate School of AI, KAIST; AITRICS, South Korea
  - **TL;DR:** This paper presents a novel framework called Adaptive Score Estimation (ASE) to enhance the sampling speed of diffusion models by adaptively allocating computational resources during score estimation. The proposed method significantly improves sampling throughput without compromising image quality and integrates well with various solvers for faster sampling.
  - **Keywords:** diffusion models, sampling speed, score estimation networks, early-exiting scheme, image synthesis, text-to-image generation, video generation, slow sampling speed, computational overhead, Adaptive Score Estimation (ASE), improved sampling throughput, GANs (Generative Adversarial Networks), ODE/SDE solvers


- [GFlowNet Training by Policy Gradients](https://icml.cc/virtual/2024/poster/34514) (Poster)
  - **Authors:** [Puhua Niu](http://openreview.net/profile?id=~Puhua_Niu2), [Shili Wu](http://openreview.net/profile?id=~Shili_Wu1), [Mingzhou Fan](http://openreview.net/profile?id=~Mingzhou_Fan1), [Xiaoning Qian](http://openreview.net/profile?id=~Xiaoning_Qian2)
  - **Affiliations:** Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA; Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computational Science Initiative, Brookhaven National Laboratory, Upton, NY, USA, Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA; Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computational Science Initiative, Brookhaven National Laboratory, Upton, NY, USA, Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA; Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computational Science Initiative, Brookhaven National Laboratory, Upton, NY, USA, Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA; Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Computational Science Initiative, Brookhaven National Laboratory, Upton, NY, USA
  - **TL;DR:** This study introduces a new training framework for Generative Flow Networks (GFlowNets) that utilizes policy-dependent rewards to optimize the expected accumulated reward, enhancing the efficiency of GFlowNet training. The proposed methods demonstrate improved performance through robust gradient estimation in both simulated and real-world datasets.
  - **Keywords:** Generative Flow Networks, Reinforcement Learning, Policy-dependent rewards, GFlowNet training methods, Combinatorial object generation, Sampling with probability proportional to a reward function, flow balance in GFlowNets, New policy-based GFlowNet training strategies, robust gradient estimation, Simulated datasets, real-world datasets, Directed Acyclic Graph (DAG), Markovian Decision Processes (MDP)


- [Distribution Alignment Optimization through Neural Collapse for Long-tailed Classification](https://icml.cc/virtual/2024/poster/34453) (Poster)
  - **Authors:** [Jintong Gao](http://openreview.net/profile?id=~Jintong_Gao2), [He Zhao](http://openreview.net/profile?id=~He_Zhao1), [Dandan Guo](http://openreview.net/profile?id=~Dan_dan_Guo1), [Hongyuan Zha](http://openreview.net/profile?id=~Hongyuan_Zha1)
  - **Affiliations:** School of Artificial Intelligence, Jilin University, CSIRO’s Data61, School of Artificial Intelligence, Jilin University; The Chinese University of Hong Kong, Shenzhen, The Chinese University of Hong Kong, Shenzhen
  - **TL;DR:** This study proposes the Distribution Alignment Optimization (DisA) method to induce the Neural Collapse phenomenon in long-tailed classification, addressing the challenges of class imbalance. The extensive experiments demonstrate the effectiveness of DisA as a promising solution for improving model performance on imbalanced datasets.
  - **Keywords:** Neural Collapse, Long-tailed Classification, Imbalanced Learning, Distribution Alignment Optimization (DisA), Optimal Transport (OT), Class imbalance, Insufficient learning for minority classes, Effective solution for imbalanced issue, Plug-and-play method for long-tailed methods, Equiangular Tight Frame (ETF), Cross-Entropy (CE), Mean Squared Error (MSE), Minority Collapse


- [Fewer Truncations Improve Language Modeling](https://icml.cc/virtual/2024/poster/33255) (Poster)
  - **Authors:** [Hantian Ding](http://openreview.net/profile?id=~Hantian_Ding1), [Zijian Wang](http://openreview.net/profile?id=~Zijian_Wang1), [Giovanni Paolini](http://openreview.net/profile?id=~Giovanni_Paolini1), [Varun Kumar](http://openreview.net/profile?id=~Varun_Kumar3), [Anoop Deoras](http://openreview.net/profile?id=~Anoop_Deoras1), [Dan Roth](http://openreview.net/profile?id=~Dan_Roth3), [Stefano Soatto](http://openreview.net/profile?id=~Stefano_Soatto3)
  - **Affiliations:** AWS AI Labs, AWS AI Labs, AWS AI Labs, AWS AI Labs, AWS AI Labs, AWS AI Labs, AWS AI Labs
  - **TL;DR:** This study introduces Best-fit Packing, a method that eliminates unnecessary truncations in large language model training while maintaining efficiency. The approach significantly improves performance in reading comprehension, context following, and program synthesis, while also reducing hallucination in generated content.
  - **Keywords:** Language Modeling, Data Integrity, Best-fit Packing, Length-aware Combinatorial Optimization, Natural Language Processing, Code Pretraining, Excessive Truncations, Loss of Information, Hallucination, Improved Performance Metrics, Reduction in Hallucination, Large Language Models (LLMs), Bin Packing Problem


- [Beyond the Norms: Detecting Prediction Errors in Regression Models](https://icml.cc/virtual/2024/poster/33759) (Spotlight Poster)
  - **Authors:** [Andres Altieri](http://openreview.net/profile?id=~Andres_Altieri1), [Marco Romanelli](http://openreview.net/profile?id=~Marco_Romanelli1), [Georg Pichler](http://openreview.net/profile?id=~Georg_Pichler1), [Florence Alberge](http://openreview.net/profile?id=~Florence_Alberge1), [Pablo Piantanida](http://openreview.net/profile?id=~Pablo_Piantanida2)
  - **Affiliations:** Laboratoire des signaux et systèmes (L2S), Université Paris-Saclay CNRS CentraleSupélec, Gif-sur-Yvette, France, New York University, New York, NY, USA, Institute of Telecommunications, TU Wien, Vienna, Austria, Systèmes et applications des technologies de l’information et de l’énergie (SATIE), CNRS Université Paris-Saclay, Gif-sur-Yvette, France, International Laboratory on Learning Systems (ILLS) and Quebec AI Institute (Mila), McGill ETS CNRS Université Paris-Saclay CentraleSupélec, Montreal (QC), Canada
  - **TL;DR:** This study presents a framework for detecting unreliable behavior in regression models by estimating prediction discrepancies and measuring statistical diversity. The proposed method shows empirical improvements in error detection across multiple regression tasks, contributing to uncertainty quantification and safe machine learning systems.
  - **Keywords:** regression algorithms, uncertainty quantification, operational safety, probabilistic modeling, statistical dissimilarity metric, machine learning, autonomous decision-making, unreliable behavior in regression, aleatoric uncertainty, model uncertainty, anomaly detection, data-driven score for uncertainty, improvements in error detection


- [Balancing Feature Similarity and Label Variability for Optimal Size-Aware One-shot Subset Selection](https://icml.cc/virtual/2024/poster/34221) (Poster)
  - **Authors:** [Abhinab Acharya](http://openreview.net/profile?id=~Abhinab_Acharya1), [Dayou Yu](http://openreview.net/profile?id=~Dayou_Yu1), [Qi Yu](http://openreview.net/profile?id=~Qi_Yu1), [Xumin Liu](http://openreview.net/profile?id=~Xumin_Liu1)
  - **Affiliations:** Rochester Institute of Technology, Rochester, NY, USA, Rochester Institute of Technology, Rochester, NY, USA, Rochester Institute of Technology, Rochester, NY, USA, Rochester Institute of Technology, Rochester, NY, USA
  - **TL;DR:** This study introduces a novel method called BOSS for one-shot subset selection that balances feature similarity and label variability to enhance data efficiency in deep learning. The findings demonstrate that this approach significantly improves subset selection performance compared to existing methods, addressing challenges related to data representation and selection size.
  - **Keywords:** subset selection, core-set selection, data-efficient deep learning, Feature similarity and Label variability Balanced One-shot Subset Selection (BOSS), Beta-scoring importance function, continual learning, data sparsity, model overfitting, high-dimensional data, balanced core-set loss bound, optimal size-aware subset


- [Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference](https://icml.cc/virtual/2024/poster/34050) (Poster)
  - **Authors:** [Luca Masserano](http://openreview.net/profile?id=~Luca_Masserano1), [Alexander Shen](http://openreview.net/profile?id=~Alexander_Shen1), [Michele Doro](http://openreview.net/profile?email=michele.doro%40unipd.it), [Tommaso Dorigo](http://openreview.net/profile?id=~Tommaso_Dorigo1), [Rafael Izbicki](http://openreview.net/profile?id=~Rafael_Izbicki1), [Ann Lee](http://openreview.net/profile?id=~Ann_B._Lee1)
  - **Affiliations:** Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA, Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, USA, Department of Physics and Astronomy, Università di Padova, Padova, Italy, Istituto Nazionale di Fisica Nucleare, Sezione di Padova, Italy; Lulea Techniska Universitet, Lulea, Sweden; Universal Scientific Education and Research Network, Italy, Department of Statistics, Universidade Federal de São Carlos, São Paulo, Brazil, Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, USA; Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA
  - **TL;DR:** This study addresses the challenge of classifying events with reliable uncertainty measures under generalized label shift (GLS) by proposing a robust uncertainty quantification method that treats classification as a hypothesis testing problem. The method effectively adapts pretrained classifiers to new domains while maintaining high predictive power and valid uncertainty estimates, demonstrated through applications in biology and astroparticle physics.
  - **Keywords:** likelihood-free inference, uncertainty quantification, generalized label shift, hypothesis testing, receiver operating characteristic (ROC) estimation, biology, astroparticle physics, distributional shift, biased predictions, uncertainty estimates, robust uncertainty quantification method, domain adaptation capabilities, nuisance parameters, mechanistic models


- [Nesting Particle Filters for Experimental Design in Dynamical Systems](https://icml.cc/virtual/2024/poster/33061) (Poster)
  - **Authors:** [Sahel Iqbal](http://openreview.net/profile?id=~Sahel_Iqbal1), [Adrien Corenflos](http://openreview.net/profile?id=~Adrien_Corenflos1), [Simo Särkkä](http://openreview.net/profile?id=~Simo_S%C3%A4rkk%C3%A41), [Hany Abdulsamad](http://openreview.net/profile?id=~Hany_Abdulsamad1)
  - **Affiliations:** Aalto University, Espoo, Finland, Aalto University, Espoo, Finland, Aalto University, Espoo, Finland, Aalto University, Espoo, Finland
  - **TL;DR:** This paper presents a novel approach to Bayesian experimental design for non-exchangeable data, formulating it as risk-sensitive policy optimization. The proposed Inside-Out SMC2 algorithm demonstrates improved efficacy in inferring optimal designs compared to existing state-of-the-art methods.
  - **Keywords:** Bayesian experimental design, risk-sensitive policy optimization, Inside-Out SMC2 algorithm, nested sequential Monte Carlo, particle Markov chain Monte Carlo, Dynamical systems, active learning, neuroscience, physics, psychology, robotics, Maximizing expected information gain, computational intensity in sequential designs, Novel amortization scheme, improved design inference


- [An Information-Theoretic Analysis of In-Context Learning](https://icml.cc/virtual/2024/poster/34205) (Poster)
  - **Authors:** [Hong Jun Jeon](http://openreview.net/profile?id=~Hong_Jun_Jeon1), [Jason Lee](http://openreview.net/profile?id=~Jason_D._Lee1), [Qi Lei](http://openreview.net/profile?id=~Qi_Lei1), [Benjamin Van Roy](http://openreview.net/profile?id=~Benjamin_Van_Roy1)
  - **Affiliations:** Department of Computer Science, Stanford University, Stanford, CA, USA, Princeton University, Princeton, NJ, USA, New York University, New York City, NY, USA, Stanford University, Stanford, CA, USA
  - **TL;DR:** This paper presents an information-theoretic framework for analyzing in-context learning, decomposing the error of a Bayes optimal predictor into meta-learning and intra-task errors. The findings provide insights into how large language models can effectively learn from limited data within their context window.
  - **Keywords:** In-Context Learning, Meta-Learning, Bayesian Inference, Information-Theoretic Tools, Large Language Models, Error Decay, Short Sequences, Decomposition of Error, Meta-Learning Error, Intra-Task Error, Transformers, HMMs (Hidden Markov Models)


- [Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning](https://icml.cc/virtual/2024/poster/34532) (Poster)
  - **Authors:** [Tenglong Liu](http://openreview.net/profile?id=~Tenglong_Liu1), [Yang Li](http://openreview.net/profile?id=~Yang_Li40), [Yixing Lan](http://openreview.net/profile?id=~Yixing_Lan2), [Hao Gao](http://openreview.net/profile?id=~Hao_Gao5), [Wei Pan](http://openreview.net/profile?id=~Wei_Pan2), [Xin Xu](http://openreview.net/profile?id=~Xin_Xu1)
  - **Affiliations:** National University of Defense Technology, Changsha, China, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai, China, National University of Defense Technology, Changsha, China, National University of Defense Technology, Changsha, China, Delft University of Technology, The Netherlands, National University of Defense Technology, Changsha, China
  - **TL;DR:** This study introduces Adaptive Advantage-Guided Policy Regularization (A2PR) to enhance offline reinforcement learning by selecting high-advantage actions while maintaining conservatism from out-of-distribution actions. The proposed method demonstrates state-of-the-art performance on the D4RL benchmark and effectively mitigates value overestimation.
  - **Keywords:** Offline Reinforcement Learning, Policy Regularization, Adaptive Advantage-Guided Policy Regularization (A2PR), Variational Autoencoder (VAE), Autonomous Driving, Healthcare, Out-of-Distribution (OOD) actions, Q-value overestimation, unnecessary conservativeness, Improved policy performance, bounded performance gap, D4RL benchmark


- [A New Robust Partial p-Wasserstein-Based Metric for Comparing Distributions](https://icml.cc/virtual/2024/poster/33067) (Poster)
  - **Authors:** [Sharath Raghvendra](http://openreview.net/profile?id=~Sharath_Raghvendra1), [Pouyan Shirzadian](http://openreview.net/profile?id=~Pouyan_Shirzadian1), [Kaiyi Zhang](http://openreview.net/profile?id=~Kaiyi_Zhang2)
  - **Affiliations:** North Carolina State University, Virginia Tech, Virginia Tech
  - **TL;DR:** This paper introduces a new robust distance metric, k-RPW, based on the partial p-Wasserstein distance, which addresses the sensitivity of the traditional 2-Wasserstein distance to outliers while improving convergence rates for empirical distributions. The proposed metric demonstrates higher accuracy in image retrieval tasks compared to existing metrics.
  - **Keywords:** Wasserstein distance, distribution comparison, robustness, k-RPW (partial p-Wasserstein distance), empirical p-Wasserstein distance, image retrieval, machine learning, computer vision, sensitivity to outliers, sampling discrepancy, new distance metric (k-RPW), improved convergence rates, 2-Wasserstein distance, total variation distance, Lévý-Prokhorov distance


- [Improving Open-Ended Text Generation via Adaptive Decoding](https://icml.cc/virtual/2024/poster/33683) (Poster)
  - **Authors:** [Wenhong Zhu](http://openreview.net/profile?id=~Wenhong_Zhu1), [Hongkun Hao](http://openreview.net/profile?id=~Hongkun_Hao1), [Zhiwei He](http://openreview.net/profile?id=~Zhiwei_He1), [Yiming Ai](http://openreview.net/profile?id=~Yiming_Ai1), [Rui Wang](http://openreview.net/profile?id=~Rui_Wang10)
  - **Affiliations:** MT Lab, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, MT Lab, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, MT Lab, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, MT Lab, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, MT Lab, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This study introduces adaptive decoding, a method that enhances language models' ability to select appropriate candidate tokens during text generation, improving both diversity and coherence. Experimental results indicate that this approach can generate text preferred by humans and potentially enhance reasoning capabilities in language models.
  - **Keywords:** open-ended text generation, language models, adaptive decoding, entropy-based metric, confidence, casual conversation, storytelling, token selection, generation quality, coherence, diversity, confidence-increasing process, human-preferred text generation


- [Causal Inference out of Control: Estimating Performativity without Treatment Randomization](https://icml.cc/virtual/2024/poster/34719) (Poster)
  - **Authors:** [Gary Cheng](http://openreview.net/profile?id=~Gary_Cheng2), [Moritz Hardt](http://openreview.net/profile?id=~Moritz_Hardt1), [Celestine Mendler-Dünner](http://openreview.net/profile?id=~Celestine_Mendler-D%C3%BCnner1)
  - **Affiliations:** Stanford University Department of Electrical Engineering; Max Planck Institute for Intelligent Systems, Tübingen, and Tübingen AI Center; ELLIS Institute Tübingen, Max Planck Institute for Intelligent Systems, Tübingen; Tübingen AI Center, Max Planck Institute for Intelligent Systems, Tübingen; Tübingen AI Center; ELLIS Institute Tübingen
  - **TL;DR:** The study investigates the causal effects of algorithmic actions on user consumption in digital platforms without relying on randomized treatment, proposing a framework for causal identifiability through observational data. The findings highlight the importance of modeling consumption dynamics and addressing confounding factors to draw valid conclusions about performativity.
  - **Keywords:** Causal inference, algorithmic actions, performativity, Digital platforms, user consumption, Confounding in observational data, feedback loops, Causal identifiability, finite sample estimators


- [Hybrid Inverse Reinforcement Learning](https://icml.cc/virtual/2024/poster/35087) (Poster)
  - **Authors:** [Juntao Ren](http://openreview.net/profile?id=~Juntao_Ren1), [Gokul Swamy](http://openreview.net/profile?id=~Gokul_Swamy1), [Steven Wu](http://openreview.net/profile?id=~Steven_Wu1), [J. Bagnell](http://openreview.net/profile?id=~Drew_Bagnell2), [Sanjiban Choudhury](http://openreview.net/profile?id=~Sanjiban_Choudhury3)
  - **Affiliations:** Cornell University, Carnegie Mellon University, Carnegie Mellon University, Aurora Innovation, Cornell University
  - **TL;DR:** This paper introduces hybrid inverse reinforcement learning, which combines online and expert data to enhance sample efficiency and reduce unnecessary exploration in policy learning. The proposed methods demonstrate significant improvements in performance on continuous control tasks compared to standard inverse reinforcement learning approaches.
  - **Keywords:** Inverse Reinforcement Learning, Imitation Learning, Hybrid Reinforcement Learning, Model-free algorithms, Model-based algorithms, Continuous control tasks, Autonomous driving, Error compounding, Exploration burden, Covariate shift, Sample efficiency improvements, Policy performance guarantees


- [Can Gaussian Sketching Converge Faster on a Preconditioned Landscape?](https://icml.cc/virtual/2024/poster/33437) (Poster)
  - **Authors:** [Yilong Wang](http://openreview.net/profile?id=~Yilong_Wang3), [Haishan Ye](http://openreview.net/profile?id=~Haishan_Ye2), [Guang Dai](http://openreview.net/profile?id=~Guang_Dai1), [Ivor Tsang](http://openreview.net/profile?id=~Ivor_Tsang1)
  - **Affiliations:** Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China; SGIT AI Lab, State Grid Corporation of China, Center for Intelligent Decision-Making and Machine Learning, School of Management, Xi’an Jiaotong University, China; SGIT AI Lab, State Grid Corporation of China, SGIT AI Lab, State Grid Corporation of China, CFAR and IHPC, Agency for Science, Technology and Research (A*STAR), Singapore; College of Computing and Data Science, NTU, Singapore
  - **TL;DR:** This paper introduces a novel gradient sketching method called GSGD that achieves fast convergence rates for large-scale optimization problems without the need for importance sampling. The experimental results demonstrate the effectiveness and efficiency of GSGD, particularly in scenarios with non-smooth regularization terms.
  - **Keywords:** large-scale optimization, gradient sketching, Gaussian Sketched Gradient Descent (GSGD), random coordinate descent, SEGA, high-dimensional data, non-smooth regularization, fast convergence rate, variance reduction


- [Differentiability and Optimization of Multiparameter Persistent Homology](https://icml.cc/virtual/2024/poster/33331) (Poster)
  - **Authors:** [Luis Scoccola](http://openreview.net/profile?id=~Luis_Scoccola1), [Siddharth Setlur](http://openreview.net/profile?id=~Siddharth_Setlur1), [David Loiseaux](http://openreview.net/profile?id=~David_Loiseaux1), [Mathieu Carrière](http://openreview.net/profile?id=~Mathieu_Carri%C3%A8re1), [Steve Oudot](http://openreview.net/profile?id=~Steve_Oudot1)
  - **Affiliations:** Mathematical Institute, University of Oxford, UK, Department of Mathematics, ETH Zürich, Switzerland, DataShape, Centre Inria d’Université Côte d’Azur, France, DataShape, Centre Inria d’Université Côte d’Azur, France, GeomeriX, Inria Saclay and École polytechnique, Paris, France
  - **TL;DR:** This study develops a general framework for differentiability and optimization of multiparameter persistent homology descriptors, demonstrating that optimizing these descriptors can enhance performance compared to traditional one-parameter methods. The findings suggest that various existing descriptors can be unified under this framework, leading to improved applications in machine learning.
  - **Keywords:** persistent homology, topological data analysis, optimization, multiparameter persistent homology, signed barcodes, multiparameter persistence landscape, machine learning, feature learning, topological regularization, lack of unique descriptors for multiparameter persistent homology, optimization challenges, development of a general framework for differentiability and optimization, improved performance with multiparameter descriptors


- [Leverage Class-Specific Accuracy to Guide Data Generation for Improving Image Classification](https://icml.cc/virtual/2024/poster/34350) (Poster)
  - **Authors:** [Jay Gala](http://openreview.net/profile?id=~Jay_Gala1), [Pengtao Xie](http://openreview.net/profile?id=~Pengtao_Xie3)
  - **Affiliations:** University of California San Diego, University of California San Diego; Mohamed bin Zayed University of Artificial Intelligence
  - **TL;DR:** This study proposes a needs-aware image generation framework that optimally allocates synthetic training data based on the classification performance of individual classes to mitigate model overfitting in image classification tasks. Experiments demonstrate the effectiveness of this approach in both imbalanced and balanced datasets.
  - **Keywords:** image classification, data generation, deep generative models, multi-level optimization, model overfitting, data sparsity, class imbalance, needs-aware image generation, synthetic training data


- [Generalization to New Sequential Decision Making Tasks with In-Context Learning](https://icml.cc/virtual/2024/poster/33211) (Poster)
  - **Authors:** [Sharath Chandra Raparthy](http://openreview.net/profile?id=~Sharath_Chandra_Raparthy3), [Eric Hambro](http://openreview.net/profile?id=~Eric_Hambro1), [Robert Kirk](http://openreview.net/profile?id=~Robert_Kirk1), [Mikael Henaff](http://openreview.net/profile?id=~Mikael_Henaff1), [Roberta Raileanu](http://openreview.net/profile?id=~Roberta_Raileanu2)
  - **Affiliations:** AI at Meta; UCL, AI at Meta; UCL, UCL, AI at Meta, AI at Meta
  - **TL;DR:** This paper explores the ability of transformers to learn new sequential decision-making tasks from a few demonstrations without weight updates, addressing the challenges posed by stochastic environments. The authors demonstrate that training on diverse trajectories enhances in-context learning, enabling generalization to new tasks in MiniHack and Procgen.
  - **Keywords:** In-context learning, Sequential decision making, Transformers, Meta-learning, Robotics, Virtual assistants, MiniHack, Procgen, Learning from few demonstrations, Stochastic environments, Error tolerance, Generalization to new tasks, Learning without weight updates, MiniHack, Procgen


- [Density-Softmax: Efficient Test-time Model for Uncertainty Estimation and Robustness under Distribution Shifts](https://icml.cc/virtual/2024/poster/33202) (Poster)
  - **Authors:** [Ha Manh Bui](http://openreview.net/profile?id=~Ha_Manh_Bui1), [Anqi Liu](http://openreview.net/profile?id=~Anqi_Liu2)
  - **Affiliations:** Department of Computer Science, Johns Hopkins University, Baltimore, MD, U.S.A., Department of Computer Science, Johns Hopkins University, Baltimore, MD, U.S.A.
  - **TL;DR:** The study introduces Density-Softmax, a sampling-free deterministic framework aimed at improving uncertainty estimation and robustness in deep neural networks under distribution shifts. The proposed method demonstrates competitive performance with fewer model parameters and lower latency compared to existing techniques.
  - **Keywords:** uncertainty estimation, robustness, distribution shifts, deep neural networks, Density-Softmax, Lipschitz-constrained feature extractor, softmax layer, healthcare, finance, decision-making, model over-confidence, over-fitting, distribution shifts, minimax uncertainty risk, competitive results in uncertainty and robustness, lower latency, sampling-free methods, empirical risk minimization


- [Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models](https://icml.cc/virtual/2024/poster/34546) (Poster)
  - **Authors:** [Neta Shaul](http://openreview.net/profile?id=~Neta_Shaul1), [Uriel Singer](http://openreview.net/profile?id=~Uriel_Singer1), [Ricky T. Q. Chen](http://openreview.net/profile?id=~Ricky_T._Q._Chen1), [Matthew Le](http://openreview.net/profile?id=~Matthew_Le2), [Ali Thabet](http://openreview.net/profile?id=~Ali_Thabet1), [Albert Pumarola](http://openreview.net/profile?id=~Albert_Pumarola2), [Yaron Lipman](http://openreview.net/profile?id=~Yaron_Lipman1)
  - **Affiliations:** Weizmann Institute of Science, GenAI, Meta, FAIR, Meta, FAIR, Meta, GenAI, Meta, GenAI, Meta, FAIR, Meta
  - **TL;DR:** This paper presents Bespoke Non-Stationary (BNS) solvers, a novel approach to enhance the sample efficiency of diffusion and flow models, achieving significant improvements in sample approximation metrics. The BNS solvers demonstrate faster optimization and maintain sample diversity, making them a promising alternative to traditional model distillation methods.
  - **Keywords:** Diffusion models, Flow models, Sample efficiency, Non-stationary solvers, Numerical ODE solvers, Solver distillation, Conditional image generation, Text-to-image generation, Text-to-audio generation, Sampling complexity, Sample approximation, Diversity of samples, BNS solvers, Improved PSNR and FID metrics, ImageNet-64


- [Data-free Neural Representation Compression with Riemannian Neural Dynamics](https://icml.cc/virtual/2024/poster/34294) (Oral)
  - **Authors:** [Zhengqi Pei](http://openreview.net/profile?id=~Zhengqi_Pei1), [Anran Zhang](http://openreview.net/profile?id=~Anran_Zhang2), [Shuhui Wang](http://openreview.net/profile?id=~Shuhui_Wang1), [Xiangyang Ji](http://openreview.net/profile?id=~Xiangyang_Ji1), [Qingming Huang](http://openreview.net/profile?id=~Qingming_Huang1)
  - **Affiliations:** Institute of Computing Technology, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Institute of Computing Technology, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Institute of Computing Technology, Chinese Academy of Sciences; Peng Cheng Laboratory, Department of Automation, Tsinghua University, School of Computer Science and Technology, University of Chinese Academy of Sciences
  - **TL;DR:** This study presents a novel data-free neural representation compression method using Riemannian metrics, which enhances parameter efficiency and inference accuracy without requiring additional fine-tuning with real data. Empirical results demonstrate that models compressed with this approach outperform existing data-free compression methods across various datasets.
  - **Keywords:** Neural representation, Data-free compression, Riemannian geometry, Riemannian metric (RieM), Neural dynamics, Image recognition, Object detection, Limited nonlinearity, Data-fitting ability, Data-free neural compression mechanism, Higher parameter efficiency, MNIST, CIFAR-100, ImageNet-1k, COCO


- [Variational Schrödinger Diffusion Models](https://icml.cc/virtual/2024/poster/33256) (Poster)
  - **Authors:** [Wei Deng](http://openreview.net/profile?id=~Wei_Deng1), [Weijian Luo](http://openreview.net/profile?id=~Weijian_Luo1), [Yixin Tan](http://openreview.net/profile?id=~Yixin_Tan1), [Marin Biloš](http://openreview.net/profile?id=~Marin_Bilo%C5%A11), [Yu Chen](http://openreview.net/profile?id=~Yu_Chen15), [Yuriy Nevmyvaka](http://openreview.net/profile?id=~Yuriy_Nevmyvaka1), [Ricky T. Q. Chen](http://openreview.net/profile?id=~Ricky_T._Q._Chen1)
  - **Affiliations:** Machine Learning Research, Morgan Stanley, NY, Peking University, Duke University, Machine Learning Research, Morgan Stanley, NY, Machine Learning Research, Morgan Stanley, NY, Machine Learning Research, Morgan Stanley, NY, Meta AI (FAIR)
  - **TL;DR:** This study introduces the variational Schrödinger diffusion model (VSDM) to enhance the scalability and efficiency of diffusion models by leveraging variational inference and optimizing transportation plans. The results demonstrate improved generation of anisotropic shapes and competitive performance in real-world datasets without the need for warm-up initializations.
  - **Keywords:** diffusion models, optimal transport, variational inference, Schrödinger bridge, forward-backward stochastic differential equations, stochastic approximation, image generation, video generation, audio generation, time series modeling, intractable forward score functions, costly evaluations, simulation-free training, variational Schrödinger diffusion model (VSDM), efficient transportation plans, convergence of variational scores, CIFAR10, multivariate diffusion, linear forward stochastic differential equations


- [Latent Space Symmetry Discovery](https://icml.cc/virtual/2024/poster/32974) (Poster)
  - **Authors:** [Jianke Yang](http://openreview.net/profile?id=~Jianke_Yang2), [Nima Dehmamy](http://openreview.net/profile?id=~Nima_Dehmamy1), [Robin Walters](http://openreview.net/profile?id=~Robin_Walters1), [Rose Yu](http://openreview.net/profile?id=~Rose_Yu1)
  - **Affiliations:** UCSD, IBM Research, Northeastern University, UCSD
  - **TL;DR:** This study introduces Latent LieGAN (LaLiGAN), a generative model that discovers nonlinear symmetries in high-dimensional data by mapping it to a structured latent space. The model effectively addresses the limitations of existing symmetry discovery methods and is applicable to tasks such as equation discovery and long-term forecasting.
  - **Keywords:** symmetry discovery, equivariant neural networks, generative models, Latent LieGAN (LaLiGAN), nonlinear group actions, high-dimensional dynamical systems, equation discovery, long-term forecasting, limitations of existing symmetry discovery methods, complexity of real-world data, nonlinear symmetries, discovery of intrinsic symmetry, well-structured latent space


- [Cooperative Graph Neural Networks](https://icml.cc/virtual/2024/poster/33739) (Poster)
  - **Authors:** [Ben Finkelshtein](http://openreview.net/profile?id=~Ben_Finkelshtein1), [Xingyue Huang](http://openreview.net/profile?id=~Xingyue_Huang1), [Michael Bronstein](http://openreview.net/profile?id=~Michael_M._Bronstein1), [Ismail Ceylan](http://openreview.net/profile?id=~Ismail_Ilkan_Ceylan2)
  - **Affiliations:** Department of Computer Science, University of Oxford, Department of Computer Science, University of Oxford, Department of Computer Science, University of Oxford, Department of Computer Science, University of Oxford
  - **TL;DR:** This paper introduces a novel framework for training graph neural networks that allows nodes to dynamically choose their message-passing strategies, addressing limitations of traditional methods. The proposed approach enhances flexibility in information flow and is supported by theoretical and empirical analyses.
  - **Keywords:** Graph Neural Networks, Message Passing, Graph Machine Learning, Long-range dependencies, Over-squashing, Over-smoothing, Novel framework for training GNNs, Dynamic message-passing paradigm


- [Overcoming the Optimizer's Curse: Obtaining Realistic Prescriptions from Neural Networks](https://icml.cc/virtual/2024/poster/32641) (Poster)
  - **Authors:** [Asterios Tsiourvas](http://openreview.net/profile?id=~Asterios_Tsiourvas1), [Georgia Perakis](http://openreview.net/profile?id=~Georgia_Perakis1)
  - **Affiliations:** Operations Research Center, Massachusetts Institute of Technology, USA, Operations Research Center, Massachusetts Institute of Technology, USA
  - **TL;DR:** This study addresses the challenge of obtaining optimal and realistic prescriptions from neural networks in data-driven decision-making by modeling alignment with the data manifold as an optimization constraint. The proposed adaptive sampling algorithm effectively reduces complex optimization problems into simpler ones, demonstrating efficacy and scalability in various applications.
  - **Keywords:** neural networks, data-driven decision-making, optimization, Local Outlier Factor (LOF), adaptive sampling algorithm, operations management, revenue management, healthcare, Optimizer’s Curse, unrealistic prescriptions, data manifold alignment, optimization constraints, realistic polytopes


- [Improving Transformers with Dynamically Composable Multi-Head Attention](https://icml.cc/virtual/2024/poster/34047) (Oral)
  - **Authors:** [Da Xiao](http://openreview.net/profile?id=~Da_Xiao1), [Qingye Meng](http://openreview.net/profile?id=~Qingye_Meng1), [Shengping Li](http://openreview.net/profile?id=~Shengping_Li2), [xingyuan yuan](http://openreview.net/profile?id=~xingyuan_yuan1)
  - **Affiliations:** School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China, ColorfulClouds Technology Co., Ltd., Beijing, China, ColorfulClouds Technology Co., Ltd., Beijing, China, ColorfulClouds Technology Co., Ltd., Beijing, China
  - **TL;DR:** This paper introduces Dynamically Composable Multi-Head Attention (DCMHA) as an efficient alternative to traditional Multi-Head Attention in Transformers, addressing issues like low-rank bottlenecks and head redundancy. The proposed DCFormer significantly outperforms existing Transformer models in language modeling tasks while requiring less computational resources.
  - **Keywords:** Transformer models, Multi-Head Attention, Dynamically Composable Multi-Head Attention (DCMHA), attention score and weight matrices composition, Language modeling, Low-rank bottleneck of attention score matrices, head redundancy, DCFormer, improved expressive power, parameter and computation efficiency, Multi-Head Attention (MHA), attention matrices


- [Constrained Exploration via Reflected Replica Exchange Stochastic Gradient Langevin Dynamics](https://icml.cc/virtual/2024/poster/34636) (Poster)
  - **Authors:** [Haoyang Zheng](http://openreview.net/profile?id=~Haoyang_Zheng2), [Hengrong Du](http://openreview.net/profile?id=~Hengrong_Du1), [Qi Feng](http://openreview.net/profile?id=~Qi_Feng3), [Wei Deng](http://openreview.net/profile?id=~Wei_Deng1), [Guang Lin](http://openreview.net/profile?id=~Guang_Lin1)
  - **Affiliations:** Purdue University, West Lafayette, IN, Vanderbilt University, Nashville, TN, Florida State University, Tallahassee, FL, Machine Learning Research, Morgan Stanley, New York, NY, Purdue University, West Lafayette, IN
  - **TL;DR:** This paper introduces reflected reSGLD (r2SGLD), an algorithm designed for constrained non-convex exploration that mitigates stagnation issues in high-temperature chains. The findings demonstrate that constrained exploration significantly enhances simulation efficiency in various applications.
  - **Keywords:** constrained exploration, non-convex learning, reflected reSGLD, stochastic gradient Langevin dynamics (SGLD), replica exchange Langevin dynamics (reLD), dynamical systems, constrained multi-modal distributions, image classification, stagnation issues, over-exploration, local trap phenomenon, improved simulation efficiency, enhanced mixing rates


- [Allocation Requires Prediction Only if Inequality Is Low](https://icml.cc/virtual/2024/poster/33863) (Spotlight Poster)
  - **Authors:** [Ali Shirali](http://openreview.net/profile?id=~Ali_Shirali1), [Rediet Abebe](http://openreview.net/profile?id=~Rediet_Abebe2), [Moritz Hardt](http://openreview.net/profile?id=~Moritz_Hardt1)
  - **Affiliations:** University of California, Berkeley, Harvard Society of Fellows, Max Planck Institute for Intelligent Systems; Tübingen AI Center
  - **TL;DR:** The study evaluates the effectiveness of prediction-based resource allocation compared to unit-level statistics, finding that predictions are only beneficial when between-unit inequality is low and the intervention budget is high. This highlights the limitations of improving intervention efficacy through predictions alone.
  - **Keywords:** algorithmic predictions, resource allocation, intervention efficacy, mathematical model, prediction-based allocations, unit-level statistics, societal resource allocation, healthcare, education, between-unit inequality, intervention budget constraints, comparison of individual-level and unit-level allocations, limits of prediction efficacy, Individual-level allocation (ILA), Unit-level allocation (ULA)


- [FlowMM: Generating Materials with Riemannian Flow Matching](https://icml.cc/virtual/2024/poster/33890) (Poster)
  - **Authors:** [Benjamin Kurt Miller](http://openreview.net/profile?id=~Benjamin_Kurt_Miller1), [Ricky T. Q. Chen](http://openreview.net/profile?id=~Ricky_T._Q._Chen1), [Anuroop Sriram](http://openreview.net/profile?id=~Anuroop_Sriram1), [Brandon Wood](http://openreview.net/profile?id=~Brandon_M_Wood1)
  - **Affiliations:** University of Amsterdam; FAIR, Meta AI, FAIR, Meta AI, FAIR, Meta AI, FAIR, Meta AI
  - **TL;DR:** The study presents FlowMM, a pair of generative models that efficiently predict stable crystal structures and generate novel compositions, achieving state-of-the-art performance while addressing the computational challenges in materials discovery. The framework demonstrates a significant improvement in efficiency, being approximately three times more effective in finding stable materials compared to previous methods.
  - **Keywords:** materials discovery, crystalline materials, generative modeling, Riemannian Flow Matching, generative models, energy storage, carbon capture, microprocessing, prediction of stable crystal structures, generation of novel compositions, computational challenges in materials design, state-of-the-art performance in crystal structure prediction and generation, efficiency in finding stable materials, periodic crystals, convex hull, Ehull


- [Probabilistic Conceptual Explainers: Trustworthy Conceptual Explanations for Vision Foundation Models](https://icml.cc/virtual/2024/poster/34650) (Poster)
  - **Authors:** [Hengyi Wang](http://openreview.net/profile?id=~Hengyi_Wang1), [Shiwei Tan](http://openreview.net/profile?id=~Shiwei_Tan1), [Hao Wang](http://openreview.net/profile?id=~Hao_Wang3)
  - **Affiliations:** Department of Computer Science, Rutgers University, New Jersey, USA, Department of Computer Science, Rutgers University, New Jersey, USA, Department of Computer Science, Rutgers University, New Jersey, USA
  - **TL;DR:** This paper introduces ProbAbilistic Concept Explainers (PACE), a variational Bayesian framework for providing trustworthy post-hoc explanations for Vision Transformers (ViTs). The proposed method addresses existing limitations in explainability by offering a multi-level structure of explanations and demonstrating superior performance compared to state-of-the-art methods.
  - **Keywords:** Explainability, Vision Transformers, Trustworthy Explanations, Variational Bayesian explanation framework, ProbAbilistic Concept Explainers (PACE), Computer Vision, Autonomous Driving, Trustworthy explanation methods, Post-hoc interpretations, Limitations of current methods, Improved conceptual explanations, Multi-level structure of explanations, Synthetic datasets, Real-world datasets, Patch embeddings, Self-attention blocks


- [Transitional Uncertainty with Layered Intermediate Predictions](https://icml.cc/virtual/2024/poster/32636) (Poster)
  - **Authors:** [Ryan Benkert](http://openreview.net/profile?id=~Ryan_Benkert1), [Mohit Prabhushankar](http://openreview.net/profile?id=~Mohit_Prabhushankar1), [Ghassan AlRegib](http://openreview.net/profile?id=~Ghassan_AlRegib1)
  - **Affiliations:** School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA, School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA, School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA
  - **TL;DR:** This paper introduces Transitional Uncertainty with Layered Intermediate Predictions (TULIP) to improve single-pass uncertainty estimation in neural networks by preserving feature distances through intermediate representations. The proposed method outperforms existing approaches, particularly in challenging settings such as imbalances and complex architectures.
  - **Keywords:** uncertainty estimation, feature engineering, deep learning, single-pass methods, transitional feature preservation, medical modalities, complex architectures, information compression, distributional shift, feature preservation, Transitional Uncertainty with Layered Intermediate Predictions (TULIP)


- [Memoria: Resolving Fateful Forgetting Problem through Human-Inspired Memory Architecture](https://icml.cc/virtual/2024/poster/32668) (Spotlight Poster)
  - **Authors:** [Sangjun Park](http://openreview.net/profile?id=~Sangjun_Park1), [JinYeong Bak](http://openreview.net/profile?id=~JinYeong_Bak2)
  - **Affiliations:** Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea, Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea
  - **TL;DR:** The study introduces Memoria, a memory system for neural networks inspired by human memory, addressing the issue of fateful forgetting by selectively preserving important information. Experimental results demonstrate its effectiveness in various tasks, surpassing conventional methods.
  - **Keywords:** long-term memory, artificial neural networks, human-inspired memory, external memory techniques, dynamic memory capacity, sorting, language modeling, classification, fateful forgetting, selective preservation, cue-based activation, Memoria memory system, engram analysis


- [GiLOT: Interpreting Generative Language Models via Optimal Transport](https://icml.cc/virtual/2024/poster/32996) (Poster)
  - **Authors:** [Xuhong Li](http://openreview.net/profile?id=~Xuhong_Li3), [Jiamin Chen](http://openreview.net/profile?id=~Jiamin_Chen2), [Yekun Chai](http://openreview.net/profile?id=~Yekun_Chai1), [Haoyi Xiong](http://openreview.net/profile?id=~Haoyi_Xiong1)
  - **Affiliations:** Baidu Inc., Beijing, China, Baidu Inc., Beijing, China, Baidu Inc., Beijing, China, Baidu Inc., Beijing, China
  - **TL;DR:** This study introduces GILOT, a method leveraging Optimal Transport to improve feature attribution for large language models (LLMs) by addressing challenges related to probability distributions and token similarity. The results demonstrate that GILOT outperforms existing methods in providing faithful explanations for LLM predictions.
  - **Keywords:** Generative AI, Explainability, Large Language Models, Optimal Transport, Feature Attribution, Dialogue Assistant, Code Generation, Multimodal Comprehension, Faithful Explanations, Semantic Distance, Autoregressive Language Models, GILOT, Input Attribution Estimation, Llama Families


- [Learning Universal Predictors](https://icml.cc/virtual/2024/poster/34740) (Poster)
  - **Authors:** [Jordi Grau-Moya](http://openreview.net/profile?id=~Jordi_Grau-Moya2), [Tim Genewein](http://openreview.net/profile?id=~Tim_Genewein1), [Marcus Hutter](http://openreview.net/profile?id=~Marcus_Hutter1), [Laurent Orseau](http://openreview.net/profile?id=~Laurent_Orseau1), [Gregoire Deletang](http://openreview.net/profile?id=~Gregoire_Deletang1), [Elliot Catt](http://openreview.net/profile?id=~Elliot_Catt1), [Anian Ruoss](http://openreview.net/profile?id=~Anian_Ruoss1), [Li Kevin Wenliang](http://openreview.net/profile?id=~Li_Kevin_Wenliang1), [Christopher Mattern](http://openreview.net/profile?id=~Christopher_Mattern1), [Matthew Aitchison](http://openreview.net/profile?id=~Matthew_Aitchison1), [Joel Veness](http://openreview.net/profile?id=~Joel_Veness2)
  - **Affiliations:** Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK
  - **TL;DR:** This study investigates the integration of Solomonoff Induction into neural networks through memory-based meta-learning, aiming to enhance their ability to learn from limited data. The findings indicate that training on data generated by Universal Turing Machines can significantly improve the networks' universal prediction capabilities.
  - **Keywords:** meta-learning, universal prediction, artificial general intelligence (AGI), Solomonoff Induction (SI), Universal Turing Machines (UTMs), memory-based meta-learning, Bayesian inference, learning new tasks quickly from limited data, designing broad task distributions, neural networks capable of learning universal prediction strategies, theoretical analysis of UTM data generation processes, LSTMs (Long Short-Term Memory networks), Transformers


- [Surprisingly Strong Performance Prediction with Neural Graph Features](https://icml.cc/virtual/2024/poster/34568) (Poster)
  - **Authors:** [Gabriela Kadlecová](http://openreview.net/profile?id=~Gabriela_Kadlecov%C3%A11), [Jovita Lukasik](http://openreview.net/profile?id=~Jovita_Lukasik1), [Martin Pilát](http://openreview.net/profile?id=~Martin_Pil%C3%A1t1), [Petra Vidnerová](http://openreview.net/profile?id=~Petra_Vidnerov%C3%A11), [Mahmoud Safari](http://openreview.net/profile?id=~Mahmoud_Safari1), [Roman Neruda](http://openreview.net/profile?id=~Roman_Neruda1), [Frank Hutter](http://openreview.net/profile?id=~Frank_Hutter1)
  - **Affiliations:** Charles University, Faculty of Mathematics and Physics; The Czech Academy of Sciences, Institute of Computer Science, University of Siegen, Charles University, Faculty of Mathematics and Physics, The Czech Academy of Sciences, Institute of Computer Science, University of Freiburg; ELLIS Institute Tübingen, The Czech Academy of Sciences, Institute of Computer Science, University of Freiburg; ELLIS Institute Tübingen
  - **TL;DR:** This paper introduces neural graph features (GRAF) as a method for performance prediction in neural architecture search (NAS), addressing the limitations of existing zero-cost proxies. GRAF provides fast, interpretable predictions and outperforms many existing performance predictors while requiring no training of networks.
  - **Keywords:** Neural Architecture Search (NAS), Performance Prediction, Zero-cost proxies, Neural graph features (GRAF), Deep learning, Neural network architecture optimization, High costs of neural network training, Biases in performance predictors, Strong and interpretable performance prediction, Improved performance over existing predictors


- [Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks](https://icml.cc/virtual/2024/poster/34251) (Oral)
  - **Authors:** [Liam Collins](http://openreview.net/profile?id=~Liam_Collins1), [Hamed Hassani](http://openreview.net/profile?id=~Hamed_Hassani2), [Mahdi Soltanolkotabi](http://openreview.net/profile?id=~Mahdi_Soltanolkotabi1), [Aryan Mokhtari](http://openreview.net/profile?id=~Aryan_Mokhtari3), [Sanjay Shakkottai](http://openreview.net/profile?id=~Sanjay_Shakkottai1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, Texas, USA, Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, Pennsylvania, USA, Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, California, USA, Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, Texas, USA, Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, Texas, USA
  - **TL;DR:** This study demonstrates that multi-task pretraining with a two-layer ReLU neural network effectively learns meaningful feature representations by recovering low-dimensional projections from high-dimensional data. The findings highlight the advantages of multi-task learning over single-task training in terms of generalization capabilities.
  - **Keywords:** Multi-task learning, Feature learning, Neural networks, Two-layer ReLU neural networks, Gradient-based algorithms, Binary classification tasks, Learning representations, Generalization to downstream tasks, Pseudo-contrastive loss, Recovery of low-dimensional projections


- [Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss](https://icml.cc/virtual/2024/poster/32862) (Poster)
  - **Authors:** [Zhenlong Liu](http://openreview.net/profile?id=~Zhenlong_Liu1), [Lei Feng](http://openreview.net/profile?id=~Lei_Feng1), [HUIPING ZHUANG](http://openreview.net/profile?id=~Huiping_Zhuang2), [Xiaofeng Cao](http://openreview.net/profile?id=~Xiaofeng_Cao2), [Hongxin Wei](http://openreview.net/profile?id=~Hongxin_Wei1)
  - **Affiliations:** Southern University of Science and Technology, Singapore University of Technology and Design, South China University of Technology, Jilin University, Southern University of Science and Technology
  - **TL;DR:** This study introduces a novel method called Convex-Concave Loss (CCL) to mitigate privacy risks associated with membership inference attacks in deep neural networks by increasing the variance of training loss distribution. The proposed method demonstrates a superior balance in the privacy-utility trade-off through extensive evaluations on multiple datasets.
  - **Keywords:** Membership Inference Attacks, Privacy Risk, Deep Neural Networks, Convex-Concave Loss (CCL), Gradient Descent, Health Care, Financial Services, DNA Sequence Analysis, Membership Inference Attacks, Loss Variance, Privacy Risk, High Variance of Training Loss Distribution, Privacy-Utility Trade-off, Texas100, Purchase100, CIFAR-10, CIFAR-100


- [Overcoming Data and Model heterogeneities in Decentralized Federated Learning via Synthetic Anchors](https://icml.cc/virtual/2024/poster/33176) (Poster)
  - **Authors:** [Chun-Yin Huang](http://openreview.net/profile?id=~Chun-Yin_Huang1), [Kartik Srinivas](http://openreview.net/profile?id=~Kartik_Srinivas1), [Xin Zhang](http://openreview.net/profile?id=~Xin_Zhang16), [Xiaoxiao Li](http://openreview.net/profile?id=~Xiaoxiao_Li1)
  - **Affiliations:** Department of Electrical and Computer Engineering, The University of British Columbia, Canada; Vector Institute, Canada, Indian Institute of Technology Hyderabad, India, Meta, U.S.A., Department of Electrical and Computer Engineering, The University of British Columbia, Canada; Vector Institute, Canada
  - **TL;DR:** This study introduces a novel decentralized federated learning technique called DESA, which utilizes synthetic anchors to enhance model generalizability among clients with heterogeneous data and models. The proposed method effectively facilitates mutual knowledge transfer and improves both inter- and intra-domain accuracy through specific regularization strategies.
  - **Keywords:** Federated Learning, Decentralized Learning, Knowledge Distillation, Domain Adaptation, Data Heterogeneity, Model Heterogeneity, Generalization Capability, Synthetic Anchors, REG loss, KD loss


- [Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations](https://icml.cc/virtual/2024/poster/32684) (Poster)
  - **Authors:** [Jiaqi Zhai](http://openreview.net/profile?id=~Jiaqi_Zhai1), [Yunxing Liao](http://openreview.net/profile?id=~Lucy_Liao1), [Xing Liu](http://openreview.net/profile?id=~Xing_Liu5), [Yueming Wang](http://openreview.net/profile?id=~Yueming_Wang4), [Rui Li](http://openreview.net/profile?id=~Rui_Li32), [Xuan Cao](http://openreview.net/profile?id=~Xuan_Cao2), [Yazhi Gao](http://openreview.net/profile?id=~Leon_Gao1), [Zhaojie Gong](http://openreview.net/profile?id=~Zhaojie_Gong1), [Fangda Gu](http://openreview.net/profile?id=~Fangda_Gu1), [Michael He](http://openreview.net/profile?id=~Jiayuan_He2), [Yinghai Lu](http://openreview.net/profile?id=~Yinghai_Lu1), [Yu Shi](http://openreview.net/profile?id=~Yu_Shi1)
  - **Affiliations:** MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI, MRS, Meta AI
  - **TL;DR:** This paper presents a new architecture, HSTU, for generative recommendation systems that reformulates recommendation tasks as sequential transduction problems, achieving significant performance improvements over existing models. The findings indicate that model quality scales with training compute, paving the way for future foundation models in recommendations while reducing carbon footprint.
  - **Keywords:** recommendation systems, generative modeling, sequential transduction, deep learning recommendation models (DLRMs), Transformers, online content platforms, e-commerce, high cardinality features, scaling with compute, data sparsity, HSTU architecture, improved NDCG metrics, online A/B test improvements, synthetic datasets, public datasets, Generative Recommenders, FlashAttention2, power-law scaling


- [Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products](https://icml.cc/virtual/2024/poster/34907) (Poster)
  - **Authors:** [Guy Bar Shalom](http://openreview.net/profile?id=~Guy_Bar-Shalom1), [Beatrice Bevilacqua](http://openreview.net/profile?id=~Beatrice_Bevilacqua1), [Haggai Maron](http://openreview.net/profile?id=~Haggai_Maron1)
  - **Affiliations:** Department of Computer Science, Technion - Israel Institute of Technology, Department of Computer Science, Purdue University, Department of Electrical & Computer Engineering, Technion - Israel Institute of Technology; NVIDIA Research
  - **TL;DR:** This paper introduces Subgraphormer, a novel architecture that unifies Subgraph GNNs and Graph Transformers by leveraging their strengths through a new attention mechanism and positional encoding scheme. Experimental results show significant performance improvements over both approaches across various datasets.
  - **Keywords:** Graph Neural Networks (GNNs), Subgraph GNNs, Graph Transformers, Message Passing Neural Networks (MPNNs), Subgraph Attention Block (SAB), product graph positional encoding, Expressive power limitations of traditional GNNs, Subgraphormer architecture, performance improvements over existing GNNs and Graph Transformers


- [Diversified Batch Selection for Training Acceleration](https://icml.cc/virtual/2024/poster/34979) (Poster)
  - **Authors:** [Feng Hong](http://openreview.net/profile?id=~Feng_Hong1), [Yueming LYU](http://openreview.net/profile?id=~Yueming_Lyu1), [Jiangchao Yao](http://openreview.net/profile?id=~Jiangchao_Yao1), [Ya Zhang](http://openreview.net/profile?id=~Ya_Zhang1), [Ivor Tsang](http://openreview.net/profile?id=~Ivor_Tsang1), [Yanfeng Wang](http://openreview.net/profile?id=~Yanfeng_Wang1)
  - **Affiliations:** Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China, CFAR, Agency for Science, Technology and Research (A*STAR), Singapore; IHPC, Agency for Science, Technology and Research (A*STAR), Singapore, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, CFAR, Agency for Science, Technology and Research (A*STAR), Singapore; IHPC, Agency for Science, Technology and Research (A*STAR), Singapore; College of Computing and Data Science, NTU, Singapore, Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China
  - **TL;DR:** This paper introduces Diversified Batch Selection (DivBS), a reference-model-free method for selecting diverse and representative samples to accelerate training in machine learning models. The proposed method effectively addresses redundancy in sample selection, demonstrating significant improvements in performance-speedup trade-off through extensive experiments.
  - **Keywords:** online batch selection, training acceleration, machine learning, Diversified Batch Selection (DivBS), reference-model-free methods, extensive training time, resource consumption, data quality issues, redundancy in data selection, group-wise orthogonalized representativeness, performance-speedup trade-off


- [A Geometric Decomposition of Finite Games: Convergence vs. Recurrence under Exponential Weights](https://icml.cc/virtual/2024/poster/34881) (Spotlight Poster)
  - **Authors:** [Davide Legacci](http://openreview.net/profile?email=davide.legacci%40univ-grenoble-alpes.fr), [Panayotis Mertikopoulos](http://openreview.net/profile?id=~Panayotis_Mertikopoulos1), [Bary Pradelski](http://openreview.net/profile?id=~Bary_Pradelski1)
  - **Affiliations:** Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France, Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France, CNRS, Maison Française d’Oxford, 2–10 Norham Road, Oxford, OX2 6SE, United Kingdom
  - **TL;DR:** This study explores the dynamics of learning in games by decomposing them into simpler components, establishing that continuous-time exponential weight dynamics in incompressible games are Poincaré recurrent. The findings connect harmonic games with the concept of incompressibility, suggesting that players' strategies can exhibit complex long-term behavior despite the challenges of convergence to Nash equilibrium.
  - **Keywords:** Game dynamics, Learning in games, Non-cooperative game theory, Exponential weights (EW) update scheme, Riemannian framework, Shahshahani metric, Machine learning, AI, Multi-agent reinforcement learning, Convergence to rational outcomes, Coarse correlated equilibria (CCE), Nash equilibrium, Poincaré recurrence, Incompressible games, Harmonic games, Coarse correlated equilibria (CCE), Nash equilibrium (NE), No-regret learning


- [MALIBO: Meta-learning for Likelihood-free Bayesian Optimization](https://icml.cc/virtual/2024/poster/35064) (Spotlight Poster)
  - **Authors:** [Jiarong Pan](http://openreview.net/profile?id=~Jiarong_Pan1), [Stefan Falkner](http://openreview.net/profile?id=~Stefan_Falkner1), [Felix Berkenkamp](http://openreview.net/profile?id=~Felix_Berkenkamp1), [Joaquin Vanschoren](http://openreview.net/profile?id=~Joaquin_Vanschoren1)
  - **Affiliations:** Bosch Center for Artificial Intelligence, Germany; Eindhoven University of Technology, Netherlands, Bosch Center for Artificial Intelligence, Germany, Bosch Center for Artificial Intelligence, Germany, Eindhoven University of Technology, Netherlands
  - **TL;DR:** This study presents a novel meta-learning approach for Bayesian optimization that bypasses traditional surrogate models, directly learning the utility of queries across tasks while explicitly modeling task uncertainty. The proposed method demonstrates strong performance and outperforms existing meta-learning Bayesian optimization methods across various benchmarks.
  - **Keywords:** Bayesian optimization, meta-learning, task similarity, task adaptation, observation noise, novel meta-learning BO approach, utility of queries, Gaussian process, surrogate models


- [Stability and Multigroup Fairness in Ranking with Uncertain Predictions](https://icml.cc/virtual/2024/poster/33765) (Poster)
  - **Authors:** [Siddartha Devic](http://openreview.net/profile?id=~Siddartha_Devic1), [Aleksandra Korolova](http://openreview.net/profile?id=~Aleksandra_Korolova1), [David Kempe](http://openreview.net/profile?id=~David_Kempe1), [Vatsal Sharan](http://openreview.net/profile?id=~Vatsal_Sharan1)
  - **Affiliations:** Department of Computer Science, University of Southern California, Department of Computer Science and Public Affairs, Princeton University, Department of Computer Science, University of Southern California, Department of Computer Science, University of Southern California
  - **TL;DR:** This paper explores how to derive meaningful rankings from uncertain predictions in classification tasks, focusing on the stability and fairness of ranking functions. The authors demonstrate that uncertainty aware (UA) ranking functions can achieve both group and individual fairness while maintaining stability in rankings.
  - **Keywords:** rankings, uncertainty in predictions, fairness, uncertainty aware (UA) ranking functions, job applications, ad marketplaces, social media content ranking, intrinsic uncertainty in predictions, stability in rankings, fairness towards individuals and subgroups, group fairness, individual fairness, stability guarantees


- [Prompt Sketching for Large Language Models](https://icml.cc/virtual/2024/poster/35100) (Poster)
  - **Authors:** [Luca Beurer-Kellner](http://openreview.net/profile?id=~Luca_Beurer-Kellner1), [Mark Müller](http://openreview.net/profile?id=~Mark_Niklas_Mueller2), [Marc Fischer](http://openreview.net/profile?id=~Marc_Fischer1), [Martin Vechev](http://openreview.net/profile?id=~Martin_Vechev1)
  - **Affiliations:** Department of Computer Science, ETH Zürich, Switzerland, Department of Computer Science, ETH Zürich, Switzerland, Department of Computer Science, ETH Zürich, Switzerland, Department of Computer Science, ETH Zürich, Switzerland
  - **TL;DR:** This study introduces prompt sketching, a new prompting paradigm for large language models that enhances control over the generation process by predicting multiple variables in a template. The proposed method outperforms existing sequential prompting strategies in various reasoning tasks, demonstrating improved overall results.
  - **Keywords:** prompt sketching, large language models (LLMs), template-guided inference, stop-and-go inference, reasoning tasks, state tracking, arithmetic reasoning, question answering, disconnected intermediate responses, stochasticity in LLMs, improved prompting strategies, better overall results in LLM tasks


- [Efficient Exploration for LLMs](https://icml.cc/virtual/2024/poster/34107) (Poster)
  - **Authors:** [Vikranth Dwaracherla](http://openreview.net/profile?id=~Vikranth_Dwaracherla1), [Seyed Mohammad Asghari](http://openreview.net/profile?id=~Seyed_Mohammad_Asghari1), [Botao Hao](http://openreview.net/profile?id=~Botao_Hao1), [Benjamin Van Roy](http://openreview.net/profile?id=~Benjamin_Van_Roy3)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind; Stanford University
  - **TL;DR:** This study demonstrates the significant benefits of efficient exploration in gathering human feedback to enhance large language models, showing that high performance can be achieved with fewer queries. The findings suggest that active exploration strategies, particularly those utilizing uncertainty estimation, can accelerate the development of superhuman capabilities in these models.
  - **Keywords:** Efficient exploration, Human feedback, Large language models, Reinforcement learning from human feedback (RLHF), Double Thompson sampling, Epistemic neural network (ENN), Boltzmann exploration, Gathering human feedback, Superhuman ingenuity, Active exploration, High performance with fewer queries, Reward model fitting


- [Amortizing Pragmatic Program Synthesis with Rankings](https://icml.cc/virtual/2024/poster/32658) (Poster)
  - **Authors:** [Yewen Pu](http://openreview.net/profile?id=~Yewen_Pu1), [Saujas Vaduguru](http://openreview.net/profile?id=~Saujas_Vaduguru1), [Priyan Vaithilingam](http://openreview.net/profile?id=~Priyan_Vaithilingam1), [Elena Glassman](http://openreview.net/profile?id=~Elena_Glassman1), [Daniel Fried](http://openreview.net/profile?id=~Daniel_Fried1)
  - **Affiliations:** Autodesk AI Research, Carnegie Mellon University, Harvard SEAS, Harvard SEAS, Carnegie Mellon University
  - **TL;DR:** This paper presents a method for amortizing the slow Rational Speech Acts (RSA) synthesizer by creating a global ranking of programs based on user-generated examples. The proposed approach significantly speeds up the synthesis process while maintaining accuracy in understanding user intent.
  - **Keywords:** Pragmatic program synthesis, User intent inference, Rational Speech Acts (RSA), Bayesian reasoning, Program synthesis, Interactive systems, Ambiguity resolution, Informative example selection, Amortized RSA synthesizer, Global ranking of programs


- [Learning and Forgetting Unsafe Examples in Large Language Models](https://icml.cc/virtual/2024/poster/34049) (Poster)
  - **Authors:** [Jiachen Zhao](http://openreview.net/profile?id=~Jiachen_Zhao4), [Zhun Deng](http://openreview.net/profile?id=~Zhun_Deng1), [David Madras](http://openreview.net/profile?id=~David_Madras1), [James Zou](http://openreview.net/profile?id=~James_Zou1), [Mengye Ren](http://openreview.net/profile?id=~Mengye_Ren1)
  - **Affiliations:** University of Massachusetts Amherst, Columbia University, Google, Stanford University, New York University
  - **TL;DR:** This study investigates the safety implications of large language models (LLMs) finetuned on unsafe data and introduces the ForgetFilter algorithm to mitigate the assimilation of harmful content during custom finetuning. The findings demonstrate that ForgetFilter significantly reduces toxicity scores while maintaining performance on downstream tasks.
  - **Keywords:** Large Language Models, AI Safety, Custom Finetuning, ForgetFilter algorithm, safety finetuning, Customer service chatbots, downstream task performance, Unsafe content learning, catastrophic forgetting, Improved safety in customized finetuning, reduced toxicity scores


- [In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering](https://icml.cc/virtual/2024/poster/33561) (Poster)
  - **Authors:** [Sheng Liu](http://openreview.net/profile?id=~Sheng_Liu2), [Haotian Ye](http://openreview.net/profile?id=~Haotian_Ye1), [Lei Xing](http://openreview.net/profile?id=~Lei_Xing1), [James Zou](http://openreview.net/profile?id=~James_Y._Zou1)
  - **Affiliations:** Department of Biomedical Data Science, Stanford University; Department of Biomedical Data Science, Stanford University, Department of Computer Science, Stanford University, Department of Radiation Oncology, Stanford University, Department of Biomedical Data Science, Stanford University
  - **TL;DR:** This paper introduces the In-Context Vector (ICV) approach to enhance the effectiveness and controllability of in-context learning in large language models. The ICV method demonstrates improved performance across various tasks while reducing computational overhead and prompt length.
  - **Keywords:** In-context learning, Large language models, In-context vectors (ICV), Latent space steering, Safety, Style transfer, Role-playing, Formatting, Limited effectiveness of in-context learning, Difficulty in quantitative control, Context window limitations, Improved performance over standard in-context learning and fine-tuning, Efficient task information extraction


- [Prospector Heads: Generalized Feature Attribution for Large Models & Data](https://icml.cc/virtual/2024/poster/34113) (Poster)
  - **Authors:** [Gautam Machiraju](http://openreview.net/profile?id=~Gautam_Machiraju1), [Alexander Derry](http://openreview.net/profile?id=~Alexander_Derry1), [Arjun Desai](http://openreview.net/profile?id=~Arjun_D_Desai1), [Neel Guha](http://openreview.net/profile?id=~Neel_Guha1), [Amir-Hossein Karimi](http://openreview.net/profile?id=~Amir-Hossein_Karimi1), [James Zou](http://openreview.net/profile?id=~James_Zou1), [Russ B Altman](http://openreview.net/profile?id=~Russ_B_Altman1), [Christopher Re](http://openreview.net/profile?id=~Christopher_Re1), [Parag Mallick](http://openreview.net/profile?id=~Parag_Mallick1)
  - **Affiliations:** Department of Biomedical Data Science, Stanford University, Department of Biomedical Data Science, Stanford University, Cartesia AI, Department of Computer Science, Stanford University, Department of Electrical & Computer Engineering, University of Waterloo, Department of Biomedical Data Science, Stanford University, Department of Biomedical Data Science, Stanford University, Department of Computer Science, Stanford University, Department of Radiology, Stanford University
  - **TL;DR:** This study introduces prospector heads as an efficient and interpretable method for feature attribution in machine learning, addressing challenges in localization and data efficiency. The method demonstrates significant improvements in interpreting class-specific patterns across various data modalities, enhancing trust and transparency in complex domains.
  - **Keywords:** Feature attribution, Machine learning, Interpretability, Prospector heads, Explanation-based attribution methods, Scientific applications, Biomedical applications, Image analysis, Imprecise feature localization, Data inefficiency, Computational challenges, High-dimensional datasets, Improved feature attribution, Generalization across modalities, Enhanced interpretation of input data, Trust and transparency in ML models


- [Rethinking Data Shapley for Data Selection Tasks: Misleads and Merits](https://icml.cc/virtual/2024/poster/33177) (Oral)
  - **Authors:** [Jiachen Wang](http://openreview.net/profile?id=~Jiachen_T._Wang1), [Tianji Yang](http://openreview.net/profile?id=~Tianji_Yang1), [James Zou](http://openreview.net/profile?id=~James_Zou1), [Yongchan Kwon](http://openreview.net/profile?id=~Yongchan_Kwon1), [Ruoxi Jia](http://openreview.net/profile?id=~Ruoxi_Jia1)
  - **Affiliations:** Princeton University, East China Normal University, Stanford University, Columbia University, Virginia Tech
  - **TL;DR:** This study investigates the inconsistent performance of Data Shapley in data selection tasks, introducing a hypothesis testing framework to analyze its effectiveness. The findings reveal that Data Shapley can perform no better than random selection without specific constraints on utility functions, providing insights into when it may succeed or fail.
  - **Keywords:** Data Shapley, data valuation, data selection, data-centric machine learning, Hypothesis testing framework, utility functions, monotonically transformed modular functions, Healthcare, machine learning model training, Inconsistent data selection performance, noise and bias in data, Heuristic for predicting Data Shapley’s effectiveness, insights into data selection success, Shapley value, cooperative game theory


- [Scaling Laws for the Value of Individual Data Points in Machine Learning](https://icml.cc/virtual/2024/poster/32901) (Poster)
  - **Authors:** [Ian Covert](http://openreview.net/profile?id=~Ian_Connick_Covert1), [Wenlong Ji](http://openreview.net/profile?id=~Wenlong_Ji1), [Tatsunori Hashimoto](http://openreview.net/profile?id=~Tatsunori_Hashimoto1), [James Zou](http://openreview.net/profile?id=~James_Zou1)
  - **Affiliations:** Stanford University, Stanford University, Stanford University, Stanford University
  - **TL;DR:** This study introduces individualized scaling laws that describe how the contribution of individual data points to model performance diminishes predictably as dataset size increases. The findings highlight the variability in the value of data points, providing insights for data selection and prioritization in machine learning.
  - **Keywords:** scaling laws, individual data points, machine learning, maximum likelihood estimator, amortized estimator, data valuation, data subset selection, diminishing contribution of data points, variability in data point value, individualized data scaling laws, insights into data point scaling behavior


- [PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels](https://icml.cc/virtual/2024/poster/33417) (Poster)
  - **Authors:** [Praneeth Kacham](http://openreview.net/profile?id=~Praneeth_Kacham1), [Vahab Mirrokni](http://openreview.net/profile?id=~Vahab_Mirrokni2), [Peilin Zhong](http://openreview.net/profile?id=~Peilin_Zhong1)
  - **Affiliations:** Google Research; Carnegie Mellon University, Google Research, Google Research
  - **TL;DR:** This paper introduces PolySketchFormer, a linear-time Transformer architecture that utilizes polynomial attention to address the computational bottlenecks of traditional self-attention mechanisms. The proposed method achieves a 2x speedup in training for long context lengths without sacrificing model quality, validated through experiments on various datasets.
  - **Keywords:** Fast Transformers, Polynomial Attention, Language Modeling, Polynomial Sketching, Self-Attention Mechanisms, Natural Language Processing, Language Modeling, Quadratic Time Complexity, Memory Complexity, Scalability Issues, Linear-Time Polynomial Attention, Training Speedup, Approximation Guarantees, PG19, Wikipedia, C4, Google Cloud TPUs, Transformer, Softmax Attention, FlashAttention


- [Assessing Large Language Models on Climate Information](https://icml.cc/virtual/2024/poster/34000) (Poster)
  - **Authors:** [Jannis Bulian](http://openreview.net/profile?id=~Jannis_Bulian1), [Mike Schäfer](http://openreview.net/profile?id=~Mike_S._Sch%C3%A4fer1), [Afra Amini](http://openreview.net/profile?id=~Afra_Amini1), [Heidi Lam](http://openreview.net/profile?id=~Heidi_Lam1), [Massimiliano Ciaramita](http://openreview.net/profile?id=~Massimiliano_Ciaramita2), [Ben Gaiarin](http://openreview.net/profile?id=~Ben_Gaiarin1), [Michelle Chen Huebscher](http://openreview.net/profile?id=~Michelle_Chen_Huebscher1), [Christian Buck](http://openreview.net/profile?id=~Christian_Buck1), [Niels Mede](http://openreview.net/profile?id=~Niels_G._Mede1), [Markus Leippold](http://openreview.net/profile?id=~Markus_Leippold1), [Nadine Strauss](http://openreview.net/profile?id=~Nadine_Strauss1)
  - **Affiliations:** Google DeepMind, IKMZ - Dept. of Communication and Media Research, University of Zurich, Switzerland, Google DeepMind; ETH AI Center, ETH, Zurich, Switzerland, Google, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, IKMZ - Dept. of Communication and Media Research, University of Zurich, Switzerland, Google DeepMind; Dept. of Finance, University of Zurich, Switzerland, IKMZ - Dept. of Communication and Media Research, University of Zurich, Switzerland
  - **TL;DR:** This study presents a comprehensive evaluation framework for assessing Large Language Models' responses to climate change questions, highlighting significant gaps in their epistemological qualities. The findings emphasize the need for scalable oversight protocols to improve the quality of AI-generated climate information.
  - **Keywords:** Large Language Models, Climate Change, Science Communication, Evaluation Framework, AI Assistance, Climate Information, Digital Media, Climate Literacy Gap, Mis- and Disinformation, Factuality Concerns, Scalable Oversight Protocols, Evaluation of LLM Responses, Epistemological Adequacy, Presentational Adequacy


- [Reweighted Solutions for Weighted Low Rank Approximation](https://icml.cc/virtual/2024/poster/32968) (Poster)
  - **Authors:** [David Woodruff](http://openreview.net/profile?id=~David_Woodruff1), [Taisuke Yasuda](http://openreview.net/profile?id=~Taisuke_Yasuda1)
  - **Affiliations:** School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA
  - **TL;DR:** This paper introduces a new relaxed solution for the weighted low rank approximation (WLRA) problem, which efficiently utilizes the weight matrix to improve approximation guarantees and communication complexity. The proposed method demonstrates strong empirical performance in model compression and other applications.
  - **Keywords:** Weighted low rank approximation, matrix approximation, Singular value decomposition (SVD), reweighting algorithms, Statistical analysis, model compression, signal processing, recommender systems, NP-hardness, computational challenges, feature selection, New relaxed solution for WLRA, provable approximation guarantees, communication complexity bounds


- [How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis](https://icml.cc/virtual/2024/poster/34663) (Poster)
  - **Authors:** [Federico Bianchi](http://openreview.net/profile?id=~Federico_Bianchi1), [Patrick John Chia](http://openreview.net/profile?id=~Patrick_John_Chia1), [Mert Yuksekgonul](http://openreview.net/profile?id=~Mert_Yuksekgonul1), [Jacopo Tagliabue](http://openreview.net/profile?id=~Jacopo_Tagliabue1), [Dan Jurafsky](http://openreview.net/profile?id=~Dan_Jurafsky1), [James Zou](http://openreview.net/profile?id=~James_Zou1)
  - **Affiliations:** Stanford University, Stanford, California, Independent, Stanford University, Stanford, California, Bauplan, New York, New York, Stanford University, Stanford, California, Stanford University, Stanford, California
  - **TL;DR:** This study investigates the negotiation capabilities of large language models (LLMs) using a framework called NEGOTIATION ARENA, revealing that LLMs can enhance their negotiation outcomes through specific behavioral tactics. The findings highlight the potential for LLMs to exhibit both rational and irrational negotiation behaviors, offering insights into their reasoning abilities and theory of mind.
  - **Keywords:** negotiation, large language models (LLMs), negotiation behaviors, irrational negotiation behaviors, NEGOITATION ARENA framework, behavioral tactics in negotiation


- [Align Your Steps: Optimizing Sampling Schedules in Diffusion Models](https://icml.cc/virtual/2024/poster/33134) (Poster)
  - **Authors:** [Amirmojtaba Sabour](http://openreview.net/profile?id=~Amirmojtaba_Sabour1), [Sanja Fidler](http://openreview.net/profile?id=~Sanja_Fidler1), [Karsten Kreis](http://openreview.net/profile?id=~Karsten_Kreis1)
  - **Affiliations:** NVIDIA, Toronto, Canada; Department of Computer Science, University of Toronto, Toronto, Ontario; Vector Institute, Toronto, Canada, NVIDIA, Toronto, Canada; Department of Computer Science, University of Toronto, Toronto, Ontario; Vector Institute, Toronto, Canada, NVIDIA, Toronto, Canada
  - **TL;DR:** This study introduces Align Your Steps (AYS), a novel framework for optimizing sampling schedules in diffusion models, addressing the slow sampling speed issue. The proposed method significantly enhances output quality, particularly in few-step synthesis scenarios, outperforming traditional hand-crafted schedules.
  - **Keywords:** diffusion models, generative modeling, stochastic calculus, sampling schedules, image synthesis, video synthesis, 2D toy data synthesis, slow sampling speed, optimization of sampling schedules, Align Your Steps (AYS), optimized sampling schedules


- [Fast, Scalable, Warm-Start Semidefinite Programming with Spectral Bundling and Sketching](https://icml.cc/virtual/2024/poster/33411) (Poster)
  - **Authors:** [Rico Angell](http://openreview.net/profile?id=~Rico_Angell1), [Andrew McCallum](http://openreview.net/profile?id=~Andrew_McCallum1)
  - **Affiliations:** Manning College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, MA, USA, Manning College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, MA, USA
  - **TL;DR:** This paper presents Unified Spectral Bundling with Sketching (USBS), a fast and scalable algorithm for solving large semidefinite programming problems that effectively utilizes warm-start initialization to enhance convergence. The method demonstrates a significant speed-up over existing solvers, achieving a 500x improvement on large instances with over 2 billion decision variables.
  - **Keywords:** semidefinite programming, optimization, Unified Spectral Bundling with Sketching (USBS), matrix sketching, spectral bundle method, combinatorial optimization, neural network verification, robotics, optimal experiment design, VLSI, systems and control theory, scalability, convergence speed, high computational complexity, fast and scalable algorithm, warm-start initialization, improved scalability, JAX


- [Incremental Topological Ordering and Cycle Detection with Predictions](https://icml.cc/virtual/2024/poster/32743) (Poster)
  - **Authors:** [Samuel McCauley](http://openreview.net/profile?id=~Samuel_McCauley1), [Benjamin Moseley](http://openreview.net/profile?id=~Benjamin_Moseley1), [Aidin Niaparast](http://openreview.net/profile?id=~Aidin_Niaparast1), [Shikha Singh](http://openreview.net/profile?id=~Shikha_Singh2)
  - **Affiliations:** Department of Computer Science, Williams College, Williamstown, MA 01267 USA, Tepper School of Business, Carnegie Mellon University, Pittsburgh, PA 15213 USA, Tepper School of Business, Carnegie Mellon University, Pittsburgh, PA 15213 USA, Department of Computer Science, Williams College, Williamstown, MA 01267 USA
  - **TL;DR:** This paper presents a novel data structure leveraging predictions to efficiently solve dynamic graph problems, specifically incremental topological ordering and cycle detection. The proposed method demonstrates significant performance improvements over traditional algorithms, particularly in scenarios with high update costs.
  - **Keywords:** dynamic graph problems, algorithms-with-predictions, topological ordering, cycle detection, warm starting, high update cost, cycle creation detection, learned data structures, performance optimization, small training dataset


- [Distributional Bellman Operators over Mean Embeddings](https://icml.cc/virtual/2024/poster/33330) (Poster)
  - **Authors:** [Li Kevin Wenliang](http://openreview.net/profile?id=~Li_Kevin_Wenliang1), [Gregoire Deletang](http://openreview.net/profile?id=~Gregoire_Deletang1), [Matthew Aitchison](http://openreview.net/profile?id=~Matthew_Aitchison1), [Marcus Hutter](http://openreview.net/profile?id=~Marcus_Hutter1), [Anian Ruoss](http://openreview.net/profile?id=~Anian_Ruoss1), [Arthur Gretton](http://openreview.net/profile?id=~Arthur_Gretton1), [Mark Rowland](http://openreview.net/profile?id=~Mark_Rowland1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Gatsby Unit, University College London, Google DeepMind
  - **TL;DR:** This paper introduces a novel framework for distributional reinforcement learning that utilizes mean embeddings of return distributions, leading to efficient algorithms that update these embeddings through simple linear operations. The proposed methods demonstrate competitive performance in deep reinforcement learning tasks while addressing computational challenges associated with traditional approaches.
  - **Keywords:** distributional reinforcement learning, return distributions, sketch Bellman operator, mean embeddings, dynamic programming, temporal-difference algorithms, deep reinforcement learning, simulation, real-world applications, computational overhead in Bellman updates, decoding statistical functional values, new algorithms for distributional RL, theoretical convergence analysis


- [Lyapunov-stable Neural Control for State and Output Feedback: A Novel Formulation](https://icml.cc/virtual/2024/poster/35042) (Poster)
  - **Authors:** [Lujie Yang](http://openreview.net/profile?id=~Lujie_Yang1), [Hongkai Dai](http://openreview.net/profile?id=~Hongkai_Dai1), [Zhouxing Shi](http://openreview.net/profile?id=~Zhouxing_Shi1), [Cho-Jui Hsieh](http://openreview.net/profile?id=~Cho-Jui_Hsieh1), [Russ Tedrake](http://openreview.net/profile?id=~Russ_Tedrake1), [Huan Zhang](http://openreview.net/profile?id=~Huan_Zhang1)
  - **Affiliations:** MIT; Toyota Research Institute, Toyota Research Institute, UCLA, UCLA, MIT; Toyota Research Institute, UIUC
  - **TL;DR:** This paper presents a novel framework for synthesizing neural network controllers with Lyapunov stability guarantees, addressing the challenges of formal stability in nonlinear dynamical systems. The approach allows for efficient training and verification, demonstrating Lyapunov-stable output feedback control for the first time in literature.
  - **Keywords:** Neural network control, Lyapunov stability, Robotics, Lyapunov certificates, Empirical falsification, Linear bound propagation, Nonlinear dynamical systems, Output feedback control, Stability guarantees, Region-of-attraction (ROA) challenges, New framework for NN controllers, Formal stability guarantees


- [Private Heterogeneous Federated Learning Without a Trusted Server Revisited: Error-Optimal and Communication-Efficient Algorithms for Convex Losses](https://icml.cc/virtual/2024/poster/32910) (Poster)
  - **Authors:** [Changyu Gao](http://openreview.net/profile?id=~Changyu_Gao1), [Andrew Lowy](http://openreview.net/profile?id=~Andrew_Lowy1), [Xingyu Zhou](http://openreview.net/profile?id=~Xingyu_Zhou2), [Stephen Wright](http://openreview.net/profile?id=~Stephen_Wright1)
  - **Affiliations:** University of Wisconsin-Madison, Madison, WI, USA, University of Wisconsin-Madison, Madison, WI, USA, Wayne State University, Detroit, MI, USA, University of Wisconsin-Madison, Madison, WI, USA
  - **TL;DR:** This paper addresses the challenges of federated learning with private data in a heterogeneous setting, proposing novel algorithms that achieve optimal excess risk bounds while being more communication-efficient than previous methods. The findings highlight the importance of Inter-Silo Record-Level Differential Privacy in protecting sensitive data across multiple silos.
  - **Keywords:** Federated Learning, Privacy Protection, Inter-Silo Record-Level Differential Privacy (ISRL-DP), Differential Privacy (DP), Healthcare, Finance, Large Language Models (LLMs), Data privacy, Heterogeneous data, Communication efficiency, Optimal excess risk bounds, Communication-efficient algorithms


- [When Do Skills Help Reinforcement Learning? A Theoretical Analysis of Temporal Abstractions](https://icml.cc/virtual/2024/poster/35079) (Poster)
  - **Authors:** [Zhening Li](http://openreview.net/profile?id=~Zhening_Li1), [Gabriel Poesia](http://openreview.net/profile?id=~Gabriel_Poesia1), [Armando Solar-Lezama](http://openreview.net/profile?id=~Armando_Solar-Lezama1)
  - **Affiliations:** MIT CSAIL, Cambridge, MA, USA, Stanford University, Stanford, CA, USA, MIT CSAIL, Cambridge, MA, USA
  - **TL;DR:** This study provides a theoretical analysis of the utility of skills in reinforcement learning, particularly in deterministic sparse-reward environments. It concludes that skills can enhance exploration but may not always improve learning efficiency, highlighting the need for better understanding in automatic skill discovery.
  - **Keywords:** reinforcement learning, hierarchical reinforcement learning, temporal abstractions, sparse rewards, exploration challenges, compressibility of solutions, automatic skill discovery, theoretical analysis of skills, skills, macroactions, options


- [Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities](https://icml.cc/virtual/2024/poster/33859) (Poster)
  - **Authors:** [Zhifeng Kong](http://openreview.net/profile?id=~Zhifeng_Kong1), [ARUSHI GOEL](http://openreview.net/profile?id=~Arushi_Goel2), [Rohan Badlani](http://openreview.net/profile?id=~Rohan_Badlani1), [Wei Ping](http://openreview.net/profile?id=~Wei_Ping1), [Rafael Valle](http://openreview.net/profile?id=~Rafael_Valle1), [Bryan Catanzaro](http://openreview.net/profile?id=~Bryan_Catanzaro1)
  - **Affiliations:** NVIDIA, CA, USA, NVIDIA, CA, USA, NVIDIA, CA, USA, NVIDIA, CA, USA, NVIDIA, CA, USA, NVIDIA, CA, USA
  - **TL;DR:** The study introduces Audio Flamingo, a novel audio language model designed to enhance the understanding of audio, including non-speech sounds, and to facilitate multi-turn dialogues. It achieves state-of-the-art performance on various audio understanding tasks through innovative training techniques and architecture design.
  - **Keywords:** audio understanding, large language models (LLMs), multimodal learning, in-context learning (ICL), retrieval augmented generation (RAG), cross attention, audio processing, dialogue systems, understanding non-speech sounds, adapting to unseen tasks, feature extraction from variable-length audio, Audio Flamingo model, state-of-the-art results on audio understanding tasks, heterogeneous dataset with 5.9 million audio-text pairs


- [Model-Free Robust $\phi$-Divergence Reinforcement Learning Using Both Offline and Online Data](https://icml.cc/virtual/2024/poster/33754) (Poster)
  - **Authors:** [Kishan Panaganti](http://openreview.net/profile?id=~Kishan_Panaganti1), [Adam Wierman](http://openreview.net/profile?id=~Adam_Wierman1), [Eric Mazumdar](http://openreview.net/profile?id=~Eric_Mazumdar1)
  - **Affiliations:** Computing + Mathematical Sciences Department, California Institute of Technology, Computing + Mathematical Sciences Department, California Institute of Technology, Computing + Mathematical Sciences Department, California Institute of Technology
  - **TL;DR:** This study presents a model-free algorithm for robust reinforcement learning that addresses parameter uncertainties and out-of-data-distribution issues by leveraging both historical and online data. The proposed methods achieve ε-optimal robust policies in high-dimensional state spaces, enhancing the robustness of RL agents in real-world applications.
  - **Keywords:** Robust Reinforcement Learning, Model-Free Learning, Robust φ-regularized fitted Q-iteration, Hybrid robust Total-variation-regularized Q-iteration, Parameter uncertainties, Out-of-data-distribution, Sample inefficiency, ε-optimal robust policy, Unified analysis for φ-divergences, Robust Markov Decision Process (RMDP), φ-divergences, High-dimensional systems


- [Learning from Integral Losses in Physics Informed Neural Networks](https://icml.cc/virtual/2024/poster/33334) (Poster)
  - **Authors:** [Ehsan Saleh](http://openreview.net/profile?id=~Ehsan_Saleh1), [Saba Ghaffari](http://openreview.net/profile?id=~Saba_Ghaffari1), [Timothy Bretl](http://openreview.net/profile?id=~Tim_Bretl1), [Luke Olson](http://openreview.net/profile?id=~Luke_Olson1), [Matthew West](http://openreview.net/profile?id=~Matthew_West1)
  - **Affiliations:** Department of Computer Science; University of Illinois Urbana-Champaign, Department of Computer Science; University of Illinois Urbana-Champaign, Department of Aerospace Engineering; University of Illinois Urbana-Champaign, Department of Computer Science; University of Illinois Urbana-Champaign, Department of Mechanical Science and Engineering; University of Illinois Urbana-Champaign
  - **TL;DR:** This study addresses the challenges of training physics-informed neural networks under partial integro-differential equations, highlighting the biases introduced by naive integral approximations. The proposed delayed target method effectively reduces bias and variance, yielding accurate solutions comparable to those obtained with larger sample sizes.
  - **Keywords:** Physics Informed Neural Networks (PINNs), Partial Integro-Differential Equations, Deterministic sampling approaches, double-sampling trick, delayed target method, Quantum physics, aerosol modeling, ecology, Accurate evaluation of PDE residuals, biased loss functions, Solutions to training PINNs with integral forms, methods to reduce bias and variance in parameter gradients


- [WAVES: Benchmarking the Robustness of Image Watermarks](https://icml.cc/virtual/2024/poster/33944) (Poster)
  - **Authors:** [Bang An](http://openreview.net/profile?id=~Bang_An1), [Mucong Ding](http://openreview.net/profile?id=~Mucong_Ding1), [Tahseen Rabbani](http://openreview.net/profile?id=~Tahseen_Rabbani1), [Aakriti Agrawal](http://openreview.net/profile?id=~Aakriti_Agrawal1), [Yuancheng Xu](http://openreview.net/profile?id=~Yuancheng_Xu1), [Chenghao Deng](http://openreview.net/profile?id=~Chenghao_Deng1), [Sicheng Zhu](http://openreview.net/profile?id=~Sicheng_Zhu1), [Abdirisak Mohamed](http://openreview.net/profile?id=~Abdirisak_Mohamed1), [Yuxin Wen](http://openreview.net/profile?id=~Yuxin_Wen2), [Tom Goldstein](http://openreview.net/profile?id=~Tom_Goldstein1), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1)
  - **Affiliations:** University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, SAP Labs, LLC, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park
  - **TL;DR:** The study introduces WA VES, a benchmark for evaluating the robustness of image watermarks against various attacks, revealing vulnerabilities in existing watermarking algorithms. It aims to establish a standardized evaluation framework to enhance the development of more resilient watermarks in the context of generative AI.
  - **Keywords:** watermark robustness, generative AI, image watermarking, stress-testing, detection and identification tasks, adversarial embedding attacks, multi-regeneration attacks, image content provenance, AI-generated content detection, vulnerabilities of watermarking algorithms, image quality degradation, watermark removal schemes, WA VES benchmark, standardized evaluation protocol, comprehensive suite of stress tests


- [DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving](https://icml.cc/virtual/2024/poster/34760) (Poster)
  - **Authors:** [Foteini Strati](http://openreview.net/profile?id=~Foteini_Strati1), [Sara McAllister](http://openreview.net/profile?id=~Sara_McAllister1), [Amar Phanishayee](http://openreview.net/profile?id=~Amar_Phanishayee1), [Jakub Tarnawski](http://openreview.net/profile?id=~Jakub_Tarnawski1), [Ana Klimovic](http://openreview.net/profile?id=~Ana_Klimovic1)
  - **Affiliations:** MSR Project Fiddle Intern; ETH Zurich, MSR Project Fiddle Intern; Carnegie Mellon University, Microsoft Research, Microsoft Research, ETH Zurich
  - **TL;DR:** The paper presents D´ej`aVu, a fault-tolerant LLM serving system that addresses key challenges in distributed inference, such as pipeline bubbles and GPU memory overprovisioning, through efficient KV cache streaming techniques. The proposed methods significantly enhance performance and resource utilization in serving large language models across cloud environments.
  - **Keywords:** Large Language Models, Generative LLM Serving, Distributed Inference, KV Cache Streaming, Prompt-Token Disaggregation, Microbatch Swapping, State Replication, Cloud Deployments, Machine Learning Model Serving, Pipeline Bubbles, GPU Memory Overprovisioning, Long Recovery Times, Latency Discrepancy, Efficient KV Cache Management, Fault-Tolerant LLM Serving


- [A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM)](https://icml.cc/virtual/2024/poster/33079) (Poster)
  - **Authors:** [Dehao Yuan](http://openreview.net/profile?id=~Dehao_Yuan1), [Cornelia Fermuller](http://openreview.net/profile?id=~Cornelia_Fermuller3), [Tahseen Rabbani](http://openreview.net/profile?id=~Tahseen_Rabbani1), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1), [Yiannis Aloimonos](http://openreview.net/profile?id=~Yiannis_Aloimonos1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Maryland, College Park, USA
  - **TL;DR:** The paper introduces VecKM, a local point cloud geometry encoder that efficiently utilizes all neighboring points for encoding, resulting in significant improvements in speed and accuracy compared to existing methods. It demonstrates 100x faster inference and superior performance in various tasks, including normal estimation, classification, and segmentation.
  - **Keywords:** local point cloud geometry, point cloud analysis, vectorized kernel mixture (VecKM), multi-layer perceptrons (MLP), autonomous driving, robotics, remote sensing, encoding local geometry, computational efficiency, faster inference speed, improved accuracy, scalable encoding


- [How Deep Do We Need: Accelerating Training and Inference of Neural ODEs via Control Perspective](https://icml.cc/virtual/2024/poster/33471) (Poster)
  - **Authors:** [Keyan Miao](http://openreview.net/profile?id=~Keyan_Miao1), [Konstantinos Gatsis](http://openreview.net/profile?id=~Konstantinos_Gatsis1)
  - **Affiliations:** Department of Engineering, University of Oxford, Oxford, United Kingdom, School of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom
  - **TL;DR:** This study focuses on optimizing Neural Ordinary Differential Equations (ODEs) to accelerate training and inference by determining an appropriate network depth. The proposed methods demonstrate effectiveness in improving performance while addressing challenges like slow computation and overfitting.
  - **Keywords:** Neural Ordinary Differential Equations, deep learning, optimization, minimum-time optimal control, Lyapunov method, continuous dynamics, trajectory planning, state observer design, slow training and inference speed, gradient vanishing, model overfitting, acceleration of training and inference, network performance optimization, Residual Networks, dynamic systems


- [GPT-4V(ision) is a Generalist Web Agent, if Grounded](https://icml.cc/virtual/2024/poster/33031) (Poster)
  - **Authors:** [Boyuan Zheng](http://openreview.net/profile?id=~Boyuan_Zheng1), [Boyu Gou](http://openreview.net/profile?id=~Boyu_Gou1), [Jihyung Kil](http://openreview.net/profile?id=~Jihyung_Kil1), [Huan Sun](http://openreview.net/profile?id=~Huan_Sun1), [Yu Su](http://openreview.net/profile?id=~Yu_Su2)
  - **Affiliations:** The Ohio State University, Columbus, OH, The Ohio State University, Columbus, OH, The Ohio State University, Columbus, OH, The Ohio State University, Columbus, OH, The Ohio State University, Columbus, OH
  - **TL;DR:** This study explores the capabilities of large multimodal models like GPT-4V(ision) as generalist web agents that can perform tasks on websites by following natural language instructions. The findings indicate that while GPT-4V can successfully complete a significant portion of tasks, grounding its plans into actions remains a major challenge, highlighting the need for improved strategies.
  - **Keywords:** Large Multimodal Models, Generalist Web Agent, GPT-4V(ision), SEEACT, Web Interaction, Visual Understanding, Grounding challenges, Task completion on live websites, Evaluation on MIND2WEB benchmark, Grounding strategies, MIND2WEB benchmark, GitHub repository for tools


- [Comparing Graph Transformers via Positional Encodings](https://icml.cc/virtual/2024/poster/32777) (Poster)
  - **Authors:** [Mitchell Black](http://openreview.net/profile?id=~Mitchell_Black1), [Zhengchao Wan](http://openreview.net/profile?id=~Zhengchao_Wan1), [Gal Mishne](http://openreview.net/profile?id=~Gal_Mishne1), [Amir Nayyeri](http://openreview.net/profile?id=~Amir_Nayyeri1), [Yusu Wang](http://openreview.net/profile?id=~Yusu_Wang1)
  - **Affiliations:** School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon, USA, Halıcıoğlu Data Science Institute, University of California San Diego, San Diego, California, USA, Halıcıoğlu Data Science Institute, University of California San Diego, San Diego, California, USA, School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon, USA, Halıcıoğlu Data Science Institute, University of California San Diego, San Diego, California, USA
  - **TL;DR:** This paper investigates the effectiveness of absolute and relative positional encodings in graph transformers, demonstrating their equivalence in distinguishing non-isomorphic graphs while highlighting potential advantages of relative encodings in the presence of node features. The findings aim to guide future designs of positional encodings for graph transformers.
  - **Keywords:** Graph Transformers, Positional Encodings, Absolute Positional Encodings (APEs), Relative Positional Encodings (RPEs), Graph Representation Learning, Distinguishing Non-Isomorphic Graphs, Oversquashing, Oversmoothing, Interchangeability of APEs and RPEs, Comparison of Distinguishing Power, Shortest-Path Distance, Resistance Distance, Stable and Expressive Positional Encoding (SPE)


- [An Explicit Frame Construction for Normalizing 3D Point Clouds](https://icml.cc/virtual/2024/poster/34002) (Poster)
  - **Authors:** [Justin Baker](http://openreview.net/profile?id=~Justin_Baker1), [Shih-Hsin Wang](http://openreview.net/profile?id=~Shih-Hsin_Wang1), [Tommaso de Fernex](http://openreview.net/profile?id=~Tommaso_de_Fernex1), [Bao Wang](http://openreview.net/profile?id=~Bao_Wang1)
  - **Affiliations:** Department of Mathematics, University of Utah, Salt Lake City, Utah, USA; Scientific Computing and Imaging (SCI) Institute, University of Utah, Salt Lake City, Utah, USA, Department of Mathematics, University of Utah, Salt Lake City, Utah, USA; Scientific Computing and Imaging (SCI) Institute, University of Utah, Salt Lake City, Utah, USA, Department of Mathematics, University of Utah, Salt Lake City, Utah, USA, Department of Mathematics, University of Utah, Salt Lake City, Utah, USA; Scientific Computing and Imaging (SCI) Institute, University of Utah, Salt Lake City, Utah, USA
  - **TL;DR:** This study presents a new algorithm for normalizing 3D point clouds that addresses the challenges of orientation variations and lack of a predefined reference frame. The proposed method demonstrates superior effectiveness and generalizability compared to existing normalization techniques across various benchmark datasets.
  - **Keywords:** 3D point clouds, machine learning, data analysis, normalization algorithms, equivariance, invariance, molecular modeling, geometric processing, robotics, lack of predefined reference frame, orientation variations, biases in data-driven methods, new normalization algorithm, universality, compatibility with learnable frameworks


- [LIDAO: Towards Limited Interventions for Debiasing (Large) Language Models](https://icml.cc/virtual/2024/poster/34188) (Spotlight Poster)
  - **Authors:** [Tianci Liu](http://openreview.net/profile?id=~Tianci_Liu1), [Haoyu Wang](http://openreview.net/profile?id=~Haoyu_Wang6), [Shiyang Wang](http://openreview.net/profile?email=wang5348%40purdue.edu), [Yu Cheng](http://openreview.net/profile?id=~Yu_Cheng1), [Jing Gao](http://openreview.net/profile?id=~Jing_Gao2)
  - **Affiliations:** Purdue University, Purdue University, Purdue University, The Chinese University of Hong Kong, Purdue University
  - **TL;DR:** This study introduces LIDAO, a framework designed to debias large language models while maintaining fluency, addressing the excessive interventions of previous methods. The findings demonstrate that LIDAO effectively mitigates biases in language generation without significantly degrading text quality.
  - **Keywords:** Debiasing, Fairness in Language Models, Information-theoretic perspective, Lightweight decoding-time intervention, Natural Language Processing, Language Generation, Bias in language models, Fairness-fluency trade-offs, LIDAO framework for debiasing, Improved fluency in text generation, Large Language Models (LLMs), Global biases, Adversarial scenarios


- [ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections](https://icml.cc/virtual/2024/poster/32672) (Poster)
  - **Authors:** [Massimo Bini](http://openreview.net/profile?id=~Massimo_Bini1), [Karsten Roth](http://openreview.net/profile?id=~Karsten_Roth1), [Zeynep Akata](http://openreview.net/profile?id=~Zeynep_Akata1), [Anna Khoreva](http://openreview.net/profile?id=~Anna_Khoreva1)
  - **Affiliations:** Bosch IoC Lab, University of Tübingen; Helmholtz Munich, Tübingen AI Center, University of Tübingen, Technical University of Munich; Munich Center for Machine Learning, Bosch Center for Artificial Intelligence
  - **TL;DR:** The study introduces ETHER, a new transformation family for parameter-efficient finetuning of large-scale models, which significantly reduces the number of parameters needed while maintaining model performance across various tasks. The proposed methods outperform existing techniques with fewer parameters and less need for hyperparameter tuning.
  - **Keywords:** Parameter-efficient finetuning, foundation models, model adaptation, ETHER transformations, Hyperplane Reflections, orthogonal transformations (OFT), Image synthesis, natural language tasks, Parameter explosion, hyperparameter tuning, model performance retention, ETHER and ETHER+ methods, reduced parameter counts compared to existing methods, PEFT (Parameter-Efficient Finetuning), Hyperspherical Energy (HE)


- [NExT-Chat: An LMM for Chat, Detection and Segmentation](https://icml.cc/virtual/2024/poster/33745) (Poster)
  - **Authors:** [Ao Zhang](http://openreview.net/profile?id=~Ao_Zhang2), [Yuan Yao](http://openreview.net/profile?id=~Yuan_Yao12), [Wei Ji](http://openreview.net/profile?id=~Wei_Ji1), [Zhiyuan Liu](http://openreview.net/profile?id=~Zhiyuan_Liu1), [Tat-Seng Chua](http://openreview.net/profile?id=~Tat-Seng_Chua2)
  - **Affiliations:** National University of Singapore, National University of Singapore, National University of Singapore, Tsinghua University, National University of Singapore
  - **TL;DR:** This paper introduces NExT-Chat, a large multimodal model that utilizes the pix2emb method for enhanced visual comprehension and region-level understanding. The model demonstrates superior performance in tasks such as visual grounding and region captioning compared to existing models.
  - **Keywords:** large language models, multimodal models, visual comprehension, pix2seq, pix2emb, visual grounding, region captioning, grounded reasoning, region-level reasoning, holistic image understanding, NExT-Chat model, location embeddings, POPE-Random, LISA, Kosmos-2


- [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://icml.cc/virtual/2024/poster/34710) (Poster)
  - **Authors:** [Haowei Lin](http://openreview.net/profile?id=~Haowei_Lin1), [Baizhou Huang](http://openreview.net/profile?id=~Baizhou_Huang1), [Haotian Ye](http://openreview.net/profile?id=~Haotian_Ye1), [Qinyu Chen](http://openreview.net/profile?id=~Qinyu_Chen2), [Zihao Wang](http://openreview.net/profile?id=~Zihao_Wang23), [Sujian Li](http://openreview.net/profile?id=~Sujian_Li1), [Jianzhu Ma](http://openreview.net/profile?id=~Jianzhu_Ma2), [Xiaojun Wan](http://openreview.net/profile?id=~Xiaojun_Wan1), [James Zou](http://openreview.net/profile?id=~James_Zou1), [Yitao Liang](http://openreview.net/profile?id=~Yitao_Liang1)
  - **Affiliations:** Institute for Artificial Intelligence, Peking University; Peking University, Peking University, Stanford University, Peking University, Institute for Artificial Intelligence, Peking University; Peking University, Peking University, Tsinghua University, Peking University, Stanford University, Institute for Artificial Intelligence, Peking University; Peking University
  - **TL;DR:** This study addresses the challenge of selecting the optimal large language model for fine-tuning under resource constraints by introducing a novel Rectified Scaling Law that predicts fine-tuning performance. The proposed method significantly reduces resource consumption while improving selection accuracy compared to existing approaches.
  - **Keywords:** Large Language Models, Fine-tuning, Model Selection, Rectified Scaling Law, Scaling Law, Downstream Tasks, Natural Language Processing, Resource-constrained model selection, Predicting fine-tuning performance, Novel LLM selection algorithm, Pre-learned data size concept


- [Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind](https://icml.cc/virtual/2024/poster/33732) (Poster)
  - **Authors:** [Mo Yu](http://openreview.net/profile?id=~Mo_Yu1), [Qiujing Wang](http://openreview.net/profile?id=~Qiujing_Wang1), [Shunchi Zhang](http://openreview.net/profile?id=~Shunchi_Zhang1), [Yisi Sang](http://openreview.net/profile?id=~Yisi_Sang1), [Kangsheng Pu](http://openreview.net/profile?id=~Kangsheng_Pu1), [Zekai Wei](http://openreview.net/profile?id=~Zekai_Wei1), [Han Wang](http://openreview.net/profile?id=~Han_Wang9), [Liyan Xu](http://openreview.net/profile?id=~Liyan_Xu1), [Jing Li](http://openreview.net/profile?id=~Jing_Li21), [Yue Yu](http://openreview.net/profile?id=~Yue_Yu3), [Jie Zhou](http://openreview.net/profile?id=~Jie_Zhou8)
  - **Affiliations:** Pattern Recognition Center, WeChat AI, Xi’an Jiaotong University, Xi’an Jiaotong University, Syracuse University, Syracuse University, Syracuse University, Pattern Recognition Center, WeChat AI, Pattern Recognition Center, WeChat AI, New Jersey Institute of Technology, Lehigh University, Pattern Recognition Center, WeChat AI
  - **TL;DR:** This study introduces a novel dataset, TOM-IN-AMC, to evaluate few-shot character understanding in movies as a measure of theory-of-mind (ToM) capabilities. The findings reveal that while humans excel at inferring characters' mental states from minimal information, current AI systems significantly lag behind, highlighting a critical gap in AI's ToM abilities.
  - **Keywords:** Few-shot learning, Theory-of-mind (ToM), Narrative understanding, ToM prompting approach, NLP dataset, Movie scripts analysis, Character understanding, Limited information for character understanding, Human-AI comparison in ToM capabilities, Novel dataset (TOM-IN-AMC), Assessment of multiple ToM dimensions, TOM-IN-AMC


- [Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity](https://icml.cc/virtual/2024/poster/32842) (Poster)
  - **Authors:** [Hagyeong Lee](http://openreview.net/profile?id=~Hagyeong_Lee1), [Minkyu Kim](http://openreview.net/profile?id=~Minkyu_Kim4), [Jun-Hyuk Kim](http://openreview.net/profile?id=~Jun-Hyuk_Kim1), [Seungeon Kim](http://openreview.net/profile?id=~Seungeon_Kim1), [Dokwan Oh](http://openreview.net/profile?id=~Dokwan_Oh1), [Jaeho Lee](http://openreview.net/profile?id=~Jaeho_Lee3)
  - **Affiliations:** POSTECH, POSTECH, Samsung Advanced Institute of Technology, Samsung Advanced Institute of Technology, Samsung Advanced Institute of Technology, POSTECH
  - **TL;DR:** This study presents a novel text-guided image compression algorithm that effectively balances high perceptual and pixel-wise fidelity by utilizing text information through text-adaptive encoding and joint image-text loss. Experimental results demonstrate that the proposed method outperforms existing baselines, achieving superior quality in reconstructed images.
  - **Keywords:** text-guided image compression, perceptual quality, pixel-wise fidelity, text-adaptive encoding, joint image-text loss, image compression, visual computing, degraded pixel-wise fidelity, practical limitations of existing methods, new text-guided image compression algorithm, high pixel-level and perceptual quality, MS-COCO, text-guided generative models, LPIPS, FID


- [Coresets for Multiple $\ell_p$ Regression](https://icml.cc/virtual/2024/poster/35021) (Poster)
  - **Authors:** [David Woodruff](http://openreview.net/profile?id=~David_Woodruff1), [Taisuke Yasuda](http://openreview.net/profile?id=~Taisuke_Yasuda1)
  - **Affiliations:** School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA
  - **TL;DR:** This paper presents nearly optimal constructions of coresets for multiple ℓp regression, achieving dimension-free size bounds that approximate the regression objective with a specified relative error. The findings include applications for approximating Euclidean power means and subspace approximation, addressing challenges in regression with multiple responses.
  - **Keywords:** Coresets, Multiple ℓp Regression, Linear Regression, ℓp Sampling, Strong Coresets, Weak Coresets, Data Analytics, Optimization, Multiple responses in regression, Sample complexity, Nearly optimal constructions of coresets, Approximation of regression objectives, ℓp Regression, Euclidean power means, Subspace approximation


- [Position: Social Environment Design Should be Further Developed for AI-based Policy-Making](https://icml.cc/virtual/2024/poster/34026) (Poster)
  - **Authors:** [Edwin Zhang](http://openreview.net/profile?id=~Edwin_Zhang2), [Sadie Zhao](http://openreview.net/profile?id=~Sadie_Zhao1), [Tonghan Wang](http://openreview.net/profile?id=~Tonghan_Wang1), [Safwan Hossain](http://openreview.net/profile?id=~Safwan_Hossain1), [Henry Gasztowtt](http://openreview.net/profile?email=henry.gasztowtt%40chch.ox.ac.uk), [Stephan Zheng](http://openreview.net/profile?id=~Stephan_Zheng1), [David Parkes](http://openreview.net/profile?id=~David_C._Parkes1), [Milind Tambe](http://openreview.net/profile?id=~Milind_Tambe1), [Yiling Chen](http://openreview.net/profile?id=~Yiling_Chen1)
  - **Affiliations:** Harvard University; Founding, None, None, None, Oxford University, Asari AI, Harvard University, Harvard University; Google Research, Harvard University
  - **TL;DR:** This paper proposes a framework called Social Environment Design to enhance AI-based policy-making by simulating economic environments and addressing the complexities of policy formulation. The research aims to align policy-makers with public interests and improve ethical decision-making through systematic analysis and simulation.
  - **Keywords:** AI-based policy-making, Social Environment Design, Reinforcement Learning, Partially Observable Markov Game (POMG), Government policy, Economic policy, Complexity in economic policy formulation, Misalignment of policy-maker incentives, Framework for automated policy-making, Simulation of economic environments, Social welfare objectives, Ethical decision making


- [CKGConv: General Graph Convolution with Continuous Kernels](https://icml.cc/virtual/2024/poster/34331) (Poster)
  - **Authors:** [Liheng Ma](http://openreview.net/profile?id=~Liheng_Ma1), [Soumyasundar Pal](http://openreview.net/profile?id=~Soumyasundar_Pal1), [Yitian Zhang](http://openreview.net/profile?id=~Yitian_Zhang2), [Jiaming Zhou](http://openreview.net/profile?id=~Jiaming_Zhou3), [Yingxue Zhang](http://openreview.net/profile?id=~Yingxue_Zhang1), [Mark Coates](http://openreview.net/profile?id=~Mark_Coates1)
  - **Affiliations:** Department of ECE, McGill University, Montreal, Canada; Mila - Quebec AI Institute, Montreal, Canada; ILLS - International Laboratory on Learning Systems, Montreal, Canada, Huawei Noah’s Ark Lab, Montreal, Canada, Department of ECE, McGill University, Montreal, Canada; Mila - Quebec AI Institute, Montreal, Canada; ILLS - International Laboratory on Learning Systems, Montreal, Canada, Huawei Noah’s Ark Lab, Montreal, Canada, Huawei Noah’s Ark Lab, Montreal, Canada, Department of ECE, McGill University, Montreal, Canada; Mila - Quebec AI Institute, Montreal, Canada; ILLS - International Laboratory on Learning Systems, Montreal, Canada
  - **TL;DR:** This paper introduces Continuous Kernel Graph Convolution (CKGConv), a flexible and expressive graph convolution framework that addresses the limitations of existing definitions. CKGConv demonstrates superior performance compared to traditional graph convolutional networks and is competitive with state-of-the-art graph transformers across various datasets.
  - **Keywords:** graph convolution, graph neural networks, continuous kernels, Continuous Kernel Graph Convolution (CKGConv), graph positional encoding, graph learning tasks, inflexibility of existing graph convolution definitions, challenges in defining convolution operators in graph domains, CKGConv outperforms existing graph convolutional networks, comparable performance to graph transformers, graph transformers, message-passing neural networks (MPNNs), spectral GNNs


- [CaPS: Collaborative and Private Synthetic Data Generation from Distributed Sources](https://icml.cc/virtual/2024/poster/33733) (Poster)
  - **Authors:** [Sikha Pentyala](http://openreview.net/profile?id=~Sikha_Pentyala1), [Mayana Pereira](http://openreview.net/profile?id=~Mayana_Pereira1), [Martine De Cock](http://openreview.net/profile?id=~Martine_De_Cock1)
  - **Affiliations:** School of Engineering and Technology, University of Washington Tacoma, USA, Department of Electrical Engineering, Universidade de Brasilia, Brazil, Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Belgium
  - **TL;DR:** The study proposes a framework for collaboratively and privately generating synthetic tabular data from distributed sources, addressing privacy concerns associated with data sharing. It leverages secure multi-party computation and differential privacy to ensure input and output privacy while demonstrating scalability with existing synthetic data generation algorithms.
  - **Keywords:** synthetic data generation, privacy concerns, data sharing, secure multi-party computation (MPC), differential privacy (DP), marginal-based SDG, healthcare, finance, automation, privacy concerns in data sharing, legal barriers to data sharing, framework for collaborative and private generation of synthetic tabular data


- [The Effect of Weight Precision on the Neuron Count in Deep ReLU Networks](https://icml.cc/virtual/2024/poster/32999) (Poster)
  - **Authors:** [Songhua He](http://openreview.net/profile?id=~Songhua_He1), [Periklis Papakonstantinou](http://openreview.net/profile?id=~Periklis_A._Papakonstantinou1)
  - **Affiliations:** Department of Computer Science, Rutgers University, New Brunswick, NJ, USA, Department of Management Science and Information Systems, Rutgers University, New Brunswick, NJ, USA
  - **TL;DR:** This study investigates the impact of weight precision on the neuron count in deep ReLU networks, revealing that while high weight precision can lead to a reduction in neuron count, it comes at a cost, and a new notion of network size is proposed to account for this trade-off. The findings suggest that using polynomial time algorithms does not allow for significant reductions in neuron count, regardless of weight precision.
  - **Keywords:** Deep Neural Networks, Weight Precision, ReLU Networks, Exponential Time Preprocessing Algorithm, Neuron Count Reduction, Trade-off between Neurons and Weight Precision, Redefined Notion of Network Size, Impossibility of Neuron Reduction with High Weight Precision, ReLU (Rectified Linear Unit), Boolean Input Encoding


- [PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses](https://icml.cc/virtual/2024/poster/32985) (Spotlight Poster)
  - **Authors:** [Adel Javanmard](http://openreview.net/profile?id=~Adel_Javanmard1), [Matthew Fahrbach](http://openreview.net/profile?id=~Matthew_Fahrbach1), [Vahab Mirrokni](http://openreview.net/profile?id=~Vahab_Mirrokni2)
  - **Affiliations:** University of Southern California; Google Research, Google Research, Google Research
  - **TL;DR:** This study introduces the PriorBoost algorithm for learning from aggregate responses, demonstrating that adaptive bag formation significantly enhances model quality compared to non-adaptive methods. The research also addresses privacy concerns by employing label differential privacy in the context of aggregate learning.
  - **Keywords:** Learning from aggregate responses, Privacy in machine learning, PriorBoost algorithm, k-means clustering, linear regression, generalized linear models (GLMs), Medical diagnostics, population screening, quality control, political elections, Privacy concerns, revealing individual responses, optimal bagging problem, Improved model quality, adaptive bag formation, label differential privacy, Aggregate learning, k-anonymity, label proportions


- [Perturb-and-Project: Differentially Private Similarities and Marginals](https://icml.cc/virtual/2024/poster/35037) (Spotlight Poster)
  - **Authors:** [Vincent Cohen-Addad](http://openreview.net/profile?id=~Vincent_Cohen-Addad1), [Tommaso d'Orsi](http://openreview.net/profile?id=~Tommaso_d%27Orsi1), [Alessandro Epasto](http://openreview.net/profile?id=~Alessandro_Epasto3), [Vahab Mirrokni](http://openreview.net/profile?id=~Vahab_Mirrokni2), [Peilin Zhong](http://openreview.net/profile?id=~Peilin_Zhong1)
  - **Affiliations:** Google Research, Google Research; BIDSA, Bocconi, Google Research, Google Research, Google Research
  - **TL;DR:** This paper explores input perturbation techniques for differential privacy, presenting efficient algorithms for releasing pair-wise cosine similarities and computing k-way marginal queries. The authors demonstrate that input perturbations can yield optimal privacy guarantees while maintaining the utility of the data.
  - **Keywords:** Differential Privacy, Input Perturbation, Algorithms for pair-wise cosine similarities, k-way marginal queries, sum-of-squares certiﬁcates, Machine Learning, Empirical Risk Minimization, Noise addition in algorithms, privacy guarantees, utility loss in differential privacy, Efficient algorithms for private data release, stronger guarantees for t-sparse datasets


- [Grokking Group Multiplication with Cosets](https://icml.cc/virtual/2024/poster/33385) (Poster)
  - **Authors:** [Dashiell Stander](http://openreview.net/profile?id=~Dashiell_Stander1), [Qinan Yu](http://openreview.net/profile?id=~Qinan_Yu1), [Honglu Fan](http://openreview.net/profile?id=~Honglu_Fan1), [Stella Biderman](http://openreview.net/profile?id=~Stella_Biderman1)
  - **Affiliations:** EleutherAI, Brown University, University of Geneva, EleutherAI
  - **TL;DR:** This study successfully reverse engineers a one-hidden layer neural network that has learned to multiply permutations in the symmetric groups S5 and S6, revealing the underlying subgroup structure and neural circuits. The findings highlight significant challenges in mechanistic interpretability, especially in reconciling differing interpretations of the same model.
  - **Keywords:** deep neural networks, mechanistic interpretability, group arithmetic, fully connected one-hidden layer networks, causal experiments, interpretability challenges, limitations of existing techniques, reverse engineering of neural circuits, subgroup structure discovery, symmetric groups S5, S6, neural circuits


- [A Field Guide for Pacing Budget and ROS Constraints](https://icml.cc/virtual/2024/poster/34463) (Poster)
  - **Authors:** [Santiago Balseiro](http://openreview.net/profile?id=~Santiago_R._Balseiro1), [Kshipra Bhawalkar](http://openreview.net/profile?id=~Kshipra_Bhawalkar1), [Zhe Feng](http://openreview.net/profile?id=~Zhe_Feng3), [Haihao Lu](http://openreview.net/profile?id=~Haihao_Lu2), [Vahab Mirrokni](http://openreview.net/profile?id=~Vahab_Mirrokni2), [Balasubramanian Sivan](http://openreview.net/profile?id=~Balasubramanian_Sivan1), [Di Wang](http://openreview.net/profile?id=~Di_Wang4)
  - **Affiliations:** Google Research, USA; Columbia University, NYC, USA, Google Research, USA, Google Research, USA, Google Research, USA, Google Research, USA, Google Research, USA, Google Research, USA
  - **TL;DR:** This study compares different algorithms for budget pacing and return-on-spend (ROS) constraints in online advertising, focusing on a min-pacing algorithm that achieves similar guarantees to a fully-coupled dual-based algorithm. The findings suggest that while decoupled systems may introduce inefficiencies, a minimally-coupled approach can effectively balance performance and coordination.
  - **Keywords:** budget pacing, return-on-spend (ROS) constraints, real-time bidding, min-pacing algorithm, dual-based algorithm, sequential algorithm, online advertising, auction systems, inefficiencies in bidding process, coordination between pacing systems, theoretical analysis of min-pacing algorithm, empirical validation of algorithms, semi-synthetic dataset from online advertising platform


- [Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews](https://icml.cc/virtual/2024/poster/33635) (Oral)
  - **Authors:** [Weixin Liang](http://openreview.net/profile?id=~Weixin_Liang1), [Zachary Izzo](http://openreview.net/profile?id=~Zachary_Izzo1), [Yaohui Zhang](http://openreview.net/profile?id=~Yaohui_Zhang2), [Haley Lepp](http://openreview.net/profile?id=~Haley_Lepp1), [Hancheng Cao](http://openreview.net/profile?id=~Hancheng_Cao1), [Xuandong Zhao](http://openreview.net/profile?id=~Xuandong_Zhao1), [Lingjiao Chen](http://openreview.net/profile?id=~Lingjiao_Chen1), [Haotian Ye](http://openreview.net/profile?id=~Haotian_Ye1), [Sheng Liu](http://openreview.net/profile?id=~Sheng_Liu2), [Zhi Huang](http://openreview.net/profile?email=zhihuang%40stanford.edu), [Daniel McFarland](http://openreview.net/profile?id=~Daniel_McFarland1), [James Zou](http://openreview.net/profile?id=~James_Y._Zou1)
  - **Affiliations:** Department of Computer Science, Stanford University, Machine Learning Department, NEC Labs America, Department of Electrical Engineering, Stanford University, Graduate School of Education, Stanford University, Department of Computer Science, Stanford University; Department of Management Science and Engineering, Stanford University, Department of Computer Science, UC Santa Barbara, Department of Biomedical Data Science, Stanford University, Department of Computer Science, Stanford University, Department of Biomedical Data Science, Stanford University, Department of Biomedical Data Science, Stanford University, Graduate School of Education, Stanford University; Department of Sociology, Stanford University; Graduate School of Business, Stanford University, Department of Computer Science, Stanford University; Department of Electrical Engineering, Stanford University; Department of Biomedical Data Science, Stanford University
  - **TL;DR:** This study estimates the extent of AI modification in peer reviews submitted to major AI conferences post-ChatGPT release, finding that 6.5% to 16.9% of reviews may be significantly altered by large language models. The findings highlight user behavior patterns and call for further interdisciplinary research on the implications of LLM usage in knowledge practices.
  - **Keywords:** AI-generated content, peer review, large language models, maximum likelihood model, scientific peer review, AI conferences, distinguishing AI-generated text from human-written content, impact of LLMs on information ecosystems, estimation of LLM-modified text in peer reviews, insights into user behavior, large language models (LLMs), ChatGPT


- [Contrastive Predict-and-Search for Mixed Integer Linear Programs](https://icml.cc/virtual/2024/poster/32626) (Poster)
  - **Authors:** [Taoan Huang](http://openreview.net/profile?id=~Taoan_Huang2), [Aaron Ferber](http://openreview.net/profile?id=~Aaron_M_Ferber1), [Arman Zharmagambetov](http://openreview.net/profile?id=~Arman_Zharmagambetov1), [Yuandong Tian](http://openreview.net/profile?id=~Yuandong_Tian1), [Bistra Dilkina](http://openreview.net/profile?id=~Bistra_Dilkina2)
  - **Affiliations:** Department of Computer Science, University of Southern California, Los Angeles, CA, USA, Department of Computer Science, Cornell University, Ithaca, NY, USA, AI at Meta (FAIR), Menlo Park, CA, USA, AI at Meta (FAIR), Menlo Park, CA, USA, Department of Computer Science, University of Southern California, Los Angeles, CA, USA
  - **TL;DR:** This paper introduces ConPaS, a novel machine learning framework that utilizes contrastive learning to predict solutions for Mixed Integer Linear Programs (MILP), achieving state-of-the-art results in solution quality and speed. The framework addresses challenges in combinatorial optimization by contrasting high-quality and low-quality solutions to enhance prediction accuracy.
  - **Keywords:** Mixed Integer Linear Programs (MILP), Combinatorial Optimization, Contrastive Learning, Predict-and-Search (PaS), Neural Diving (ND), Resource Allocation, Traffic Management, Network Design, Production Planning, NP-hard problems, Solution Quality, Feasibility, ConPaS framework, High-quality solution prediction, SCIP, Gurobi, CPLEX, Branch-and-Bound (BnB), Branch-and-Cut, Heuristics


- [Deep Networks Always Grok and Here is Why](https://icml.cc/virtual/2024/poster/32632) (Poster)
  - **Authors:** [Ahmed Imtiaz Humayun](http://openreview.net/profile?id=~Ahmed_Imtiaz_Humayun1), [Randall Balestriero](http://openreview.net/profile?id=~Randall_Balestriero1), [Richard Baraniuk](http://openreview.net/profile?id=~Richard_Baraniuk1)
  - **Affiliations:** Rice University, Brown University, Rice University
  - **TL;DR:** This study explores the phenomenon of grokking in deep neural networks, demonstrating that generalization and robustness can occur long after achieving low training error. The authors provide an analytical explanation for this delayed generalization and robustness, highlighting the role of local complexity in the DNN's input-output mapping.
  - **Keywords:** grokking, delayed generalization, deep neural networks, convolutional neural networks, ResNet, Projected Gradient Descent, CIFAR10, Imagenette, adversarial examples, robustness, interpolation, delayed robustness, phase transition in training, CIFAR10, Imagenette, linear regions, spline partition regions


- [Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint](https://icml.cc/virtual/2024/poster/33609) (Poster)
  - **Authors:** [Wei Xiong](http://openreview.net/profile?id=~Wei_Xiong9), [Hanze Dong](http://openreview.net/profile?id=~Hanze_Dong1), [Chenlu Ye](http://openreview.net/profile?id=~Chenlu_Ye1), [Ziqi Wang](http://openreview.net/profile?id=~Ziqi_Wang2), [Han Zhong](http://openreview.net/profile?id=~Han_Zhong1), [Heng Ji](http://openreview.net/profile?id=~Heng_Ji3), [Nan Jiang](http://openreview.net/profile?id=~Nan_Jiang2), [Tong Zhang](http://openreview.net/profile?id=~Tong_Zhang2)
  - **Affiliations:** University of Illinois Urbana-Champaign, Salesforce AI Research, The Hong Kong University of Science and Technology, University of Illinois Urbana-Champaign, The Hong Kong University of Science and Technology, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign, University of Illinois Urbana-Champaign
  - **TL;DR:** This study presents a theoretical framework for aligning generative models using Reinforcement Learning from Human Feedback (RLHF) under a KL-constraint, exploring its behavior in offline, online, and hybrid settings. The proposed algorithms demonstrate significant improvements over existing methods in real-world applications, addressing challenges like reward misspecification and model optimization instability.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), generative models alignment, reverse-KL regularized contextual bandit, Direct Preference Optimization (DPO), rejection sampling, Large Language Models (LLMs), AI alignment, reward misspecification, model optimization instability, performance degeneration, reward hacking, novel RLHF algorithms, finite-sample theoretical guarantees


- [PRISE: LLM-Style Sequence Compression for Learning Temporal Action Abstractions in Control](https://icml.cc/virtual/2024/poster/33060) (Oral)
  - **Authors:** [Ruijie Zheng](http://openreview.net/profile?id=~Ruijie_Zheng1), [Ching-An Cheng](http://openreview.net/profile?id=~Ching-An_Cheng1), [Hal Daumé](http://openreview.net/profile?id=~Hal_Daum%C3%A9_III1), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1), [Andrey Kolobov](http://openreview.net/profile?id=~Andrey_Kolobov1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park; None, Microsoft Research, Department of Computer Science, University of Maryland, College Park; Microsoft Research, Department of Computer Science, University of Maryland, College Park, Microsoft Research
  - **TL;DR:** This study introduces Primitive Sequence Encoding (PRISE), a novel method for learning temporal action abstractions in continuous control domains by applying sequence compression techniques from NLP. The findings demonstrate that skills learned through PRISE significantly enhance the performance of Behavior Cloning in robotic manipulation tasks.
  - **Keywords:** Temporal action abstractions, sequential decision making, continuous control, Primitive Sequence Encoding (PRISE), Byte Pair Encoding (BPE), Behavior Cloning (BC), Robotics, multi-task learning, Learning skills of variable time span, high-dimensional observations, complex action spaces, Improved performance of Behavior Cloning using action primitives


- [${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning](https://icml.cc/virtual/2024/poster/32670) (Poster)
  - **Authors:** [Dingyang Chen](http://openreview.net/profile?id=~Dingyang_Chen1), [Qi Zhang](http://openreview.net/profile?id=~Qi_Zhang12)
  - **Affiliations:** Artificial Intelligence Institute, University of South Carolina, Columbia, SC, USA, Artificial Intelligence Institute, University of South Carolina, Columbia, SC, USA
  - **TL;DR:** This paper explores the exploitation of Euclidean symmetries in cooperative multi-agent reinforcement learning (MARL) problems, proposing neural network architectures with symmetric constraints that enhance performance and generalization capabilities. The findings indicate significant improvements in benchmarks and the ability to adapt to unseen scenarios with symmetric patterns.
  - **Keywords:** Cooperative Multi-Agent Reinforcement Learning (MARL), Euclidean Symmetries, Actor-Critic Methods, Neural Network Architectures, Symmetric Constraints, Symmetric Optimal Values and Policies, Continuous E(3) Multi-Agent Symmetries, Superior Performance in Cooperative MARL Benchmarks, Zero-Shot Learning, Transfer Learning, Markov Games, Permutation Invariance, Euclidean Transformations


- [Outlier-Efficient Hopfield Layers for Large Transformer-Based Models](https://icml.cc/virtual/2024/poster/33261) (Poster)
  - **Authors:** [Jerry Yao-Chieh Hu](http://openreview.net/profile?id=~Jerry_Yao-Chieh_Hu1), [Pei-Hsuan Chang](http://openreview.net/profile?email=b09202022%40ntu.edu.tw), [Haozheng Luo](http://openreview.net/profile?id=~Haozheng_Luo2), [Hong-Yu Chen](http://openreview.net/profile?email=b0976960890%40gmail.com), [Weijian Li](http://openreview.net/profile?id=~Weijian_Li2), [Wei-Po Wang](http://openreview.net/profile?email=b09202009%40ntu.edu.tw), [Han Liu](http://openreview.net/profile?id=~Han_Liu4)
  - **Affiliations:** Department of Computer Science, Northwestern University, Evanston, USA, Department of Physics, National Taiwan University, Taipei, Taiwan, Department of Computer Science, Northwestern University, Evanston, USA, Department of Physics, National Taiwan University, Taipei, Taiwan, Department of Computer Science, Northwestern University, Evanston, USA, Department of Physics, National Taiwan University, Taipei, Taiwan, Department of Computer Science, Northwestern University, Evanston, USA; Department of Statistics and Data Science, Northwestern University, Evanston, USA
  - **TL;DR:** This study introduces the Outlier-Efficient Modern Hopfield Model (OutEffHop) to tackle the outlier inefficiency problem in large transformer-based models, enhancing memory retrieval and attention mechanisms. The proposed model demonstrates significant improvements in performance metrics and efficiency across various large-scale models.
  - **Keywords:** Outlier efficiency, Transformer-based models, Associative memory, Outlier-Efficient Modern Hopfield Model, Attention mechanism, Softmax approximation, Natural language processing, Finance, Genomics, Outlier inefficiency, Attention allocation to low-information tokens, GPU memory usage, Outlier-efficient Hopfield layers, Improved post-quantization performance, Fixed point convergence, Exponential storage capacity, BERT, OPT, ViT, STanHop-Net, Hopfield models, Attention mechanisms, No-op outliers


- [Scalable and Flexible Causal Discovery with an Efficient Test for Adjacency](https://icml.cc/virtual/2024/poster/34984) (Poster)
  - **Authors:** [Alan Amin](http://openreview.net/profile?id=~Alan_Nawzad_Amin1), [Andrew Wilson](http://openreview.net/profile?id=~Andrew_Gordon_Wilson1)
  - **Affiliations:** New York University, New York, USA, New York University, New York, USA
  - **TL;DR:** The study presents a scalable method for causal discovery using the Differentiable Adjacency Test (DAT) to efficiently evaluate adjacency in causal graphs. The proposed DAT-Graph method demonstrates state-of-the-art accuracy in learning causal graphs from large-scale data and improves predictions of intervention effects in complex systems.
  - **Keywords:** causal discovery, causal graphs, large scale data, Differentiable Adjacency Test (DAT), neural networks, genetics, microbiology, healthcare, RNA sequencing, searching for causal relationships, conditional independence, model search complexity, DAT-Graph, improved predictions of intervention effects, directed acyclic graphs, hybrid causal models


- [Editing Partially Observable Networks via Graph Diffusion Models](https://icml.cc/virtual/2024/poster/35098) (Poster)
  - **Authors:** [Puja Trivedi](http://openreview.net/profile?id=~Puja_Trivedi1), [Ryan A Rossi](http://openreview.net/profile?id=~Ryan_A._Rossi2), [David Arbour](http://openreview.net/profile?id=~David_Arbour1), [Tong Yu](http://openreview.net/profile?id=~Tong_Yu3), [Franck Dernoncourt](http://openreview.net/profile?id=~Franck_Dernoncourt1), [Sungchul Kim](http://openreview.net/profile?id=~Sungchul_Kim1), [Nedim Lipka](http://openreview.net/profile?id=~Nedim_Lipka1), [Namyong Park](http://openreview.net/profile?id=~Namyong_Park1), [Nesreen Ahmed](http://openreview.net/profile?id=~Nesreen_K._Ahmed2), [Danai Koutra](http://openreview.net/profile?id=~Danai_Koutra1)
  - **Affiliations:** CSE Dept, University of Michigan, Ann Arbor, Adobe Research Inc, Adobe Research Inc, Adobe Research Inc, Adobe Research Inc, Adobe Research Inc, Adobe Research Inc, Carnegie Mellon University, Intel AI Research, CSE Dept, University of Michigan, Ann Arbor
  - **TL;DR:** This study introduces a novel graph generative framework, SGDM, that enhances the scalability and fidelity of graph diffusion models to refine partially observable networks. The framework effectively addresses various editing tasks such as denoising, expanding subgraphs, and performing style transfer, thereby improving downstream performance in network analysis.
  - **Keywords:** graph generative models, network refinement, partially observable networks, graph diffusion models, subgraph diffusion, generative adversarial networks (GANs), network analysis, node classification, image generation, noisy networks, incomplete samples, data corruption, SGDM framework, conditional generation tasks, denoising, expansion, style transfer


- [Continuous Treatment Effects with Surrogate Outcomes](https://icml.cc/virtual/2024/poster/33588) (Poster)
  - **Authors:** [Zhenghao Zeng](http://openreview.net/profile?id=~Zhenghao_Zeng1), [David Arbour](http://openreview.net/profile?id=~David_Arbour1), [Avi Feller](http://openreview.net/profile?id=~Avi_Feller1), [Raghavendra Addanki](http://openreview.net/profile?id=~Raghavendra_Addanki1), [Ryan A Rossi](http://openreview.net/profile?id=~Ryan_A._Rossi2), [Ritwik Sinha](http://openreview.net/profile?id=~Ritwik_Sinha1), [Edward Kennedy](http://openreview.net/profile?id=~Edward_Kennedy1)
  - **Affiliations:** Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, PA, USA, Adobe Research, San Jose, CA, USA, Goldman School of Public Policy and Department of Statistics, University of California, Berkeley, Berkeley, CA, USA, Adobe Research, San Jose, CA, USA, Adobe Research, San Jose, CA, USA, Adobe Research, San Jose, CA, USA, Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, PA, USA
  - **TL;DR:** This study proposes a novel doubly robust method for estimating continuous treatment effects using both labeled and unlabeled data, addressing the challenges of missing primary outcomes by incorporating surrogate outcomes. The method demonstrates improved variance reduction and establishes the asymptotic normality of the proposed estimator, enhancing the reliability of causal inference analyses.
  - **Keywords:** causal inference, treatment effects, surrogate outcomes, doubly robust methods, outcome modeling, treatment process modeling, health interventions, online advertising effectiveness, missing outcome data, selection bias, variance reduction, novel estimation methods, asymptotic normality of estimators


- [Meta Evidential Transformer for Few-Shot Open-Set Recognition](https://icml.cc/virtual/2024/poster/34658) (Poster)
  - **Authors:** [Hitesh Sapkota](http://openreview.net/profile?id=~Hitesh_Sapkota1), [Krishna Neupane](http://openreview.net/profile?id=~Krishna_Prasad_Neupane1), [Qi Yu](http://openreview.net/profile?id=~Qi_Yu1)
  - **Affiliations:** Amazon Inc.; Rochester Institute of Technology (RIT), Amazon Inc.; Rochester Institute of Technology (RIT), Rochester Institute of Technology (RIT)
  - **TL;DR:** The study introduces the Meta Evidential Transformer (MET) for few-shot open-set recognition, addressing challenges in accurately rejecting open-set instances. The proposed model demonstrates improved performance in recognizing unseen classes while maintaining closed-set accuracy.
  - **Keywords:** Few-shot open-set recognition, Open-set recognition, Meta Evidential Transformer, Attention mapping, Evidence-guided cross-attention mechanism, Detecting instances from unseen classes, Rejecting instances from open-set classes, Challenges in few-shot settings, Evidential open-set loss, Compact closed-set class representations, Real-world datasets


- [Beyond the Federation: Topology-aware Federated Learning for Generalization to Unseen Clients](https://icml.cc/virtual/2024/poster/35086) (Poster)
  - **Authors:** [Mengmeng Ma](http://openreview.net/profile?id=~Mengmeng_Ma1), [Tang Li](http://openreview.net/profile?id=~Tang_Li1), [Xi Peng](http://openreview.net/profile?id=~Xi_Peng1)
  - **Affiliations:** DeepREAL Lab, Department of Computer & Information Sciences, University of Delaware, None, None
  - **TL;DR:** This study introduces Topology-aware Federated Learning (TFL) to enhance the robustness of models against out-of-federation data by leveraging client relationships. Empirical evaluations demonstrate TFL's superior performance and scalability in addressing the challenges posed by unseen clients in federated learning settings.
  - **Keywords:** Federated Learning, Out-of-Federation Generalization, Topology-aware Federated Learning (TFL), Client Topology Learning, Healthcare, Distributed Data, Data Heterogeneity, Performance Degradation on Unseen Clients, Robust Models, Optimization Problem Formulation, eICU Dataset


- [High-Dimensional Geometric Streaming for Nearly Low Rank Data](https://icml.cc/virtual/2024/poster/32671) (Poster)
  - **Authors:** [Hossein Esfandiari](http://openreview.net/profile?id=~Hossein_Esfandiari1), [Praneeth Kacham](http://openreview.net/profile?id=~Praneeth_Kacham1), [Vahab Mirrokni](http://openreview.net/profile?id=~Vahab_Mirrokni2), [David Woodruff](http://openreview.net/profile?id=~David_Woodruff1), [Peilin Zhong](http://openreview.net/profile?id=~Peilin_Zhong1)
  - **Affiliations:** Google Research, USA, Google Research, USA; Computer Science Department, Carnegie Mellon University, Pittsburgh, USA, Google Research, USA, Computer Science Department, Carnegie Mellon University, Pittsburgh, USA, Google Research, USA
  - **TL;DR:** This paper presents streaming algorithms for the `p` subspace approximation problem, focusing on constructing a strong coreset to efficiently approximate the maximum distance from points to a k-dimensional subspace. The proposed methods are efficient and applicable to high-dimensional datasets, improving upon previous results in geometric problems.
  - **Keywords:** high-dimensional geometric streaming, subspace approximation, streaming algorithms, strong coreset construction, `p` subspace approximation problem, outer (d-k) radius estimation, poly(k, log n) approximation, efficient single-pass streaming algorithm


- [Bayesian Design Principles for Offline-to-Online Reinforcement Learning](https://icml.cc/virtual/2024/poster/34472) (Poster)
  - **Authors:** [Hao Hu](http://openreview.net/profile?id=~Hao_Hu3), [yiqin yang](http://openreview.net/profile?id=~Yiqin_Yang1), [Jianing Ye](http://openreview.net/profile?id=~Jianing_Ye1), [Chengjie Wu](http://openreview.net/profile?id=~Chengjie_Wu1), [Ziqing Mai](http://openreview.net/profile?id=~Ziqing_Mai1), [Yujing Hu](http://openreview.net/profile?id=~Yujing_Hu2), [Tangjie Lv](http://openreview.net/profile?id=~Tangjie_Lv1), [Changjie Fan](http://openreview.net/profile?id=~Changjie_Fan1), [Qianchuan Zhao](http://openreview.net/profile?id=~Qianchuan_Zhao1), [Chongjie Zhang](http://openreview.net/profile?id=~Chongjie_Zhang1)
  - **Affiliations:** Institute for Interdisciplinary Sciences, Tsinghua University, China, Institute of Automation, Chinese Academy of Sciences, China, Institute for Interdisciplinary Sciences, Tsinghua University, China, Institute for Interdisciplinary Sciences, Tsinghua University, China, Institute for Interdisciplinary Sciences, Tsinghua University, China, Fuxi AI Lab, Netease, Inc., Hangzhou, China, Fuxi AI Lab, Netease, Inc., Hangzhou, China, Fuxi AI Lab, Netease, Inc., Hangzhou, China, Department of Automation, Tsinghua University, China, Washington University in St. Louis, USA
  - **TL;DR:** This paper addresses the challenges of fine-tuning offline reinforcement learning policies by proposing a probability-matching approach based on Bayesian design principles. The proposed method effectively balances optimism and pessimism, leading to improved performance and the potential for more effective learning from offline data.
  - **Keywords:** Offline reinforcement learning, online fine-tuning, Bayesian design principles, probability-matching agent, Real-world applications, decision-making problems, Suboptimal offline policies, exploration challenges, optimistic-pessimistic dilemma, Novel algorithm, performance improvement, theoretical findings


- [Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks](https://icml.cc/virtual/2024/poster/33075) (Poster)
  - **Authors:** [Zhuomin Chen](http://openreview.net/profile?id=~Zhuomin_Chen1), [Jiaxing Zhang](http://openreview.net/profile?id=~Jiaxing_Zhang4), [Jingchao Ni](http://openreview.net/profile?id=~Jingchao_Ni1), [Xiaoting Li](http://openreview.net/profile?id=~Xiaoting_Li3), [Yuchen Bian](http://openreview.net/profile?id=~Yuchen_Bian1), [Md Mezbahul Isam](http://openreview.net/profile?id=~Md_Mezbahul_Islam1), [Ananda Mondal](http://openreview.net/profile?id=~Ananda_Mondal1), [Hua Wei](http://openreview.net/profile?id=~Hua_Wei1), [Dongsheng Luo](http://openreview.net/profile?id=~Dongsheng_Luo1)
  - **Affiliations:** Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, USA, New Jersey Institute of Technology, Newark, USA, Department of Computer Science, University of Houston, Houston, USA, Visa Research, USA, Amazon Search A9, USA, Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, USA, Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, USA, School of Computing and Augmented Intelligence, Arizona State University, Tempe, USA, Knight Foundation School of Computing and Information Sciences, Florida International University, Miami, USA
  - **TL;DR:** This paper proposes a novel method for generating in-distribution proxy graphs to enhance the explainability of Graph Neural Networks (GNNs) by addressing the challenges posed by distributional shifts in explainable subgraphs. Empirical evaluations show that the proposed method achieves more accurate explanations for GNNs across various datasets.
  - **Keywords:** Explainability, Graph Neural Networks (GNNs), Graph Information Bottleneck (GIB), parametric graph generators, Healthcare, fraud detection, Distributional shift, explainable subgraphs, Proxy graphs for explainable subgraphs, new training objective based on information theory


- [Differentiable Distributionally Robust Optimization Layers](https://icml.cc/virtual/2024/poster/34461) (Poster)
  - **Authors:** [Xutao Ma](http://openreview.net/profile?id=~Xutao_Ma1), [Chao Ning](http://openreview.net/profile?id=~Chao_Ning2), [WenLi Du](http://openreview.net/profile?id=~WenLi_Du2)
  - **Affiliations:** Department of Automation, Shanghai Jiao Tong University, Shanghai 200240, China, Department of Automation, Shanghai Jiao Tong University, Shanghai 200240, China; The Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai 200237, China, The Key Laboratory of Smart Manufacturing in Energy Chemical Process, Ministry of Education, East China University of Science and Technology, Shanghai 200237, China
  - **TL;DR:** This paper develops differentiable distributionally robust optimization layers for mixed-integer decision-making under uncertainty, introducing a novel dual-view methodology to enhance decision quality while maintaining robustness. The proposed approach is validated through a decision-focused learning pipeline, demonstrating its superiority over traditional prediction-focused methods.
  - **Keywords:** decision-focused learning, distributionally robust optimization (DRO), differentiable optimization layers, dual-view methodology, importance sampling, contextual distributionally robust decision-making, decision-making under uncertainty, mixed-integer decisions, differentiable DRO layers, energy-based surrogate, asymptotic convergency under regularization, ambiguity set, second-order conic ambiguity sets, Wasserstein ambiguity sets


- [Augmenting Decision with Hypothesis in Reinforcement Learning](https://icml.cc/virtual/2024/poster/34195) (Poster)
  - **Authors:** [Nguyen Minh Quang](http://openreview.net/profile?id=~Nguyen_Minh_Quang1), [Hady Lauw](http://openreview.net/profile?id=~Hady_W._Lauw1)
  - **Affiliations:** School of Computing and Information Systems, Singapore Management University, 80 Stamford Road, Singapore 178902, School of Computing and Information Systems, Singapore Management University, 80 Stamford Road, Singapore 178902
  - **TL;DR:** This study proposes augmenting the decision-making process in reinforcement learning with hypotheses to address issues of low exploitation and bias sensitivity. The proposed ALH algorithm demonstrates significant improvements over existing value-based learning methods across various benchmarks.
  - **Keywords:** reinforcement learning, decision-making, hypothesis augmentation, value-based reinforcement learning, ALH algorithm, Q-learning, Mujoco benchmarks, dynamic environments, low exploitation in early training, bias sensitiveness, significant improvement over value-based learning algorithms, Mujoco, Markov Decision Process (MDP), Bellman optimal Q value


- [Learning Decision Trees and Forests with Algorithmic Recourse](https://icml.cc/virtual/2024/poster/33625) (Spotlight Poster)
  - **Authors:** [Kentaro Kanamori](http://openreview.net/profile?id=~Kentaro_Kanamori1), [Takuya Takagi](http://openreview.net/profile?id=~Takuya_Takagi1), [Ken Kobayashi](http://openreview.net/profile?id=~Ken_Kobayashi1), [Yuichi Ike](http://openreview.net/profile?id=~Yuichi_Ike1)
  - **Affiliations:** Fujitsu Limited, Japan, Fujitsu Limited, Japan, Tokyo Institute of Technology, Japan, Kyushu University, Japan
  - **TL;DR:** This paper introduces a new framework called Recourse-Aware Classification Tree (RACT) for learning tree-based models that ensure the existence of actionable recourse while maintaining accuracy. The proposed method outperforms existing models in providing reasonable actions for more instances without significantly compromising predictive performance.
  - **Keywords:** Algorithmic Recourse, Decision Trees, Random Forests, Top-down greedy algorithm, Adversarial training techniques, Loan approvals, Algorithmic decision-making, Existence of recourse actions, Predictive performance, Recourse-Aware Classification Tree (RACT)


- [Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures](https://icml.cc/virtual/2024/poster/33542) (Poster)
  - **Authors:** [Zenan Ling](http://openreview.net/profile?id=~Zenan_Ling1), [Longbo Li](http://openreview.net/profile?id=~Longbo_Li1), [Zhanbo Feng](http://openreview.net/profile?id=~Zhanbo_Feng1), [YIXUAN ZHANG](http://openreview.net/profile?id=~YIXUAN_ZHANG1), [Feng Zhou](http://openreview.net/profile?id=~Feng_Zhou9), [Robert Qiu](http://openreview.net/profile?id=~Robert_C_Qiu1), [Zhenyu Liao](http://openreview.net/profile?id=~Zhenyu_Liao1)
  - **Affiliations:** School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Ezhou Industrial Technology Research Institute, Huazhong University of Science and Technology, Wuhan, China, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Ezhou Industrial Technology Research Institute, Huazhong University of Science and Technology, Wuhan, China, Department of CSE, Shanghai Jiao Tong University, Shanghai, China, China-Austria Belt and Road Joint Laboratory on AI and AM, Hangzhou Dianzi University, Hangzhou, China, Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing, China, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Ezhou Industrial Technology Research Institute, Huazhong University of Science and Technology, Wuhan, China, School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Ezhou Industrial Technology Research Institute, Huazhong University of Science and Technology, Wuhan, China
  - **TL;DR:** This paper investigates the theoretical connections between deep equilibrium models (DEQs) and explicit neural networks, demonstrating that a shallow explicit network can be designed to replicate the behavior of DEQs when analyzing high-dimensional Gaussian mixtures. The findings suggest that DEQs, while computationally intensive, can be effectively understood and approximated through explicit models.
  - **Keywords:** Deep Equilibrium Models, Implicit Neural Networks, Conjugate Kernel (CK), Neural Tangent Kernel (NTK), Random Matrix Theory (RMT), Computer Vision, Natural Language Processing, Neural Rendering, Inverse Problems, Theoretical understanding of implicit vs explicit neural networks, computational costs of DEQs, Equivalence of DEQs and explicit networks, design principles for shallow networks, Gaussian Mixture Model, None


- [TimeX++: Learning Time-Series Explanations with Information Bottleneck](https://icml.cc/virtual/2024/poster/32881) (Poster)
  - **Authors:** [Zichuan Liu](http://openreview.net/profile?id=~Zichuan_Liu3), [Tianchun Wang](http://openreview.net/profile?id=~Tianchun_Wang1), [Jimeng Shi](http://openreview.net/profile?id=~Jimeng_Shi1), [Xu Zheng](http://openreview.net/profile?id=~Xu_Zheng3), [Zhuomin Chen](http://openreview.net/profile?id=~Zhuomin_Chen1), [Lei Song](http://openreview.net/profile?id=~Lei_Song3), [Wenqian Dong](http://openreview.net/profile?id=~Wenqian_Dong2), [Jayantha Obeysekera](http://openreview.net/profile?email=jobeysek%40fiu.edu), [Farhad Shirani](http://openreview.net/profile?id=~Farhad_Shirani1), [Dongsheng Luo](http://openreview.net/profile?id=~Dongsheng_Luo1)
  - **Affiliations:** Nanjing University; Microsoft Research Asia, Pennsylvania State University, Florida International University, Florida International University, Florida International University, Microsoft Research Asia, Florida International University, Florida International University, Florida International University, Florida International University
  - **TL;DR:** This study introduces TIMEX++, a novel framework for enhancing explainability in deep learning models for time series data by leveraging an information bottleneck approach. The framework demonstrates significant improvements in explanation quality across various datasets, addressing critical issues like trivial solutions and distributional shifts.
  - **Keywords:** Explainability, Deep Learning, Time Series Analysis, Information Bottleneck, Parametric Network, Environmental Science, Healthcare, Finance, Lack of explainability, Distributional shift issues, TIMEX++, Explanation-embedded instances, Synthetic datasets, Real-world datasets, ECG dataset


- [Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond](https://icml.cc/virtual/2024/poster/33866) (Poster)
  - **Authors:** [Kyriakos Axiotis](http://openreview.net/profile?id=~Kyriakos_Axiotis1), [Vincent Cohen-Addad](http://openreview.net/profile?id=~Vincent_Cohen-Addad1), [Monika Henzinger](http://openreview.net/profile?id=~Monika_Henzinger1), [Sammy Jerome](http://openreview.net/profile?id=~Sammy_Jerome1), [Vahab Mirrokni](http://openreview.net/profile?id=~Vahab_Mirrokni2), [David Saulpic](http://openreview.net/profile?id=~David_Saulpic1), [David Woodruff](http://openreview.net/profile?id=~David_Woodruff1), [Michael Wunder](http://openreview.net/profile?id=~Michael_Wunder2)
  - **Affiliations:** Google Research, Google Research, Institute for Science and Technology Austria (ISTA), Klosterneuburg, Austria, Google Research, Google Research, CNRS & IRIF, Université Paris Cité, Paris, France, Carnegie Mellon University, Google Research
  - **TL;DR:** This study presents a novel data selection method utilizing k-means clustering and sensitivity sampling to efficiently train machine learning models. The approach demonstrates improved performance and scalability, particularly in fine-tuning foundation models, while also matching the effectiveness of existing sampling strategies in linear regression.
  - **Keywords:** Data selection, Machine learning, k-means clustering, Sensitivity sampling, Foundation models, Linear regression, Data sparsity, Model training efficiency, New data selection approach, Improved sampling strategy, H ¨older continuous, k-means cost


- [AMPA: Adaptive Mixed Precision Allocation for Low-Bit Integer Training](https://icml.cc/virtual/2024/poster/34456) (Poster)
  - **Authors:** [Li Ding](http://openreview.net/profile?id=~Li_Ding5), [Wen Fei](http://openreview.net/profile?id=~Wen_Fei1), [Yuyang Huang](http://openreview.net/profile?id=~Yuyang_Huang3), [Shuangrui Ding](http://openreview.net/profile?id=~Shuangrui_Ding1), [Wenrui Dai](http://openreview.net/profile?id=~Wenrui_Dai1), [Chenglin Li](http://openreview.net/profile?id=~Chenglin_Li2), [Junni Zou](http://openreview.net/profile?id=~Junni_Zou1), [Hongkai Xiong](http://openreview.net/profile?id=~Hongkai_Xiong1)
  - **Affiliations:** School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, Department of Information Engineering, The Chinese University of Hong Kong, Shatin, NT, Hong Kong, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This paper presents a novel low-bit integer training framework called AMPA, which achieves adaptive mixed-precision allocation for weights, activations, and gradients, reducing BitOPs by over 38% with less than 2% performance loss compared to traditional INT8 quantization. The proposed method addresses the challenges of quantization losses and enhances model performance in various deep learning applications.
  - **Keywords:** low-bit integer training, mixed-precision quantization, adaptive mixed-precision allocation (AMPA), magnitude-based sensitivity measurement, image classification, image segmentation, language modeling, computational demands, resource consumption, quantization losses, BitOPs reduction, layer-wise precision update strategy, CIFAR-10, CIFAR-100, ImageNet, DNNs (Deep Neural Networks), INT8 precision


- [Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs](https://icml.cc/virtual/2024/poster/34657) (Poster)
  - **Authors:** [Andries Smit](http://openreview.net/profile?id=~Andries_Petrus_Smit1), [Nathan Grinsztajn](http://openreview.net/profile?id=~Nathan_Grinsztajn1), [Paul Duckworth](http://openreview.net/profile?id=~Paul_Duckworth1), [Thomas Barrett](http://openreview.net/profile?id=~Thomas_D_Barrett1), [Arnu Pretorius](http://openreview.net/profile?id=~Arnu_Pretorius1)
  - **Affiliations:** InstaDeep, London UK, InstaDeep, London UK, InstaDeep, London UK, InstaDeep, London UK, InstaDeep, London UK
  - **TL;DR:** This study investigates multi-agent debate strategies to enhance the truthfulness of large language models (LLMs) and benchmarks various prompting strategies. The findings indicate that while current MAD systems do not consistently outperform other methods, specific adjustments can significantly improve their performance.
  - **Keywords:** Multi-Agent Debate, Large Language Models, Hyperparameter tuning, Debating strategies, Medicine, Education, Law, Ensuring accuracy and reliability of generative agents, Improved reasoning abilities, Enhanced performance through agent agreement levels, Open-source repository, Benchmarking strategies


- [Larimar: Large Language Models with Episodic Memory Control](https://icml.cc/virtual/2024/poster/32878) (Poster)
  - **Authors:** [Payel Das](http://openreview.net/profile?id=~Payel_Das1), [Subhajit Chaudhury](http://openreview.net/profile?id=~Subhajit_Chaudhury1), [Elliot Nelson](http://openreview.net/profile?id=~Elliot_Nelson1), [Igor Melnyk](http://openreview.net/profile?id=~Igor_Melnyk1), [Sarath Swaminathan](http://openreview.net/profile?id=~Sarathkrishna_Swaminathan1), [Sophie Dai](http://openreview.net/profile?id=~Sihui_Dai1), [Aurelie Lozano](http://openreview.net/profile?id=~Aurelie_Lozano1), [Georgios Kollias](http://openreview.net/profile?id=~Georgios_Kollias1), [Vijil Chenthamarakshan](http://openreview.net/profile?id=~Vijil_Chenthamarakshan1), [Jiri Navratil](http://openreview.net/profile?id=~Jiri_Navratil1), [Soham Dan](http://openreview.net/profile?id=~Soham_Dan1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1)
  - **Affiliations:** IBM AI Research, IBM AI Research, IBM AI Research, IBM AI Research, IBM AI Research, Princeton University; IBM AI Research, IBM AI Research, IBM AI Research, IBM AI Research, IBM AI Research, IBM AI Research, IBM AI Research
  - **TL;DR:** This paper introduces Larimar, a brain-inspired architecture that enhances Large Language Models (LLMs) with a distributed episodic memory, enabling efficient and accurate one-shot updates of knowledge without costly retraining. Experimental results show that Larimar achieves competitive accuracy and significant speed improvements, addressing challenges in knowledge updating and selective fact forgetting.
  - **Keywords:** Large Language Models, Knowledge Updating, Episodic Memory, Natural Language Processing, Knowledge updating, Model overfitting, Catastrophic forgetting, Input context length generalization, Distributed episodic memory, Dynamic one-shot updates, Selective fact forgetting, Information leakage prevention, LLM-agnostic, Sequential editing, Fact editing, AI Safety, Hallucination


- [DiNADO: Norm-Disentangled Neurally-Decomposed Oracles for Controlling Language Models](https://icml.cc/virtual/2024/poster/33017) (Poster)
  - **Authors:** [Sidi Lu](http://openreview.net/profile?id=~Sidi_Lu1), [Wenbo Zhao](http://openreview.net/profile?id=~Wenbo_Zhao1), [Chenyang Tao](http://openreview.net/profile?id=~Chenyang_Tao1), [Arpit Gupta](http://openreview.net/profile?id=~Arpit_Gupta1), [Shanchan Wu](http://openreview.net/profile?id=~Shanchan_Wu2), [Tagyoung Chung](http://openreview.net/profile?id=~Tagyoung_Chung2), [Nanyun Peng](http://openreview.net/profile?id=~Nanyun_Peng1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, Amazon AGI, Amazon AGI, Amazon AGI, Samsung Research America, Amazon AGI, Department of Computer Science, University of California, Los Angeles
  - **TL;DR:** This paper introduces DiNADO, an improved version of the NADO algorithm for controllable generation with large language models, addressing issues like catastrophic forgetting and gradient vanishing. The proposed method demonstrates enhanced performance and flexibility in tasks such as machine translation and constrained generation.
  - **Keywords:** controllable generation, large language models, NeurAlly-Decomposed Oracle (NADO), DiNADO, LoRA, machine translation, lexically constrained generation, catastrophic forgetting, gradient vanishing, limited capacity, improved performance, better capacity, stability, and flexibility, CommonGen


- [MusicFlow: Cascaded Flow Matching for Text Guided Music Generation](https://icml.cc/virtual/2024/poster/33259) (Poster)
  - **Authors:** [Prajwal K R](http://openreview.net/profile?id=~K_R_Prajwal1), [Bowen Shi](http://openreview.net/profile?id=~Bowen_Shi1), [Matthew Le](http://openreview.net/profile?id=~Matthew_Le2), [Apoorv Vyas](http://openreview.net/profile?id=~Apoorv_Vyas1), [Andros Tjandra](http://openreview.net/profile?id=~Andros_Tjandra1), [Mahi Luthra](http://openreview.net/profile?email=mahiluthra%40meta.com), [Baishan Guo](http://openreview.net/profile?id=~Baishan_Guo1), [Huiyu Wang](http://openreview.net/profile?id=~Huiyu_Wang1), [Triantafyllos Afouras](http://openreview.net/profile?id=~Triantafyllos_Afouras1), [David Kant](http://openreview.net/profile?id=~David_Kant1), [Wei-Ning Hsu](http://openreview.net/profile?id=~Wei-Ning_Hsu2)
  - **Affiliations:** VGG, University of Oxford, UK; Meta, USA, Meta, USA, Meta, USA, Meta, USA, Meta, USA, Meta, USA, Meta, USA, Meta, USA, Meta, USA, Meta, USA, Meta, USA
  - **TL;DR:** The study presents MusicFlow, a cascaded text-to-music generation model that utilizes flow matching to effectively bridge text descriptions and music audio. It demonstrates superior quality and coherence in generated music while also enabling tasks like music infilling and continuation with fewer computational resources.
  - **Keywords:** text-to-music generation, audio generation, flow matching, self-supervised representations, masked prediction, music infilling, music continuation, modeling long-term structures, complex interactions in audio, superior quality music generation, competitive performance in music tasks, MusicCaps


- [Listening to the noise: Blind Denoising with Gibbs Diffusion](https://icml.cc/virtual/2024/poster/32940) (Poster)
  - **Authors:** [David Heurtel-Depeiges](http://openreview.net/profile?id=~David_Heurtel-Depeiges1), [Charles Margossian](http://openreview.net/profile?id=~Charles_Margossian1), [Ruben Ohana](http://openreview.net/profile?id=~Ruben_Ohana1), [Bruno Régaldo-Saint Blancard](http://openreview.net/profile?id=~Bruno_R%C3%A9galdo-Saint_Blancard1)
  - **Affiliations:** Ecole Polytechnique, Institut Polytechnique de Paris, Flatiron Institute, Flatiron Institute, Flatiron Institute
  - **TL;DR:** This paper introduces Gibbs Diffusion (GDiff), a novel methodology for blind denoising that simultaneously recovers both signal and noise characteristics by addressing posterior sampling of noise parameters. The method is demonstrated on natural images and cosmological data, highlighting its effectiveness in complex noise scenarios.
  - **Keywords:** blind denoising, deep generative models, Bayesian inference, Gibbs Diffusion (GDiff), conditional diffusion model, Monte Carlo sampler, natural images, cosmic microwave background data, medical imaging, astronomy, speech recognition, financial market analysis, blind denoising, noise characterization, posterior sampling, Gibbs algorithm, theoretical analysis of Gibbs stationary distribution, Gaussian noise, noise covariance, signal prior


- [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://icml.cc/virtual/2024/poster/34913) (Oral)
  - **Authors:** [Shusheng Xu](http://openreview.net/profile?id=~Shusheng_Xu1), [Wei Fu](http://openreview.net/profile?id=~Wei_Fu1), [Jiaxuan Gao](http://openreview.net/profile?id=~Jiaxuan_Gao1), [Wenjie Ye](http://openreview.net/profile?id=~Wenjie_Ye1), [Weilin Liu](http://openreview.net/profile?id=~Weilin_Liu1), [Zhiyu Mei](http://openreview.net/profile?id=~Zhiyu_Mei1), [Guangju Wang](http://openreview.net/profile?id=~Guangju_Wang1), [Chao Yu](http://openreview.net/profile?id=~Chao_Yu1), [Yi Wu](http://openreview.net/profile?id=~Yi_Wu1)
  - **Affiliations:** Tsinghua University, Beijing China, Tsinghua University, Beijing China, Tsinghua University, Beijing China, OpenPsi Inc., OpenPsi Inc., Tsinghua University, Beijing China, OpenPsi Inc., Tsinghua University, Beijing China; Shanghai Qi Zhi Institute, Shanghai, China, Tsinghua University, Beijing China; Shanghai Qi Zhi Institute, Shanghai, China
  - **TL;DR:** This study investigates the effectiveness of Direct Preference Optimization (DPO) versus Proximal Policy Optimization (PPO) in aligning large language models (LLMs) with human preferences. The findings reveal that while DPO has limitations, PPO consistently outperforms DPO across various benchmarks, achieving state-of-the-art results in challenging tasks.
  - **Keywords:** Large Language Models (LLMs), AI Alignment, Reinforcement Learning from Human Feedback (RLHF), Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), actor-critic algorithms, Dialogue systems, code generation, Performance comparison between reward-based and reward-free methods, limitations of DPO, challenges in RLHF benchmarks, Theoretical and empirical analysis of DPO and PPO, identification of key factors for PPO performance


- [Open-Domain Text Evaluation via Contrastive Distribution Methods](https://icml.cc/virtual/2024/poster/34810) (Poster)
  - **Authors:** [Sidi Lu](http://openreview.net/profile?id=~Sidi_Lu1), [Hongyi Liu](http://openreview.net/profile?id=~Hongyi_Liu4), [Asli Celikyilmaz](http://openreview.net/profile?id=~Asli_Celikyilmaz1), [Tianlu Wang](http://openreview.net/profile?id=~Tianlu_Wang1), [Nanyun Peng](http://openreview.net/profile?id=~Nanyun_Peng1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, Shanghai Jiao Tong University, Meta FAIR, Meta FAIR, Department of Computer Science, University of California, Los Angeles
  - **TL;DR:** This paper introduces Contrastive Distribution Methods (CDM) for evaluating open-domain text generation, addressing the challenges of assessing generation quality. The proposed method demonstrates a superior correlation with human judgment compared to existing automatic evaluation metrics.
  - **Keywords:** open-domain text generation, evaluation metrics, Contrastive Distribution Methods (CDM), discriminator-based metrics, dialogue systems, commonsense evaluation, assessing generation quality, limitations of existing evaluation metrics, novel evaluation method (CDM), improved correlation with human judgment, large pre-trained language models (LLMs), probabilistic distributions


- [NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](https://icml.cc/virtual/2024/poster/33552) (Oral)
  - **Authors:** [Zeqian Ju](http://openreview.net/profile?id=~Zeqian_Ju1), [Yuancheng Wang](http://openreview.net/profile?id=~Yuancheng_Wang1), [Kai Shen](http://openreview.net/profile?id=~Kai_Shen2), [Xu Tan](http://openreview.net/profile?id=~Xu_Tan1), [Detai Xin](http://openreview.net/profile?id=~Detai_Xin1), [Dongchao Yang](http://openreview.net/profile?id=~Dongchao_Yang1), [Eric Liu](http://openreview.net/profile?id=~Eric_Liu1), [Yichong Leng](http://openreview.net/profile?id=~Yichong_Leng1), [Kaitao Song](http://openreview.net/profile?id=~Kaitao_Song1), [Siliang Tang](http://openreview.net/profile?id=~Siliang_Tang1), [Zhizheng Wu](http://openreview.net/profile?id=~Zhizheng_Wu1), [Tao Qin](http://openreview.net/profile?id=~Tao_Qin1), [Xiangyang Li](http://openreview.net/profile?id=~Xiangyang_Li4), [Wei Ye](http://openreview.net/profile?id=~Wei_Ye2), [Shikun Zhang](http://openreview.net/profile?id=~Shikun_Zhang2), [Jiang Bian](http://openreview.net/profile?id=~Jiang_Bian1), [Lei He](http://openreview.net/profile?id=~Lei_He6), [Jinyu Li](http://openreview.net/profile?id=~Jinyu_Li1), [sheng zhao](http://openreview.net/profile?id=~sheng_zhao1)
  - **Affiliations:** University of Science and Technology of China; Microsoft Research & Microsoft Azure, The Chinese University of Hong Kong, Shenzhen, Zhejiang University, Microsoft Research & Microsoft Azure, Microsoft Research & Microsoft Azure; The University of Tokyo, Microsoft Research & Microsoft Azure, Microsoft Research & Microsoft Azure, Microsoft Research & Microsoft Azure, Microsoft Research & Microsoft Azure, Zhejiang University, The Chinese University of Hong Kong, Shenzhen, Microsoft Research & Microsoft Azure, University of Science and Technology of China, Peking University, Peking University, Microsoft Research & Microsoft Azure, Microsoft Research & Microsoft Azure, Microsoft Research & Microsoft Azure, Microsoft Research & Microsoft Azure
  - **TL;DR:** The study presents NaturalSpeech 3, a novel text-to-speech synthesis system that utilizes factorized diffusion models and a neural codec to generate high-quality natural speech in a zero-shot manner. Experimental results demonstrate that it significantly outperforms existing TTS systems in terms of quality, similarity, prosody, and intelligibility.
  - **Keywords:** text-to-speech synthesis, zero-shot speech synthesis, factorized diffusion models, factorized vector quantization (FVQ), neural codec, speech generation, natural speech synthesis, speech quality, similarity, prosody, intricate information in speech, NaturalSpeech 3, improved TTS system performance


- [End-to-End Neuro-Symbolic Reinforcement Learning with Textual Explanations](https://icml.cc/virtual/2024/poster/35201) (Spotlight Poster)
  - **Authors:** [Lirui Luo](http://openreview.net/profile?id=~Lirui_Luo1), [Guoxi Zhang](http://openreview.net/profile?id=~Guoxi_Zhang1), [Hongming Xu](http://openreview.net/profile?id=~Hongming_Xu2), [Yaodong Yang](http://openreview.net/profile?id=~Yaodong_Yang1), [Cong Fang](http://openreview.net/profile?id=~Cong_Fang1), [Qing Li](http://openreview.net/profile?id=~Qing_Li1)
  - **Affiliations:** School of Intelligence Science and Technology, Peking University; State Key Laboratory of General Artificial Intelligence, BIGAI, State Key Laboratory of General Artificial Intelligence, BIGAI, State Key Laboratory of General Artificial Intelligence, BIGAI, School of Intelligence Science and Technology, Peking University; State Key Laboratory of General Artificial Intelligence, BIGAI, School of Intelligence Science and Technology, Peking University; State Key Laboratory of General Artificial Intelligence, BIGAI, State Key Laboratory of General Artificial Intelligence, BIGAI
  - **TL;DR:** This paper presents INSIGHT, a neuro-symbolic framework that jointly learns structured states and symbolic policies while generating textual explanations for decision-making, addressing inefficiencies and accessibility issues in neuro-symbolic reinforcement learning. The approach is validated through experiments on nine Atari tasks, demonstrating improved performance and interpretability.
  - **Keywords:** Neuro-symbolic reinforcement learning, explainable decision-making, Structured state representations, vision foundation models, equation learner (EQL), GPT-4, Visual reinforcement learning, Atari games, Inefficiency in refining structured states with rewards, accessibility of symbolic policies, INSIGHT framework for learning structured states and symbolic policies, textual explanations for policies and decisions, Atari tasks


- [GATE: How to Keep Out Intrusive Neighbors](https://icml.cc/virtual/2024/poster/33995) (Poster)
  - **Authors:** [Nimrah Mustafa](http://openreview.net/profile?id=~Nimrah_Mustafa1), [Rebekka Burkholz](http://openreview.net/profile?id=~Rebekka_Burkholz1)
  - **Affiliations:** CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany, CISPA Helmholtz Center for Information Security, 66123 Saarbrücken, Germany
  - **TL;DR:** The study introduces GATE, an extension of Graph Attention Networks (GATs), which effectively addresses the issue of over-smoothing by minimizing unnecessary neighborhood aggregation. The findings suggest that GATE outperforms traditional GATs on real-world heterophilic datasets, highlighting the importance of task-specific neighborhood aggregation in graph neural networks.
  - **Keywords:** Graph Attention Networks, neighborhood aggregation, over-smoothing, GAT (Graph Attention Network), GNN (Graph Neural Network), perceptrons, Node classification, graph-structured data, Over-smoothing, task-irrelevant neighborhood aggregation, GATE (GAT extension), improved neighborhood aggregation, Heterophilic datasets


- [What Would Gauss Say About Representations? Probing Pretrained Image Models using Synthetic Gaussian Benchmarks](https://icml.cc/virtual/2024/poster/34226) (Poster)
  - **Authors:** [Ching-Yun (Irene) Ko](http://openreview.net/profile?id=~Ching-Yun_Ko1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1), [Payel Das](http://openreview.net/profile?id=~Payel_Das1), [Jeet Mohapatra](http://openreview.net/profile?id=~Jeet_Mohapatra1), [Luca Daniel](http://openreview.net/profile?id=~Luca_Daniel1)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, MIT, Cambridge, USA; IBM Research, Yorktown, USA, IBM Research, Yorktown, USA, IBM Research, Yorktown, USA, Department of Electrical Engineering and Computer Science, MIT, Cambridge, USA, Department of Electrical Engineering and Computer Science, MIT, Cambridge, USA
  - **TL;DR:** This paper proposes a task-agnostic method for evaluating pretrained image models using synthetic Gaussian benchmarks, revealing intrinsic model capabilities and addressing evaluation data bias. The findings indicate that the proposed evaluation correlates with actual performance on downstream tasks and can guide better robustness-accuracy trade-offs.
  - **Keywords:** task-agnostic representation learning, pretrained models, linear probing, Gaussian mixtures, Fisher’s linear discriminant, image models, downstream tasks, evaluation data bias, model deficiency, holistic evaluation, robustness-accuracy trade-off


- [Matrix Information Theory for Self-Supervised Learning](https://icml.cc/virtual/2024/poster/32737) (Poster)
  - **Authors:** [Yifan Zhang](http://openreview.net/profile?id=~Yifan_Zhang16), [Zhiquan Tan](http://openreview.net/profile?id=~Zhiquan_Tan1), [Jingqin Yang](http://openreview.net/profile?id=~Jingqin_Yang2), [Weiran Huang](http://openreview.net/profile?id=~Weiran_Huang1), [Yang Yuan](http://openreview.net/profile?id=~Yang_Yuan4)
  - **Affiliations:** IIIS, Tsinghua University, Beijing, China, Department of Mathematical Sciences, Tsinghua University, Beijing, China, IIIS, Tsinghua University, Beijing, China, MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, IIIS, Tsinghua University, Beijing, China; Shanghai Qizhi Institute, Shanghai, China
  - **TL;DR:** This study introduces Matrix-SSL, a novel self-supervised learning approach that utilizes matrix information theory to enhance maximum entropy encoding methods. Experimental results demonstrate that Matrix-SSL outperforms existing state-of-the-art methods on benchmark datasets like ImageNet and MS-COCO, achieving significant improvements in transfer learning tasks.
  - **Keywords:** Self-Supervised Learning, Maximum Entropy Encoding, Matrix Information Theory, Matrix-SSL, Maximum Entropy Encoding Loss, Matrix Alignment Loss, Image Recognition, Transfer Learning, Outperforms state-of-the-art methods, Enhanced maximum entropy encoding, ImageNet, MS-COCO, GSM8K, Non-contrastive Learning, Contrastive Learning, Covariance Matrices


- [CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks](https://icml.cc/virtual/2024/poster/34678) (Poster)
  - **Authors:** [Shashank Agnihotri](http://openreview.net/profile?id=~Shashank_Agnihotri1), [Steffen Jung](http://openreview.net/profile?id=~Steffen_Jung1), [Margret Keuper](http://openreview.net/profile?id=~Margret_Keuper1)
  - **Affiliations:** Data and Web Science Group, University of Mannheim, Germany, Max-Planck-Institute for Informatics, Saarland Informatics Campus, Germany, None
  - **TL;DR:** The paper introduces CosPGD, an efficient white-box adversarial attack designed to enhance the robustness evaluation of neural networks in pixel-wise prediction tasks. CosPGD outperforms previous state-of-the-art methods by balancing errors across the entire image domain while maintaining optimization stability.
  - **Keywords:** Adversarial attacks, Neural networks, Robustness evaluation, Projected Gradient Descent (PGD), CosPGD, Semantic segmentation, Optical flow estimation, Image restoration, Lack of robustness, Input perturbations, Optimization stability, Efficient adversarial attack methods, Alignment score for loss scaling, Sintel dataset


- [Information Flow in Self-Supervised Learning](https://icml.cc/virtual/2024/poster/32767) (Poster)
  - **Authors:** [Zhiquan Tan](http://openreview.net/profile?id=~Zhiquan_Tan1), [Jingqin Yang](http://openreview.net/profile?id=~Jingqin_Yang2), [Weiran Huang](http://openreview.net/profile?id=~Weiran_Huang1), [Yang Yuan](http://openreview.net/profile?id=~Yang_Yuan4), [Yifan Zhang](http://openreview.net/profile?id=~Yifan_Zhang16)
  - **Affiliations:** Department of Mathematical Sciences, Tsinghua University, Beijing, China, IIIS, Tsinghua University, Beijing, China, MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, IIIS, Tsinghua University, Beijing, China; Shanghai Qizhi Institute, Shanghai, China, IIIS, Tsinghua University, Beijing, China
  - **TL;DR:** This paper analyzes dual-branch self-supervised learning methods, specifically Barlow Twins and spectral contrastive learning, through matrix mutual information, leading to the introduction of a new method, M-MAE, which shows improved performance on ImageNet. The findings highlight the implicit optimization of mutual information and joint entropy in these learning approaches.
  - **Keywords:** Self-supervised learning, Dual-branch learning, Barlow Twins, Spectral contrastive learning, Matrix Variational Masked Auto-Encoder (M-MAE), MAE, U-MAE, Image classification, Image segmentation, Optimization of mutual information, Joint entropy, Improvement in linear probing and fine-tuning performance, ImageNet, Siamese architecture, Matrix mutual information


- [Position: Data-driven Discovery with Large Generative Models](https://icml.cc/virtual/2024/poster/34977) (Poster)
  - **Authors:** [Bodhisattwa Prasad Majumder](http://openreview.net/profile?id=~Bodhisattwa_Prasad_Majumder1), [Harshit Surana](http://openreview.net/profile?id=~Harshit_Surana1), [Dhruv Agarwal](http://openreview.net/profile?id=~Dhruv_Agarwal2), [Sanchaita Hazra](http://openreview.net/profile?id=~Sanchaita_Hazra1), [Ashish Sabharwal](http://openreview.net/profile?id=~Ashish_Sabharwal1), [Peter Clark](http://openreview.net/profile?id=~Peter_Clark1)
  - **Affiliations:** Allen Institute for AI, OpenLocus, University of Massachusetts Amherst, University of Utah, Allen Institute for AI, Allen Institute for AI
  - **TL;DR:** This position paper advocates for the use of large generative models to automate data-driven scientific discovery, emphasizing the need for systems that can generate and verify hypotheses from existing datasets. The authors highlight the limitations of current methods and propose integrating user feedback and robust tools to enhance the discovery process.
  - **Keywords:** data-driven discovery, large generative models, scientific discovery, large generative models (LGMs), GPT-4, statistical tests (OLS, GLM), scientific research, hypothesis generation and verification, challenges in data absorption, hypothesis formulation, and meaningful conclusions, automated systems for data-driven discovery, verification plans


- [Better & Faster Large Language Models via Multi-token Prediction](https://icml.cc/virtual/2024/poster/33048) (Poster)
  - **Authors:** [Fabian Gloeckle](http://openreview.net/profile?id=~Fabian_Gloeckle1), [Badr Youbi Idrissi](http://openreview.net/profile?id=~Badr_Youbi_Idrissi1), [Baptiste Roziere](http://openreview.net/profile?id=~Baptiste_Roziere1), [David Lopez-Paz](http://openreview.net/profile?id=~David_Lopez-Paz2), [Gabriel Synnaeve](http://openreview.net/profile?id=~Gabriel_Synnaeve1)
  - **Affiliations:** FAIR at Meta; CERMICS Ecole des Ponts ParisTech; LISN Université Paris-Saclay, FAIR at Meta; CERMICS Ecole des Ponts ParisTech; LISN Université Paris-Saclay, FAIR at Meta, FAIR at Meta, FAIR at Meta
  - **TL;DR:** This study proposes a multi-token prediction approach for training large language models, which enhances sample efficiency and improves performance on generative tasks. The method allows models to predict multiple tokens simultaneously, resulting in significant gains in problem-solving capabilities and faster inference times.
  - **Keywords:** Large Language Models, Multi-token Prediction, Next-token Prediction, Auxiliary Training Task, Code Generation, Natural Language Processing, Sample Efficiency, Data Requirements, Improved Downstream Capabilities, Self-Speculative Decoding, HumanEval, MBPP


- [ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data](https://icml.cc/virtual/2024/poster/34843) (Poster)
  - **Authors:** [Carmen Martin-Turrero](http://openreview.net/profile?email=cmartur%40gmail.com), [Maxence Bouvier](http://openreview.net/profile?id=~Maxence_Bouvier1), [Manuel Breitenstein](http://openreview.net/profile?email=manuel.breitenstein%40gmail.com), [Pietro Zanuttigh](http://openreview.net/profile?id=~Pietro_Zanuttigh1), [Vincent Parret](http://openreview.net/profile?id=~Vincent_Parret1)
  - **Affiliations:** Sony Semiconductor Solutions Europe; University of Padova, Sony Semiconductor Solutions Europe, Sony Semiconductor Solutions Europe, University of Padova, Sony Semiconductor Solutions Europe
  - **TL;DR:** The study presents the ALERT-Transformer, a hybrid pipeline that integrates asynchronous sensing with synchronous processing to efficiently handle ultra-sparse spatiotemporal data from event-based sensors. The proposed method achieves state-of-the-art performance in object and gesture recognition with lower latency compared to existing approaches.
  - **Keywords:** event-based sensors, spatiotemporal data processing, machine learning, PointNet, ALERT module, Vision Transformer, transformer model, object recognition, gesture recognition, data sparsity, real-time processing, continuous time representation, asynchronous embedding updates, flexible readout, patch-based approach, time encoding solution, PyTorch, TensorFlow


- [Debating with More Persuasive LLMs Leads to More Truthful Answers](https://icml.cc/virtual/2024/poster/33360) (Best Paper)
  - **Authors:** [Akbir Khan](http://openreview.net/profile?id=~Akbir_Khan1), [John Hughes](http://openreview.net/profile?id=~John_Hughes4), [Dan Valentine](http://openreview.net/profile?id=~Dan_Valentine1), [Laura Ruis](http://openreview.net/profile?id=~Laura_Ruis1), [Kshitij Sachan](http://openreview.net/profile?id=~Kshitij_Sachan1), [Ansh Radhakrishnan](http://openreview.net/profile?id=~Ansh_Radhakrishnan1), [Edward Grefenstette](http://openreview.net/profile?id=~Edward_Grefenstette1), [Samuel Bowman](http://openreview.net/profile?id=~Samuel_R._Bowman1), [Tim Rocktäschel](http://openreview.net/profile?id=~Tim_Rockt%C3%A4schel1), [Ethan Perez](http://openreview.net/profile?id=~Ethan_Perez1)
  - **Affiliations:** University College London, Speechmatics, MATS, University College London, Anthropic, Anthropic, University College London, Anthropic, University College London, Anthropic; FAR AI
  - **TL;DR:** This study investigates the use of debate between large language models (LLMs) to enhance the ability of non-expert models and humans to identify correct answers in the absence of ground truth. The findings demonstrate that debate significantly improves accuracy in answering comprehension questions, suggesting a viable method for aligning models without relying on human-labelled data.
  - **Keywords:** Large Language Models, AI Alignment, Model Evaluation, Debate, Persuasiveness Optimization, Comprehension Tasks, AI Oversight, Lack of Ground Truth, Human Evaluation Limitations, Improved Accuracy in Answering Questions, Scalable Oversight Mechanisms, QuALITY Comprehension Task


- [Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution](https://icml.cc/virtual/2024/poster/34801) (Poster)
  - **Authors:** [Chrisantha Fernando](http://openreview.net/profile?id=~Chrisantha_Fernando1), [Dylan Banarse](http://openreview.net/profile?id=~Dylan_Sunil_Banarse1), [Henryk Michalewski](http://openreview.net/profile?id=~Henryk_Michalewski1), [Simon Osindero](http://openreview.net/profile?id=~Simon_Osindero1), [Tim Rocktäschel](http://openreview.net/profile?id=~Tim_Rockt%C3%A4schel1)
  - **Affiliations:** Google DeepMind, London, None, None, None, None
  - **TL;DR:** This paper introduces PROMPTBREEDER, a self-referential self-improvement mechanism that evolves and adapts prompts for Large Language Models, outperforming existing strategies like Chain-of-Thought and Plan-and-Solve Prompting. The findings suggest that this approach can enhance reasoning abilities and tackle complex tasks such as hate speech classification.
  - **Keywords:** self-referential self-improvement, prompt evolution, Large Language Models, evolutionary algorithm, mutation-prompts, commonsense reasoning, hate speech classification, sub-optimal prompt strategies, diminishing returns in prompt engineering, Promptbreeder, improved task-prompts, Chain-of-Thought Prompting, Plan-and-Solve Prompting


- [Position: Open-Endedness is Essential for Artificial Superhuman Intelligence](https://icml.cc/virtual/2024/poster/34718) (Oral)
  - **Authors:** [Edward Hughes](http://openreview.net/profile?id=~Edward_Hughes1), [Michael Dennis](http://openreview.net/profile?id=~Michael_D_Dennis1), [Jack Parker-Holder](http://openreview.net/profile?id=~Jack_Parker-Holder1), [Feryal Behbahani](http://openreview.net/profile?id=~Feryal_Behbahani1), [Aditi Mavalankar](http://openreview.net/profile?id=~Aditi_Mavalankar1), [Yuge Shi](http://openreview.net/profile?id=~Yuge_Shi2), [Tom Schaul](http://openreview.net/profile?id=~Tom_Schaul2), [Tim Rocktäschel](http://openreview.net/profile?id=~Tim_Rockt%C3%A4schel1)
  - **Affiliations:** Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK, Google DeepMind, London, UK
  - **TL;DR:** The paper argues that open-endedness is a crucial property for achieving artificial superhuman intelligence (ASI) and provides a formal definition of open-endedness through novelty and learnability. It emphasizes the need for AI systems to self-improve and make human-relevant discoveries while addressing safety implications.
  - **Keywords:** Open-endedness, Artificial Superhuman Intelligence (ASI), Foundation Models, AI systems, Human-relevant discoveries, Self-improvement, Creative and diverse discoveries, Data scarcity, Formal definition of open-endedness, Novelty and learnability, Large Language Models (LLMs), Artificial General Intelligence (AGI), AI Safety, AI Alignment


- [ATraDiff: Accelerating Online Reinforcement Learning with Imaginary Trajectories](https://icml.cc/virtual/2024/poster/33264) (Poster)
  - **Authors:** [Qianlan Yang](http://openreview.net/profile?id=~Qianlan_Yang1), [Yu-Xiong Wang](http://openreview.net/profile?id=~Yu-Xiong_Wang1)
  - **Affiliations:** Department of Computer Science, University of Illinois Urbana-Champaign, Urbana, Illinois, USA, Department of Computer Science, University of Illinois Urbana-Champaign, Urbana, Illinois, USA
  - **TL;DR:** The study introduces Adaptive Trajectory Diffuser (ATraDiff), a generative model that enhances online reinforcement learning by generating synthetic trajectories to address challenges posed by sparse rewards and high sample costs. Empirical results demonstrate that ATraDiff significantly improves the performance of various RL methods across complex environments.
  - **Keywords:** online reinforcement learning, data efficiency, generative models, Adaptive Trajectory Diffuser (ATraDiff), data augmentation, autonomous driving, robotic manipulation, sparse rewards, high sample costs, distribution shifts, state-of-the-art performance, adaptability in trajectory generation


- [HAMLET: Graph Transformer Neural Operator for Partial Differential Equations](https://icml.cc/virtual/2024/poster/33118) (Poster)
  - **Authors:** [Andrey Bryutkin](http://openreview.net/profile?id=~Andrey_Bryutkin1), [Jiahao Huang](http://openreview.net/profile?id=~Jiahao_Huang1), [Zhongying Deng](http://openreview.net/profile?id=~Zhongying_Deng1), [Guang Yang](http://openreview.net/profile?id=~Guang_Yang1), [Carola-Bibiane Schönlieb](http://openreview.net/profile?id=~Carola-Bibiane_Sch%C3%B6nlieb1), [Angelica I Aviles-Rivero](http://openreview.net/profile?id=~Angelica_I_Aviles-Rivero1)
  - **Affiliations:** Department of Mathematics, MIT, USA, Bioengineering Department and Imperial-X, National Heart and Lung Institute & Cardiovascular Research Centre, Imperial College London, UK, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK, Bioengineering Department and Imperial-X, National Heart and Lung Institute & Cardiovascular Research Centre, Imperial College London, UK, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK
  - **TL;DR:** The study introduces HAMLET, a novel graph transformer framework designed to solve partial differential equations (PDEs) using neural networks, enhancing adaptability and performance across various domains. Experimental results demonstrate its superiority over existing techniques, particularly in scenarios with limited data and complex geometries.
  - **Keywords:** graph transformers, neural networks, partial differential equations (PDEs), graph transformer framework, modular input encoders, neural operators, fluid dynamics, electromagnetics, finance, healthcare, computational expense, high-dimensional problems, complex geometries, limited generalisability, discretisation invariance, novel framework (HAMLET), improved model resilience and performance, adaptability to irregular meshes, Physics-Informed Neural Networks (PINNs), neural operators


- [Learning from Memory: Non-Parametric Memory Augmented Self-Supervised Learning of Visual Features](https://icml.cc/virtual/2024/poster/34573) (Poster)
  - **Authors:** [Thalles Silva](http://openreview.net/profile?id=~Thalles_Silva1), [Helio Pedrini](http://openreview.net/profile?id=~Helio_Pedrini1), [Adín Ramírez Rivera](http://openreview.net/profile?id=~Ad%C3%ADn_Ram%C3%ADrez_Rivera1)
  - **Affiliations:** Institute of Computing, University of Campinas, Campinas-SP, Brazil, Institute of Computing, University of Campinas, Campinas-SP, Brazil, Department of Informatics, University of Oslo, Oslo, Norway
  - **TL;DR:** This paper presents a novel method called Memory Augmented Self-Supervised Learning (MaSSL) that enhances the stability of self-supervised learning by utilizing a non-parametric memory of previously seen concepts. The approach demonstrates improved performance on various vision tasks while reducing computational costs and training time.
  - **Keywords:** Self-Supervised Learning (SSL), Memory Augmented Learning, Non-Parametric Memory, Stochastic Memory Blocks, Clustering-based Approaches, Image Retrieval, Transfer Learning, Low-Shot Classification, Training Instability, Training Collapse, Lack of Human Labels, Stable SSL Training, Highly Transferable Representations


- [HexGen: Generative Inference of Large Language Model over Heterogeneous Environment](https://icml.cc/virtual/2024/poster/34819) (Poster)
  - **Authors:** [Youhe Jiang](http://openreview.net/profile?id=~YOUHE_JIANG1), [Ran Yan](http://openreview.net/profile?email=ryanaf%40connect.ust.hk), [Xiaozhe Yao](http://openreview.net/profile?id=~Xiaozhe_Yao1), [Yang Zhou](http://openreview.net/profile?email=yangzho6%40andrew.cmu.edu), [Beidi Chen](http://openreview.net/profile?id=~Beidi_Chen1), [Binhang Yuan](http://openreview.net/profile?id=~Binhang_Yuan1)
  - **Affiliations:** Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China, Department of Computer Science, ETH Zurich, Zürich, Switzerland, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China
  - **TL;DR:** This paper presents HEXGEN, a flexible distributed inference engine designed to optimize generative inference of large language models in heterogeneous and cross-datacenter environments, significantly reducing inference costs and improving latency. The evaluation shows that HEXGEN can achieve up to 2.3× lower latency or tolerate up to 4× more request rates compared to traditional homogeneous setups.
  - **Keywords:** Generative inference, Large language models, Heterogeneous environments, Tensor model parallelism, Pipeline parallelism, Scheduling algorithms, AI applications, Cloud services, Inference costs, Latency levels, Deployment challenges, HEXGEN, Asymmetric partitioning of computations, Efficiency improvements, LLAMA-2 (70B)


- [Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts](https://icml.cc/virtual/2024/poster/34070) (Poster)
  - **Authors:** [Shengzhuang Chen](http://openreview.net/profile?id=~Shengzhuang_Chen1), [Jihoon Tack](http://openreview.net/profile?id=~Jihoon_Tack1), [Yunqiao Yang](http://openreview.net/profile?id=~Yunqiao_Yang1), [Yee-Whye Teh](http://openreview.net/profile?id=~Yee_Whye_Teh2), [Jonathan Richard Schwarz](http://openreview.net/profile?id=~Jonathan_Richard_Schwarz1), [Ying WEI](http://openreview.net/profile?id=~Ying_Wei1)
  - **Affiliations:** City University of Hong Kong, Korea Advanced Institute of Science and Technology, City University of Hong Kong, University of Oxford, Harvard University, Nanyang Technological University
  - **TL;DR:** This paper introduces Sparse MetA-Tuning (SMAT), a novel method that enhances the transfer abilities of vision foundation models by addressing out-of-distribution sensitivity. The authors demonstrate that SMAT achieves state-of-the-art results on challenging tasks, surpassing traditional parameter-efficient fine-tuning approaches.
  - **Keywords:** meta-tuning, few-shot generalization, transfer learning, Sparse MetA-Tuning (SMAT), sparse mixture-of-experts, vision foundation models, out-of-distribution (OOD) sensitivity, in-distribution generalization, state-of-the-art results, optimization of foundation models, Meta-Dataset, parameter-efficient fine-tuning, self-supervised objectives


- [Position: Future Directions in the Theory of Graph Machine Learning](https://icml.cc/virtual/2024/poster/32757) (Poster)
  - **Authors:** [Christopher Morris](http://openreview.net/profile?id=~Christopher_Morris1), [Fabrizio Frasca](http://openreview.net/profile?id=~Fabrizio_Frasca1), [Nadav Dym](http://openreview.net/profile?id=~Nadav_Dym1), [Haggai Maron](http://openreview.net/profile?id=~Haggai_Maron1), [Ismail Ceylan](http://openreview.net/profile?id=~Ismail_Ilkan_Ceylan2), [Ron Levie](http://openreview.net/profile?id=~Ron_Levie1), [Derek Lim](http://openreview.net/profile?id=~Derek_Lim1), [Michael Bronstein](http://openreview.net/profile?id=~Michael_M._Bronstein1), [Martin Grohe](http://openreview.net/profile?id=~Martin_Grohe1), [Stefanie Jegelka](http://openreview.net/profile?id=~Stefanie_Jegelka3)
  - **Affiliations:** RWTH Aachen University, Technion - Israel Institute of Technology, Technion - Israel Institute of Technology, Technion - Israel Institute of Technology; NVIDIA Research, University of Oxford, Technion - Israel Institute of Technology, MIT, University of Oxford, RWTH Aachen University, MIT; TU Munich
  - **TL;DR:** The paper discusses the need for a balanced theory of graph machine learning, emphasizing the interplay between expressive power, generalization, and optimization of graph neural networks (GNNs). It highlights the current limitations in understanding GNNs' theoretical properties and calls for further research in this area.
  - **Keywords:** Graph Machine Learning, Graph Neural Networks (GNNs), Message-Passing Neural Networks (MPNNs), Combinatorial Techniques, Weisfeiler–Leman Algorithm, Life Sciences, Social Sciences, Engineering Sciences, Theoretical Understanding of GNNs, Generalization Behavior, Expressive Power, Graph Isomorphism, Graph Structure, Graph Transformers


- [PlanDQ: Hierarchical Plan Orchestration via D-Conductor and Q-Performer](https://icml.cc/virtual/2024/poster/35165) (Poster)
  - **Authors:** [Chang Chen](http://openreview.net/profile?id=~Chang_Chen1), [Junyeob Baek](http://openreview.net/profile?id=~Junyeob_Baek1), [Fei Deng](http://openreview.net/profile?id=~Fei_Deng1), [Kenji Kawaguchi](http://openreview.net/profile?id=~Kenji_Kawaguchi1), [Caglar Gulcehre](http://openreview.net/profile?id=~Caglar_Gulcehre1), [Sungjin Ahn](http://openreview.net/profile?id=~Sungjin_Ahn1)
  - **Affiliations:** Rutgers University, KAIST, Rutgers University, National University of Singapore, EPFL, KAIST
  - **TL;DR:** The study introduces PlanDQ, a hierarchical planner for offline reinforcement learning that combines a diffusion-based planner (D-Conductor) with a Q-learning approach (Q-Performer) to effectively tackle sparse-reward, long-horizon tasks. Experimental results demonstrate that PlanDQ achieves competitive performance across various benchmark tasks, addressing the challenges of credit assignment and extrapolation errors.
  - **Keywords:** offline reinforcement learning, hierarchical planning, diffusion-based planner, Q-learning, healthcare, autonomous driving, robotics, sparse-reward tasks, long-horizon tasks, credit assignment, extrapolation errors, PlanDQ, D-Conductor, Q-Performer, D4RL continuous control benchmark, AntMaze, Kitchen, Calvin


- [Data-Efficient Molecular Generation with Hierarchical Textual Inversion](https://icml.cc/virtual/2024/poster/34851) (Poster)
  - **Authors:** [Seojin Kim](http://openreview.net/profile?id=~Seojin_Kim2), [Jaehyun Nam](http://openreview.net/profile?id=~Jaehyun_Nam2), [Sihyun Yu](http://openreview.net/profile?id=~Sihyun_Yu2), [Younghoon Shin](http://openreview.net/profile?id=~Younghoon_Shin2), [Jinwoo Shin](http://openreview.net/profile?id=~Jinwoo_Shin1)
  - **Affiliations:** Korea Advanced Institute of Science and Technology (KAIST), Korea Advanced Institute of Science and Technology (KAIST), Korea Advanced Institute of Science and Technology (KAIST), Korea University, Korea Advanced Institute of Science and Technology (KAIST)
  - **TL;DR:** The study introduces HI-Mol, a novel data-efficient molecular generation method that leverages hierarchical information to effectively generate molecules with limited data. It demonstrates superior performance in generating novel molecules and low-shot molecular property prediction compared to existing methods.
  - **Keywords:** molecular generation, data efficiency, drug discovery, Hierarchical Textual Inversion, multi-level embeddings, drug discovery, material design, limited data regimes, data sparsity, HI-Mol method, low-shot molecular property prediction, QM9


- [Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience](https://icml.cc/virtual/2024/poster/34941) (Poster)
  - **Authors:** [Martina G. Vilas](http://openreview.net/profile?id=~Martina_G._Vilas1), [Federico Adolfi](http://openreview.net/profile?id=~Federico_Adolfi1), [David Poeppel](http://openreview.net/profile?id=~David_Poeppel1), [Gemma Roig](http://openreview.net/profile?id=~Gemma_Roig1)
  - **Affiliations:** Ernst Strüngmann Institute for Neuroscience, Frankfurt am Main, Germany; Department of Computer Science, Goethe University, Frankfurt am Main, Germany, Ernst Strüngmann Institute for Neuroscience, Frankfurt am Main, Germany, Department of Psychology, New York University, New York, USA, The Hessian Center for AI, Hessen, Germany
  - **TL;DR:** The paper proposes a conceptual framework for Inner Interpretability in AI, drawing parallels with Cognitive Neuroscience to address critiques and enhance mechanistic explanations of AI systems. It emphasizes the need for a structured approach to interpret the internal mechanisms of AI models effectively.
  - **Keywords:** Inner Interpretability, AI systems, Cognitive Neuroscience, Mechanistic explanations, Causal abstraction approaches, Lack of conceptual framework, Critiques of interpretability, General conceptual framework, Methodological strategies


- [Differentially Private Representation Learning via Image Captioning](https://icml.cc/virtual/2024/poster/34185) (Poster)
  - **Authors:** [Tom Sander](http://openreview.net/profile?id=~Tom_Sander1), [Yaodong Yu](http://openreview.net/profile?id=~Yaodong_Yu4), [Maziar Sanjabi](http://openreview.net/profile?id=~Maziar_Sanjabi1), [Alain Oliviero Durmus](http://openreview.net/profile?id=~Alain_Oliviero_Durmus1), [Yi Ma](http://openreview.net/profile?id=~Yi_Ma4), [Kamalika Chaudhuri](http://openreview.net/profile?id=~Kamalika_Chaudhuri1), [Chuan Guo](http://openreview.net/profile?id=~Chuan_Guo1)
  - **Affiliations:** Meta; CMAP, École polytechnique, Meta; UC Berkeley, Meta, CMAP, École polytechnique, UC Berkeley, UCSD, Meta
  - **TL;DR:** This study demonstrates that effective differentially private representation learning can be achieved through image captioning on large multimodal datasets, overcoming the limitations of previous methods. The proposed DP image captioner (DP-Cap) significantly improves representation quality, achieving a new state-of-the-art accuracy on ImageNet-1K under a privacy budget.
  - **Keywords:** Differentially Private Machine Learning, Representation Learning, Image Captioning, DP-SGD, Masked Autoencoder (MAE), Vision Tasks, Vision-Language Tasks, Privacy-Utility Trade-off, Sub-optimal Representation Learning, High-Quality Image Features, Improved Accuracy on ImageNet-1K, LAION-2B, ImageNet-1K


- [Universality of Linear Recurrences Followed by Non-linear Projections: Finite-Width Guarantees and Benefits of Complex Eigenvalues](https://icml.cc/virtual/2024/poster/35035) (Poster)
  - **Authors:** [Antonio Orvieto](http://openreview.net/profile?id=~Antonio_Orvieto3), [Soham De](http://openreview.net/profile?id=~Soham_De2), [Caglar Gulcehre](http://openreview.net/profile?email=caglar.gulcehre%40epfl.ch), [Razvan Pascanu](http://openreview.net/profile?id=~Razvan_Pascanu1), [Samuel Smith](http://openreview.net/profile?id=~Samuel_L_Smith1)
  - **Affiliations:** ELLIS Institute Tübingen, Max Planck Institute for Intelligent Systems, Tübingen AI Center, Tübingen, Germany, Google Deepmind, EPFL, Google Deepmind, Google Deepmind
  - **TL;DR:** This paper investigates the combination of linear RNNs and MLPs for sequence modeling, demonstrating that both real and complex linear recurrences can achieve universal approximation. The findings highlight the advantages of using complex eigenvalues for improved information storage and address challenges related to the vanishing gradient issue.
  - **Keywords:** deep neural networks, sequence modeling, state-space models, linear RNNs, position-wise MLPs, complex eigenvalues, text, genetics, long-range reasoning, expressive power, vanishing gradient issue, universal approximation, lossless encoding, S4, LRU, Mamba, causal sequence-to-sequence maps


- [Evaluation of Trajectory Distribution Predictions with Energy Score](https://icml.cc/virtual/2024/poster/34547) (Poster)
  - **Authors:** [Novin Shahroudi](http://openreview.net/profile?id=~Novin_Shahroudi1), [Mihkel Lepson](http://openreview.net/profile?id=~Mihkel_Lepson1), [Meelis Kull](http://openreview.net/profile?id=~Meelis_Kull1)
  - **Affiliations:** Institute of Computer Science, University of Tartu, Tartu, Tartu County, Estonia, Institute of Computer Science, University of Tartu, Tartu, Tartu County, Estonia, Institute of Computer Science, University of Tartu, Tartu, Tartu County, Estonia
  - **TL;DR:** This study critiques the Minimum of N metric for evaluating multimodal trajectory predictions in autonomous systems, highlighting its potential for misleading assessments. It proposes Energy Score-based evaluation measures as a more reliable alternative for assessing trajectory distribution predictions.
  - **Keywords:** trajectory prediction, uncertainty in autonomous systems, Minimum of N (MoN), Energy Score, autonomous vehicles, self-driving cars, inherent uncertainty in trajectory predictions, evaluation of multimodal trajectory predictions, evaluation measures for trajectory distribution predictions, ETH/UCY dataset, Multimodal Trajectory Prediction (MTP), proper scoring rules


- [Dynamic Facility Location in High Dimensional Euclidean Spaces](https://icml.cc/virtual/2024/poster/32936) (Spotlight Poster)
  - **Authors:** [Sayan Bhattacharya](http://openreview.net/profile?id=~Sayan_Bhattacharya2), [Gramoz Goranci](http://openreview.net/profile?id=~Gramoz_Goranci1), [Shaofeng Jiang](http://openreview.net/profile?id=~Shaofeng_H.-C._Jiang1), [Yi Qian](http://openreview.net/profile?email=qianyi%40stu.pku.edu.cn), [Yubo Zhang](http://openreview.net/profile?id=~Yubo_Zhang4)
  - **Affiliations:** University of Warwick, UK, Faculty of Computer Science, University of Vienna, Austria, Peking University, China, Peking University, China, Peking University, China
  - **TL;DR:** This study presents the first fully dynamic algorithm for the facility location problem in high-dimensional Euclidean spaces, achieving O(c)-approximation with efficient point updates. The algorithm demonstrates high-quality solutions with low running time and minimal recourse, addressing the challenges of dynamic data in clustering applications.
  - **Keywords:** Facility Location, Dynamic Clustering, Fully dynamic algorithm, O(c)-approximation, dynamic nearest neighbour oracles, Social network analysis, image segmentation, anomaly detection, Dynamic facility location, point insertions and deletions, high-dimensional spaces, High-quality solutions, low running time, minimal recourse, NP-complete problem, metric space


- [Tabular Insights, Visual Impacts: Transferring Expertise from Tables to Images](https://icml.cc/virtual/2024/poster/32795) (Spotlight Poster)
  - **Authors:** [Jun-Peng Jiang](http://openreview.net/profile?id=~Jun-Peng_Jiang2), [Han-Jia Ye](http://openreview.net/profile?id=~Han-Jia_Ye1), [Leye Wang](http://openreview.net/profile?id=~Leye_Wang1), [Yang Yang](http://openreview.net/profile?id=~Yang_Yang17), [Yuan Jiang](http://openreview.net/profile?id=~Yuan_Jiang1), [De-Chuan Zhan](http://openreview.net/profile?id=~De-Chuan_Zhan1)
  - **Affiliations:** School of Artificial Intelligence, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China, School of Artificial Intelligence, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China, Key Lab of High Confidence Software Technologies (Peking University), Ministry of Education & School of Computer Science, Peking University, Beijing, China, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China, School of Artificial Intelligence, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China, School of Artificial Intelligence, Nanjing University, Nanjing, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
  - **TL;DR:** This paper presents CHARMS, a method for transferring knowledge from expert-derived tabular data to enhance image-based predictions, addressing the challenges of aligning diverse data modalities. The results show that CHARMS improves both the performance and interpretability of visual classifiers by effectively utilizing tabular knowledge.
  - **Keywords:** Knowledge transfer, Multimodal learning, Channel Tabular alignment with optimal transport (CHARMS), Image-based predictions, Healthcare, Mapping tabular data to visual contexts, Lack of tabular data during inference, Enhanced performance of image classifiers, Improved interpretability


- [On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box](https://icml.cc/virtual/2024/poster/33782) (Poster)
  - **Authors:** [Yi Cai](http://openreview.net/profile?id=~Yi_Cai6), [Gerhard Wunder](http://openreview.net/profile?email=g.wunder%40fu-berlin.de)
  - **Affiliations:** Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin, Germany, Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin, Germany
  - **TL;DR:** This paper introduces GEEX, a method for generating gradient-like explanations in black-box settings, addressing the limitations of traditional gradient-based approaches. The experimental results show that GEEX outperforms existing black-box methods while maintaining competitive performance against white-box methods.
  - **Keywords:** Explainability, Deep Learning, Gradient-based methods, GEEX (gradient-estimation-based explanation), Image data, Medical image classification, Autonomous driving, Black-box model limitations, Adversarial attacks, Clever-Hans-Effect, Gradient-like explanations, Feature attribution methods


- [Neural operators meet conjugate gradients: The FCG-NO method for efficient PDE solving](https://icml.cc/virtual/2024/poster/34406) (Poster)
  - **Authors:** [Alexander Rudikov](http://openreview.net/profile?id=~Alexander_Rudikov1), [Fanaskov Vladimir](http://openreview.net/profile?id=~Vladimir_Fanaskov2), [Ekaterina Muravleva](http://openreview.net/profile?id=~Ekaterina_Muravleva1), [Yuri Laevsky](http://openreview.net/profile?id=~Yuri_M._Laevsky1), [Ivan Oseledets](http://openreview.net/profile?id=~Ivan_Oseledets1)
  - **Affiliations:** Artificial Intelligence Research Institute; Skolkovo Institute of Science and Technology, Skolkovo Institute of Science and Technology, Skolkovo Institute of Science and Technology; Sberbank PJSC, Institute of Computational Mathematics and Mathematical Geophysics SB RAS, Artificial Intelligence Research Institute; Skolkovo Institute of Science and Technology
  - **TL;DR:** This study proposes a novel method that utilizes neural operators as nonlinear preconditioners for the flexible conjugate gradient method to improve the accuracy of deep learning solvers for partial differential equations. The approach demonstrates that preconditioners learned at lower resolutions can effectively be applied to higher resolution data, enhancing computational efficiency.
  - **Keywords:** deep learning, partial differential equations (PDEs), preconditioners, neural operators, flexible conjugate gradient method (FCG), discretization-invariant methods, elliptic boundary value problems, limited accuracy of deep learning solvers, need for efficient preconditioners, learning efficient preconditioners, convergence guarantees, training on lower resolutions for higher resolutions, Krylov subspace, nonlinear operator, asymptotic complexity O(N)


- [No Double Descent in Principal Component Regression: A High-Dimensional Analysis](https://icml.cc/virtual/2024/poster/34254) (Poster)
  - **Authors:** [Daniel Gedon](http://openreview.net/profile?id=~Daniel_Gedon1), [Antonio Ribeiro](http://openreview.net/profile?id=~Antonio_H._Ribeiro1), [Thomas Schön](http://openreview.net/profile?id=~Thomas_B._Sch%C3%B6n1)
  - **Affiliations:** Department of Information Technology, Uppsala University, Sweden, Department of Information Technology, Uppsala University, Sweden, Department of Information Technology, Uppsala University, Sweden
  - **TL;DR:** This study analyzes Principal Component Regression (PCR) on data sampled from a spiked covariance model to understand its generalization properties in high-dimensional settings. The findings indicate that PCR effectively regularizes the model and mitigates the interpolation peak associated with double descent, with empirical validation through simulations.
  - **Keywords:** Principal Component Regression, Generalization Properties, High-Dimensional Data, Principal Component Analysis, Linear Regression, Spiked Covariance Model, Statistical Research, Econometrics, Genetics, Robotics, Generalization Risk, Model Misspecification, Distribution Shift, Asymptotic Guarantees, Regularization Effect, Diverse MAGIC Wheat Dataset, Double Descent, Overparameterization, Random Matrix Theory


- [Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization](https://icml.cc/virtual/2024/poster/35148) (Poster)
  - **Authors:** [Xingyi Zhao](http://openreview.net/profile?id=~Xingyi_Zhao1), [Depeng Xu](http://openreview.net/profile?id=~Depeng_Xu2), [Shuhan Yuan](http://openreview.net/profile?id=~Shuhan_Yuan2)
  - **Affiliations:** Computer Science Department, Utah State University, Logan UT, USA, Department of Software & Information Systems, Charlotte NC, USA, Computer Science Department, Utah State University, Logan UT, USA
  - **TL;DR:** This study proposes a backdoor mitigation approach called PURE, which utilizes head pruning and attention normalization to defend against backdoor attacks on pre-trained language models. The experimental results demonstrate that PURE effectively reduces the attack success rate while maintaining performance on clean texts.
  - **Keywords:** backdoor attacks, pre-trained language models, natural language processing, head pruning, attention normalization, classification tasks, vulnerability to backdoor attacks, mislabeling of poisoned samples, PURE (backdoor mitigation approach), effectiveness in lowering attack success rate


- [Stochastic positional embeddings improve masked image modeling](https://icml.cc/virtual/2024/poster/33377) (Poster)
  - **Authors:** [Amir Bar](http://openreview.net/profile?id=~Amir_Bar1), [Florian Bordes](http://openreview.net/profile?id=~Florian_Bordes1), [Assaf Shocher](http://openreview.net/profile?id=~Assaf_Shocher1), [Mahmoud Assran](http://openreview.net/profile?id=~Mido_Assran1), [Pascal Vincent](http://openreview.net/profile?id=~Pascal_Vincent1), [Nicolas Ballas](http://openreview.net/profile?id=~Nicolas_Ballas1), [Trevor Darrell](http://openreview.net/profile?id=~Trevor_Darrell2), [Amir Globerson](http://openreview.net/profile?id=~Amir_Globerson1), [Yann LeCun](http://openreview.net/profile?id=~Yann_LeCun1)
  - **Affiliations:** Tel Aviv University; UC Berkeley; Meta AI (FAIR), Meta AI (FAIR), UC Berkeley, Meta AI (FAIR), Meta AI (FAIR), Meta AI (FAIR), UC Berkeley, Tel Aviv University; Google Research, Meta AI (FAIR); New York University
  - **TL;DR:** This study introduces Stochastic Positional Embeddings (StoP) to address location uncertainty in Masked Image Modeling (MIM), enhancing the model's ability to learn robust features. The proposed method significantly improves downstream performance on various tasks, including a notable increase in accuracy on ImageNet.
  - **Keywords:** Masked Image Modeling (MIM), self-supervised learning, Stochastic Positional Embeddings (StoP), Gaussian distribution, Image reconstruction, computer vision, Location uncertainty, overfitting to location features, Improved MIM performance, enhanced feature learning, ImageNet, ViT-B, ViT-H, Masked Auto-Encoders (MAE), I-JEPA


- [Hierarchical Integral Probability Metrics: A distance on random probability measures with low sample complexity](https://icml.cc/virtual/2024/poster/33578) (Poster)
  - **Authors:** [Marta Catalano](http://openreview.net/profile?id=~Marta_Catalano1), [Hugo Lavenant](http://openreview.net/profile?id=~Hugo_Lavenant1)
  - **Affiliations:** Luiss University, Rome, Italy, Bocconi University, Milan, Italy
  - **TL;DR:** This paper introduces a new distance metric for comparing laws of random probabilities that maintains desirable properties of existing metrics while achieving a parametric rate of convergence. The findings highlight the challenges of sample complexity in infinite-dimensional probability spaces and propose solutions for effective numerical estimation.
  - **Keywords:** random probabilities, nonparametric methods, statistics, machine learning, Wasserstein distance, integral probability metrics, distribution-free methods, Bayesian nonparametrics, sample complexity, weak convergence, new distance metric, parametric rate of convergence, Dirichlet process, empirical processes


- [Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States](https://icml.cc/virtual/2024/poster/33824) (Poster)
  - **Authors:** [Noam Razin](http://openreview.net/profile?id=~Noam_Razin1), [Yotam Alexander](http://openreview.net/profile?id=~Yotam_Alexander1), [Edo Cohen-Karlik](http://openreview.net/profile?id=~Edo_Cohen-Karlik1), [Raja Giryes](http://openreview.net/profile?id=~Raja_Giryes1), [Amir Globerson](http://openreview.net/profile?id=~Amir_Globerson1), [Nadav Cohen](http://openreview.net/profile?id=~Nadav_Cohen1)
  - **Affiliations:** Tel Aviv University; Google, Tel Aviv University; Google, Tel Aviv University, Tel Aviv University, Tel Aviv University, Tel Aviv University
  - **TL;DR:** This paper investigates the implicit bias of policy gradient methods in optimal control, particularly focusing on how well learned controllers can extrapolate to unseen initial states. The findings suggest that the degree of exploration from training initial states significantly affects this extrapolation capability, with implications for improving real-world optimal control through better initial state selection.
  - **Keywords:** implicit bias, policy gradient, optimal control, reinforcement learning, gradient descent, Linear Quadratic Regulator (LQR), extrapolation to unseen initial states, generalization from training data, informed selection of initial states for training


- [All-in-one simulation-based inference](https://icml.cc/virtual/2024/poster/34630) (Oral)
  - **Authors:** [Manuel Gloeckler](http://openreview.net/profile?id=~Manuel_Gloeckler1), [Michael Deistler](http://openreview.net/profile?id=~Michael_Deistler1), [Christian Weilbach](http://openreview.net/profile?id=~Christian_Dietrich_Weilbach1), [Frank Wood](http://openreview.net/profile?id=~Frank_Wood2), [Jakob Macke](http://openreview.net/profile?id=~Jakob_H._Macke1)
  - **Affiliations:** Machine Learning in Science, University of Tübingen; Tübingen AI Center, Tübingen, Germany, Machine Learning in Science, University of Tübingen; Tübingen AI Center, Tübingen, Germany, Department of Computer Science, University of British Columbia, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada, Machine Learning in Science, University of Tübingen; Tübingen AI Center, Tübingen, Germany; Max Planck Institute for Intelligent Systems, Department Empirical Inference, Tübingen, Germany
  - **TL;DR:** The study introduces the Simformer, a new amortized inference method that utilizes probabilistic diffusion models with transformer architectures to enhance flexibility and performance in Bayesian inference tasks. It addresses limitations of existing methods by enabling inference with unstructured data and sampling various conditionals of the joint distribution, thus expanding the applicability of simulation-based models in diverse scientific fields.
  - **Keywords:** Amortized Bayesian inference, simulation-based inference, Probabilistic diffusion model, transformer architectures, Ecology, epidemiology, neuroscience, Simulation-hungry methods, inflexible inference tasks, unobservable parameters, Simformer, flexible inference method, sampling conditionals of joint distribution


- [Rethinking the Flat Minima Searching in Federated Learning](https://icml.cc/virtual/2024/poster/34919) (Poster)
  - **Authors:** [Taehwan Lee](http://openreview.net/profile?id=~Taehwan_Lee2), [Sung Whan Yoon](http://openreview.net/profile?id=~Sung_Whan_Yoon1)
  - **Affiliations:** Graduate School of Artificial Intelligence, Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea, Graduate School of Artificial Intelligence, Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea; Department of Electrical Engineering, UNIST, Ulsan, South Korea
  - **TL;DR:** This paper addresses the challenge of generalization in Federated Learning (FL) by analyzing the flatness discrepancy between local and global models. The authors propose a new method, FedGF, which aims to achieve flatter global minima, resulting in improved performance in heterogeneous FL scenarios.
  - **Keywords:** Federated Learning, Generalization, Sharpness-Aware Minimization (SAM), Optimization methods, Decentralized training, Heterogeneous settings, Heterogeneity across clients, Flatness discrepancy, Federated Learning for Global Flatness (FedGF), Performance gains


- [Deconstructing the Goldilocks Zone of Neural Network Initialization](https://icml.cc/virtual/2024/poster/34634) (Poster)
  - **Authors:** [Artem Vysogorets](http://openreview.net/profile?id=~Artem_M_Vysogorets1), [Anna Dawid](http://openreview.net/profile?id=~Anna_Dawid1), [Julia Kempe](http://openreview.net/profile?id=~Julia_Kempe1)
  - **Affiliations:** Center for Data Science, New York University; Courant Institute, New York University, Center for Computational Quantum Physics, Flatiron Institute, Center for Data Science, New York University; Courant Institute, New York University
  - **TL;DR:** This study rigorously analyzes the "Goldilocks zone" of neural network initialization, revealing that excessive positive curvature and local convexity of the loss Hessian are crucial for trainability. The findings suggest that strong model performance does not always align with the Goldilocks zone, indicating a need for further research into this relationship.
  - **Keywords:** Goldilocks zone, neural network initialization, optimization dynamics, Hessian analysis, fully-connected networks, convolutional networks, Deep learning, Excess positive curvature, local convexity, vanishing gradients, Analysis of initialization norms, relationship between curvature and model performance


- [Performance Bounds for Active Binary Testing with Information Maximization](https://icml.cc/virtual/2024/poster/32669) (Poster)
  - **Authors:** [Aditya Chattopadhyay](http://openreview.net/profile?id=~Aditya_Chattopadhyay1), [Benjamin Haeffele](http://openreview.net/profile?id=~Benjamin_David_Haeffele1), [Rene Vidal](http://openreview.net/profile?id=~Rene_Vidal1), [Donald Geman](http://openreview.net/profile?id=~Donald_Geman2)
  - **Affiliations:** Johns Hopkins University, USA, Johns Hopkins University, USA, University of Pennsylvania, USA, Johns Hopkins University, USA
  - **TL;DR:** This paper establishes tight performance bounds for the Information Maximization (InfoMax) algorithm used in active binary testing, demonstrating its near-optimal efficiency in predicting a target variable with a limited number of tests. The findings highlight the algorithm's effectiveness in practical applications such as experimental design and medical diagnosis.
  - **Keywords:** Active testing, Information Maximization, Greedy heuristic, InfoMax algorithm, Experimental design, Group testing, Medical diagnosis, Minimizing expected number of tests, Predicting a target variable, Performance bounds, Near-optimal performance guarantees, Entropy, Mutual information


- [Provable Benefits of Local Steps in Heterogeneous Federated Learning for Neural Networks: A Feature Learning Perspective](https://icml.cc/virtual/2024/poster/32676) (Poster)
  - **Authors:** [Yajie Bao](http://openreview.net/profile?id=~Yajie_Bao2), [Michael Crawshaw](http://openreview.net/profile?id=~Michael_Crawshaw1), [Mingrui Liu](http://openreview.net/profile?id=~Mingrui_Liu2)
  - **Affiliations:** School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China, Department of Computer Science, George Mason University, Fairfax, VA 22030, USA, Department of Computer Science, George Mason University, Fairfax, VA 22030, USA; School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This study investigates the benefits of local steps in heterogeneous federated learning, demonstrating that local updates enable convolutional neural networks to learn more distinguishing features and improve generalization performance compared to global updates. The findings suggest that local steps significantly enhance test accuracy in real-world data scenarios.
  - **Keywords:** Federated Learning, Generalization Performance, Local SGD, Mini-batch SGD, Gradient Descent (GD), One-shot Model Averaging, Deep Neural Networks, Data Heterogeneity, Communication Costs, Feature Learning, Improved Test Accuracy, CIFAR-10, Convolutional Neural Networks (CNN)


- [Prompt-based Visual Alignment for Zero-shot Policy Transfer](https://icml.cc/virtual/2024/poster/34124) (Poster)
  - **Authors:** [Haihan Gao](http://openreview.net/profile?id=~Haihan_Gao1), [Rui Zhang](http://openreview.net/profile?id=~Rui_Zhang1), [Qi Yi](http://openreview.net/profile?id=~Qi_Yi1), [Hantao Yao](http://openreview.net/profile?id=~Hantao_Yao2), [Haochen Li](http://openreview.net/profile?id=~Haochen_Li2), [Jiaming Guo](http://openreview.net/profile?id=~Jiaming_Guo2), [Shaohui Peng](http://openreview.net/profile?id=~Shaohui_Peng2), [Yunkai Gao](http://openreview.net/profile?id=~Yunkai_Gao1), [QiCheng Wang](http://openreview.net/profile?id=~QiCheng_Wang2), [Xing Hu](http://openreview.net/profile?id=~Xing_Hu3), [Yuanbo Wen](http://openreview.net/profile?id=~Yuanbo_Wen1), [Zihao Zhang](http://openreview.net/profile?id=~Zihao_Zhang4), [Zidong Du](http://openreview.net/profile?id=~Zidong_Du1), [Ling Li](http://openreview.net/profile?id=~Ling_Li6), [Qi Guo](http://openreview.net/profile?id=~Qi_Guo4), [Yunji Chen](http://openreview.net/profile?id=~Yunji_Chen1)
  - **Affiliations:** University of Science and Technology of China; SKL of Processors, Institute of Computing Technology, CAS, SKL of Processors, Institute of Computing Technology, CAS, University of Science and Technology of China, Institute of Automation, Chinese Academy of Sciences, Institute of Software, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China, SKL of Processors, Institute of Computing Technology, CAS, Institute of Software, Chinese Academy of Sciences, University of Science and Technology of China; SKL of Processors, Institute of Computing Technology, CAS, SKL of Processors, Institute of Computing Technology, CAS; University of Chinese Academy of Sciences, China, SKL of Processors, Institute of Computing Technology, CAS; Shanghai Innovation Center for Processor Technologies, SKL of Processors, Institute of Computing Technology, CAS, SKL of Processors, Institute of Computing Technology, CAS, SKL of Processors, Institute of Computing Technology, CAS; Shanghai Innovation Center for Processor Technologies, Institute of Software, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China, SKL of Processors, Institute of Computing Technology, CAS, SKL of Processors, Institute of Computing Technology, CAS; University of Chinese Academy of Sciences, China
  - **TL;DR:** This study introduces Prompt-based Visual Alignment (PVA) to address overfitting in reinforcement learning by providing a framework for zero-shot policy transfer that mitigates domain bias. The proposed method demonstrates strong generalization capabilities in unseen domains, validated through experiments in a vision-based autonomous driving task using the CARLA simulator.
  - **Keywords:** Reinforcement Learning, Zero-shot Policy Transfer, Prompt-based Visual Alignment, Visual-Language Model, Prompt Tuning, Autonomous Driving, Overfitting, Domain Bias, Generalization Performance, Unified Cross-domain Representation, Good Generalization Ability, CARLA Simulator


- [Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping](https://icml.cc/virtual/2024/poster/32867) (Poster)
  - **Authors:** [Ben Lonnqvist](http://openreview.net/profile?id=~Ben_Lonnqvist1), [Zhengqing Wu](http://openreview.net/profile?id=~Zhengqing_Wu1), [Michael Herzog](http://openreview.net/profile?id=~Michael_Herzog1)
  - **Affiliations:** Ecole Polytechnique Fédérale de Lausanne (EPFL), Ecole Polytechnique Fédérale de Lausanne (EPFL), Ecole Polytechnique Fédérale de Lausanne (EPFL)
  - **TL;DR:** This study proposes that neural noise can facilitate unsupervised perceptual grouping and segmentation in images, demonstrating that adding noise to DNNs allows for effective segmentation without prior training on labels. The findings align with human perceptual phenomena and show a significant performance improvement on newly introduced datasets.
  - **Keywords:** unsupervised perceptual grouping, segmentation, neural noise, Deep Neural Networks (DNNs), mathematical demonstration, image segmentation, human perception, robustness in object recognition, challenges in segmentation, novel unsupervised segmentation method, improved performance on Good Gestalt datasets, Good Gestalt (GG) datasets, Gestalt principles, illusory contours, closure, continuity, proximity, occlusion


- [Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs](https://icml.cc/virtual/2024/poster/33159) (Poster)
  - **Authors:** [Shenzhi Yang](http://openreview.net/profile?id=~Shenzhi_Yang1), [Bin Liang](http://openreview.net/profile?id=~Bin_Liang6), [An Liu](http://openreview.net/profile?id=~An_Liu1), [Lin Gui](http://openreview.net/profile?id=~Lin_Gui3), [Xingkai Yao](http://openreview.net/profile?id=~Xingkai_Yao1), [Xiaofang Zhang](http://openreview.net/profile?id=~Xiaofang_Zhang1)
  - **Affiliations:** School of Computer Science and Technology, Soochow University, Suzhou, China, Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, China, School of Computer Science and Technology, Soochow University, Suzhou, China, Department of Informatics, King’s College London, London, UK, School of Computer Science and Technology, Soochow University, Suzhou, China, School of Computer Science and Technology, Soochow University, Suzhou, China
  - **TL;DR:** This study introduces NODESAFE, a framework designed to enhance the detection of out-of-distribution data in graph neural networks by addressing the issues of extreme score generation and logit shifts. Experimental results demonstrate significant improvements in detection accuracy, particularly in scenarios involving structure manipulation.
  - **Keywords:** Graph Neural Networks (GNNs), Out-of-Distribution (OOD) Detection, Negative Energy Scores, Score Aggregation, Optimization Terms, Knowledge Graphs, Social Networks, Chemical Analysis, Detection of OOD data, Extreme Score Generation, Logit Shift, NODESAFE framework, Improved OOD detection metrics (FPR95), Structure Manipulation


- [Conditional Language Learning with Context](https://icml.cc/virtual/2024/poster/33299) (Poster)
  - **Authors:** [Xiao Zhang](http://openreview.net/profile?id=~Xiao_Zhang9), [Miao Li](http://openreview.net/profile?email=miao-li%40tsinghua.edu.cn), [Ji Wu](http://openreview.net/profile?id=~Ji_Wu3)
  - **Affiliations:** Department of Electronics Engineering, Tsinghua University, Department of Electronics Engineering, Tsinghua University, Department of Electronics Engineering, Tsinghua University; College of AI, Tsinghua University
  - **TL;DR:** This paper introduces conditional finetuning, a method that enhances causal language modeling by incorporating context to achieve selective learning from domain-specific corpora. The approach helps mitigate issues like forgetting and biases, potentially benefiting lifelong learning in language models.
  - **Keywords:** Conditional finetuning, language modeling, selective learning, Causal language modeling, Domain-specific corpora, lifelong learning, Forgetting of existing knowledge, over-adaptation to corpus statistics, topic biases, Selective learning from corpus, improved stability-plasticity tradeoff


- [Optimization without Retraction on the Random Generalized Stiefel Manifold](https://icml.cc/virtual/2024/poster/34087) (Poster)
  - **Authors:** [Simon Vary](http://openreview.net/profile?id=~Simon_Vary1), [Pierre Ablin](http://openreview.net/profile?id=~Pierre_Ablin2), [Bin Gao](http://openreview.net/profile?id=~Bin_Gao6), [P.-A. Absil](http://openreview.net/profile?id=~Pierre-Antoine_Absil1)
  - **Affiliations:** ICTEAM Institute, UCLouvain, Louvain-la-neuve, Belgium; Department of Statistics, University of Oxford, Oxford, United Kingdom, Apple Machine Learning Group, Paris, France, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China, ICTEAM Institute, UCLouvain, Louvain-la-neuve, Belgium
  - **TL;DR:** This paper presents a stochastic iterative method for optimizing matrices on the generalized Stiefel manifold, which converges to critical points without enforcing constraints in every iteration. The proposed method is computationally efficient and applicable to various machine learning problems involving generalized orthogonality constraints.
  - **Keywords:** optimization, generalized Stiefel manifold, stochastic iterative method, Riemannian optimization, canonical correlation analysis (CCA), independent component analysis (ICA), generalized eigenvalue problem (GEVP), optimization under constraints, stochastic feasible set, convergence to critical points, lower per-iteration cost


- [AutoOS: Make Your OS More Powerful by Exploiting Large Language Models](https://icml.cc/virtual/2024/poster/34039) (Poster)
  - **Authors:** [Huilai Chen](http://openreview.net/profile?id=~Huilai_Chen1), [Yuanbo Wen](http://openreview.net/profile?id=~Yuanbo_Wen1), [Limin Cheng](http://openreview.net/profile?id=~Limin_Cheng1), [Shouxu Kuang](http://openreview.net/profile?id=~Shouxu_Kuang1), [Yumeng Liu](http://openreview.net/profile?id=~Yumeng_Liu3), [Weijia Li](http://openreview.net/profile?id=~Weijia_Li4), [Ling Li](http://openreview.net/profile?id=~Ling_Li6), [Rui Zhang](http://openreview.net/profile?id=~Rui_Zhang1), [Xinkai Song](http://openreview.net/profile?id=~Xinkai_Song1), [Wei Li](http://openreview.net/profile?id=~Wei_Li96), [Qi Guo](http://openreview.net/profile?id=~Qi_Guo4), [Yunji Chen](http://openreview.net/profile?id=~Yunji_Chen1)
  - **Affiliations:** State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China, State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, Intelligent Software Research Center, Institute of Software, CAS, Beijing, China, University of Chinese Academy of Sciences, Beijing, China; Intelligent Software Research Center, Institute of Software, CAS, Beijing, China, Intelligent Software Research Center, Institute of Software, CAS, Beijing, China, University of Chinese Academy of Sciences, Beijing, China; Intelligent Software Research Center, Institute of Software, CAS, Beijing, China, Intelligent Software Research Center, Institute of Software, CAS, Beijing, China, State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
  - **TL;DR:** The study introduces AutoOS, a framework that utilizes Large Language Models to automatically customize and optimize operating system kernel configurations for AIoT applications. Experimental results demonstrate that AutoOS can enhance performance by up to 25% compared to vendor-provided configurations, addressing the complexities and challenges of kernel optimization.
  - **Keywords:** AIoT, Operating System Optimization, Large Language Models, State Machine-based Traversal Algorithm, Embedded Systems, AIoT Applications, Complexity of Kernel Configuration, Performance Evaluation Costs, Error-prone Optimization Process, AutoOS Framework, Optimization of OS Kernel Configurations, Linux Kernel, Configuration Options


- [Gradient-based Visual Explanation for Transformer-based CLIP](https://icml.cc/virtual/2024/poster/33867) (Poster)
  - **Authors:** [Chenyang ZHAO](http://openreview.net/profile?id=~Chenyang_ZHAO3), [Kun Wang](http://openreview.net/profile?id=~Kun_Wang8), [Xingyu Zeng](http://openreview.net/profile?id=~Xingyu_Zeng1), [Rui Zhao](http://openreview.net/profile?id=~Rui_Zhao6), [Antoni Chan](http://openreview.net/profile?id=~Antoni_B._Chan1)
  - **Affiliations:** Department of Computer Science, City University of Hong Kong, Hong Kong; SenseTime Group Ltd, SenseTime Group Ltd, SenseTime Group Ltd, SenseTime Group Ltd, Department of Computer Science, City University of Hong Kong, Hong Kong
  - **TL;DR:** This paper introduces Grad-ECLIP, a gradient-based visual explanation method for the CLIP model, which effectively interprets image-text matching results by producing heat maps that highlight the influence of specific image regions and words. The method outperforms existing techniques in providing high-quality visual explanations and offers insights into the workings of CLIP.
  - **Keywords:** Explainability, Vision-Language Models, Gradient-based Visual Explanation, CLIP, Transformer, Image-Text Matching, Multimodal Learning, Interpretation of CLIP, Attribution Identification, Grad-ECLIP, Heat Maps for Image Regions and Words, CLIP (Contrastive Language-Image Pre-training), Self-Attention


- [Forget Sharpness: Perturbed Forgetting of Model Biases Within SAM Dynamics](https://icml.cc/virtual/2024/poster/33595) (Poster)
  - **Authors:** [Ankit Vani](http://openreview.net/profile?id=~Ankit_Vani1), [Frederick Tung](http://openreview.net/profile?id=~Frederick_Tung1), [Gabriel Oliveira](http://openreview.net/profile?id=~Gabriel_L._Oliveira1), [Hossein Sharifi-Noghabi](http://openreview.net/profile?id=~Hossein_Sharifi-Noghabi1)
  - **Affiliations:** Mila, Université de Montréal; Borealis AI, Borealis AI, Borealis AI, Borealis AI
  - **TL;DR:** This study proposes a new perspective on sharpness-aware minimization (SAM) by introducing the concept of "perturbed forgetting," which helps discard undesirable model biases and improve generalization. The findings indicate that this approach can outperform standard SAM methods while sometimes converging to sharper regions of the loss surface.
  - **Keywords:** sharpness-aware minimization, generalization, model biases, SAM (Sharpness-Aware Minimization), perturbations, gradient descent, image recognition, robustness benchmarks, model overfitting, undesirable model biases, spurious correlations, perturbed forgetting, output bias forgetting perturbations, improved generalization, ImageNet, CIFAR-10, CIFAR-100, information bottleneck principle


- [Towards a Better Theoretical Understanding of Independent Subnetwork Training](https://icml.cc/virtual/2024/poster/33820) (Poster)
  - **Authors:** [Egor Shulgin](http://openreview.net/profile?id=~Egor_Shulgin1), [Peter Richtarik](http://openreview.net/profile?id=~Peter_Richt%C3%A1rik1)
  - **Affiliations:** King Abdullah University of Science and Technology, Thuwal, Saudi Arabia, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia
  - **TL;DR:** This paper explores Independent Subnetwork Training (IST) as a solution to the challenges of scaling large neural network models in distributed computing environments. It highlights the advantages of IST over traditional data parallelism and provides a theoretical analysis of its optimization performance.
  - **Keywords:** Independent Subnetwork Training, model parallelism, data-parallel distributed computing, communication bottleneck, memory constraints, scaling model size, optimization performance analysis


- [Relaxed Quantile Regression: Prediction Intervals for Asymmetric Noise](https://icml.cc/virtual/2024/poster/34312) (Poster)
  - **Authors:** [Thomas Pouplin](http://openreview.net/profile?id=~Thomas_Pouplin1), [Alan Jeffares](http://openreview.net/profile?id=~Alan_Jeffares1), [Nabeel Seedat](http://openreview.net/profile?id=~Nabeel_Seedat1), [M van der Schaar](http://openreview.net/profile?id=~Mihaela_van_der_Schaar2)
  - **Affiliations:** Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, UK
  - **TL;DR:** This paper introduces Relaxed Quantile Regression (RQR) as an alternative to traditional quantile regression for constructing prediction intervals, addressing the limitations of symmetric intervals in skewed distributions. The proposed method enhances interval quality while ensuring coverage guarantees, making it suitable for high-stakes applications.
  - **Keywords:** uncertainty quantification, prediction intervals, regression, quantile regression, Relaxed Quantile Regression (RQR), medical decision-making, autonomous driving, energy forecasting, asymmetric noise, skewed distributions, coverage guarantees, improved interval width, flexible interval construction


- [Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations](https://icml.cc/virtual/2024/poster/33591) (Poster)
  - **Authors:** [Stefan Sylvius Wagner Martinez](http://openreview.net/profile?id=~Stefan_Sylvius_Wagner1), [Stefan Harmeling](http://openreview.net/profile?id=~Stefan_Harmeling1)
  - **Affiliations:** Department of Computer Science, Heinrich Heine University Düsseldorf, Germany, Department of Computer Science, Technical University Dortmund, Germany
  - **TL;DR:** This study presents a method for exploration in reinforcement learning that utilizes clustering of representations to estimate state density in 3-D environments. The findings indicate that both random and pre-trained representations can effectively count states, with pre-trained DINO representations showing superior performance in visually complex scenarios.
  - **Keywords:** exploration in reinforcement learning, density estimation, clustering, pre-trained representations, pseudo-counts, 3-D environments, VizDoom, Habitat, data sparsity, high-dimensional data, effective state counting, integration of pre-trained biases, DINO representations, Random Network Distillation, BYOL-Explore


- [Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss](https://icml.cc/virtual/2024/poster/34425) (Poster)
  - **Authors:** [Yahong Yang](http://openreview.net/profile?id=~Yahong_Yang1), [Juncai He](http://openreview.net/profile?id=~Juncai_He1)
  - **Affiliations:** Department of Mathematics, The Pennsylvania State University, University Park, State College, PA 16802, USA, Computer, Electrical and Mathematical Science and Engineering Division, The King Abdullah University of Science and Technology, Thuwal 23955, Saudi Arabia
  - **TL;DR:** This paper investigates the optimal generalization error of deeper neural networks (DeNNs) versus wider neural networks (WeNNs) in the context of Sobolev losses, revealing that the choice of architecture is influenced by factors such as the number of sample points and parameters. The findings suggest that DeNNs are preferable with more sample points and regular loss functions, while WeNNs are favored with a higher number of parameters.
  - **Keywords:** neural networks, architecture comparison, generalization error, Sobolev training, deep Ritz method, physics-informed neural networks (PINNs), partial differential equations, model architecture design, approximation capability, optimal generalization error analysis, deeper neural networks (DeNNs), wider neural networks (WeNNs), ReLU activation function


- [Getting the most out of your tokenizer for pre-training and domain adaptation](https://icml.cc/virtual/2024/poster/33743) (Poster)
  - **Authors:** [Gautier Dagan](http://openreview.net/profile?id=~Gautier_Dagan1), [Gabriel Synnaeve](http://openreview.net/profile?id=~Gabriel_Synnaeve1), [Baptiste Roziere](http://openreview.net/profile?id=~Baptiste_Roziere1)
  - **Affiliations:** University of Edinburgh, Edinburgh, UK, Meta AI, Paris, France, Meta AI, Paris, France
  - **TL;DR:** This paper investigates the impact of tokenizer design on the performance of large language models, demonstrating that optimizing tokenization can significantly enhance generation speed and effective context size. The authors provide recommendations for tokenizer hyper-parameters and highlight the importance of adapting tokenizers for specific tasks.
  - **Keywords:** Tokenization, Large Language Models (LLMs), Byte-Pair Encoding (BPE), Unigram algorithm, Code generation, Inefficiency in tokenizer usage, sub-optimal tokenization for specific tasks, Recommendations for tokenizer hyper-parameters, improvements in generation speed and effective context size, HumanEval, MBPP


- [One for All: A Universal Generator for Concept Unlearnability via Multi-Modal Alignment](https://icml.cc/virtual/2024/poster/32780) (Poster)
  - **Authors:** [Chaochao Chen](http://openreview.net/profile?id=~Chaochao_Chen3), [Jiaming Zhang](http://openreview.net/profile?id=~Jiaming_Zhang7), [Yuyuan Li](http://openreview.net/profile?id=~Yuyuan_Li1), [Zhongxuan Han](http://openreview.net/profile?id=~Zhongxuan_Han1)
  - **Affiliations:** Zhongxuan Han; College of Computer Science and Technology, Zhejiang University, Hangzhou, China, Zhongxuan Han; College of Computer Science and Technology, Zhejiang University, Hangzhou, China, School of Communication Engineering, Hangzhou Dianzi University, Hangzhou, China, Zhongxuan Han; College of Computer Science and Technology, Zhejiang University, Hangzhou, China
  - **TL;DR:** This study proposes a universal perturbation generator that enhances concept unlearnability by leveraging multi-modal pre-trained models, addressing privacy risks associated with data usage without consent. The findings demonstrate the generator's effectiveness in achieving cross-dataset transferability and label-agnostic utility, making data unlearnable for unauthorized models.
  - **Keywords:** Concept unlearnability, Privacy protection, Multi-modal pre-trained models, Concept-wise discriminant loss, Data privacy, Machine learning, Privacy risks, Data utilization without consent, Universal perturbation generator, Cross-dataset transferability, Label-agnostic utility, Unlearnable examples, Perturbations


- [DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep Learning](https://icml.cc/virtual/2024/poster/33358) (Poster)
  - **Authors:** [Ashwin Hebbar](http://openreview.net/profile?id=~S_Ashwin_Hebbar1), [Sravan Kumar Ankireddy](http://openreview.net/profile?id=~Sravan_Kumar_Ankireddy1), [Hyeji Kim](http://openreview.net/profile?id=~Hyeji_Kim1), [Sewoong Oh](http://openreview.net/profile?id=~Sewoong_Oh3), [Pramod Viswanath](http://openreview.net/profile?id=~Pramod_Viswanath2)
  - **Affiliations:** Princeton University, University of Texas at Austin, University of Texas at Austin, University of Washington, Princeton University
  - **TL;DR:** This study introduces DEEPPOLAR codes, a novel non-linear generalization of Polar codes that utilizes larger kernel sizes and neural networks to enhance reliability in error correction coding. The findings demonstrate improved performance over existing neural codes and conventional Polar codes, addressing limitations in finite-length performance.
  - **Keywords:** Polar codes, channel coding, error correction, Non-linear generalization, neural networks, larger kernel size, Digital communication, wireless communication, 5G standards, Finite-length performance limitations, reliability in noisy channels, DEEPPOLAR codes, enhanced reliability, improved decoding performance, Polarization kernel, Kronecker products, channel polarization, successive cancellation (SC) decoder


- [S$\Omega$I: Score-based O-INFORMATION Estimation](https://icml.cc/virtual/2024/poster/34265) (Oral)
  - **Authors:** [Mustapha BOUNOUA](http://openreview.net/profile?id=~Mustapha_BOUNOUA1), [Giulio Franzese](http://openreview.net/profile?id=~Giulio_Franzese1), [Pietro Michiardi](http://openreview.net/profile?id=~Pietro_Michiardi1)
  - **Affiliations:** Ampere Software Technology, France; Department of Data Science, Eurecom, France, Department of Data Science, Eurecom, France, Department of Data Science, Eurecom, France
  - **TL;DR:** This paper introduces SΩI, a method for computing O-INFORMATION that addresses the limitations of classical mutual information in analyzing multivariate systems. The effectiveness of SΩI is validated through experiments on synthetic data and a real-world use case.
  - **Keywords:** Information theory, Multivariate systems, Synergy and redundancy, O-INFORMATION, Partial Information Decomposition (PID), Neuroscience, Climate models, Economics, Machine learning, Limitations of mutual information in multivariate analysis, Computational complexity of PID, SΩI method for computing O-INFORMATION


- [SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks](https://icml.cc/virtual/2024/poster/33447) (Poster)
  - **Authors:** [Jiwon Song](http://openreview.net/profile?id=~Jiwon_Song1), [Kyungseok Oh](http://openreview.net/profile?id=~Kyungseok_Oh1), [Taesu Kim](http://openreview.net/profile?id=~Taesu_Kim1), [Hyungjun Kim](http://openreview.net/profile?id=~Hyungjun_Kim2), [Yulhwa Kim](http://openreview.net/profile?id=~Yulhwa_Kim1), [jae-joon kim](http://openreview.net/profile?id=~jae-joon_kim1)
  - **Affiliations:** Seoul National University, Seoul National University, SqueezeBits Inc., SqueezeBits Inc., Sungkyunkwan University, Seoul National University
  - **TL;DR:** This paper introduces SLEB, a novel approach for streamlining large language models by eliminating redundant transformer blocks, which significantly enhances inference speed while maintaining accuracy. The results indicate that SLEB outperforms existing pruning methods, addressing the challenges of deploying LLMs in real-world applications.
  - **Keywords:** Large Language Models (LLMs), Pruning Techniques, Transformer Blocks, Redundancy Verification, Natural Language Processing (NLP), Chatbots, Question-Answering Systems, High Parameter Count, Memory Consumption, Computational Demands, SLEB Method, Enhanced Processing Speed, Improved Efficiency


- [Towards a Self-contained Data-driven Global Weather Forecasting Framework](https://icml.cc/virtual/2024/poster/33791) (Poster)
  - **Authors:** [Yi Xiao](http://openreview.net/profile?id=~Yi_Xiao4), [LEI BAI](http://openreview.net/profile?id=~LEI_BAI1), [Wei Xue](http://openreview.net/profile?id=~Wei_Xue1), [Hao Chen](http://openreview.net/profile?id=~Hao_Chen14), [Kun Chen](http://openreview.net/profile?id=~Kun_Chen5), [kang chen](http://openreview.net/profile?id=~kang_chen3), [Tao Han](http://openreview.net/profile?id=~Tao_Han4), [Wanli Ouyang](http://openreview.net/profile?id=~Wanli_Ouyang1)
  - **Affiliations:** Department of Computer Science and Technology, Tsinghua University, Beijing, China, Shanghai Artificial Intelligence Laboratory, Shanghai, China, Department of Computer Science and Technology, Tsinghua University, Beijing, China; Qinghai University and Intelligent Computing and Application Laboratory of Qinghai Province, China, Shanghai Artificial Intelligence Laboratory, Shanghai, China, Shanghai Artificial Intelligence Laboratory, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China, Shanghai Artificial Intelligence Laboratory, Shanghai, China, Shanghai Artificial Intelligence Laboratory, Shanghai, China, Shanghai Artificial Intelligence Laboratory, Shanghai, China
  - **TL;DR:** This study presents FengWu-4DVar, a self-contained data-driven global weather forecasting framework that integrates the AI forecasting model FengWu with the 4DVar data assimilation algorithm. The framework demonstrates significant computational efficiency, achieving accurate weather forecasts for an entire year while being approximately 100 times faster than traditional 4DVar methods.
  - **Keywords:** Data-driven weather forecasting, AI forecasting models, Four-dimensional variational assimilation (4DVar), AI-embedded 4DVar algorithm, spherical-harmonic-transform-based approximation, auto-differentiation scheme, Global weather forecasting, numerical weather prediction, Computational expense of traditional methods, inconsistency with AI models, high computational complexity, FengWu-4DVar framework, accurate analysis fields, stable global weather forecasts, significant computational efficiency, ERA5 simulated observational data


- [Towards Causal Foundation Model: on Duality between Optimal Balancing and Attention](https://icml.cc/virtual/2024/poster/33599) (Poster)
  - **Authors:** [Jiaqi Zhang](http://openreview.net/profile?id=~Jiaqi_Zhang2), [Joel Jennings](http://openreview.net/profile?id=~Joel_Jennings1), [Agrin Hilmkil](http://openreview.net/profile?id=~Agrin_Hilmkil1), [Nick Pawlowski](http://openreview.net/profile?id=~Nick_Pawlowski2), [Cheng Zhang](http://openreview.net/profile?id=~Cheng_Zhang1), [Chao Ma](http://openreview.net/profile?id=~Chao_Ma2)
  - **Affiliations:** Massachusetts Institute of Technology, Google DeepMind; Work done while at Microsoft, Microsoft Research Cambridge, Microsoft Research Cambridge, Work done while at Microsoft, Microsoft Research Cambridge
  - **TL;DR:** This study introduces Causal Inference with Attention (CInA), a novel method for self-supervised causal learning that enables zero-shot causal inference for treatment effect estimations. The method demonstrates effective generalization to out-of-distribution datasets, suggesting its potential as a foundation for developing causally-aware models.
  - **Keywords:** causal inference, foundation models, treatment effect estimation, Causal Inference with Attention (CInA), self-supervised causal learning, transformer-type architecture, healthcare, economics, statistics, intricate reasoning, high numerical precision, out-of-distribution generalization, zero-shot causal inference, optimal covariate balancing


- [A Fixed-Point Approach for Causal Generative Modeling](https://icml.cc/virtual/2024/poster/34365) (Poster)
  - **Authors:** [Meyer Scetbon](http://openreview.net/profile?id=~Meyer_Scetbon1), [Joel Jennings](http://openreview.net/profile?id=~Joel_Jennings1), [Agrin Hilmkil](http://openreview.net/profile?id=~Agrin_Hilmkil1), [Cheng Zhang](http://openreview.net/profile?id=~Cheng_Zhang1), [Chao Ma](http://openreview.net/profile?id=~Chao_Ma2)
  - **Affiliations:** Microsoft Research, Microsoft Research; Google DeepMind, Microsoft Research, Microsoft Research; Google DeepMind, Microsoft Research
  - **TL;DR:** This study introduces a novel framework for learning Structural Causal Models (SCMs) as fixed-point problems without the need for Directed Acyclic Graphs (DAGs). The proposed two-stage causal generative model effectively infers topological order from observations and learns the generative SCM, outperforming existing methods on out-of-distribution problems.
  - **Keywords:** Causal Generative Modeling, Structural Causal Models (SCMs), Fixed-point problems, Topological ordering (TO), Transformer-based architecture, Attention mechanism, Economics, Biology, Genetics, Healthcare, Recovery of SCMs, DAG learning, NP-hard complexity, Ill-posed problems, New framework for learning SCMs, Zero-shot TO inference method, Directed Acyclic Graphs (DAGs), Autoregressive flows, Triangular monotonic increasing (TMI) maps


- [Scaling Laws for Fine-Grained Mixture of Experts](https://icml.cc/virtual/2024/poster/32651) (Poster)
  - **Authors:** [Jan Ludziejewski](http://openreview.net/profile?id=~Jan_Ludziejewski1), [Jakub Krajewski](http://openreview.net/profile?id=~Jakub_Krajewski1), [Kamil Adamczewski](http://openreview.net/profile?id=~Kamil_Adamczewski1), [Maciej Pióro](http://openreview.net/profile?id=~Maciej_Pi%C3%B3ro1), [Michał Krutul](http://openreview.net/profile?id=~Micha%C5%82_Krutul1), [Szymon Antoniak](http://openreview.net/profile?id=~Szymon_Antoniak1), [Kamil Ciebiera](http://openreview.net/profile?id=~Kamil_Ciebiera1), [Krystian Król](http://openreview.net/profile?id=~Krystian_Kr%C3%B3l2), [Tomasz Odrzygóźdź](http://openreview.net/profile?id=~Tomasz_Odrzyg%C3%B3%C5%BAd%C5%BA1), [Piotr Sankowski](http://openreview.net/profile?id=~Piotr_Sankowski1), [Marek Cygan](http://openreview.net/profile?id=~Marek_Cygan1), [Sebastian Jaszczur](http://openreview.net/profile?id=~Sebastian_Jaszczur1)
  - **Affiliations:** IDEAS NCBR; University of Warsaw, IDEAS NCBR; University of Warsaw, IDEAS NCBR, IDEAS NCBR; Polish Academy of Sciences, IDEAS NCBR; University of Warsaw, IDEAS NCBR; University of Warsaw, IDEAS NCBR; University of Warsaw, IDEAS NCBR; University of Warsaw, TradeLink, IDEAS NCBR; University of Warsaw, University of Warsaw; Nomagic, IDEAS NCBR; University of Warsaw
  - **TL;DR:** This study investigates the scaling properties of Mixture of Experts (MoE) models, introducing a new hyperparameter called granularity to optimize expert size and improve efficiency. The findings indicate that with optimal configurations, MoE models can consistently outperform traditional dense models across various computational budgets.
  - **Keywords:** Mixture of Experts, Large Language Models, Scaling laws, granularity adjustment, High computational costs, efficiency of model training, Optimal training configuration, compute savings, MoE models, dense models, Transformer


- [Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection](https://icml.cc/virtual/2024/poster/32772) (Poster)
  - **Authors:** [Nils Palumbo](http://openreview.net/profile?id=~Nils_Palumbo1), [Yang Guo](http://openreview.net/profile?id=~Yang_Guo4), [Xi Wu](http://openreview.net/profile?id=~Xi_Wu1), [Jiefeng Chen](http://openreview.net/profile?id=~Jiefeng_Chen2), [Yingyiu Liang](http://openreview.net/profile?id=~Yingyu_Liang1), [Somesh Jha](http://openreview.net/profile?id=~Somesh_Jha1)
  - **Affiliations:** Depart of Computer Sciences, University of Wisconsin-Madison, Madison, WI, USA, Depart of Computer Sciences, University of Wisconsin-Madison, Madison, WI, USA, Google, Depart of Computer Sciences, University of Wisconsin-Madison, Madison, WI, USA, Depart of Computer Sciences, University of Wisconsin-Madison, Madison, WI, USA, Depart of Computer Sciences, University of Wisconsin-Madison, Madison, WI, USA
  - **TL;DR:** This paper explores the combination of transduction and rejection techniques to enhance adversarial robustness in deep learning models. The authors demonstrate that their novel approach significantly improves robust accuracy against state-of-the-art adversarial attacks, achieving notable results on CIFAR datasets.
  - **Keywords:** Adversarial Robustness, Transduction, Rejection, Transductive algorithms, Reduction techniques, Deep Learning, Adversarial Machine Learning, Adversarial perturbations, Sample complexity, Robust generalization, Improved robust accuracy, Effective defenses, CIFAR-10, CIFAR-100


- [To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models](https://icml.cc/virtual/2024/poster/34529) (Poster)
  - **Authors:** [George-Octavian Bărbulescu](http://openreview.net/profile?id=~George-Octavian_B%C4%83rbulescu1), [Peter Triantafillou](http://openreview.net/profile?id=~Peter_Triantafillou1)
  - **Affiliations:** Department of Computer Science, University of Warwick, Coventry, United Kingdom, None
  - **TL;DR:** This study focuses on improving the unlearning of memorized data in Large Language Models (LLMs) to address privacy and copyright issues. It introduces new methods and metrics for unlearning that consider the degree of memorization, while maintaining model utility across various NLP tasks.
  - **Keywords:** Large Language Models, Unlearning, Privacy, Gradient Ascent, Task Arithmetic, Natural Language Processing, Memorization, Privacy Violations, Copyright Issues, New metric for unlearning quality, New unlearning methods, State-of-the-Art (SOTA) algorithms


- [Position: On the Possibilities of AI-Generated Text Detection](https://icml.cc/virtual/2024/poster/34689) (Poster)
  - **Authors:** [Souradip Chakraborty](http://openreview.net/profile?id=~Souradip_Chakraborty1), [Amrit Singh Bedi](http://openreview.net/profile?id=~Amrit_Bedi1), [Sicheng Zhu](http://openreview.net/profile?id=~Sicheng_Zhu1), [Bang An](http://openreview.net/profile?id=~Bang_An1), [Dinesh Manocha](http://openreview.net/profile?id=~Dinesh_Manocha3), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1)
  - **Affiliations:** University of Maryland, College Park, MD, USA, University of Central Florida, FL, USA, University of Maryland, College Park, MD, USA, University of Maryland, College Park, MD, USA, University of Maryland, College Park, MD, USA, University of Maryland, College Park, MD, USA
  - **TL;DR:** This study demonstrates that it is generally feasible to detect AI-generated text, particularly when sufficient text data is available. The research provides guidelines on the necessary text data quantity for reliable detection and counters pessimistic views on the capabilities of AI text detection.
  - **Keywords:** AI-generated text detection, Large Language Models (LLMs), Information theory, sample complexity bounds, Natural Language Processing (NLP), automated content generation, Distinguishing human-written text from machine-generated text, ethical challenges of LLMs, Guidelines for text data quantity for reliable AI text detection, Xsum, Squad, IMDb, Kaggle FakeNews, GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, Llama-2-70B-Chat-HF, RoBERTa-Large/Base-Detector, GPTZero, Ethical concerns regarding LLMs, misinformation, public trust


- [PIPER: Primitive-Informed Preference-based Hierarchical Reinforcement Learning via Hindsight Relabeling](https://icml.cc/virtual/2024/poster/33225) (Poster)
  - **Authors:** [Utsav Singh](http://openreview.net/profile?id=~Utsav_Singh1), [Wesley A. Suttle](http://openreview.net/profile?id=~Wesley_A_Suttle1), [Brian Sadler](http://openreview.net/profile?id=~Brian_M._Sadler1), [Vinay Namboodiri](http://openreview.net/profile?id=~Vinay_P_Namboodiri1), [Amrit Singh Bedi](http://openreview.net/profile?id=~Amrit_Bedi1)
  - **Affiliations:** CSE dept., IIT Kanpur, Kanpur, India, U.S. Army Research Laboratory, Adelphi, MD, USA, University of Texas, Austin, Texas, USA, Department of Computer Science, University of Bath, Bath, UK, CS dept., University of Central Florida, Orlando, Florida, USA
  - **TL;DR:** This paper introduces PIPER, a novel approach to hierarchical reinforcement learning that utilizes preference-based learning to create a reward model, effectively addressing issues of non-stationarity and infeasible subgoal generation. The method demonstrates over 50% success rates in challenging sparse-reward robotic environments, outperforming existing baselines.
  - **Keywords:** Hierarchical Reinforcement Learning, Preference-based Learning, Hindsight Relabeling, Reward Model, Primitive-Informed Regularization, Robotic Manipulation, Sparse-Reward Tasks, Non-stationarity, Infeasible Subgoal Generation, Sparse Rewards, Improved Sample Efficiency, Higher Success Rates in Sparse-Reward Environments


- [Risk Aware Benchmarking of Large Language Models](https://icml.cc/virtual/2024/poster/34220) (Poster)
  - **Authors:** [Apoorva Nitsure](http://openreview.net/profile?id=~Apoorva_Nitsure1), [Youssef Mroueh](http://openreview.net/profile?id=~Youssef_Mroueh1), [Mattia Rigotti](http://openreview.net/profile?id=~Mattia_Rigotti1), [Kristjan Greenewald](http://openreview.net/profile?id=~Kristjan_Greenewald1), [Brian Belgodere](http://openreview.net/profile?id=~Brian_Belgodere1), [Mikhail Yurochkin](http://openreview.net/profile?id=~Mikhail_Yurochkin1), [Jiri Navratil](http://openreview.net/profile?id=~Jiri_Navratil1), [Igor Melnyk](http://openreview.net/profile?id=~Igor_Melnyk1), [Jarret Ross](http://openreview.net/profile?id=~Jarret_Ross1)
  - **Affiliations:** IBM Research; MIT-IBM Watson AI Lab, IBM Research, IBM Research, IBM Research; MIT-IBM Watson AI Lab, IBM Research, IBM Research; MIT-IBM Watson AI Lab, IBM Research, IBM Research, IBM Research
  - **TL;DR:** This paper presents a distributional framework for benchmarking the socio-technical risks of large language models, utilizing statistical methods to assess and compare model performance based on various risk metrics. The findings emphasize the importance of risk-aware model selection to ensure alignment with human values and mitigate potential harmful outputs.
  - **Keywords:** Risk Aware Benchmarking, Large Language Models, Stochastic Dominance, Statistical Relative Testing, Mean-Risk Models, Foundation Models, Socio-Technical Risk Evaluation, Trustworthiness of Outputs, Alignment with Human Values, Toxic Content Generation, Metrics Portfolio, Model Selection Framework, Foundation Models, Large Language Models (LLMs), Central Limit Theorems


- [Two-timescale Derivative Free Optimization for Performative Prediction with Markovian Data](https://icml.cc/virtual/2024/poster/34757) (Poster)
  - **Authors:** [Haitong LIU](http://openreview.net/profile?id=~Haitong_LIU1), [Qiang Li](http://openreview.net/profile?id=~Qiang_LI7), [Hoi To Wai](http://openreview.net/profile?id=~Hoi_To_Wai1)
  - **Affiliations:** Department of Computer Science, ETH Zurich; None, Department of System Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, China, Department of System Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, China
  - **TL;DR:** This paper presents a two-timescale derivative free optimization algorithm for minimizing expected loss in performative prediction settings, where data distribution is influenced by the prediction model. The proposed method effectively balances updates and bias reduction, requiring a specific number of samples to achieve a near-stationary solution.
  - **Keywords:** performative prediction, decision-dependent data, stochastic derivative free optimization (DFO), two-timescale DFO, strategic classification, economic practices, price promotion mechanisms, ride sharing, non-convex optimization, distributional shift, performative risk, near-stationary solution, sample accumulation mechanism, bias reduction, Markovian data, expected loss


- [Et Tu Certifications: Robustness Certificates Yield Better Adversarial Examples](https://icml.cc/virtual/2024/poster/34054) (Poster)
  - **Authors:** [Andrew C. Cullen](http://openreview.net/profile?id=~Andrew_Craig_Cullen1), [Shijie Liu](http://openreview.net/profile?id=~Shijie_Liu4), [Paul Montague](http://openreview.net/profile?id=~Paul_Montague1), [Sarah Erfani](http://openreview.net/profile?id=~Sarah_Monazam_Erfani1), [Benjamin Rubinstein](http://openreview.net/profile?id=~Benjamin_I._P._Rubinstein1)
  - **Affiliations:** School of Computing and Information Systems, University of Melbourne, Parkville, Australia, School of Computing and Information Systems, University of Melbourne, Parkville, Australia, Defence Science and Technology Group, Adelaide, Australia, School of Computing and Information Systems, University of Melbourne, Parkville, Australia, School of Computing and Information Systems, University of Melbourne, Parkville, Australia
  - **TL;DR:** This paper investigates the paradoxical effect of certification mechanisms in adversarial machine learning, revealing that they can be exploited to create more effective adversarial examples. The findings suggest that releasing certification information may inadvertently reduce the security of the models they aim to protect.
  - **Keywords:** Adversarial Examples, Certified Robustness, Certification Aware Attack, Norm-Minimising Attacks, Neural Network Robustness, Security of Certification Mechanisms, False Sense of Security, Improved Understanding of Certification Tightness, Smaller Adversarial Perturbations


- [AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training](https://icml.cc/virtual/2024/poster/34701) (Poster)
  - **Authors:** [Ziyu Wan](http://openreview.net/profile?id=~Ziyu_Wan2), [Xidong Feng](http://openreview.net/profile?id=~Xidong_Feng1), [Muning Wen](http://openreview.net/profile?id=~Muning_Wen2), [Stephen Mcaleer](http://openreview.net/profile?id=~Stephen_Marcus_McAleer1), [Ying Wen](http://openreview.net/profile?id=~Ying_Wen1), [Weinan Zhang](http://openreview.net/profile?id=~Weinan_Zhang1), [Jun Wang](http://openreview.net/profile?id=~Jun_Wang2)
  - **Affiliations:** Shanghai Jiao Tong University, University College London, Shanghai Jiao Tong University, Carnegie Mellon University, Shanghai Jiao Tong University, Shanghai Jiao Tong University, University College London
  - **TL;DR:** This paper introduces TS-LLM, an AlphaZero-like tree-search framework designed to enhance the performance of large language models (LLMs) in multi-step reasoning tasks. The approach leverages a learned value function to improve LLM decoding and training, demonstrating superior performance across various reasoning and planning tasks compared to existing methods.
  - **Keywords:** Large Language Models, Multi-step Reasoning, Tree-Search Algorithms, AlphaZero-like Algorithms, Reinforcement Learning, Reasoning, Planning, Decision-Making, Limitations of existing tree-search methods, Low search depth, In-domain training challenges, TS-LLM framework, Improved LLM performance, Adaptability to various tasks


- [Guarantees for Nonlinear Representation Learning: Non-identical Covariates, Dependent Data, Fewer Samples](https://icml.cc/virtual/2024/poster/32637) (Poster)
  - **Authors:** [Thomas T. Zhang](http://openreview.net/profile?id=~Thomas_TCK_Zhang1), [Bruce Lee](http://openreview.net/profile?id=~Bruce_D_Lee1), [Ingvar Ziemann](http://openreview.net/profile?id=~Ingvar_Ziemann1), [George J. Pappas](http://openreview.net/profile?id=~George_J._Pappas1), [Nikolai Matni](http://openreview.net/profile?id=~Nikolai_Matni2)
  - **Affiliations:** Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, USA, Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, USA, Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, USA, Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, USA, Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, USA
  - **TL;DR:** This study establishes statistical guarantees for learning nonlinear representations from multiple data sources with non-identical distributions and dependent data. The findings indicate that the excess risk of the learned function can be minimized with fewer samples than previously required, enhancing the applicability of transfer learning in complex scenarios.
  - **Keywords:** Nonlinear representation learning, Transfer learning, Statistical guarantees, Sample complexity, Function class, Machine learning, Imitation learning, Non-linear dynamical systems, Non-identical covariates, Dependent data, Sample limitations, Generalization bounds, Excess risk analysis


- [HyperFields: Towards Zero-Shot Generation of NeRFs from Text](https://icml.cc/virtual/2024/poster/34847) (Poster)
  - **Authors:** [Sudarshan Babu](http://openreview.net/profile?id=~Sudarshan_Babu1), [Richard Liu](http://openreview.net/profile?id=~Richard_Liu1), [Zi Yu Zhou](http://openreview.net/profile?id=~Avery_Zhou1), [Michael Maire](http://openreview.net/profile?id=~Michael_Maire1), [Greg Shakhnarovich](http://openreview.net/profile?id=~Greg_Shakhnarovich1), [Rana Hanocka](http://openreview.net/profile?id=~Rana_Hanocka1)
  - **Affiliations:** Toyota Technological Institute at Chicago, University of Chicago, Toyota Technological Institute at Chicago; University of Chicago, University of Chicago, Toyota Technological Institute at Chicago, University of Chicago
  - **TL;DR:** The paper introduces HyperFields, a hypernetwork-based method for generating text-conditioned Neural Radiance Fields (NeRFs) that allows for efficient scene synthesis with minimal fine-tuning. It demonstrates significant improvements in speed and generalization over existing methods, enabling the generation of novel scenes in a zero-shot manner.
  - **Keywords:** text-to-3D synthesis, Neural Radiance Fields (NeRFs), hypernetwork, NeRF distillation training, scene generation, 3D representation, per-prompt NeRF optimization, generalization to novel prompts, accelerated convergence, efficient scene synthesis


- [Don't be so Negative! Score-based Generative Modeling with Oracle-assisted Guidance](https://icml.cc/virtual/2024/poster/34477) (Poster)
  - **Authors:** [Saeid Naderiparizi](http://openreview.net/profile?id=~Saeid_Naderiparizi1), [Xiaoxuan Liang](http://openreview.net/profile?id=~Xiaoxuan_Liang2), [Setareh Cohan](http://openreview.net/profile?id=~Setareh_Cohan1), [Berend Zwartsenberg](http://openreview.net/profile?id=~Berend_Zwartsenberg1), [Frank Wood](http://openreview.net/profile?id=~Frank_Wood2)
  - **Affiliations:** Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada; Montréal Institute for Learning Algorithms (MILA), Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada, Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada; Montréal Institute for Learning Algorithms (MILA)
  - **TL;DR:** This paper presents Gen-neG, a novel denoising diffusion probabilistic modeling methodology that utilizes oracle-assisted guidance to improve generative modeling by reducing the generation of invalid samples. The approach is empirically validated in applications such as collision avoidance in autonomous driving and safe human motion generation.
  - **Keywords:** Score-based generative modeling, diffusion models, oracle-assisted guidance, Denoising diffusion probabilistic modeling, classifier guidance, Collision avoidance in self-driving simulators, safety-guarded human motion generation, Generative modeling with constraints, invalid sample generation, model generalization, Gen-neG methodology, reduction of probability mass on invalid outputs


- [MaxMin-RLHF: Alignment with Diverse Human Preferences](https://icml.cc/virtual/2024/poster/34828) (Poster)
  - **Authors:** [Souradip Chakraborty](http://openreview.net/profile?id=~Souradip_Chakraborty1), [Jiahao Qiu](http://openreview.net/profile?id=~Jiahao_Qiu1), [Hui Yuan](http://openreview.net/profile?id=~Hui_Yuan2), [Alec Koppel](http://openreview.net/profile?id=~Alec_Koppel1), [Dinesh Manocha](http://openreview.net/profile?id=~Dinesh_Manocha3), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1), [Amrit Singh Bedi](http://openreview.net/profile?id=~Amrit_Bedi1), [Mengdi Wang](http://openreview.net/profile?id=~Mengdi_Wang1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, MD, USA, Department of Electrical and Computer Engineering, Princeton University, NJ, USA, Department of Electrical and Computer Engineering, Princeton University, NJ, USA, JP Morgan Chase AI Research, New York, USA, Department of Computer Science, University of Maryland, College Park, MD, USA, Department of Computer Science, University of Maryland, College Park, MD, USA, Department of Computer Science, University of Central Florida, FL, USA, Department of Electrical and Computer Engineering, Princeton University, NJ, USA
  - **TL;DR:** This study addresses the limitations of traditional RLHF approaches that rely on a single reward model, proposing a mixture of reward models to better represent diverse human preferences. The findings demonstrate the efficacy of the new alignment methodology in improving the representation of varied user preferences in language models.
  - **Keywords:** AI Alignment, Human Preferences, Reinforcement Learning from Human Feedback (RLHF), Expectation-Maximization Algorithm, Mixture of Reward Models, MaxMin Alignment Objective, Language Models, Large Language Models (LLMs), Diversity of Human Preferences, Societal Biases, Alignment Challenges, New Alignment Methodology, Improved Representation of Diverse Preferences, GPT-2, Tulu2-7B, Egalitarian Principle, Social Choice Theory


- [Towards Global Optimality for Practical Average Reward Reinforcement Learning without Mixing Time Oracles](https://icml.cc/virtual/2024/poster/34664) (Poster)
  - **Authors:** [Bhrij Patel](http://openreview.net/profile?id=~Bhrij_Patel1), [Wesley A. Suttle](http://openreview.net/profile?id=~Wesley_A_Suttle1), [Alec Koppel](http://openreview.net/profile?id=~Alec_Koppel1), [Vaneet Aggarwal](http://openreview.net/profile?id=~Vaneet_Aggarwal1), [Brian Sadler](http://openreview.net/profile?id=~Brian_M._Sadler1), [Dinesh Manocha](http://openreview.net/profile?id=~Dinesh_Manocha3), [Amrit Singh Bedi](http://openreview.net/profile?id=~Amrit_Bedi1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, USA, US Army Research Laboratory, Adelphi, MD, USA, JP Morgan AI Research, New York, USA, School of Industrial Engineering, Purdue University, Indiana, USA, Department of Computer Science, University of Texas, Austin, USA, Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Central Florida, Florida, USA
  - **TL;DR:** This study presents a novel approach to average-reward reinforcement learning that eliminates the need for oracle knowledge of mixing times, utilizing the Multi-level Actor-Critic framework to achieve global convergence. The proposed method demonstrates superior performance in a 2D grid world navigation experiment compared to existing state-of-the-art policy gradient methods.
  - **Keywords:** average-reward reinforcement learning, global convergence, Multi-level Actor-Critic (MAC), Multi-level Monte-Carlo (MLMC) gradient estimator, policy gradient methods, robotic locomotion, traffic engineering, healthcare, mixing time estimation, gradient estimation issues, temporal dependence in data, alleviation of mixing time dependency, improved convergence results


- [Amortized Variational Deep Kernel Learning](https://icml.cc/virtual/2024/poster/34238) (Poster)
  - **Authors:** [Alan Matias](http://openreview.net/profile?id=~Alan_L._S._Matias1), [César Lincoln Mattos](http://openreview.net/profile?id=~C%C3%A9sar_Lincoln_Mattos1), [Joao Paulo Gomes](http://openreview.net/profile?id=~Jo%C3%A3o_Paulo_Pordeus_Gomes1), [Diego Mesquita](http://openreview.net/profile?id=~Diego_Mesquita1)
  - **Affiliations:** Federal University of Ceará, Federal University of Ceará, Federal University of Ceará, Getulio Vargas Foundation
  - **TL;DR:** This study introduces Amortized Variational Deep Kernel Learning (A VDKL), which addresses the challenges of overfitting and spurious correlations in deep kernel learning by employing amortized inducing points and a parameter-sharing scheme. The proposed method consistently outperforms traditional deep kernel learning and standard Gaussian processes across various tasks, including tabular data and node classification.
  - **Keywords:** Deep Kernel Learning, Gaussian Processes, Uncertainty Quantification, Amortized Inducing Points, Parameter-sharing Scheme, Variational Inference, Tabular Data, Node Classification, Image Classification, Overfitting, Non-local Kernels, Spurious Correlations, Amortized Variational DKL (A VDKL), Improved Model Fit and Capacity, CIFAR100, None


- [Causal Bandits: The Pareto Optimal Frontier of Adaptivity, a Reduction to Linear Bandits, and Limitations around Unknown Marginals](https://icml.cc/virtual/2024/poster/33956) (Poster)
  - **Authors:** [Ziyi Liu](http://openreview.net/profile?id=~Ziyi_Liu7), [Idan Attias](http://openreview.net/profile?id=~Idan_Attias1), [Daniel Roy](http://openreview.net/profile?id=~Daniel_M._Roy1)
  - **Affiliations:** Department of Statistical Sciences, University of Toronto, Canada; Vector Institute, Canada, Vector Institute, Canada; Department of Computer Science, Ben-Gurion University, Israel, Department of Statistical Sciences, University of Toronto, Canada; Vector Institute, Canada
  - **TL;DR:** This study investigates how to adapt to the presence or absence of causal structures in multi-armed bandit problems, aiming to achieve better regret rates when favorable conditions exist while maintaining worst-case guarantees otherwise. The authors establish the Pareto optimal frontier of adaptive rates and demonstrate the necessity of nontrivial estimates for achieving better-than-worst-case minimax rates.
  - **Keywords:** causal bandits, adaptivity, multi-armed bandit problems, d-separation, minimax rates of regret, instance-dependent bounds, decision making, sequential decision-making, adapting to causal structure, recovering worst-case minimax regret, Pareto optimal frontier of adaptive rates, trade-offs in performance, conditionally benign environments


- [Towards Neural Architecture Search through Hierarchical Generative Modeling](https://icml.cc/virtual/2024/poster/33906) (Poster)
  - **Authors:** [Lichuan Xiang](http://openreview.net/profile?id=~Lichuan_Xiang1), [Łukasz Dudziak](http://openreview.net/profile?id=~%C5%81ukasz_Dudziak1), [Mohamed Abdelfattah](http://openreview.net/profile?id=~Mohamed_S_Abdelfattah1), [Abhinav Mehrotra](http://openreview.net/profile?id=~Abhinav_Mehrotra1), [Nicholas Lane](http://openreview.net/profile?id=~Nicholas_Donald_Lane1), [Hongkai Wen](http://openreview.net/profile?id=~Hongkai_Wen1)
  - **Affiliations:** University of Warwick, UK, Samsung AI Centre Cambridge, UK, Cornell University, US, Samsung AI Centre Cambridge, UK, University of Cambridge, UK; Flower Labs, UK, University of Warwick, UK; Samsung AI Centre Cambridge, UK
  - **TL;DR:** This paper presents a novel approach to Neural Architecture Search (NAS) using a two-level generative model hierarchy to efficiently navigate a large search space. The proposed method achieves state-of-the-art performance in low-cost NAS across various tasks, addressing challenges related to search space design and computational efficiency.
  - **Keywords:** Neural Architecture Search (NAS), deep neural network design, Conditional Continuous Normalizing Flow (CCNF), transformer-based sequence generator, CIFAR-10, CIFAR-100, ImageNet, NAS-Bench-360, Search space design, search efficiency, computational feasibility, State-of-the-art performance in low-cost NAS methods, CIFAR-10, CIFAR-100, ImageNet, NAS-Bench-360, Generative modeling, hierarchical generative model hierarchy


- [Few-Shot Unsupervised Implicit Neural Shape Representation Learning with Spatial Adversaries](https://icml.cc/virtual/2024/poster/34012) (Poster)
  - **Authors:** [Amine Ouasfi](http://openreview.net/profile?id=~Amine_Ouasfi1), [Adnane Boukhayma](http://openreview.net/profile?id=~Adnane_Boukhayma2)
  - **Affiliations:** Inria, Univ. Rennes, CNRS, IRISA, M2S, France, Inria, Univ. Rennes, CNRS, IRISA, M2S, France
  - **TL;DR:** This study presents a novel method for learning Neural Signed Distance Functions (SDFs) from sparse 3D point clouds using adversarial samples to mitigate overfitting and improve shape representation. The proposed approach demonstrates significant improvements over existing methods in both synthetic and real data scenarios.
  - **Keywords:** Implicit Neural Representations, 3D Shape Representation, Neural Signed Distance Functions (SDF), Adversarial Samples, Computer Vision, Graphics, Data Sparsity, Model Overfitting, Noise in Input Data, Improved SDF Learning, Regularization Techniques, Shape Hallucinations


- [Learning with Partial-Label and Unlabeled Data: A Uniform Treatment for Supervision Redundancy and Insufficiency](https://icml.cc/virtual/2024/poster/34269) (Spotlight Poster)
  - **Authors:** [Yangfan Liu](http://openreview.net/profile?id=~Yangfan_Liu1), [JIAQI LYU](http://openreview.net/profile?id=~Jiaqi_Lv1), [Xin Geng](http://openreview.net/profile?id=~Xin_Geng1), [Ning Xu](http://openreview.net/profile?id=~Ning_Xu5)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China
  - **TL;DR:** This paper presents a novel method to address the challenges of supervision redundancy and insufficiency in weakly supervised learning by utilizing a mutual information-based approach and a dynamic label exchange mechanism. Experimental results show that the proposed method outperforms existing state-of-the-art techniques in partial label and semi-supervised learning contexts.
  - **Keywords:** weakly supervised learning, inexact supervision, semi-supervised learning, mutual information-based approach, label channel, partial label learning, semi-supervised partial label learning, supervision redundancy, supervision insufficiency, error accumulation, novel approach for label exchange, improved learning from partial labels and unlabeled data, partial labels (PLs), complementary labels (CLs), semi-supervised learning (SSL)


- [Disentanglement Learning via Topology](https://icml.cc/virtual/2024/poster/33014) (Poster)
  - **Authors:** [Nikita Balabin](http://openreview.net/profile?id=~Nikita_Balabin1), [Daria Voronkova](http://openreview.net/profile?id=~Daria_Voronkova1), [Ilya Trofimov](http://openreview.net/profile?id=~Ilya_Trofimov1), [Evgeny Burnaev](http://openreview.net/profile?id=~Evgeny_Burnaev1), [Serguei Barannikov](http://openreview.net/profile?id=~Serguei_Barannikov1)
  - **Affiliations:** Skolkovo Institute of Science and Technology, Moscow, Russia; AIRI, Moscow, Russia, Skolkovo Institute of Science and Technology, Moscow, Russia; AIRI, Moscow, Russia, Skolkovo Institute of Science and Technology, Moscow, Russia, Skolkovo Institute of Science and Technology, Moscow, Russia; AIRI, Moscow, Russia, Skolkovo Institute of Science and Technology, Moscow, Russia; CNRS, IMJ, Paris Cité University
  - **TL;DR:** The study introduces TopDis, a novel method for learning disentangled representations through a multi-scale topological loss, which enhances disentanglement scores while maintaining reconstruction quality. The method operates unsupervised and is effective even with correlated factors of variation, contributing to advancements in explainability and robustness in deep learning models.
  - **Keywords:** Disentangled representations, Explainability, Robustness in deep learning, TopDis (Topological Disentanglement), Differentiable topological loss, Variational Autoencoder (VAE), Unsupervised domain adaptation, Zero-shot learning, Few-shot learning, Controllable image editing, Data representation, Factors of variation, Correlated factors of variation, Improved disentanglement scores (MIG, FactorVAE score, SAP score, DCI disentanglement score), Preservation of reconstruction quality, Topological properties of data manifolds


- [How Graph Neural Networks Learn: Lessons from Training Dynamics](https://icml.cc/virtual/2024/poster/34612) (Poster)
  - **Authors:** [Chenxiao Yang](http://openreview.net/profile?id=~Chenxiao_Yang1), [Qitian Wu](http://openreview.net/profile?id=~Qitian_Wu1), [David Wipf](http://openreview.net/profile?id=~David_Wipf1), [Ruoyu Sun](http://openreview.net/profile?id=~Ruoyu_Sun1), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Amazon Web Services, School of Data Science, The Chinese University of Hong Kong, Shenzhen; Shenzhen International Center for Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University
  - **TL;DR:** This study investigates the training dynamics of Graph Neural Networks (GNNs) to understand how they learn functions that generalize, revealing that their optimization process leverages graph structure through a phenomenon termed kernel-graph alignment. The findings suggest a new parameter-free algorithm that can achieve similar effectiveness to traditional GNNs while being significantly faster.
  - **Keywords:** Graph Neural Networks (GNNs), Training Dynamics, Optimization, Gradient Descent, Kernel-Graph Alignment, Neural Tangent Kernel (NTK), Learning behavior of black-box models, Generalization in GNNs, Limitations in heterophilic graphs, Parameter-free algorithm for function updates, Insights into GNN optimization


- [Stay on Topic with Classifier-Free Guidance](https://icml.cc/virtual/2024/poster/34043) (Spotlight Poster)
  - **Authors:** [Guillaume Sanchez](http://openreview.net/profile?id=~Guillaume_Sanchez1), [Alexander Spangher](http://openreview.net/profile?id=~Alexander_Spangher2), [Honglu Fan](http://openreview.net/profile?id=~Honglu_Fan1), [Elad Levi](http://openreview.net/profile?id=~Elad_Levi1), [Stella Biderman](http://openreview.net/profile?id=~Stella_Biderman1)
  - **Affiliations:** LightOn, France; EleutherAI, None, Information Sciences Institute, University of Southern California, University of Geneva; EleutherAI, Sightful, EleutherAI
  - **TL;DR:** This study demonstrates the effectiveness of Classifier-Free Guidance (CFG) as an inference-time technique to enhance prompt adherence in language modeling, achieving state-of-the-art performance on various benchmarks. The findings indicate that CFG can significantly improve model alignment and coherence, outperforming traditional methods.
  - **Keywords:** Classifier-Free Guidance, Language Modeling, CFG (Classifier-Free Guidance), autoregressive text generation, Natural Language Processing, Code Generation, Question & Answering, Hallucination, Prompt Misadherence, Model Degradation, Improved model performance, alignment to prompts, state-of-the-art results, LAMBADA, Pythia, GPT-2, LLaMA, Large Language Models, Chain-of-Thought, Self-Consistency


- [Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics](https://icml.cc/virtual/2024/poster/34429) (Poster)
  - **Authors:** [Luca Grillotti](http://openreview.net/profile?id=~Luca_Grillotti1), [Maxence Faldor](http://openreview.net/profile?id=~Maxence_Faldor1), [Borja G. León](http://openreview.net/profile?id=~Borja_G._Le%C3%B3n1), [Antoine Cully](http://openreview.net/profile?id=~Antoine_Cully1)
  - **Affiliations:** Department of Computing, Imperial College London, London, United Kingdom, Department of Computing, Imperial College London, London, United Kingdom, Department of Computing, Imperial College London, London, United Kingdom; Iconic AI, Department of Computing, Imperial College London, London, United Kingdom
  - **TL;DR:** The study introduces the Quality-Diversity Actor-Critic (QDAC) algorithm, which combines value and successor features critics to learn high-performing and diverse behaviors in reinforcement learning. QDAC outperforms existing methods in continuous control tasks and demonstrates improved adaptability in perturbed environments.
  - **Keywords:** Quality-Diversity Optimization, Reinforcement Learning, Actor-Critic Algorithms, Value Function Critic, Successor Features Critic, Continuous Control Tasks, Locomotion, Adapting to Unexpected Situations, Learning Diverse Behaviors, Quality-Diversity Actor-Critic (QDAC), High-Performing and Diverse Behaviors


- [Learning Divergence Fields for Shift-Robust Graph Representations](https://icml.cc/virtual/2024/poster/33308) (Poster)
  - **Authors:** [Qitian Wu](http://openreview.net/profile?id=~Qitian_Wu1), [Fan Nie](http://openreview.net/profile?id=~Fan_Nie1), [Chenxiao Yang](http://openreview.net/profile?id=~Chenxiao_Yang1), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This study introduces a geometric diffusion model with learnable divergence fields to address the generalization challenges posed by interdependent data in machine learning. The proposed model demonstrates robustness against distribution shifts and effectively captures generalizable patterns of interdependence across different domains.
  - **Keywords:** Generalization in machine learning, Interdependent data, Geometric diffusion model, Stochastic diffusivity, Causal inference, Graph-based learning, Out-of-distribution generalization, Distribution shifts, Interdependence among data points, Learnable divergence fields, Generalizable patterns of interdependence, GCN (Graph Convolutional Networks), GAT (Graph Attention Networks), Transformers


- [Learning Label Shift Correction for Test-Agnostic Long-Tailed Recognition](https://icml.cc/virtual/2024/poster/34403) (Poster)
  - **Authors:** [Tong Wei](http://openreview.net/profile?id=~Tong_Wei1), [Zhen Mao](http://openreview.net/profile?id=~Zhen_Mao2), [Zi-Hao Zhou](http://openreview.net/profile?id=~Zi-Hao_Zhou1), [Yuanyu Wan](http://openreview.net/profile?id=~Yuanyu_Wan1), [Min-Ling Zhang](http://openreview.net/profile?id=~Min-Ling_Zhang2)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China, School of Software Technology, Zhejiang University, Ningbo, China, School of Computer Science and Engineering, Southeast University, Nanjing, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China
  - **TL;DR:** This study introduces a method called label shift correction (LSC) to address the challenge of unknown test label distributions in long-tail learning, demonstrating that accurate estimation of test distributions can significantly reduce generalization error. The proposed method outperforms existing approaches and enhances the performance of current long-tail learning techniques.
  - **Keywords:** Long-tail learning, Test-agnostic learning, Label shift correction (LSC), Generalized black box shift estimation, Autonomous driving, Recommender systems, Label distribution shift, Generalization error, Improved model performance, Integration with existing long-tail learning techniques


- [Graph Out-of-Distribution Detection Goes Neighborhood Shaping](https://icml.cc/virtual/2024/poster/33026) (Poster)
  - **Authors:** [Tianyi Bao](http://openreview.net/profile?id=~Tianyi_Bao1), [Qitian Wu](http://openreview.net/profile?id=~Qitian_Wu1), [Zetian Jiang](http://openreview.net/profile?id=~Zetian_Jiang1), [Yiting Chen](http://openreview.net/profile?id=~Yiting_Chen1), [Jiawei Sun](http://openreview.net/profile?id=~Jiawei_Sun2), [Junchi Yan](http://openreview.net/profile?id=~Junchi_Yan2)
  - **Affiliations:** School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China, School of Artificial Intelligence & Department of Computer Science and Engineering & MoE Lab of AI, Shanghai Jiao Tong University, Shanghai, China
  - **TL;DR:** This paper introduces TopoOOD, a novel approach for detecting out-of-distribution node instances in graph-structured data by leveraging neighborhood topology and interdependent node features. The proposed method shows significant improvements in detection performance, achieving up to a 15% increase in AUROC and a 50% decrease in FPR compared to existing methods.
  - **Keywords:** Out-of-Distribution (OOD) detection, Graph Neural Networks (GNNs), TopoOOD, k-hop Dirichlet energy, Graph-structured data, Machine learning robustness, OOD detection challenges, Distribution shifts, Node-level OOD detection, New detection metric, Experimental benchmarks, Dirichlet energy, Neighborhood topology


- [On the Recoverability of Causal Relations from Temporally Aggregated I.I.D. Data](https://icml.cc/virtual/2024/poster/33632) (Poster)
  - **Authors:** [Shunxing Fan](http://openreview.net/profile?id=~Shunxing_Fan1), [Mingming Gong](http://openreview.net/profile?id=~Mingming_Gong1), [Kun Zhang](http://openreview.net/profile?id=~Kun_Zhang1)
  - **Affiliations:** Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates, School of Mathematics and Statistics, The University of Melbourne, Melbourne, Australia, Department of Philosophy, Carnegie Mellon University, Pittsburgh, PA, United States; Department of Machine Learning, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates
  - **TL;DR:** This study investigates how temporal aggregation affects the discovery of causal relationships in data, revealing that high aggregation can distort causal inference, particularly in nonlinear cases. However, it also finds that causal relationships can still be recovered under certain conditions, such as partial linearity or appropriate prior knowledge.
  - **Keywords:** Causal discovery, Temporal aggregation, Functional causal model-based methods, Conditional independence-based methods, Economics, Social sciences, Neuroscience, Temporal aggregation effects, Causal frequency alignment, Conditions for consistency in causal discovery, Distortion of causal discovery results


- [SaVeR: Optimal Data Collection Strategy for Safe Policy Evaluation in Tabular MDP](https://icml.cc/virtual/2024/poster/32944) (Poster)
  - **Authors:** [Subhojyoti Mukherjee](http://openreview.net/profile?id=~Subhojyoti_Mukherjee1), [Josiah Hanna](http://openreview.net/profile?id=~Josiah_P._Hanna1), [Robert Nowak](http://openreview.net/profile?id=~Robert_D_Nowak1)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Wisconsin-Madison, Madison, USA, Computer Sciences Department, University of Wisconsin-Madison, Madison, USA, Department of Electrical and Computer Engineering, University of Wisconsin-Madison, Madison, USA
  - **TL;DR:** This paper presents SaVeR, an algorithm for optimal data collection under safety constraints for policy evaluation in tabular Markov decision processes. The study demonstrates that SaVeR effectively approximates a safe oracle algorithm while ensuring low mean squared error in policy evaluation.
  - **Keywords:** Safe data collection, Policy evaluation, Reinforcement learning, Markov decision processes (MDPs), Safe oracle algorithm, SaVeR algorithm, Web marketing, Autonomous driving, Robotics, Healthcare, Finance, Safety constraints, Data collection accuracy, Cumulative cost evaluation, Lower bound for safe data collection, Finite-sample mean squared error bounds, Off-policy evaluation (OPE), Behavior policy


- [Position: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination](https://icml.cc/virtual/2024/poster/32690) (Spotlight Poster)
  - **Authors:** [Zhiyao Luo](http://openreview.net/profile?id=~Zhiyao_Luo1), [Yangchen Pan](http://openreview.net/profile?id=~Yangchen_Pan2), [Peter Watkinson](http://openreview.net/profile?id=~Peter_Watkinson1), [Tingting Zhu](http://openreview.net/profile?id=~Tingting_Zhu1)
  - **Affiliations:** Department of Engineering Science, University of Oxford, Parks Road, Oxford OX1 3PJ, United Kingdom, Department of Engineering Science, University of Oxford, Parks Road, Oxford OX1 3PJ, United Kingdom, Nuffield Department of Population Health (NDPH), University of Oxford, Richard Doll Building, Old Road Campus, Headington, Oxford OX3 7LF, United Kingdom, Department of Engineering Science, University of Oxford, Parks Road, Oxford OX1 3PJ, United Kingdom
  - **TL;DR:** This paper critically examines the application of offline reinforcement learning in dynamic treatment regimes, highlighting significant variability in algorithm performance based on evaluation metrics and MDP formulations. The authors call for standardized evaluation methods and careful algorithm development to enhance the reliability of RL-based treatment strategies in healthcare.
  - **Keywords:** Reinforcement Learning, Dynamic Treatment Regimes, Offline Reinforcement Learning, Markov Decision Process (MDP), Weighted Dueling Double Deep Q-Network, V-learning, Healthcare, Treatment Optimization, Inconsistent evaluation metrics, Lack of standardized baselines, Diverse MDP formulations, Policy evaluation methods, Reward design, Algorithm development, Sepsis dataset


- [Active Ranking and Matchmaking, with Perfect Matchings](https://icml.cc/virtual/2024/poster/33941) (Poster)
  - **Authors:** [Hafedh El Ferchichi](http://openreview.net/profile?id=~Hafedh_El_Ferchichi1), [Matthieu LERASLE](http://openreview.net/profile?id=~Matthieu_LERASLE1), [Vianney Perchet](http://openreview.net/profile?id=~Vianney_Perchet3)
  - **Affiliations:** CREST, ENSAE, IP Paris; FAIRPLAY joint team, CREST, ENSAE, IP Paris; FAIRPLAY joint team, CREST, ENSAE, IP Paris; FAIRPLAY joint team; Criteo AI Lab
  - **TL;DR:** The study presents an algorithm for actively ranking players with varying strengths through perfect matchings, addressing the challenge of noisy comparisons while ensuring all players are paired in each iteration. The proposed method optimally balances ranking accuracy and player engagement, achieving cost efficiency in the process.
  - **Keywords:** Active ranking, matchmaking, AKS sorting networks, bandit theory, Video games, sports ranking, Noisy comparisons, player engagement, pairing players of similar skills, Optimal matching algorithms, cost function analysis, Microsoft TrueSkill, Perfect matching, sample complexity


- [Evaluation of Test-Time Adaptation Under Computational Time Constraints](https://icml.cc/virtual/2024/poster/34931) (Poster)
  - **Authors:** [Motasem Alfarra](http://openreview.net/profile?id=~Motasem_Alfarra1), [Hani Itani](http://openreview.net/profile?id=~Hani_Itani1), [Alejandro Pardo](http://openreview.net/profile?id=~Alejandro_Pardo1), [Shyma Alhuwaider](http://openreview.net/profile?id=~shyma_yaser_alhuwaider1), [Merey Ramazanova](http://openreview.net/profile?id=~Merey_Ramazanova1), [Juan C Perez](http://openreview.net/profile?id=~Juan_Camilo_Perez1), [zhipeng cai](http://openreview.net/profile?id=~zhipeng_cai3), [Matthias Müller](http://openreview.net/profile?id=~Matthias_M%C3%BCller1), [Bernard Ghanem](http://openreview.net/profile?id=~Bernard_Ghanem1)
  - **Affiliations:** King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Intel Labs, Munich, Germany, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia, Intel Labs, Munich, Germany, Intel Labs, Munich, Germany, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia
  - **TL;DR:** This paper introduces a novel online evaluation protocol for Test Time Adaptation (TTA) methods that penalizes slower methods by providing fewer samples for adaptation. The findings indicate that faster, simpler TTA approaches can outperform more complex methods when accounting for inference speed, highlighting the need for efficient TTA solutions.
  - **Keywords:** Test Time Adaptation (TTA), Deep Neural Networks (DNNs), distribution shifts, out-of-distribution data, computational overhead, adaptation speed, online evaluation protocol, benchmarking TTA methods


- [Self-Driven Entropy Aggregation for Byzantine-Robust Heterogeneous Federated Learning](https://icml.cc/virtual/2024/poster/33274) (Poster)
  - **Authors:** [Wenke Huang](http://openreview.net/profile?id=~Wenke_Huang1), [Zekun Shi](http://openreview.net/profile?id=~Zekun_Shi1), [Mang Ye](http://openreview.net/profile?id=~Mang_Ye1), [He Li](http://openreview.net/profile?id=~He_Li4), [Bo Du](http://openreview.net/profile?id=~Bo_Du3)
  - **Affiliations:** National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; Taikang Center for Life and Medical Sciences, Wuhan University, Wuhan, China, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China, National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China
  - **TL;DR:** This study introduces Self-Driven Entropy Aggregation (SDEA) to enhance Byzantine robustness in heterogeneous federated learning by leveraging random public datasets. The method effectively identifies benign clients and mitigates the impact of malicious updates, demonstrating significant improvements in model performance.
  - **Keywords:** Federated Learning, Byzantine Robustness, Self-Driven Entropy Aggregation (SDEA), learnable aggregation weight, Heterogeneous Federated Learning, Byzantine attacks, malicious client updates, data heterogeneity, Detection of Byzantine attackers, diverse predictions, batch-prediction entropy maximization, Random public dataset


- [Towards Interpretable Deep Local Learning with Successive Gradient Reconciliation](https://icml.cc/virtual/2024/poster/35142) (Poster)
  - **Authors:** [Yibo Yang](http://openreview.net/profile?id=~Yibo_Yang2), [Xiaojie Li](http://openreview.net/profile?id=~Xiaojie_Li3), [Motasem Alfarra](http://openreview.net/profile?id=~Motasem_Alfarra1), [Hasan Hammoud](http://openreview.net/profile?id=~Hasan_Abed_Al_Kader_Hammoud1), [Adel Bibi](http://openreview.net/profile?id=~Adel_Bibi1), [Phil Torr](http://openreview.net/profile?id=~Philip_Torr1), [Bernard Ghanem](http://openreview.net/profile?id=~Bernard_Ghanem1)
  - **Affiliations:** King Abdullah University of Science and Technology, Harbin Institute of Technology (Shenzhen); Peng Cheng Laboratory, King Abdullah University of Science and Technology, King Abdullah University of Science and Technology, University of Oxford, University of Oxford, King Abdullah University of Science and Technology
  - **TL;DR:** This study addresses the limitations of global back-propagation in neural network training by proposing a local training strategy that reconciles gradients between modules, achieving significant performance improvements while reducing memory consumption by over 40% on architectures like CNN and Transformer.
  - **Keywords:** Deep learning, Local learning, Neural network training, Back-propagation (BP), Local errors, Gradient reconciliation, Non-greedy layer-wise training, Image recognition, Large-scale datasets, Biological implausibility, Memory consumption, Update locking, Weight transport problem, Local training strategy, Performance improvements, Gradient isolation, ImageNet, Convolutional Neural Networks (CNN), Transformer


- [Codebook Features: Sparse and Discrete Interpretability for Neural Networks](https://icml.cc/virtual/2024/poster/33910) (Poster)
  - **Authors:** [Alex Tamkin](http://openreview.net/profile?id=~Alex_Tamkin1), [Mohammad Taufeeque](http://openreview.net/profile?id=~Mohammad_Taufeeque2), [Noah Goodman](http://openreview.net/profile?id=~Noah_Goodman1)
  - **Affiliations:** Anthropic; Stanford University, FAR AI, Stanford University
  - **TL;DR:** This study introduces codebook features to enhance the interpretability of neural networks by quantizing hidden states into sparse and discrete representations. The findings demonstrate that neural networks can maintain performance while allowing for controlled behavior through the activation of specific codes, providing a promising avenue for analysis and control in neural network models.
  - **Keywords:** Neural Networks, Interpretability, Sparse and Discrete Features, Vector Quantization, Codebook Features, Finetuning, Natural Language Processing, Language Models, Superposition Problem, Complexity of Hidden States, Codebook Features for Control and Analysis, Modest Performance Degradation, Finite State Machine Dataset, Transformer Language Models


- [An Empirical Study of Realized GNN Expressiveness](https://icml.cc/virtual/2024/poster/33880) (Poster)
  - **Authors:** [Yanbo Wang](http://openreview.net/profile?id=~Yanbo_Wang2), [Muhan Zhang](http://openreview.net/profile?id=~Muhan_Zhang1)
  - **Affiliations:** Institute of Artificial Intelligence, Peking University, Beijing, China, Institute of Artificial Intelligence, Peking University, Beijing, China
  - **TL;DR:** This study investigates the realized expressiveness of Graph Neural Networks (GNNs) using a new dataset, BREC, which includes 800 1-WL-indistinguishable graphs. The findings reveal a significant gap between theoretical expressiveness and practical performance of state-of-the-art GNN models.
  - **Keywords:** Graph Neural Networks (GNNs), expressiveness, k-dimensional Weisfeiler-Lehman (k-WL) test, message-passing neural networks (MPNNs), bioinformatics, recommender systems, social networks, limited ability to distinguish non-isomorphic graphs, challenges in expressiveness measurement, novel expressiveness dataset (BREC), measurement of realized expressiveness, BREC, EXP, CSL, SR25, 1-WL, 3-WL, 4-WL, subgraph extraction, equivariance maintenance


- [Self-attention Networks Localize When QK-eigenspectrum Concentrates](https://icml.cc/virtual/2024/poster/33685) (Poster)
  - **Authors:** [Han Bao](http://openreview.net/profile?id=~Han_Bao2), [Ryuichiro Hataya](http://openreview.net/profile?id=~Ryuichiro_Hataya1), [Ryo Karakida](http://openreview.net/profile?id=~Ryo_Karakida2)
  - **Affiliations:** Kyoto University, RIKEN AIP, AIST
  - **TL;DR:** This study investigates the self-attention mechanism in machine learning, focusing on how attention localization affects model performance. It finds that a small eigenspectrum variance can prevent both rank and entropy collapses, enhancing model expressivity and trainability.
  - **Keywords:** self-attention, attention localization, model performance, language modeling, vision tasks, speech recognition, rank collapse, entropy collapse, attention dynamics, better model expressivity, improved trainability, Transformers, attention mechanism, eigenspectrum


- [Understanding MLP-Mixer as a wide and sparse MLP](https://icml.cc/virtual/2024/poster/35140) (Poster)
  - **Authors:** [Tomohiro Hayase](http://openreview.net/profile?id=~Tomohiro_Hayase1), [Ryo Karakida](http://openreview.net/profile?id=~Ryo_Karakida2)
  - **Affiliations:** Metaverse Lab, Cluster Inc., Artificial Intelligence Research Center, AIST
  - **TL;DR:** This study investigates the MLP-Mixer, revealing that its superior performance is attributed to its inherent sparseness and effective expression as a wider MLP with structured weights. The findings suggest that optimizing token and channel sizes can further enhance prediction performance.
  - **Keywords:** MLP-Mixer, deep learning, sparseness, Multi-layer perceptron (MLP), Kronecker product, image classification, performance improvement, model architecture, implicit sparse regularization, effective expression of MLP-Mixer


- [eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data](https://icml.cc/virtual/2024/poster/34290) (Poster)
  - **Authors:** [Peng](http://openreview.net/profile?id=~Bo_Peng14), [Xinyi Ling](http://openreview.net/profile?id=~Xinyi_Ling1), [Ziru Chen](http://openreview.net/profile?id=~Ziru_Chen1), [Huan Sun](http://openreview.net/profile?id=~Huan_Sun1), [Xia Ning](http://openreview.net/profile?id=~Xia_Ning1)
  - **Affiliations:** Department of Computer Science and Engineering, The Ohio State University, USA, Department of Computer Science and Engineering, The Ohio State University, USA, Department of Computer Science and Engineering, The Ohio State University, USA, Translational Data Analytics Institute, The Ohio State University, USA, Department of Computer Science and Engineering; Translational Data Analytics Institute; Department of Biomedical Informatics, The Ohio State University, USA
  - **TL;DR:** This study introduces eCeLLM, a series of large language models specifically designed for e-commerce, leveraging a new dataset called ECInstruct. The models significantly outperform existing task-specific models and demonstrate strong generalizability to new users and products, addressing critical challenges in the e-commerce domain.
  - **Keywords:** e-commerce, generalist modeling, out-of-domain generalization, instruction tuning, large language models (LLMs), e-commerce platforms, user and product recommendation, cold start, out-of-domain generalization challenge, ECInstruct dataset, eCeLLM models, improved generalizability, ECInstruct, large language models (LLMs), GPT-4


- [Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](https://icml.cc/virtual/2024/poster/33837) (Poster)
  - **Authors:** [Vithursan Thangarasa](http://openreview.net/profile?id=~Vithursan_Thangarasa1), [Shreyas Saxena](http://openreview.net/profile?id=~Shreyas_Saxena1), [Abhay Gupta](http://openreview.net/profile?id=~Abhay_Gupta1), [Sean Lie](http://openreview.net/profile?id=~Sean_Lie1)
  - **Affiliations:** Cerebras Systems Inc, California, USA, None, None, None
  - **TL;DR:** The study introduces Sparse Iso-FLOP Transformations (Sparse-IFT) to enhance the accuracy of deep neural networks while maintaining the computational efficiency of dense models. The approach demonstrates significant accuracy improvements on benchmarks like ResNet-18 and GPT-3 without requiring adjustments to training hyperparameters.
  - **Keywords:** weight sparsity, training efficiency, deep neural networks, Sparse Iso-FLOP Transformations (Sparse-IFT), dynamic sparse training (DST), image recognition, large language models, accuracy compromise in sparse weight training, high training costs, significant accuracy improvements, optimal sparse masks, ImageNet, GPT-3


- [Indirectly Parameterized Concrete Autoencoders](https://icml.cc/virtual/2024/poster/34485) (Poster)
  - **Authors:** [Alfred Nilsson](http://openreview.net/profile?id=~Alfred_Nilsson1), [Klas Wijk](http://openreview.net/profile?id=~Klas_Wijk1), [Sai bharath chandra Gutha](http://openreview.net/profile?id=~Sai_bharath_chandra_Gutha1), [Erik Englesson](http://openreview.net/profile?id=~Erik_Englesson1), [Alexandra Hotti](http://openreview.net/profile?id=~Alexandra_Hotti1), [Carlo Saccardi](http://openreview.net/profile?id=~Carlo_Saccardi1), [Oskar Kviman](http://openreview.net/profile?id=~Oskar_Kviman1), [Jens Lagergren](http://openreview.net/profile?id=~Jens_Lagergren1), [Ricardo Vinuesa](http://openreview.net/profile?id=~Ricardo_Vinuesa_Motilva1), [Hossein Azizpour](http://openreview.net/profile?id=~Hossein_Azizpour2)
  - **Affiliations:** KTH Royal Institute of Technology, Science for Life Laboratory, None, KTH Royal Institute of Technology, Science for Life Laboratory, None, KTH Royal Institute of Technology, None, KTH Royal Institute of Technology, None, KTH Royal Institute of Technology; Klarna, None, KTH Royal Institute of Technology, None, KTH Royal Institute of Technology, None, KTH Royal Institute of Technology, None, KTH Royal Institute of Technology; Swedish e-Science Research Centre (SeRC), None, KTH Royal Institute of Technology; Swedish e-Science Research Centre (SeRC), None
  - **TL;DR:** This study introduces Indirectly Parameterized Concrete Autoencoders (IP-CAEs) to address the instability and redundancy issues in traditional Concrete Autoencoders (CAEs) for feature selection. The proposed method demonstrates significant improvements in generalization and training efficiency across various datasets.
  - **Keywords:** Feature selection, Neural network-based embedded feature selection, Concrete Autoencoders (CAEs), Indirectly Parameterized CAEs (IP-CAEs), Gumbel-Softmax distributions, Bioinformatics, Neuroscience, Fluid mechanics, High-dimensional data, Training instability, Redundant feature selection, Improved generalization, Enhanced training time, Unique feature selections


- [Efficient Mixture Learning in Black-Box Variational Inference](https://icml.cc/virtual/2024/poster/34484) (Poster)
  - **Authors:** [Alexandra Hotti](http://openreview.net/profile?id=~Alexandra_Hotti1), [Oskar Kviman](http://openreview.net/profile?id=~Oskar_Kviman1), [Ricky Molén](http://openreview.net/profile?id=~Ricky_Mol%C3%A9n1), [Víctor Elvira](http://openreview.net/profile?id=~V%C3%ADctor_Elvira1), [Jens Lagergren](http://openreview.net/profile?id=~Jens_Lagergren1)
  - **Affiliations:** KTH Royal Institute of Technology; Science for Life Laboratory; Klarna, KTH Royal Institute of Technology; Science for Life Laboratory, KTH Royal Institute of Technology; Science for Life Laboratory, University of Edinburgh, KTH Royal Institute of Technology; Science for Life Laboratory
  - **TL;DR:** This study introduces the Multiple Importance Sampling Variational Autoencoder (MISVAE) to enhance black-box variational inference by efficiently scaling mixture components with minimal parameter increase and reduced inference time. The proposed methods achieve state-of-the-art results on MNIST and improve inference times in Bayesian phylogenetic inference across multiple datasets.
  - **Keywords:** Black-Box Variational Inference, Mixture Variational Distributions, Multiple Importance Sampling, Variational Autoencoder, Evidence Lower Bound (ELBO), Density Estimation, Image Processing, Bayesian Phylogenetic Inference, Computational complexities, Scaling mixture components, Inference time, New estimators of ELBO, Scalability to hundreds of mixture components, Improved estimation performance, MNIST, None


- [STELLA: Continual Audio-Video Pre-training with SpatioTemporal Localized Alignment](https://icml.cc/virtual/2024/poster/32844) (Poster)
  - **Authors:** [Jaewoo Lee](http://openreview.net/profile?id=~Jaewoo_Lee4), [Jaehong Yoon](http://openreview.net/profile?id=~Jaehong_Yoon1), [Wonjae Kim](http://openreview.net/profile?id=~Wonjae_Kim1), [Yunji Kim](http://openreview.net/profile?id=~Yunji_Kim1), [Sung Ju Hwang](http://openreview.net/profile?id=~Sung_Ju_Hwang1)
  - **Affiliations:** KAIST, UNC Chapel Hill, NAVER AI Lab, NAVER AI Lab, KAIST; DeepAuto
  - **TL;DR:** This study presents a continual audio-video pre-training method that addresses challenges in learning evolving audio-video semantics by introducing localized importance scoring and correlation assessment techniques. Experimental results demonstrate a significant performance improvement in zero-shot retrieval tasks while reducing memory usage.
  - **Keywords:** Continual learning, Multimodal learning, Audio-video semantics, Localized Patch Importance Scoring, Replay-guided Correlation Assessment, Multimodal encoder, Audio-related reasoning tasks, Zero-shot retrieval tasks, Sparse spatio-temporal correlation, Multimodal correlation overwriting, Knowledge drift, Probabilistic patch selection, Performance gain in retrieval tasks, Reduced memory consumption


- [Differentiable Combinatorial Scheduling at Scale](https://icml.cc/virtual/2024/poster/35065) (Poster)
  - **Authors:** [Mingju Liu](http://openreview.net/profile?id=~Mingju_Liu1), [Yingjie Li](http://openreview.net/profile?id=~Yingjie_Li1), [Jiaqi Yin](http://openreview.net/profile?email=jyin629%40umd.edu), [Zhiru Zhang](http://openreview.net/profile?id=~Zhiru_Zhang2), [CUNXI YU](http://openreview.net/profile?id=~CUNXI_YU1)
  - **Affiliations:** University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, Cornell University, University of Maryland, College Park
  - **TL;DR:** This paper presents a novel differentiable combinatorial scheduling framework that addresses the NP-hard problem of resource-constrained scheduling, significantly improving optimization efficiency compared to existing commercial and open-source solvers. The proposed method utilizes Gumbel-Softmax sampling and introduces the constrained Gumbel Trick to efficiently handle inequality constraints without requiring training data.
  - **Keywords:** resource-constrained scheduling, NP-hard problems, combinatorial scheduling, Gumbel-Softmax differentiable sampling, linear programming (LP), constrained Gumbel Trick, chip design, high-performance computing, scalability challenges, speed-quality trade-off, limited scalability of existing methods, efficient and scalable scheduling via gradient descent, improved optimization efficiency, integer linear programming (ILP), satisfiability (SAT), constraint programming (CP), system of difference constraints (SDC)


- [Bottleneck-Minimal Indexing for Generative Document Retrieval](https://icml.cc/virtual/2024/poster/34248) (Oral)
  - **Authors:** [Xin Du](http://openreview.net/profile?id=~Xin_Du4), [Lixin Xiu](http://openreview.net/profile?id=~Lixin_Xiu1), [Kumiko Tanaka-Ishii](http://openreview.net/profile?id=~Kumiko_Tanaka-Ishii2)
  - **Affiliations:** Waseda Research Institute for Science and Engineering, Waseda University, Department of Mathematical Informatics, The University of Tokyo, Department of Computer Science and Engineering, Waseda University
  - **TL;DR:** This study proposes a bottleneck-minimal indexing method for generative document retrieval (GDR) by applying an information-theoretic perspective, specifically using Shannon's rate-distortion theory. The proposed method outperforms previous indexing techniques, demonstrating improved efficiency in document retrieval tasks.
  - **Keywords:** Generative Document Retrieval (GDR), Information Retrieval, Neural Autoregressive Model, Rate-Distortion Theory, Information Bottleneck Theory, Document Retrieval, Query Mapping, Indexing Optimization, Information Transmission, Bottleneck-Minimal Indexing Method, Empirical Quantification of Bottleneck, NQ320K, MARCO, Mutual Information, Distortion Optimality


- [Faster Streaming and Scalable Algorithms for Finding Directed Dense Subgraphs in Large Graphs](https://icml.cc/virtual/2024/poster/34906) (Poster)
  - **Authors:** [Slobodan Mitrovic](http://openreview.net/profile?id=~Slobodan_Mitrovic1), [Theodore Pan](http://openreview.net/profile?id=~Theodore_Pan1)
  - **Affiliations:** Department of Computer Science, University of California, Davis, CA, Department of Computer Science, University of California, Davis, CA
  - **TL;DR:** The study presents a new algorithm for finding directed dense subgraphs that achieves a (2 + ε) approximation in a single pass using O(n·poly log n) memory, significantly improving speed and efficiency compared to previous methods. The results demonstrate that this approach is twice as fast on large graphs while maintaining output quality similar to existing algorithms.
  - **Keywords:** directed dense subgraphs, data mining, community detection, clustering, (2 + ε) approximation, randomized streams, single-pass algorithms, finance, web graphs, computational neuroscience, finding induced subgraphs, maximizing edge-to-vertex ratio, faster streaming algorithms, scalable algorithms, quadratic improvement in performance, semi-streaming, Massively Parallel Computation (MPC)


- [BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation](https://icml.cc/virtual/2024/poster/34954) (Poster)
  - **Authors:** [Daeun Lee](http://openreview.net/profile?id=~Daeun_Lee2), [Jaehong Yoon](http://openreview.net/profile?id=~Jaehong_Yoon1), [Sung Ju Hwang](http://openreview.net/profile?id=~Sung_Ju_Hwang1)
  - **Affiliations:** Statistics, Korea University, Computer Science, UNC-Chapel Hill, Korea Advanced Institute of Science and Technology; DeepAuto
  - **TL;DR:** This paper introduces BECoTTA, a modular framework for Continual Test Time Adaptation (CTTA) that efficiently adapts to multiple unseen domains while minimizing forgetting of previously learned knowledge. The proposed method outperforms existing CTTA approaches with significantly fewer trainable parameters, addressing key challenges in model adaptability and efficiency.
  - **Keywords:** Continual Test Time Adaptation (CTTA), Test-Time Adaptation (TTA), Mixture-of-Domain Low-rank Experts (MoDE), Domain-Adaptive Routing, Domain-Expert Synergy Loss, Autonomous driving, vision models, Forgetting-adaptation trade-off, computational efficiency, adaptation to unseen domains, Improved model performance with fewer trainable parameters


- [Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks](https://icml.cc/virtual/2024/poster/33000) (Poster)
  - **Authors:** [Duy Nguyen](http://openreview.net/profile?id=~Duy_Minh_Ho_Nguyen1), [Nina Lukashina](http://openreview.net/profile?id=~Nina_Lukashina1), [Tai Nguyen](http://openreview.net/profile?id=~Tai_Nguyen2), [An Thai Le](http://openreview.net/profile?id=~An_Thai_Le1), [TrungTin Nguyen](http://openreview.net/profile?id=~TrungTin_Nguyen1), [Nhat Ho](http://openreview.net/profile?id=~Nhat_Ho1), [Jan Peters](http://openreview.net/profile?id=~Jan_Peters3), [Daniel Sonntag](http://openreview.net/profile?id=~Daniel_Sonntag2), [Viktor Zaverkin](http://openreview.net/profile?id=~Viktor_Zaverkin1), [Mathias Niepert](http://openreview.net/profile?id=~Mathias_Niepert1)
  - **Affiliations:** Department of Computer Science, University of Stuttgart, Germany; Max Planck Research School for Intelligent Systems (IMPRS-IS); German Research Center for Artificial Intelligence (DFKI), Department of Computer Science, University of Stuttgart, Germany; Max Planck Research School for Intelligent Systems (IMPRS-IS), German Research Center for Artificial Intelligence (DFKI), Department of Computer Science, Technische Universitat Darmstadt, Germany, School of Mathematics and Physics, University of Queensland, Australia, Department of Statistics and Data Sciences, University of Texas at Austin, USA, German Research Center for Artificial Intelligence (DFKI); Department of Computer Science, Technische Universitat Darmstadt, Germany; Hessian.AI, German Research Center for Artificial Intelligence (DFKI); Department of Computer Science, Technische Universitat Darmstadt, Germany; Hessian.AI, NEC Laboratories Europe, Department of Computer Science, University of Stuttgart, Germany; Max Planck Research School for Intelligent Systems (IMPRS-IS); NEC Laboratories Europe
  - **TL;DR:** This study introduces E(3)-invariant molecular conformer aggregation networks that integrate 2D molecular representations with ensembles of 3D conformers to enhance molecular property prediction. The proposed method significantly outperforms existing approaches by addressing the limitations of using single conformers and improving the representation of molecular systems.
  - **Keywords:** molecular property prediction, machine learning, molecular representations, E(3)-invariant networks, Fused Gromov-Wasserstein Barycenter, differentiable solver, message-passing neural networks, drug discovery, material design, limitations of single geometry in molecular modeling, challenges in conformer generation, scalability issues, new aggregation mechanism, improved molecular property prediction performance, 2D molecular graphs, 3D conformers, atom types, bond types, spatial coordinates


- [Quantum Positional Encodings for Graph Neural Networks](https://icml.cc/virtual/2024/poster/34426) (Poster)
  - **Authors:** [Slimane Thabet](http://openreview.net/profile?id=~Slimane_Thabet1), [Mehdi Djellabi](http://openreview.net/profile?id=~Mehdi_Djellabi1), [Igor Sokolov](http://openreview.net/profile?id=~Igor_Olegovich_Sokolov1), [Sachin Kasture](http://openreview.net/profile?id=~Sachin_Kasture1), [Louis-Paul Henry](http://openreview.net/profile?id=~Louis-Paul_Henry1), [Loic Henriet](http://openreview.net/profile?id=~Loic_Henriet1)
  - **Affiliations:** Pasqal, Massy, France; Sorbonne University, Paris, France, Pasqal, Massy, France, Pasqal, Massy, France, Pasqal, Massy, France, Pasqal, Massy, France, Pasqal, Massy, France
  - **TL;DR:** This study introduces novel quantum-based positional encodings for graph neural networks, demonstrating that these encodings can enhance model performance on standard benchmarks. The findings suggest that leveraging quantum computing capabilities can significantly improve the handling of graph data.
  - **Keywords:** quantum computing, graph neural networks, positional encodings, Message Passing Neural Networks (MPNN), Graph Transformers, hybrid algorithms, graph machine learning, chemistry, biology, drug design, social networks, computer vision, limitations of current GNN approaches, need for efficient representations, novel positional encodings, improved model performance, tractable quantum features, eigenvectors of the laplacian matrix, random walk probabilities, Hamiltonian


- [What is Dataset Distillation Learning?](https://icml.cc/virtual/2024/poster/32642) (Poster)
  - **Authors:** [William Yang](http://openreview.net/profile?id=~William_Yang1), [Ye Zhu](http://openreview.net/profile?id=~Ye_Zhu3), [Zhiwei Deng](http://openreview.net/profile?id=~Zhiwei_Deng3), [Olga Russakovsky](http://openreview.net/profile?id=~Olga_Russakovsky1)
  - **Affiliations:** Department of Computer Science, Princeton University, Princeton NJ, United States, Department of Computer Science, Princeton University, Princeton NJ, United States, Google Research, Mountain View CA, United States, Department of Computer Science, Princeton University, Princeton NJ, United States
  - **TL;DR:** This study investigates dataset distillation as a method to create a compact synthetic dataset that retains essential information from large datasets, revealing that while distilled data can train high-performing models, it cannot fully replace real data. The findings highlight the importance of understanding the information content and behavior of distilled data for effective utilization.
  - **Keywords:** dataset distillation, synthetic data, machine learning, large datasets, data compression, information retention, high task performance, interpretation of distilled data, CIFAR-10


- [Model-based Reinforcement Learning for Parameterized Action Spaces](https://icml.cc/virtual/2024/poster/32706) (Poster)
  - **Authors:** [Renhao Zhang](http://openreview.net/profile?id=~Renhao_Zhang1), [Haotian Fu](http://openreview.net/profile?id=~Haotian_Fu3), [Yilin Miao](http://openreview.net/profile?id=~Yilin_Miao1), [George Konidaris](http://openreview.net/profile?id=~George_Konidaris1)
  - **Affiliations:** Department of Computer Science, Brown University, Department of Computer Science, Brown University, Department of Computer Science, Brown University, Department of Computer Science, Brown University
  - **TL;DR:** This paper introduces a novel model-based reinforcement learning algorithm called DLPA for PAMDPs, which significantly improves sample efficiency and performance compared to existing methods. The proposed approach leverages a parameterized-action-conditioned dynamics model and offers theoretical performance guarantees, demonstrating superior results across various benchmarks.
  - **Keywords:** Model-based reinforcement learning, Parameterized Action Markov Decision Processes (PAMDPs), Dynamics Learning and predictive control, Model Predictive Path Integral control, Lipschitz Continuity, Robot control, Game playing, RTS Games, Robot Soccer, Discrete-continuous hybrid action space, Sample efficiency, Asymptotic performance, Performance guarantee, Sample complexity, Improved algorithm performance


- [Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation](https://icml.cc/virtual/2024/poster/34921) (Poster)
  - **Authors:** [Danny Halawi](http://openreview.net/profile?id=~Danny_Halawi1), [Alexander Wei](http://openreview.net/profile?id=~Alexander_Wei2), [Eric Wallace](http://openreview.net/profile?id=~Eric_Wallace1), [Tony Wang](http://openreview.net/profile?id=~Tony_Tong_Wang1), [Nika Haghtalab](http://openreview.net/profile?id=~Nika_Haghtalab2), [Jacob Steinhardt](http://openreview.net/profile?id=~Jacob_Steinhardt1)
  - **Affiliations:** UC Berkeley, UC Berkeley, UC Berkeley, MIT, UC Berkeley, UC Berkeley
  - **TL;DR:** This study introduces covert malicious finetuning, a method that allows attackers to compromise the safety of large language models like GPT-4 while evading detection. The findings reveal that the fine-tuned model can follow harmful instructions 99% of the time, raising significant concerns about the security of black-box finetuning access.
  - **Keywords:** malicious finetuning, model safety, large language models (LLMs), black-box finetuning, encoded format, model safety compromise, evasion of detection, covert malicious finetuning method, high success rate in harmful instruction adherence, GPT-4, safety checks, dual-use concerns


- [Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations](https://icml.cc/virtual/2024/poster/34214) (Poster)
  - **Authors:** [Henrik Schopmans](http://openreview.net/profile?id=~Henrik_Schopmans1), [Pascal Friederich](http://openreview.net/profile?id=~Pascal_Friederich1)
  - **Affiliations:** Institute of Nanotechnology, Karlsruhe Institute of Technology, Kaiserstr. 12, 76131 Karlsruhe, Germany; Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Kaiserstr. 12, 76131 Karlsruhe, Germany, Institute of Nanotechnology, Karlsruhe Institute of Technology, Kaiserstr. 12, 76131 Karlsruhe, Germany; Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Kaiserstr. 12, 76131 Karlsruhe, Germany
  - **TL;DR:** This study presents a method using conditional normalizing flows for efficient sampling of the Boltzmann distribution in molecular systems by separating fine-grained and coarse-grained degrees of freedom. The approach demonstrates significant speedup in molecular dynamics simulations, achieving a performance increase of approximately 15.9 to 216.2 times compared to existing machine learning methods.
  - **Keywords:** Coarse-graining, Molecular dynamics, Generative machine learning, Normalizing flows, Active learning, Molecular systems, Biomolecular simulations, Mode collapse, Exploration of configurational space, Speedup in molecular dynamics simulations, Boltzmann distribution, Potential of mean force (PMF)


- [EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens](https://icml.cc/virtual/2024/poster/33105) (Poster)
  - **Authors:** [Sunil Hwang](http://openreview.net/profile?id=~Sunil_Hwang1), [Jaehong Yoon](http://openreview.net/profile?id=~Jaehong_Yoon1), [Youngwan Lee](http://openreview.net/profile?id=~Youngwan_Lee1), [Sung Ju Hwang](http://openreview.net/profile?id=~Sung_Ju_Hwang1)
  - **Affiliations:** Korea Military Academy, UNC Chapel Hill, KAIST; ETRI, KAIST; DeepAuto
  - **TL;DR:** The study introduces EVEREST, an efficient Masked Video Autoencoder that optimizes video representation learning by selectively focusing on informative tokens, significantly reducing computational and memory demands. The method achieves comparable performance to existing heavy models while enabling training on a single machine with fewer resources.
  - **Keywords:** Video Representation Learning, Masked Video Autoencoder, Efficient Masked Video Autoencoder (EVEREST), token selection, Video understanding, self-supervised learning, Excessive computation and memory requirements, uninformative tokens, Reduced computation and memory requirements, efficient training algorithm, Ego4D dataset


- [DPZero: Private Fine-Tuning of Language Models without Backpropagation](https://icml.cc/virtual/2024/poster/34091) (Poster)
  - **Authors:** [Liang Zhang](http://openreview.net/profile?id=~Liang_Zhang6), [Bingcong Li](http://openreview.net/profile?id=~Bingcong_Li1), [Kiran Thekumparampil](http://openreview.net/profile?id=~Kiran_Koshy_Thekumparampil1), [Sewoong Oh](http://openreview.net/profile?id=~Sewoong_Oh3), [Niao He](http://openreview.net/profile?id=~Niao_He3)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Department of Computer Science, ETH Zurich, Amazon Search, Paul G. Allen School of Computer Science and Engineering, University of Washington, Department of Computer Science, ETH Zurich
  - **TL;DR:** The study introduces DPZERO, a novel private zeroth-order algorithm for fine-tuning large language models that addresses the challenges of high memory demands and privacy concerns. It demonstrates significant memory efficiency in fine-tuning models like RoBERTa and OPT while ensuring differential privacy.
  - **Keywords:** fine-tuning, large language models (LLMs), privacy, zeroth-order methods, differential privacy, domain-specific data, downstream tasks, memory demands, privacy concerns, model memorization, DPZERO algorithm, memory efficiency, RoBERTa, OPT


- [MLI Formula: A Nearly Scale-Invariant Solution with Noise Perturbation](https://icml.cc/virtual/2024/poster/33985) (Poster)
  - **Authors:** [Bowen Tao](http://openreview.net/profile?id=~Bowen_Tao1), [Xin-Chun Li](http://openreview.net/profile?id=~Xin-Chun_Li1), [De-Chuan Zhan](http://openreview.net/profile?id=~De-Chuan_Zhan1)
  - **Affiliations:** School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China
  - **TL;DR:** This study investigates the Monotonic Linear Interpolation (MLI) phenomenon in deep neural networks, revealing that error curves decrease monotonically even when the initial model parameters are replaced with noise or zeros. The findings suggest that the MLI behavior is more closely related to the properties of the converged model rather than the optimization trajectory itself.
  - **Keywords:** Monotonic Linear Interpolation, Deep Neural Networks, Optimization, Stochastic Gradient Descent, Linear Interpolation, Error Minimization, Optimization Trajectory, Scale Invariance Properties, Noise Perturbation, Generalized Scale Invariance (GSI), Rectified Scale Invariance (RSI), Normalized Scale Invariance (NSI)


- [Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity](https://icml.cc/virtual/2024/poster/33675) (Poster)
  - **Authors:** [Lu Yin](http://openreview.net/profile?id=~Lu_Yin1), [You Wu](http://openreview.net/profile?id=~You_Wu1), [Zhenyu Zhang](http://openreview.net/profile?id=~Zhenyu_Zhang4), [Cheng-Yu Hsieh](http://openreview.net/profile?id=~Cheng-Yu_Hsieh1), [Yaqing Wang](http://openreview.net/profile?id=~Yaqing_Wang1), [Yiling Jia](http://openreview.net/profile?id=~Yiling_Jia1), [Gen Li](http://openreview.net/profile?id=~Gen_Li4), [Ajay Jaiswal](http://openreview.net/profile?id=~AJAY_KUMAR_JAISWAL1), [Mykola Pechenizkiy](http://openreview.net/profile?id=~Mykola_Pechenizkiy1), [Yi Liang](http://openreview.net/profile?id=~Yi_Liang1), [Michael Bendersky](http://openreview.net/profile?id=~Michael_Bendersky1), [Zhangyang “Atlas” Wang](http://openreview.net/profile?id=~Zhangyang_Wang1), [Shiwei Liu](http://openreview.net/profile?id=~Shiwei_Liu2)
  - **Affiliations:** University of Surrey; Eindhoven University of Technology; Google Research, Google Research, University of Texas at Austin, University of Washington, Google Research, Google Research, Clemson University, University of Texas at Austin, Eindhoven University of Technology, Google Research, Google Research, University of Texas at Austin, University of Oxford
  - **TL;DR:** This study introduces Outlier Weighed Layerwise Sparsity (OWL), a novel pruning methodology for Large Language Models (LLMs) that aligns layerwise weight sparsity with activation outlier ratios. OWL achieves significant performance improvements and inference speed-ups compared to existing methods, demonstrating its effectiveness in reducing model size without fine-tuning.
  - **Keywords:** Large Language Models, Model Pruning, Outlier Weighed Layerwise Sparsity (OWL), Network Pruning, Model Size, Performance Degradation, Fine-tuning Challenges, Performance Gain, Inference Speed-up, LLaMA-V1/V2, Vicuna, OPT, Mistral, DeepSparse


- [Sampling in Unit Time with Kernel Fisher-Rao Flow](https://icml.cc/virtual/2024/poster/32937) (Poster)
  - **Authors:** [Aimee Maurais](http://openreview.net/profile?id=~Aimee_Maurais1), [Youssef Marzouk](http://openreview.net/profile?id=~Youssef_Marzouk1)
  - **Affiliations:** Center for Computational Science and Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA, Center for Computational Science and Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA
  - **TL;DR:** This study introduces a new mean-field ODE and interacting particle systems for sampling from unnormalized target densities, utilizing a gradient-free approach that outperforms existing methods. The proposed method demonstrates high-quality sampling capabilities and addresses challenges in traditional sampling techniques.
  - **Keywords:** sampling, transport, unnormalized target density, mean-field ODE, interacting particle systems (IPS), Poisson equation, Fisher–Rao gradient flow, RKHS ansatz, Bayesian inference, data assimilation, gradient-free sampling, weight degeneracy, ensemble collapse, high-quality samples, stochastic variant, optimization of sampling methods, Monge–Ampère equations, sample-driven optimal transport


- [A New Computationally Efficient Algorithm to solve Feature Selection for Functional Data Classification in High-dimensional Spaces](https://icml.cc/virtual/2024/poster/33700) (Poster)
  - **Authors:** [Tobia Boschi](http://openreview.net/profile?id=~Tobia_Boschi1), [FRANCESCA BONIN](http://openreview.net/profile?email=fbonin%40ie.ibm.com), [Rodrigo Ordonez-Hurtado](http://openreview.net/profile?id=~Rodrigo_H._Ordonez-Hurtado1), [Alessandra Pascale](http://openreview.net/profile?id=~Alessandra_Pascale1), [Jonathan Epperlein](http://openreview.net/profile?id=~Jonathan_P_Epperlein1)
  - **Affiliations:** IBM Research Europe, Dublin, IBM Research Europe, Dublin, IBM Research Europe, Dublin, IBM Research Europe, Dublin, IBM Research Europe, Dublin
  - **TL;DR:** This paper presents a new methodology called Feature Selection for Functional Classification (FSFC) that efficiently integrates feature selection and classification for functional data in high-dimensional spaces. FSFC demonstrates superior performance in computational efficiency and classification accuracy compared to existing methods, while also effectively reducing dimensionality in data analysis contexts.
  - **Keywords:** Feature Selection, Functional Classification, High-dimensional Data, Dual Augmented Lagrangian algorithm, Functional Principal Components, Health Data Analysis, Chronic Disease Analysis, High dimensionality, Data scarcity, Model overfitting, FSFC methodology, Optimization problem integration, Functional Data Analysis (FDA), Logistic Loss


- [Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks](https://icml.cc/virtual/2024/poster/34210) (Poster)
  - **Authors:** [Arjun Karuvally](http://openreview.net/profile?id=~Arjun_Karuvally1), [Terrence Sejnowski](http://openreview.net/profile?id=~Terrence_Sejnowski2), [Hava Siegelmann](http://openreview.net/profile?id=~Hava_T_Siegelmann1)
  - **Affiliations:** Manning College of Information and Computer Sciences, University of Massachusetts Amherst, MA-01002, USA, Computational Neurobiology Laboratory, The Salk Institute for Biological Studies, La Jolla, 92037, CA, USA, Manning College of Information and Computer Sciences, University of Massachusetts Amherst, MA-01002, USA
  - **TL;DR:** This study introduces a theoretical model of neural working memory in Recurrent Neural Networks (RNNs) based on traveling wave dynamics, demonstrating its effectiveness in storing information and enhancing learning processes. The findings suggest significant implications for advancing neural network architectures in artificial intelligence.
  - **Keywords:** Traveling waves, neural working memory, Recurrent Neural Networks (RNNs), Wave dynamics, backpropagation, Artificial intelligence, memory storage, Diminishing gradient problem, history-dependent dynamical systems, Improved learnability and generalization properties of RNNs


- [Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices](https://icml.cc/virtual/2024/poster/34303) (Poster)
  - **Authors:** [Jiin Woo](http://openreview.net/profile?id=~Jiin_Woo1), [Laixi Shi](http://openreview.net/profile?id=~Laixi_Shi1), [Gauri Joshi](http://openreview.net/profile?id=~Gauri_Joshi1), [Yuejie Chi](http://openreview.net/profile?id=~Yuejie_Chi1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA, Department of Computing Mathematical Sciences, California Institute of Technology, CA 91125, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA, Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA
  - **TL;DR:** This study presents FedLCB-Q, a federated offline reinforcement learning algorithm that effectively utilizes multiple agents' datasets to learn an optimal policy without requiring high-quality individual datasets. The findings demonstrate that collaborative coverage can achieve linear speedup and maintain communication efficiency, addressing challenges in data sparsity and state-action coverage.
  - **Keywords:** Offline Reinforcement Learning, Federated Learning, Q-learning, FedLCB-Q, Markov Decision Processes (MDPs), Data sparsity, Coverage of state-action space, Linear speedup, Communication efficiency, Pessimism, Model-free RL


- [Improved Dimensionality Dependence for Zeroth-Order Optimisation over Cross-Polytopes](https://icml.cc/virtual/2024/poster/34644) (Poster)
  - **Authors:** [Weijia Shao](http://openreview.net/profile?id=~Weijia_Shao1)
  - **Affiliations:** Unit 2.6 Workplaces, Safety of Machinery, Operational Safety, Federal Institute for Occupational Safety and Health, Dresden, Germany
  - **TL;DR:** This study presents a novel algorithm that enhances the dimensionality dependence for gradient-free optimisation over cross-polytopes, achieving a significant improvement in convergence rates. The proposed method is applicable to both smooth and non-smooth functions, with implications for adversarial machine learning and explainable AI.
  - **Keywords:** Gradient-free optimisation, Cross-polytopes, Adversarial attacks, Explainable AI, Mirror descent algorithm, Negative 1/2-Tsallis entropy, ℓ1-ellipsoidal smoothing-based gradient estimator, Adversarial machine learning, Sparse regression, Dimensionality dependence, Bandit convex optimisation, Non-smooth and non-convex functions, Improved dimensionality dependence, Convergence guarantees


- [Disentangled 3D Scene Generation with Layout Learning](https://icml.cc/virtual/2024/poster/34277) (Poster)
  - **Authors:** [Dave Epstein](http://openreview.net/profile?id=~Dave_Epstein1), [Ben Poole](http://openreview.net/profile?id=~Ben_Poole1), [Ben Mildenhall](http://openreview.net/profile?id=~Ben_Mildenhall1), [Alexei Efros](http://openreview.net/profile?id=~Alexei_A_Efros1), [Aleksander Holynski](http://openreview.net/profile?id=~Aleksander_Holynski1)
  - **Affiliations:** Department of Computer Science, UC Berkeley; Google Research, Google Research, Google Research, Department of Computer Science, UC Berkeley, Department of Computer Science, UC Berkeley; Google Research
  - **TL;DR:** This paper presents a method for generating 3D scenes that are disentangled into individual objects using unsupervised learning and multiple NeRFs. The approach enables effective object-level manipulation in text-to-3D content creation, demonstrating significant capabilities in scene composition without requiring additional supervision.
  - **Keywords:** 3D scene generation, object disentanglement, NeRF (Neural Radiance Fields), layout learning, Text-to-3D content creation, Object discovery, scene composition, Effective object-level scene manipulation, unsupervised disentanglement


- [A Sparsity Principle for Partially Observable Causal Representation Learning](https://icml.cc/virtual/2024/poster/34245) (Poster)
  - **Authors:** [Danru Xu](http://openreview.net/profile?id=~Danru_Xu1), [Dingling Yao](http://openreview.net/profile?id=~Dingling_Yao1), [Sébastien Lachapelle](http://openreview.net/profile?id=~Sebastien_Lachapelle1), [Perouz Taslakian](http://openreview.net/profile?id=~Perouz_Taslakian1), [Julius von Kügelgen](http://openreview.net/profile?id=~Julius_von_K%C3%BCgelgen2), [Francesco Locatello](http://openreview.net/profile?id=~Francesco_Locatello1), [Sara Magliacane](http://openreview.net/profile?id=~Sara_Magliacane1)
  - **Affiliations:** University of Amsterdam, Institute of Science and Technology Austria; Max Planck Institute for Intelligent Systems, Tübingen, Germany, Samsung - SAIT AI Lab, Montreal; Mila, Université de Montréal, ServiceNow Research, Seminar for Statistics, ETH Zürich, Institute of Science and Technology Austria, University of Amsterdam
  - **TL;DR:** This study focuses on causal representation learning in a partially observed setting, where measurements provide information about varying subsets of causal states. The authors establish identifiability results and propose methods to estimate underlying causal variables by enforcing sparsity, demonstrating effectiveness through experiments on simulated datasets.
  - **Keywords:** Causal representation learning, Partial observability, Independent component analysis (ICA), Linear mixing functions, Piecewise linear mixing functions, Partial observability, High-dimensional observations, Instance-dependent partial observability, Identifiability results, Estimation methods, Sparsity in inferred representation, Simulated datasets, Established benchmarks


- [Implicit meta-learning may lead language models to trust more reliable sources](https://icml.cc/virtual/2024/poster/34518) (Poster)
  - **Authors:** [Dmitrii Krasheninnikov](http://openreview.net/profile?id=~Dmitrii_Krasheninnikov1), [Egor Krasheninnikov](http://openreview.net/profile?id=~Egor_Krasheninnikov1), [Bruno Mlodozeniec](http://openreview.net/profile?id=~Bruno_Kacper_Mlodozeniec2), [Tegan Maharaj](http://openreview.net/profile?id=~Tegan_Maharaj1), [David Krueger](http://openreview.net/profile?id=~David_Krueger1)
  - **Affiliations:** University of Cambridge, University of Cambridge, Max Planck Institute for Intelligent Systems; University of Cambridge, University of Toronto, University of Cambridge
  - **TL;DR:** This study investigates how language models can learn to identify and internalize useful information from documents, demonstrating that models fine-tuned with indicators of reliability can better utilize reliable sources in subsequent training. The findings suggest implications for the capabilities and controllability of future AI systems.
  - **Keywords:** Implicit meta-learning, language models, document usefulness, Fine-tuning, coreference resolution, probing, Natural language processing, question-answering, Document reliability, internalization of knowledge, cross-document coreference resolution, Indicators of usefulness, model updates based on reliability, Synthetic fine-tuning dataset, LLMs (Large Language Models), Stage1, Stage2


- [The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective](https://icml.cc/virtual/2024/poster/35636) (Poster)
  - **Authors:** Chi-Heng Lin, Chiraag Kaushik, Eva Dyer, Vidya Muthukumar
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Decomposed Linear Dynamical Systems (dLDS) for  learning the latent components of neural dynamics](https://icml.cc/virtual/2024/poster/35631) (Poster)
  - **Authors:** Noga Mudrik, Yenho Chen, Eva Yezerets, Christopher Rozell, Adam Charles
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Survival Kernets: Scalable and Interpretable Deep Kernel Survival Analysis with an Accuracy Guarantee](https://icml.cc/virtual/2024/poster/35628) (Poster)
  - **Authors:** George Chen
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Evaluating Instrument Validity using the Principle of Independent Mechanisms](https://icml.cc/virtual/2024/poster/35621) (Poster)
  - **Authors:** Patrick F. Burauel
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Graph Attention Retrospective](https://icml.cc/virtual/2024/poster/35638) (Poster)
  - **Authors:** Kimon Fountoulakis, Amit Levi, Shenghao Yang, Aseem Baranwal, Aukosh Jagannath
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [RankSEG: A Consistent Ranking-based Framework for Segmentation](https://icml.cc/virtual/2024/poster/35623) (Poster)
  - **Authors:** Ben Dai, Chunlin Li
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [A Unified Recipe for Deriving (Time-Uniform) PAC-Bayes Bounds](https://icml.cc/virtual/2024/poster/35643) (Poster)
  - **Authors:** Ben Chugg, Hongjian Wang, Aaditya Ramdas
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [On the Generalization of Stochastic Gradient Descent with Momentum](https://icml.cc/virtual/2024/poster/35629) (Poster)
  - **Authors:** Ali Ramezani-Kebrya, Kimon Antonakopoulos, Volkan Cevher, Ashish Khisti, Ben Liang
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Fair Data Representation for Machine Learning at the Pareto Frontier](https://icml.cc/virtual/2024/poster/35624) (Poster)
  - **Authors:** Shizhou Xu, Thomas Strohmer
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [T-Cal: An Optimal Test for the Calibration of Predictive Models](https://icml.cc/virtual/2024/poster/35627) (Poster)
  - **Authors:** Donghwan Lee, Xinmeng Huang, Hamed Hassani, Edgar Dobriban
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Scaling Speech Technology to 1,000+ Languages](https://icml.cc/virtual/2024/poster/35635) (Poster)
  - **Authors:** Vineel Pratap Konduru, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Adaptive Learning of Density Ratios in RKHS](https://icml.cc/virtual/2024/poster/35641) (Poster)
  - **Authors:** Werner Zellinger, Stefan Kindermann, Sergei V. Pereverzyev
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Prior Specification for Bayesian Matrix Factorization via Prior Predictive Matching](https://icml.cc/virtual/2024/poster/35639) (Poster)
  - **Authors:** Eliezer de Souza da Silva, Tomasz Kuśmierczyk, Marcelo Hartmann, Arto Klami
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Attribution-based Explanations that Provide Recourse Cannot be Robust](https://icml.cc/virtual/2024/poster/35640) (Poster)
  - **Authors:** Hidde Fokkema, Rianne de Heide, Tim van Erven
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Online Non-stochastic Control with Partial Feedback](https://icml.cc/virtual/2024/poster/35630) (Poster)
  - **Authors:** Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data](https://icml.cc/virtual/2024/poster/33830) (Poster)
  - **Authors:** [Yvonne Zhou](http://openreview.net/profile?id=~Yvonne_Zhou1), [Mingyu Liang](http://openreview.net/profile?id=~Mingyu_Liang1), [Ivan Brugere](http://openreview.net/profile?id=~Ivan_Brugere1), [Danial Dervovic](http://openreview.net/profile?id=~Danial_Dervovic1), [Antigoni Polychroniadou](http://openreview.net/profile?id=~Antigoni_Polychroniadou1), [Min Wu](http://openreview.net/profile?id=~Min_Wu1), [Dana Dachman-Soled](http://openreview.net/profile?id=~Dana_Dachman-Soled1)
  - **Affiliations:** University of Maryland, College Park, University of Maryland, College Park, J.P. Morgan AI Research, New York, J.P. Morgan AI Research, New York, AlgoCRYPT CoE; J.P. Morgan AI Research, New York, University of Maryland, College Park, University of Maryland, College Park
  - **TL;DR:** None
  - **Keywords:** None


- [Major-Minor Mean Field Multi-Agent Reinforcement Learning](https://icml.cc/virtual/2024/poster/33152) (Poster)
  - **Authors:** [Kai Cui](http://openreview.net/profile?id=~Kai_Cui3), [Christian Fabian](http://openreview.net/profile?id=~Christian_Fabian1), [Anam Tahir](http://openreview.net/profile?id=~Anam_Tahir1), [Heinz Koeppl](http://openreview.net/profile?id=~Heinz_Koeppl1)
  - **Affiliations:** Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany, Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany, Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany, Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany
  - **TL;DR:** This study introduces Major-Minor Mean Field Control (M3FC) to enhance the scalability of multi-agent reinforcement learning by modeling many similar and few complex agents. The proposed algorithm, Major-Minor Mean Field MARL (M3FMARL), demonstrates strong performance in various scenarios compared to existing policy gradient methods.
  - **Keywords:** Multi-Agent Reinforcement Learning (MARL), Mean Field Control (MFC), Major-Minor Mean Field Control (M3FC), Major-Minor Mean Field MARL (M3FMARL), Scalability in multi-agent systems, limitations of standard MFC, Approximation results for finite agent control, sufficiency of stationary policies for optimality, Mean Field Games (MFG), dynamic programming principle


- [Online Cascade Learning for Efficient Inference over Streams](https://icml.cc/virtual/2024/poster/33840) (Poster)
  - **Authors:** [Lunyiu Nie](http://openreview.net/profile?id=~Lunyiu_Nie1), [Zhimin Ding](http://openreview.net/profile?id=~Zhimin_Ding1), [Erdong Hu](http://openreview.net/profile?id=~Erdong_Hu1), [Christopher Jermaine](http://openreview.net/profile?id=~Christopher_Jermaine1), [Swarat Chaudhuri](http://openreview.net/profile?id=~Swarat_Chaudhuri1)
  - **Affiliations:** The University of Texas at Austin, Rice University, Rice University, Rice University, The University of Texas at Austin
  - **TL;DR:** This paper introduces online cascade learning to efficiently process data streams using a hierarchy of models, from simpler ones to large language models (LLMs). The proposed method significantly reduces inference costs by up to 90% while maintaining accuracy and robustness against varying input complexities.
  - **Keywords:** Online Cascade Learning, Large Language Models (LLMs), Efficient Inference, Imitation Learning, Deferral Policy, No-Regret Algorithm, Data Streams, Sentiment Analysis, High Computational Cost of LLM Inference, Input Distribution Shifts, Cost Reduction in Inference, Robustness in Stream Processing, Logistic Regression, Llama


- [Quality-Diversity with Limited Resources](https://icml.cc/virtual/2024/poster/34945) (Poster)
  - **Authors:** [Ren-Jian Wang](http://openreview.net/profile?id=~Ren-Jian_Wang1), [Ke Xue](http://openreview.net/profile?id=~Ke_Xue1), [Cong Guan](http://openreview.net/profile?id=~Cong_Guan1), [Chao Qian](http://openreview.net/profile?id=~Chao_Qian1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This paper addresses the challenge of efficiently training Quality-Diversity (QD) algorithms with limited resources by proposing a novel method called RefQD, which reduces resource overhead while maintaining performance. Experiments show that RefQD significantly lowers resource usage while achieving comparable or better results than existing sample-efficient QD algorithms.
  - **Keywords:** Quality-Diversity algorithms, optimization, RefQD, neural network decomposition, Reinforcement Learning, robotics, human-AI coordination, Sample efficiency, resource efficiency, resource overhead, Efficient training methods, reduced resource usage, QDax, Atari, Evolutionary Algorithms, archive, parent selection, variation operators


- [On the Independence Assumption in Neurosymbolic Learning](https://icml.cc/virtual/2024/poster/34030) (Poster)
  - **Authors:** [Emile van Krieken](http://openreview.net/profile?id=~Emile_van_Krieken1), [Pasquale Minervini](http://openreview.net/profile?id=~Pasquale_Minervini4), [Edoardo Ponti](http://openreview.net/profile?id=~Edoardo_Ponti1), [Antonio Vergari](http://openreview.net/profile?id=~Antonio_Vergari3)
  - **Affiliations:** School of Informatics, University of Edinburgh, School of Informatics, University of Edinburgh, School of Informatics, University of Edinburgh, School of Informatics, University of Edinburgh
  - **TL;DR:** The paper critiques the conditional independence assumption in neurosymbolic learning systems, demonstrating how it can lead to overconfidence in predictions and optimization difficulties. The authors propose a theoretical foundation for developing more expressive probabilistic models that better represent uncertainty.
  - **Keywords:** neurosymbolic learning, probabilistic reasoning, logical constraints, conditional independence assumption, uncertainty quantification, optimization challenges, expressive neurosymbolic probabilistic models


- [Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling](https://icml.cc/virtual/2024/poster/33976) (Poster)
  - **Authors:** [Raunaq Bhirangi](http://openreview.net/profile?id=~Raunaq_Bhirangi1), [Chenyu Wang](http://openreview.net/profile?id=~Chenyu_Wang9), [Venkatesh Pattabiraman](http://openreview.net/profile?id=~Venkatesh_Pattabiraman1), [Carmel Majidi](http://openreview.net/profile?id=~Carmel_Majidi1), [Abhinav Gupta](http://openreview.net/profile?id=~Abhinav_Gupta1), [Tess Hellebrekers](http://openreview.net/profile?id=~Tess_Hellebrekers2), [Lerrel Pinto](http://openreview.net/profile?id=~Lerrel_Pinto1)
  - **Affiliations:** Carnegie Mellon University, Pittsburgh, USA; FAIR, Meta, New York University, NYC, USA, New York University, NYC, USA, Carnegie Mellon University, Pittsburgh, USA, Carnegie Mellon University, Pittsburgh, USA, FAIR, Meta, New York University, NYC, USA
  - **TL;DR:** This study introduces Hierarchical State-Space models (HiSS) for continuous sequence-to-sequence prediction from raw sensory data, demonstrating significant improvements over existing models like Transformers and LSTMs. The results indicate HiSS's effectiveness in handling non-linear sensor data and its scalability to smaller datasets.
  - **Keywords:** Continuous sequence-to-sequence prediction, sensory data analysis, Hierarchical State-Space models (HiSS), structured state-space models, causal Transformers, LSTMs, S4, Mamba, Robotics, medical devices, real-time decision-making, Non-linear sensor data, extraneous variables, data-dependent drift, small labeled datasets, Improved predictive performance, efficient scaling to smaller datasets, CSP-Bench


- [Near-Optimal Regret in Linear MDPs with Aggregate Bandit Feedback](https://icml.cc/virtual/2024/poster/33391) (Poster)
  - **Authors:** [Asaf Cassel](http://openreview.net/profile?id=~Asaf_Cassel1), [Haipeng Luo](http://openreview.net/profile?id=~Haipeng_Luo1), [Aviv Rosenberg](http://openreview.net/profile?id=~Aviv_Rosenberg1), [Dmitry Sotnikov](http://openreview.net/profile?id=~Dmitry_Sotnikov1)
  - **Affiliations:** Blavatnik School of Computer Science, Tel Aviv University; None, University of Southern California, Google Research, Amazon Science
  - **TL;DR:** This paper extends the Aggregate Bandit Feedback model to Linear MDPs, developing two efficient algorithms that achieve near-optimal regret guarantees. The findings highlight the challenges of providing reward feedback in RL and introduce novel techniques for optimizing performance in environments with large state spaces.
  - **Keywords:** Reinforcement Learning, Aggregate Bandit Feedback, Linear MDPs, Randomized Ensemble Least Squares Value Iteration (RE-LSVI), Randomized Ensemble Policy Optimization (REPO), Robotics, Large Language Models, Reward feedback challenges, function approximation in RL, Near-optimal regret guarantees, ensemble randomization technique, Markov Decision Process (MDP), Q-functions ensemble


- [Instruction Tuning for Secure Code Generation](https://icml.cc/virtual/2024/poster/34230) (Poster)
  - **Authors:** [Jingxuan He](http://openreview.net/profile?id=~Jingxuan_He1), [Mark Vero](http://openreview.net/profile?id=~Mark_Vero1), [Gabriela Krasnopolska](http://openreview.net/profile?id=~Gabriela_Krasnopolska1), [Martin Vechev](http://openreview.net/profile?id=~Martin_Vechev1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Switzerland, Department of Computer Science, ETH Zurich, Switzerland, Department of Computer Science, ETH Zurich, Switzerland, Department of Computer Science, ETH Zurich, Switzerland
  - **TL;DR:** This study introduces SafeCoder, a method for enhancing the security of code generated by large language models through security-centric fine-tuning integrated with standard instruction tuning. SafeCoder significantly improves code security by approximately 30% while maintaining the utility of the generated code.
  - **Keywords:** instruction tuning, secure code generation, SafeCoder, fine-tuning, programming, software development, security of generated code, unsafe code generation, improved security, joint optimization of security and utility, large language models (LMs), instruction-following capabilities


- [Understanding Forgetting in Continual Learning with Linear Regression](https://icml.cc/virtual/2024/poster/34856) (Poster)
  - **Authors:** [Meng Ding](http://openreview.net/profile?id=~Meng_Ding3), [Kaiyi Ji](http://openreview.net/profile?id=~Kaiyi_Ji1), [Di Wang](http://openreview.net/profile?id=~Di_Wang1), [Jinhui Xu](http://openreview.net/profile?id=~Jinhui_Xu1)
  - **Affiliations:** Department of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, USA, Department of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, USA, Division of CEMSE, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia, Department of Computer Science and Engineering, State University of New York at Buffalo, Buffalo, USA
  - **TL;DR:** This study provides a theoretical analysis of forgetting in continual learning using linear regression and SGD, revealing that the sequence of task training affects forgetting rates. It also highlights that an appropriate step size can mitigate forgetting in both under-parameterized and over-parameterized settings.
  - **Keywords:** Continual learning, Catastrophic forgetting, Linear regression, Stochastic Gradient Descent (SGD), Catastrophic forgetting, Task sequence arrangement, Theoretical analysis, Mitigation of forgetting, Over-parameterized, Under-parameterized


- [Probability Distribution of Hypervolume Improvement in Bi-objective Bayesian Optimization](https://icml.cc/virtual/2024/poster/33798) (Poster)
  - **Authors:** [Hao Wang](http://openreview.net/profile?id=~Hao_Wang48), [Kaifeng Yang](http://openreview.net/profile?id=~Kaifeng_Yang1), [Michael Affenzeller](http://openreview.net/profile?id=~Michael_Affenzeller1)
  - **Affiliations:** Leiden University, Leiden, The Netherlands, University of Applied Sciences, Hagenberg, Austria, University of Applied Sciences, Hagenberg, Austria
  - **TL;DR:** This study provides the exact probability distribution of hypervolume improvement (HVI) for bi-objective problems in Bayesian optimization, demonstrating that the proposed ε-probability of hypervolume improvement (ε-PoHVI) significantly outperforms existing acquisition functions, particularly under high prediction uncertainty.
  - **Keywords:** Multi-objective Bayesian optimization, Hypervolume improvement, Gaussian process modeling, Cell partition-based method, Black-box multi-objective optimization problems, Prediction uncertainty, Exact expression of hypervolume improvement distribution, ε-probability of hypervolume improvement (ε-PoHVI), Hypervolume indicator (HV), Probability of improvement (PoI), Expected hypervolume improvement (EHVI), Pareto-compliant property


- [Mimicking Better by Matching the Approximate Action Distribution](https://icml.cc/virtual/2024/poster/34395) (Poster)
  - **Authors:** [Joao A. Candido Ramos](http://openreview.net/profile?id=~Joao_Candido_Ramos1), [Lionel Blondé](http://openreview.net/profile?id=~Lionel_Blond%C3%A91), [Naoya Takeishi](http://openreview.net/profile?id=~Naoya_Takeishi1), [Alexandros Kalousis](http://openreview.net/profile?id=~Alexandros_Kalousis1)
  - **Affiliations:** University of Geneva (UNIGE), University of Applied Sciences and Arts Western (HES-SO), University of Applied Sciences and Arts Western (HES-SO), The University of Tokyo, RIKEN Center for Advanced Intelligence Project, University of Applied Sciences and Arts Western (HES-SO)
  - **TL;DR:** This paper introduces MAAD, a novel sample-efficient on-policy algorithm for Imitation Learning from Observations, which improves sample efficiency and stability by inferring plausible action distributions from expert state transitions. The method demonstrates superior performance in various environments, often achieving expert-level results with fewer interactions compared to existing methods.
  - **Keywords:** Imitation Learning, Reinforcement Learning, MAAD (sample-efficient on-policy algorithm), inverse dynamics model, Autonomous driving, motion capture data analysis, Non-availability of expert actions, complexity of reward function design, Improved sample efficiency and stability, expert performance levels, MuJoCo environments, OpenAI Gym, DeepMind Control Suite, Imitation Learning from Observations (ILO), Imitation Learning from Demonstrations (ILD)


- [Asymmetry in Low-Rank Adapters of Foundation Models](https://icml.cc/virtual/2024/poster/32849) (Poster)
  - **Authors:** [Jiacheng Zhu](http://openreview.net/profile?id=~Jiacheng_Zhu1), [Kristjan Greenewald](http://openreview.net/profile?id=~Kristjan_Greenewald1), [Kimia Nadjahi](http://openreview.net/profile?id=~Kimia_Nadjahi1), [Haitz Sáez de Ocáriz Borde](http://openreview.net/profile?id=~Haitz_S%C3%A1ez_de_Oc%C3%A1riz_Borde1), [Rickard Gabrielsson](http://openreview.net/profile?id=~Rickard_Br%C3%BCel_Gabrielsson1), [Leshem Choshen](http://openreview.net/profile?id=~Leshem_Choshen1), [Marzyeh Ghassemi](http://openreview.net/profile?id=~Marzyeh_Ghassemi2), [Mikhail Yurochkin](http://openreview.net/profile?id=~Mikhail_Yurochkin1), [Justin Solomon](http://openreview.net/profile?id=~Justin_Solomon1)
  - **Affiliations:** MIT CSAIL, IBM Research; MIT-IBM Watson AI Lab, MIT CSAIL, University of Oxford, MIT CSAIL, MIT-IBM Watson AI Lab; IBM Research, MIT CSAIL, IBM Research; MIT-IBM Watson AI Lab, MIT CSAIL
  - **TL;DR:** This study investigates the asymmetry in the roles of low-rank adapter matrices in fine-tuning foundation models, demonstrating that fine-tuning the B matrix is more effective than fine-tuning the A matrix. The findings suggest that a randomly initialized A can perform nearly as well as a fine-tuned one, leading to practical improvements in parameter efficiency and generalization.
  - **Keywords:** Parameter-efficient fine-tuning, Low-Rank Adaptation (LoRA), Foundation models, Low-rank updates, Neural networks, Fine-tuning challenges, Model efficiency, Improved parameter efficiency, Generalization bounds, RoBERTa, BART-Large, LLaMA-2, ViTs, Asymmetry in LoRA matrices


- [Behavior Generation with Latent Actions](https://icml.cc/virtual/2024/poster/33379) (Spotlight Poster)
  - **Authors:** [Seungjae Lee](http://openreview.net/profile?id=~Seungjae_Lee2), [Yibin Wang](http://openreview.net/profile?id=~Yibin_Wang1), [Haritheja Etukuru](http://openreview.net/profile?id=~Haritheja_Etukuru1), [H. Jin Kim](http://openreview.net/profile?id=~H._Jin_Kim1), [Mahi Shafiullah](http://openreview.net/profile?id=~Nur_Muhammad_Mahi_Shafiullah1), [Lerrel Pinto](http://openreview.net/profile?id=~Lerrel_Pinto1)
  - **Affiliations:** New York University; Department of Aerospace Engineering, Seoul National University; Artificial Intelligence Institute of SNU, New York University, New York University, Department of Aerospace Engineering, Seoul National University; Artificial Intelligence Institute of SNU, New York University, New York University
  - **TL;DR:** This paper introduces the Vector-Quantized Behavior Transformer (VQ-BeT), a model designed for behavior generation that effectively handles multimodal action prediction and improves upon existing models like Behavior Transformers and Diffusion Policies. VQ-BeT demonstrates enhanced capabilities in capturing behavior modes while significantly accelerating inference speed.
  - **Keywords:** behavior generation, decision-making, Behavior Transformers (BeT), Vector-Quantized Behavior Transformer (VQ-BeT), k-means clustering, hierarchical vector quantization, simulated manipulation, autonomous driving, robotics, high-dimensional action spaces, long-range actions, multimodal action prediction, partial observations, improved ability to capture behavior modes, accelerated inference speed


- [Slicing Mutual Information Generalization Bounds for Neural Networks](https://icml.cc/virtual/2024/poster/32824) (Poster)
  - **Authors:** [Kimia Nadjahi](http://openreview.net/profile?id=~Kimia_Nadjahi1), [Kristjan Greenewald](http://openreview.net/profile?id=~Kristjan_Greenewald1), [Rickard Gabrielsson](http://openreview.net/profile?id=~Rickard_Br%C3%BCel_Gabrielsson1), [Justin Solomon](http://openreview.net/profile?id=~Justin_Solomon1)
  - **Affiliations:** MIT; MIT-IBM Watson AI Lab; IBM Research, None, None, None
  - **TL;DR:** This study introduces new information-theoretic generalization bounds for neural networks that leverage slicing of the parameter space, demonstrating improved generalization and computational efficiency. The findings suggest that model compressibility can be effectively controlled through a proposed regularization scheme, enhancing generalization without compromising performance.
  - **Keywords:** Generalization in machine learning, Information theory, Mutual information, Disintegrated mutual information, k-sliced mutual information, Rate-distortion theory, Neural networks, Machine learning algorithms, Generalization error, High-dimensional data, Model compressibility, Tighter generalization bounds, Regularization scheme for compressibility, Compression schemes, PAC-Bayes analysis


- [Position: Fundamental Limitations of LLM Censorship Necessitate New Approaches](https://icml.cc/virtual/2024/poster/33326) (Poster)
  - **Authors:** [David Glukhov](http://openreview.net/profile?id=~David_Glukhov1), [Ilia Shumailov](http://openreview.net/profile?id=~Ilia_Shumailov1), [Yarin Gal](http://openreview.net/profile?id=~Yarin_Gal1), [Nicolas Papernot](http://openreview.net/profile?id=~Nicolas_Papernot1), [Vardan Papyan](http://openreview.net/profile?id=~Vardan_Papyan1)
  - **Affiliations:** University of Toronto & Vector Institute, University of Oxford, University of Oxford, University of Toronto & Vector Institute, University of Toronto & Vector Institute
  - **TL;DR:** The paper discusses the fundamental limitations of current censorship methods for large language models (LLMs) and argues for a reevaluation of censorship goals, highlighting the challenges of verifying semantic properties and the potential for malicious use. It proposes an initial approach to censorship through syntactic methods, emphasizing the need for new definitions and strategies.
  - **Keywords:** Large Language Models (LLMs), Censorship, AI Safety, Reinforcement Learning with Human Feedback (RLHF), Fine-tuning, Semantic censorship, Misaligned behavior, Risks of malicious use, Syntactic censorship, New definitions and approaches to censorship, Compositional threats, Instruction following


- [CuTS: Customizable Tabular Synthetic Data Generation](https://icml.cc/virtual/2024/poster/33789) (Poster)
  - **Authors:** [Mark Vero](http://openreview.net/profile?id=~Mark_Vero1), [Mislav Balunovic](http://openreview.net/profile?id=~Mislav_Balunovic1), [Martin Vechev](http://openreview.net/profile?id=~Martin_Vechev1)
  - **Affiliations:** Department of Computer Science, ETH Zurich, Switzerland, Department of Computer Science, ETH Zurich, Switzerland, Department of Computer Science, ETH Zurich, Switzerland
  - **TL;DR:** This paper introduces CuTS, a customizable framework for generating synthetic tabular data that addresses privacy, data quality, and bias concerns. CuTS outperforms existing methods by allowing for a wide range of custom specifications while achieving higher accuracy in downstream tasks.
  - **Keywords:** synthetic data generation, data privacy, data quality, data sharing, customizable synthetic data framework, declarative statistical and logical expressions, differentially private training, finance, healthcare, privacy concerns, data quality issues, bias mitigation, truthfulness in data generation, CuTS framework, improved downstream accuracy, customization of synthetic data, Adult dataset, differential privacy (DP), fairness


- [diff History for Neural Language Agents](https://icml.cc/virtual/2024/poster/33977) (Poster)
  - **Authors:** [Ulyana Piterbarg](http://openreview.net/profile?id=~Ulyana_Piterbarg1), [Lerrel Pinto](http://openreview.net/profile?id=~Lerrel_Pinto1), [Rob Fergus](http://openreview.net/profile?id=~Rob_Fergus1)
  - **Affiliations:** Dept of Computer Science, Courant Institute, NYU, Dept of Computer Science, Courant Institute, NYU, Dept of Computer Science, Courant Institute, NYU
  - **TL;DR:** This paper introduces "diff history," a method that enhances the efficiency of neural language models in sequential decision-making by abstracting redundant information from environment observations. The approach demonstrates state-of-the-art performance in the NetHack video game while requiring significantly fewer training examples compared to previous methods.
  - **Keywords:** Neural Language Models, embodied control, sequential decision-making, diff history, Unix diff command, video games, AI agents, long and verbose textual prompts, limited observation size, interaction history, instruction finetuning, state-of-the-art performance, efficiency of low-sample instruction finetuning, NetHack, BabyAI-Text


- [Information Complexity of Stochastic Convex Optimization: Applications to Generalization, Memorization, and Tracing](https://icml.cc/virtual/2024/poster/34649) (Best Paper)
  - **Authors:** [Idan Attias](http://openreview.net/profile?id=~Idan_Attias1), [Gintare Karolina Dziugaite](http://openreview.net/profile?id=~Gintare_Karolina_Dziugaite1), [Mahdi Haghifam](http://openreview.net/profile?id=~Mahdi_Haghifam2), [Roi Livni](http://openreview.net/profile?id=~Roi_Livni1), [Daniel Roy](http://openreview.net/profile?id=~Daniel_M._Roy1)
  - **Affiliations:** Department of Computer Science, Ben-Gurion University; Vector Institute, Google DeepMind, Khoury College of Computer Sciences, Northeastern University, Department of Electrical Engineering, Tel Aviv University, Department of Statistical Sciences, University of Toronto; Vector Institute
  - **TL;DR:** This study explores the relationship between memorization and learning in stochastic convex optimization, revealing that a learning algorithm's accuracy is fundamentally linked to its conditional mutual information. The findings highlight the necessity of memorization for effective generalization and present implications for understanding generalization bounds in machine learning.
  - **Keywords:** Stochastic Convex Optimization, Generalization, Memorization, Conditional Mutual Information (CMI), Tradeoff between accuracy and memorization, limitations of generalization bounds, Characterization of CMI in relation to excess error, adversarial identification of training samples, L2 Lipschitz-bounded, Strong Convexity, Empirical Risk Minimizers (ERMs)


- [Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?](https://icml.cc/virtual/2024/poster/33095) (Poster)
  - **Authors:** [Khashayar Gatmiry](http://openreview.net/profile?id=~Khashayar_Gatmiry1), [Nikunj Saunshi](http://openreview.net/profile?id=~Nikunj_Saunshi1), [Sashank J. Reddi](http://openreview.net/profile?id=~Sashank_J._Reddi1), [Stefanie Jegelka](http://openreview.net/profile?id=~Stefanie_Jegelka3), [Sanjiv Kumar](http://openreview.net/profile?id=~Sanjiv_Kumar1)
  - **Affiliations:** MIT, Google Research, Google Research, MIT, Google Research
  - **TL;DR:** This study investigates whether looped Transformers can learn to implement multi-step gradient descent for in-context learning, demonstrating that the global minimizer of the training loss achieves this with a data-adaptive preconditioner. The findings reveal fast convergence for gradient flow despite the non-convex landscape, marking a significant theoretical advancement in understanding multi-layer Transformers.
  - **Keywords:** Transformers, in-context learning, multi-step algorithms, Gradient descent, preconditioned gradient descent, linear regression, Learnability of multi-step algorithms, convergence to algorithmic solutions, Fast convergence for gradient flow, theoretical analysis of multi-layer Transformers, Multi-layer model, weight sharing, gradient dominance condition


- [Smoothing Proximal Gradient Methods for Nonsmooth Sparsity Constrained Optimization: Optimality Conditions and Global Convergence](https://icml.cc/virtual/2024/poster/33721) (Poster)
  - **Authors:** [Ganzhao Yuan](http://openreview.net/profile?id=~Ganzhao_Yuan1)
  - **Affiliations:** Peng Cheng Laboratory, China
  - **TL;DR:** This paper presents Smoothing Proximal Gradient Methods (SPGM) for solving nonsmooth sparsity constrained optimization problems, demonstrating that the SPGM-BCD variant achieves stronger stationary points and improved convergence rates compared to existing methods. The findings indicate that these methods can effectively address the challenges posed by non-convex and NP-hard optimization problems in machine learning applications.
  - **Keywords:** Nonsmooth optimization, Sparsity constrained optimization, Machine learning, Smoothing Proximal Gradient Methods (SPGM), Iterative Hard Thresholding (SPGM-IHT), Block Coordinate Decomposition (SPGM-BCD), Sparse logistic regression, Sparse censored regression, Impulse noise removal, Sparse isotonic regression, Sparse quantile regression, Non-convex optimization, NP-hard problems, Cardinality constraint, Stronger stationary points, Convergence rates, Theoretical bounds


- [The Merit of River Network Topology for Neural Flood Forecasting](https://icml.cc/virtual/2024/poster/34095) (Poster)
  - **Authors:** [Nikolas Kirschstein](http://openreview.net/profile?id=~Nikolas_Kirschstein1), [Yixuan Sun](http://openreview.net/profile?id=~Yixuan_Sun3)
  - **Affiliations:** Mathematical Institute, University of Oxford, UK; Department of Informatics, Technical University of Munich, Germany, None
  - **TL;DR:** This study investigates the impact of river network topology on neural flood forecasting using GNNs, revealing that the model does not effectively utilize topology information and struggles with sudden discharge spikes. The findings suggest a broader phenomenon where neural prediction may not always benefit from graphical structures, prompting further investigation into the conditions affecting this outcome.
  - **Keywords:** flood forecasting, river discharge prediction, climate change, GNNs (Graph Neural Networks), LSTM (Long Short-Term Memory networks), riverine flood management, environmental monitoring, reliance on isolated forecasting, challenges in predicting sudden discharge spikes, evaluation of adjacency definitions in forecasting models, insights into neural prediction limitations, CAMELS-x, LamaH-CE


- [Breadth-First Exploration on Adaptive Grid for Reinforcement Learning](https://icml.cc/virtual/2024/poster/34989) (Poster)
  - **Authors:** [Youngsik Yoon](http://openreview.net/profile?id=~Youngsik_Yoon1), [Gangbok Lee](http://openreview.net/profile?id=~Gangbok_Lee1), [Sungsoo Ahn](http://openreview.net/profile?id=~Sungsoo_Ahn1), [Jungseul Ok](http://openreview.net/profile?id=~Jungseul_Ok2)
  - **Affiliations:** Department of CSE, POSTECH, Pohang, Republic of Korea, Graduate School of AI, POSTECH, Pohang, Republic of Korea, Department of CSE, POSTECH, Pohang, Republic of Korea; Graduate School of AI, POSTECH, Pohang, Republic of Korea, Department of CSE, POSTECH, Pohang, Republic of Korea; Graduate School of AI, POSTECH, Pohang, Republic of Korea
  - **TL;DR:** This study introduces Breadth-first Exploration on Adaptive Grid (BEAG) for goal-conditioned reinforcement learning, addressing the inefficiencies of previous graph-based planners by managing both achieved and unattained subgoals. The proposed method demonstrates improved exploration strategies, leading to successful outcomes in complex environments.
  - **Keywords:** Goal-conditioned reinforcement learning, Graph-based planners, Breadth-first exploration, Adaptive grid refinement, Shortest path algorithms, Navigation of robots, Object manipulation, Unattainable transitions, Inefficient exploration of subgoals, BEAG (Breadth-first Exploration on Adaptive Grid), Subgoals, Replay buffer, Hierarchical RL


- [Disguised Copyright Infringement of Latent Diffusion Models](https://icml.cc/virtual/2024/poster/33010) (Poster)
  - **Authors:** [Yiwei Lu](http://openreview.net/profile?id=~Yiwei_Lu1), [Matthew Yang](http://openreview.net/profile?id=~Matthew_Y._R._Yang1), [Zuoqiu Liu](http://openreview.net/profile?id=~Zuoqiu_Liu1), [Gautam Kamath](http://openreview.net/profile?id=~Gautam_Kamath1), [Yaoliang Yu](http://openreview.net/profile?id=~Yaoliang_Yu1)
  - **Affiliations:** School of Computer Science, University of Waterloo; Vector Institute, School of Computer Science, University of Waterloo, School of Computer Science, University of Waterloo, School of Computer Science, University of Waterloo; Vector Institute, School of Computer Science, University of Waterloo; Vector Institute
  - **TL;DR:** This paper investigates the issue of disguised copyright infringement in Latent Diffusion Models, demonstrating how such models can be trained on disguised copyrighted material that is visually different yet retains similar latent information. The authors propose methods for generating and detecting these disguises, highlighting the inadequacy of current auditing tools in identifying indirect access to copyrighted data.
  - **Keywords:** copyright infringement, generative models, Latent Diffusion Models, concealed copyright infringement, access to copyrighted material, substantial similarity, disguises generation algorithm, detection of disguised samples, LAION-5B, textual inversion, Latent Diffusion Models (LDM)


- [Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training](https://icml.cc/virtual/2024/poster/33214) (Poster)
  - **Authors:** [Ming-Kun Xie](http://openreview.net/profile?id=~Ming-Kun_Xie1), [Jia-Hao Xiao](http://openreview.net/profile?id=~Jia-Hao_Xiao1), [Pei Peng](http://openreview.net/profile?id=~Pei_Peng1), [Gang Niu](http://openreview.net/profile?id=~Gang_Niu1), [Masashi Sugiyama](http://openreview.net/profile?id=~Masashi_Sugiyama1), [Sheng-Jun Huang](http://openreview.net/profile?id=~Sheng-Jun_Huang1)
  - **Affiliations:** Nanjing University of Aeronautics and Astronautics, Nanjing, China, Nanjing University of Aeronautics and Astronautics, Nanjing, China, Nanjing University of Aeronautics and Astronautics, Nanjing, China, RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; The University of Tokyo, Tokyo, Japan, RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; The University of Tokyo, Tokyo, Japan, Nanjing University of Aeronautics and Astronautics, Nanjing, China
  - **TL;DR:** This study addresses the challenges of multi-label image classification by proposing a counterfactual reasoning method that enhances model performance while mitigating overfitting caused by co-occurring objects. The results demonstrate that the proposed patching-based training approach achieves state-of-the-art performance across various benchmark datasets.
  - **Keywords:** Multi-label image classification, causal inference, Counterfactual reasoning, patching-based training, Image recognition, Model overfitting, co-occurrence relationships, State-of-the-art performance, direct effect measurement, Benchmark datasets


- [Predictive Linear Online Tracking for Unknown Targets](https://icml.cc/virtual/2024/poster/33212) (Spotlight Poster)
  - **Authors:** [Anastasios Tsiamis](http://openreview.net/profile?id=~Anastasios_Tsiamis1), [Aren Karapetyan](http://openreview.net/profile?id=~Aren_Karapetyan1), [Yueshan Li](http://openreview.net/profile?id=~Yueshan_Li1), [Efe C. Balta](http://openreview.net/profile?id=~Efe_C._Balta1), [John Lygeros](http://openreview.net/profile?id=~John_Lygeros1)
  - **Affiliations:** Automatic Control Laboratory, ETH Zürich, Zürich, Switzerland, Automatic Control Laboratory, ETH Zürich, Zürich, Switzerland, Institute for Dynamic Systems and Control, ETH Zürich, Zürich, Switzerland, Control and Automation Group, inspire AG, Zürich, Switzerland, Automatic Control Laboratory, ETH Zürich, Zürich, Switzerland
  - **TL;DR:** This paper presents a novel algorithm called predictive linear online tracking (PLOT) for tracking unknown, non-stationary targets in linear control systems. The method effectively learns a dynamic model of the target using recursive least squares and demonstrates successful implementation on a real quadrotor, showcasing its practical applicability.
  - **Keywords:** online tracking, linear control systems, non-stationary targets, predictive linear online tracking (PLOT), recursive least squares, exponential forgetting, receding horizon control, environmental monitoring, agriculture, air shows, autonomous agents, unknown target dynamics, time-varying targets, adversarial target tracking, dynamic regret analysis, implementation on real hardware (quadrotor), Linear Quadratic Tracking (LQT), autoregressive (AR) dynamics


- [Bifurcated Attention for Single-Context Large-Batch Sampling](https://icml.cc/virtual/2024/poster/34377) (Poster)
  - **Authors:** [Ben Athiwaratkun](http://openreview.net/profile?id=~Ben_Athiwaratkun1), [Sujan Kumar Gonugondla](http://openreview.net/profile?id=~Sujan_Kumar_Gonugondla1), [Sanjay Krishna Gouda](http://openreview.net/profile?id=~Sanjay_Krishna_Gouda1), [Haifeng Qian](http://openreview.net/profile?id=~Haifeng_Qian1), [Hantian Ding](http://openreview.net/profile?id=~Hantian_Ding1), [Qing Sun](http://openreview.net/profile?id=~Qing_Sun2), [Jun Wang](http://openreview.net/profile?id=~Jun_Wang32), [Jiacheng Guo](http://openreview.net/profile?id=~Jiacheng_Guo4), [Liangfu Chen](http://openreview.net/profile?id=~Liangfu_Chen1), [parminder bhatia](http://openreview.net/profile?id=~Parminder_Bhatia1), [Ramesh M Nallapati](http://openreview.net/profile?id=~Ramesh_Nallapati1), [Sudipta Sengupta](http://openreview.net/profile?id=~Sudipta_Sengupta1), [Bing Xiang](http://openreview.net/profile?id=~Bing_Xiang2)
  - **Affiliations:** Together.ai (work conducted at AWS), AWS NGDE Science, AWS NGDE Science, AWS NGDE Science, AWS NGDE Science, AWS NGDE Science, AWS NGDE Science, AWS NGDE Science, AWS NGDE Science, GE HealthCare (work conducted at AWS), Amazon AGI (work conducted at AWS), AWS NGDE Science, Goldman Sachs (work conducted at AWS)
  - **TL;DR:** This study introduces bifurcated attention, a method designed to enhance language model inference efficiency in single-context batch sampling by reducing memory IO costs. The approach leads to lower latency and improved performance in real-time applications, making it suitable for generating multiple completions from a single context.
  - **Keywords:** bifurcated attention, language model inference, single-context batch sampling, multi-query attention, multi-head attention, GEMM operations, real-time applications, code-editing IDE tools, inference latency, memory IO costs, latency bottleneck, reduced memory IO, improved efficiency, lower latency, large language models (LLMs), KV cache


- [Learning Latent Dynamic Robust Representations for World Models](https://icml.cc/virtual/2024/poster/34700) (Poster)
  - **Authors:** [Ruixiang Sun](http://openreview.net/profile?id=~Ruixiang_Sun1), [Hongyu Zang](http://openreview.net/profile?id=~Hongyu_Zang1), [Xin Li](http://openreview.net/profile?id=~Xin_Li31), [Riashat Islam](http://openreview.net/profile?id=~Riashat_Islam1)
  - **Affiliations:** Beijing Institute of Technology, China, Beijing Institute of Technology, China, Beijing Institute of Technology, China, DreamFold AI, Canada
  - **TL;DR:** This study presents a Hybrid Recurrent State-Space Model (HRSSM) to enhance the robustness of state representations in Model-Based Reinforcement Learning (MBRL) by effectively filtering out irrelevant noise and improving performance in visually complex control tasks. The proposed methods demonstrate significant improvements over existing MBRL approaches, particularly in environments with exogenous distractions.
  - **Keywords:** Model-Based Reinforcement Learning (MBRL), World Models, Hybrid Recurrent State-Space Model (HRSSM), spatio-temporal masking, bisimulation principle, latent reconstruction, Visually complex control tasks, simulation and forecasting in reinforcement learning, Exogenous noise in observation space, instability in joint training of representations, dynamics, and policy, Improved performance in control tasks, robust state representation, Maniskill, Matterport, Recurrent State-Space Model (RSSM), Dreamer algorithms


- [Target Networks and Over-parameterization Stabilize Off-policy Bootstrapping with Function Approximation](https://icml.cc/virtual/2024/poster/34061) (Spotlight Poster)
  - **Authors:** [Fengdi Che](http://openreview.net/profile?id=~Fengdi_Che1), [Chenjun Xiao](http://openreview.net/profile?id=~Chenjun_Xiao1), [Jincheng Mei](http://openreview.net/profile?id=~Jincheng_Mei1), [Bo Dai](http://openreview.net/profile?id=~Bo_Dai1), [Ramki Gummadi](http://openreview.net/profile?id=~Ramki_Gummadi1), [Oscar Ramirez](http://openreview.net/profile?id=~Oscar_A_Ramirez1), [Christopher Harris](http://openreview.net/profile?id=~Christopher_K_Harris1), [Rupam Mahmood](http://openreview.net/profile?id=~A._Rupam_Mahmood1), [Dale Schuurmans](http://openreview.net/profile?id=~Dale_Schuurmans1)
  - **Affiliations:** Department of Computing Science, University of Alberta, School of Data Science, The Chinese University of Hong Kong, Shenzhen, Google DeepMind, School of Computational Science and Engineering, Georgia Tech; Google, Google DeepMind, Uber, None, Department of Computing Science, University of Alberta, Department of Computing Science, University of Alberta; Google DeepMind
  - **TL;DR:** This study establishes that combining target networks with over-parameterized linear function approximation can ensure convergence in off-policy bootstrapped value estimation, addressing challenges posed by offline data. The findings provide high-probability error bounds and extend to control settings like Q-learning.
  - **Keywords:** Off-policy value evaluation, Temporal difference estimation, Target networks, Over-parameterized linear function approximation, Q-learning, Self-driving vehicles, Healthcare, Off-policy data challenges, Divergence in TD algorithms, Data sparsity, Convergence conditions for TD with offline data, High-probability value estimation error bounds, Bellman equation, Deadly triad, L2-regularization, Baird’s counterexample


- [Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making](https://icml.cc/virtual/2024/poster/35032) (Poster)
  - **Authors:** [Parand A. Alamdari](http://openreview.net/profile?id=~Parand_A._Alamdari1), [Toryn Q. Klassen](http://openreview.net/profile?id=~Toryn_Q._Klassen1), [Elliot Creager](http://openreview.net/profile?id=~Elliot_Creager1), [Sheila McIlraith](http://openreview.net/profile?id=~Sheila_A._McIlraith1)
  - **Affiliations:** University of Toronto, Toronto, Canada; Vector Institute, Toronto, Canada; Schwartz Reisman Institute for Technology and Society, Toronto, Canada, University of Toronto, Toronto, Canada; Vector Institute, Toronto, Canada; Schwartz Reisman Institute for Technology and Society, Toronto, Canada, University of Waterloo, Waterloo, Canada, University of Toronto, Toronto, Canada; Vector Institute, Toronto, Canada; Schwartz Reisman Institute for Technology and Society, Toronto, Canada
  - **TL;DR:** This study investigates non-Markovian fairness in sequential decision making, emphasizing the importance of historical context and intermediate assessments of fairness. The authors introduce the FairQCM algorithm to enhance the synthesis of fair policies through improved sample efficiency in reinforcement learning.
  - **Keywords:** fairness, sequential decision making, non-Markovian fairness, FairQCM algorithm, reinforcement learning, vaccine distribution, decision-making processes, fairness assessment over time, stakeholder impact, properties of non-Markovian fairness, long-term fairness


- [Clifford-Steerable Convolutional Neural Networks](https://icml.cc/virtual/2024/poster/33823) (Poster)
  - **Authors:** [Maksim Zhdanov](http://openreview.net/profile?id=~Maksim_Zhdanov1), [David Ruhe](http://openreview.net/profile?id=~David_Ruhe1), [Maurice Weiler](http://openreview.net/profile?id=~Maurice_Weiler1), [Ana Lucic](http://openreview.net/profile?id=~Ana_Lucic1), [Johannes Brandstetter](http://openreview.net/profile?id=~Johannes_Brandstetter1), [Patrick Forré](http://openreview.net/profile?id=~Patrick_Forr%C3%A91)
  - **Affiliations:** AMLab, Informatics Institute, University of Amsterdam; AI4Science Lab, Informatics Institute, University of Amsterdam; Anton Pannekoek Institute for Astronomy, University of Amsterdam, AMLab, Informatics Institute, University of Amsterdam; AI4Science Lab, Informatics Institute, University of Amsterdam; Anton Pannekoek Institute for Astronomy, University of Amsterdam, AMLab, Informatics Institute, University of Amsterdam; AI4Science Lab, Informatics Institute, University of Amsterdam; Anton Pannekoek Institute for Astronomy, University of Amsterdam, AI4Science, Microsoft Research, ELLIS Unit Linz, Institute for Machine Learning, JKU Linz, Austria; NXAI GmbH, AMLab, Informatics Institute, University of Amsterdam; AI4Science Lab, Informatics Institute, University of Amsterdam
  - **TL;DR:** This study introduces Clifford-Steerable Convolutional Neural Networks (CS-CNNs) that are E(p, q)-equivariant and effectively process multivector fields on pseudo-Euclidean spaces. The proposed method significantly outperforms baseline approaches in predicting fluid dynamics and relativistic electrodynamics.
  - **Keywords:** Clifford-Steerable Convolutional Neural Networks, E(p, q)-equivariance, multivector fields, O(p, q)-steerable kernels, Clifford group equivariant neural networks, Fluid dynamics forecasting, relativistic electrodynamics simulations, Equivariance in neural networks, symmetry in physical systems, Implementation of implicit O(p, q)-steerable kernels, performance improvement over baseline methods, Clifford algebra, pseudo-Euclidean spaces, Maxwell equation, Dirac equation, General Relativity


- [Learning to Model the World With Language](https://icml.cc/virtual/2024/poster/34876) (Oral)
  - **Authors:** [Jessy Lin](http://openreview.net/profile?id=~Jessy_Lin1), [Yuqing Du](http://openreview.net/profile?id=~Yuqing_Du1), [Olivia Watkins](http://openreview.net/profile?id=~Olivia_Watkins1), [Danijar Hafner](http://openreview.net/profile?id=~Danijar_Hafner1), [Pieter Abbeel](http://openreview.net/profile?id=~Pieter_Abbeel2), [Dan Klein](http://openreview.net/profile?id=~Dan_Klein1), [Anca Dragan](http://openreview.net/profile?id=~Anca_Dragan1)
  - **Affiliations:** UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley, UC Berkeley
  - **TL;DR:** The study presents Dynalang, an agent designed to leverage diverse language for predicting future states in the environment, enhancing its ability to interact naturally with humans. Key findings indicate that Dynalang excels in tasks by integrating language inputs continuously, improving performance over traditional language-conditioned policies.
  - **Keywords:** language understanding, future prediction, multimodal interaction, Dynalang, multimodal world model, DreamerV3 algorithm, game-playing, home navigation, integration of diverse language with vision and action, complex dependencies between language and actions, generative model for language and vision, self-supervised learning


- [BWS: Best Window Selection Based on Sample Scores for Data Pruning across Broad Ranges](https://icml.cc/virtual/2024/poster/33080) (Poster)
  - **Authors:** [Hoyong Choi](http://openreview.net/profile?id=~Hoyong_Choi1), [Nohyun Ki](http://openreview.net/profile?id=~Nohyun_Ki1), [Hye Won Chung](http://openreview.net/profile?id=~Hye_Won_Chung2)
  - **Affiliations:** Samsung Research, Seoul, South Korea, School of Electronic Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, School of Electronic Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea
  - **TL;DR:** This paper introduces Best Window Selection (BWS), a universal method for data subset selection that efficiently identifies informative subsets from large datasets. Experimental results show that BWS outperforms existing methods across various selection ratios, enhancing training efficiency for neural networks.
  - **Keywords:** Data subset selection, Neural networks, Large-scale datasets, Best Window Selection (BWS), Kernel ridge regression, Image classification, Data pruning, High computational costs, Storage requirements, Performance approximation, Efficient data subset selection method, Competitive performance across selection ratios, CIFAR-10, CIFAR-100, ImageNet, Score-based selection, Optimization-based selection


- [Diffusion Language Models Are Versatile Protein Learners](https://icml.cc/virtual/2024/poster/34203) (Poster)
  - **Authors:** [Xinyou Wang](http://openreview.net/profile?id=~Xinyou_Wang1), [Zaixiang Zheng](http://openreview.net/profile?id=~Zaixiang_Zheng2), [Fei YE](http://openreview.net/profile?id=~Fei_YE4), [Dongyu Xue](http://openreview.net/profile?id=~Dongyu_Xue1), [Shujian Huang](http://openreview.net/profile?id=~Shujian_Huang1), [Quanquan Gu](http://openreview.net/profile?id=~Quanquan_Gu1)
  - **Affiliations:** Dept. of Computer Science, Nanjing University; ByteDance Research, ByteDance Research, ByteDance Research, ByteDance Research, Dept. of Computer Science, Nanjing University; ByteDance Research, ByteDance Research
  - **TL;DR:** This study presents the diffusion protein language model (DPLM), which excels in generating and understanding protein sequences through a novel generative self-supervised framework. DPLM demonstrates superior performance in both generative and predictive tasks compared to existing models, highlighting its versatility in protein research.
  - **Keywords:** protein language models, generative modeling, predictive tasks, diffusion probabilistic framework, generative self-supervised learning, protein sequence generation, protein structure prediction, limitations of masked prediction and autoregression in protein modeling, diffusion protein language model (DPLM), improved understanding of protein sequences


- [Dynamic Metric Embedding into lp Space](https://icml.cc/virtual/2024/poster/32645) (Poster)
  - **Authors:** [Kiarash Banihashem](http://openreview.net/profile?id=~Kiarash_Banihashem1), [MohammadTaghi Hajiaghayi](http://openreview.net/profile?id=~MohammadTaghi_Hajiaghayi1), [Dariusz Kowalski](http://openreview.net/profile?id=~Dariusz_Rafal_Kowalski1), [Jan Olkowski](http://openreview.net/profile?id=~Jan_Olkowski1), [Max Springer](http://openreview.net/profile?id=~Max_Springer1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Maryland, College Park, USA, School of Computer and Cyber Sciences, Augusta University, Georgia, USA, Department of Computer Science, University of Maryland, College Park, USA, Department of Mathematics, University of Maryland, College Park, USA
  - **TL;DR:** This paper presents a dynamic algorithm for embedding weighted, undirected graphs into ℓp space with low distortion, achieving expected distortion of O(log²n) while handling edge weight increases. The results extend previous work in static settings to dynamic scenarios, highlighting the challenges of maintaining low distortion during graph modifications.
  - **Keywords:** dynamic metric embedding, low distortion embedding, randomized mapping, dynamic algorithms, graph theory, metric spaces, edge weight increases, dynamic sequence of edge updates, maintaining low distortion, dynamic algorithm with expected distortion O(log²n), total update time analysis, ℓp space, distortion, randomized decompositions


- [Graph2Tac: Online Representation Learning of Formal Math Concepts](https://icml.cc/virtual/2024/poster/34785) (Poster)
  - **Authors:** [Lasse Blaauwbroek](http://openreview.net/profile?id=~Lasse_Blaauwbroek1), [Mirek Olšák](http://openreview.net/profile?id=~Mirek_Ol%C5%A1%C3%A1k1), [Jason Rute](http://openreview.net/profile?id=~Jason_Rute1), [Fidel I. Schaposnik Massolo](http://openreview.net/profile?id=~Fidel_Ivan_Schaposnik_Massolo1), [Jelle Piepenbrock](http://openreview.net/profile?id=~Jelle_Piepenbrock1), [Vasily Pestun](http://openreview.net/profile?id=~Vasily_Pestun1)
  - **Affiliations:** IHES, University of Cambridge, IBM Research, IHES, Radboud University; Czech Technical University in Prague, IHES; IBM Research
  - **TL;DR:** This study presents two online solvers, a k-nearest neighbor solver and a graph neural network called Graph2Tac, that significantly improve theorem proving in the Coq proof assistant by leveraging online representation learning. The solvers demonstrate substantial performance gains over offline methods, highlighting the importance of locality in formal mathematical concepts.
  - **Keywords:** online representation learning, formal mathematics, proof assistants, k-nearest neighbor solver, graph neural network, hierarchical representations, Coq proof assistant, theorem proving, online learning in unseen mathematical settings, improving theorem proving efficiency, significant improvements in theorem proving, complementary online solvers, Tactician platform, Coq, locality property, formal mathematical concepts


- [Data Engineering for Scaling Language Models to 128K Context](https://icml.cc/virtual/2024/poster/33969) (Poster)
  - **Authors:** [Yao Fu](http://openreview.net/profile?id=~Yao_Fu3), [Rameswar Panda](http://openreview.net/profile?id=~Rameswar_Panda1), [Xinyao Niu](http://openreview.net/profile?id=~Xinyao_Niu1), [Xiang Yue](http://openreview.net/profile?id=~Xiang_Yue1), [Hannaneh Hajishirzi](http://openreview.net/profile?id=~Hannaneh_Hajishirzi1), [Yoon Kim](http://openreview.net/profile?id=~Yoon_Kim1), [Hao Peng](http://openreview.net/profile?id=~Hao_Peng4)
  - **Affiliations:** University Edinburgh, MIT-IBM Watson AI Lab, University of Melbourne, CMU, University of Washington, Massachusetts Institute of Technology, University of Illinois at Urbana-Champaign
  - **TL;DR:** This study explores data engineering methods for scaling language models to utilize a context length of 128K tokens through continual pretraining on a balanced data mixture. The findings indicate that a relatively small amount of high-quality data can significantly enhance the model's ability to retrieve information across long contexts, outperforming existing models and approaching the performance of leading models like GPT-4.
  - **Keywords:** long context modeling, continual pretraining, data engineering, language models, multi-document question answering, autonomous agents, scaling context lengths, information retrieval in long contexts, effective continual pretraining strategies, performance improvement over existing models, 128K context, Needle-in-a-Haystack test, LLaMA-2


- [Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making](https://icml.cc/virtual/2024/poster/32712) (Poster)
  - **Authors:** [Vivek Myers](http://openreview.net/profile?id=~Vivek_Myers1), [Chongyi Zheng](http://openreview.net/profile?id=~Chongyi_Zheng1), [Anca Dragan](http://openreview.net/profile?id=~Anca_Dragan1), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1), [Benjamin Eysenbach](http://openreview.net/profile?id=~Benjamin_Eysenbach1)
  - **Affiliations:** University of California, Berkeley, Princeton University, University of California, Berkeley, University of California, Berkeley, Princeton University
  - **TL;DR:** This paper introduces a method for defining temporal distances in stochastic settings that satisfies the triangle inequality, enhancing the ability to generalize and find shortest paths in reinforcement learning. The proposed approach demonstrates improved learning efficiency and combinatorial generalization compared to previous methods.
  - **Keywords:** temporal distances, reinforcement learning, stochastic settings, contrastive learning, quasimetrics, dynamic programming, planning, control, autonomous vehicles, healthcare systems, triangle inequality, stochastic transitions, generalization, new temporal distance metrics, combinatorial generalization


- [The Emergence of Reproducibility and Consistency in Diffusion Models](https://icml.cc/virtual/2024/poster/34446) (Poster)
  - **Authors:** [Huijie Zhang](http://openreview.net/profile?id=~Huijie_Zhang2), [Jinfan Zhou](http://openreview.net/profile?id=~Jinfan_Zhou2), [Yifu Lu](http://openreview.net/profile?id=~Yifu_Lu1), [Minzhe Guo](http://openreview.net/profile?id=~Minzhe_Guo1), [Peng Wang](http://openreview.net/profile?id=~Peng_Wang23), [Liyue Shen](http://openreview.net/profile?id=~Liyue_Shen1), [Qing Qu](http://openreview.net/profile?id=~Qing_Qu2)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, Location, Ann Arbor, MI 48109-2122, USA, Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, Location, Ann Arbor, MI 48109-2122, USA, Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, Location, Ann Arbor, MI 48109-2122, USA, Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, Location, Ann Arbor, MI 48109-2122, USA, Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, Location, Ann Arbor, MI 48109-2122, USA, Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, Location, Ann Arbor, MI 48109-2122, USA, Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, Location, Ann Arbor, MI 48109-2122, USA
  - **TL;DR:** This study investigates the phenomenon of "consistent model reproducibility" in diffusion models, revealing that different models trained on the same dataset produce similar outputs when using the same noise input. The findings indicate that diffusion models can learn distinct distributions based on training data size, highlighting their generalization capabilities across various applications.
  - **Keywords:** diffusion models, consistent model reproducibility, denoising diffusion probabilistic models (DDPM), deterministic ODE sampler, image generation, image-to-image translation, text-to-image synthesis, solving inverse problems, model overfitting, learning underlying data distribution, generalization property of diffusion models, distinct distributions influenced by training data size, CIFAR-10


- [Stealing part of a production language model](https://icml.cc/virtual/2024/poster/33922) (Best Paper)
  - **Authors:** [Nicholas Carlini](http://openreview.net/profile?id=~Nicholas_Carlini1), [Daniel Paleka](http://openreview.net/profile?id=~Daniel_Paleka1), [Krishnamurthy Dvijotham](http://openreview.net/profile?id=~Krishnamurthy_Dj_Dvijotham1), [Thomas Steinke](http://openreview.net/profile?id=~Thomas_Steinke2), [Jonathan Hayase](http://openreview.net/profile?id=~Jonathan_Hayase2), [A. Feder Cooper](http://openreview.net/profile?id=~A._Feder_Cooper1), [Katherine Lee](http://openreview.net/profile?id=~Katherine_Lee1), [Matthew Jagielski](http://openreview.net/profile?id=~Matthew_Jagielski1), [Milad Nasr](http://openreview.net/profile?id=~Milad_Nasr2), [Arthur Conmy](http://openreview.net/profile?id=~Arthur_Conmy1), [Eric Wallace](http://openreview.net/profile?id=~Eric_Wallace1), [David Rolnick](http://openreview.net/profile?id=~David_Rolnick1), [Florian Tramer](http://openreview.net/profile?id=~Florian_Tram%C3%A8r1)
  - **Affiliations:** Google DeepMind, ETH Zurich, Google DeepMind, Google DeepMind, University of Washington, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, OpenAI, McGill University, ETH Zurich
  - **TL;DR:** This paper presents a novel model-stealing attack that successfully extracts the embedding projection layer from black-box language models like OpenAI's ChatGPT and Google's PaLM-2. The findings reveal hidden dimensions of these models and raise concerns about the security of production language models exposed via APIs.
  - **Keywords:** model-stealing attack, black-box language models, transformer model, embedding projection layer, information extraction from production models, model secrecy, recovery of embedding dimension, estimation of query costs for model extraction, hidden dimension, logit vector, low-rank projection


- [EvoluNet: Advancing Dynamic Non-IID Transfer Learning on Graphs](https://icml.cc/virtual/2024/poster/33670) (Poster)
  - **Authors:** [Haohui Wang](http://openreview.net/profile?id=~Haohui_Wang1), [Yuzhen Mao](http://openreview.net/profile?id=~Yuzhen_Mao2), [Yujun Yan](http://openreview.net/profile?id=~Yujun_Yan1), [Yaoqing Yang](http://openreview.net/profile?id=~Yaoqing_Yang1), [Jianhui Sun](http://openreview.net/profile?id=~Jianhui_Sun1), [Kevin Choi](http://openreview.net/profile?id=~Kevin_Choi1), [Balaji Veeramani](http://openreview.net/profile?id=~Balaji_Veeramani1), [Alison Hu](http://openreview.net/profile?id=~Alison_Hu1), [Edward Bowen](http://openreview.net/profile?id=~Edward_Bowen1), [Tyler Cody](http://openreview.net/profile?id=~Tyler_Cody1), [Dawei Zhou](http://openreview.net/profile?id=~Dawei_Zhou1)
  - **Affiliations:** Department of Computer Science, Virginia Tech, Blacksburg, VA, USA, Department of Computer Science, Virginia Tech, Blacksburg, VA, USA, Department of Computer Science, Dartmouth College, Hanover, NH, USA, Department of Computer Science, Dartmouth College, Hanover, NH, USA, Department of Computer Science, Virginia Tech, Blacksburg, VA, USA, Deloitte & Touche LLP, USA, Deloitte & Touche LLP, USA, Deloitte & Touche LLP, USA, Deloitte & Touche LLP, USA, Virginia Tech National Security Institute, Arlington, VA, USA, Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Virginia Tech National Security Institute, Arlington, VA, USA
  - **TL;DR:** This study introduces EVOLU NET, a framework for dynamic non-IID transfer learning on graphs, addressing the challenges of domain evolution and discrepancy. The proposed method outperforms existing models by up to 12.1%, demonstrating its effectiveness in knowledge transfer between dynamic source and target graphs.
  - **Keywords:** Non-IID transfer learning, dynamic graphs, Transformer-based temporal encoding, dynamic domain unification, Graph-structured data, machine learning, Domain evolution, dynamic discrepancy between source and target domains, Generalization bound for dynamic non-IID transfer learning, improved generalization performance


- [Localizing Task Information for Improved Model Merging and Compression](https://icml.cc/virtual/2024/poster/34624) (Poster)
  - **Authors:** [Ke Wang](http://openreview.net/profile?id=~Ke_Wang19), [Nikolaos Dimitriadis](http://openreview.net/profile?id=~Nikolaos_Dimitriadis1), [Guillermo Ortiz-Jimenez](http://openreview.net/profile?id=~Guillermo_Ortiz-Jimenez1), [François Fleuret](http://openreview.net/profile?id=~Fran%C3%A7ois_Fleuret2), [Pascal Frossard](http://openreview.net/profile?id=~Pascal_Frossard1)
  - **Affiliations:** Ecole Polytechnique Federale de Lausanne, Lausanne, Switzerland, Ecole Polytechnique Federale de Lausanne, Lausanne, Switzerland, Google DeepMind; Work done while at EPFL, University of Geneva, Geneva, Switzerland, Ecole Polytechnique Federale de Lausanne, Lausanne, Switzerland
  - **TL;DR:** This study introduces TALL-masks and Consensus Merging to effectively merge multiple single-task models into a multi-task model while preserving task-specific information and significantly reducing storage requirements. The proposed methods achieve over 99% accuracy retention from individual models and improve overall performance in multi-task settings.
  - **Keywords:** model merging, multi-task learning, task arithmetic, TALL-masks, Consensus Merging, vision, natural language processing (NLP), performance loss in model merging, weight interference, task-specific feature erasure, improved model merging performance, significant storage reduction, selfish weights, catastrophic weights


- [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://icml.cc/virtual/2024/poster/35145) (Poster)
  - **Authors:** [Fuzhao Xue](http://openreview.net/profile?id=~Fuzhao_Xue1), [Zian Zheng](http://openreview.net/profile?id=~Zian_Zheng1), [Yao Fu](http://openreview.net/profile?id=~Yao_Fu3), [Jinjie Ni](http://openreview.net/profile?id=~Jinjie_Ni1), [Zangwei Zheng](http://openreview.net/profile?id=~Zangwei_Zheng1), [Wangchunshu Zhou](http://openreview.net/profile?id=~Wangchunshu_Zhou1), [Yang You](http://openreview.net/profile?id=~Yang_You1)
  - **Affiliations:** National University of Singapore, National University of Singapore, University of Edinburgh, National University of Singapore, National University of Singapore, ETH Zurich, National University of Singapore
  - **TL;DR:** The study introduces OpenMoE, a series of open-sourced Mixture-of-Experts language models, demonstrating their cost-effectiveness compared to dense models. Key findings include insights into routing mechanisms and strategies for improving MoE LLM designs, aiming to advance the open-source MoE community.
  - **Keywords:** Mixture-of-Experts (MoE), Large Language Models (LLMs), Decoder-only MoE models, Routing mechanisms, Natural Language Processing (NLP), Open-source LLM development, Computational expense of LLMs, Performance degradation in sequential tasks, Cost-effectiveness trade-off, Insights into routing mechanisms, Strategies for MoE LLM improvement, WildChat dataset, Trillion-level diverse datasets


- [Sub-token ViT Embedding via Stochastic Resonance Transformers](https://icml.cc/virtual/2024/poster/34934) (Poster)
  - **Authors:** [Dong Lao](http://openreview.net/profile?id=~Dong_Lao1), [Yangchao Wu](http://openreview.net/profile?id=~Yangchao_Wu1), [Tian Yu Liu](http://openreview.net/profile?id=~Tian_Yu_Liu2), [Alex Wong](http://openreview.net/profile?id=~Alex_Wong2), [Stefano Soatto](http://openreview.net/profile?id=~Stefano_Soatto1)
  - **Affiliations:** UCLA Vision Lab, UCLA Vision Lab, UCLA Vision Lab, Yale Vision Lab, UCLA Vision Lab
  - **TL;DR:** This study introduces the Stochastic Resonance Transformer (SRT), a training-free method that enhances Vision Transformer (ViT) architectures by performing sub-token spatial transformations to improve spatial detail representation. The method consistently boosts performance across various tasks, achieving up to a 14.9% improvement without fine-tuning.
  - **Keywords:** Vision Transformers, spatial quantization, fine-grained inference, Stochastic Resonance, sub-token spatial transformations, aggregation of ViT features, Segmentation, classification, depth estimation, Coarse spatial tokenization, limitations of fixed quantization, Stochastic Resonance Transformer (SRT), performance improvement by up to 14.9%, ViT (Vision Transformer), anti-aliasing, sub-token embedding


- [Detecting and Identifying Selection Structure in Sequential Data](https://icml.cc/virtual/2024/poster/34982) (Poster)
  - **Authors:** [Yujia Zheng](http://openreview.net/profile?id=~Yujia_Zheng1), [Zeyu Tang](http://openreview.net/profile?id=~Zeyu_Tang1), [Yiwen Qiu](http://openreview.net/profile?id=~Yiwen_Qiu1), [Bernhard Schölkopf](http://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1), [Kun Zhang](http://openreview.net/profile?id=~Kun_Zhang1)
  - **Affiliations:** Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, Max Planck Institute for Intelligent Systems, Carnegie Mellon University; MBZUAI
  - **TL;DR:** This study explores the causal structure of selection in sequential data, arguing that understanding selection is crucial for accurate statistical analysis and modeling. The authors present a nonparametric method to identify selection structures, demonstrating its applicability through empirical validation on synthetic and real-world music data.
  - **Keywords:** Selection structure, Sequential data, Causal inference, Nonparametric identifiability, Algorithm for detecting selection structures, Music composition, Statistical analysis, Bias in statistical inference, Overcomplicated dependence models, Identification of selection structures, Insights into generative processes, Synthetic data, Real-world music, Latent variables, Autoregressive structure, Latent confounders


- [Enhancing Size Generalization in Graph Neural Networks through Disentangled Representation Learning](https://icml.cc/virtual/2024/poster/35203) (Poster)
  - **Authors:** [Zheng Huang](http://openreview.net/profile?id=~Zheng_Huang2), [Qihui Yang](http://openreview.net/profile?id=~Qihui_Yang1), [Dawei Zhou](http://openreview.net/profile?id=~Dawei_Zhou1), [Yujun Yan](http://openreview.net/profile?id=~Yujun_Yan1)
  - **Affiliations:** Department of Computer Science, Dartmouth College, Hanover, NH, USA, Electrical and Computer Engineering, UCSD, San Diego, USA, Department of Computer Science, Virginia Tech, Blacksburg, VA, USA, Department of Computer Science, Dartmouth College, Hanover, NH, USA
  - **TL;DR:** This study introduces DISGEN, a model-agnostic framework aimed at enhancing size generalization in Graph Neural Networks by disentangling size factors from graph representations. Empirical results demonstrate that DISGEN improves classification performance on larger graphs by up to 6% compared to state-of-the-art models.
  - **Keywords:** Graph Neural Networks, Size Generalization, Disentangled Representation Learning, Augmentation Strategies, Performance Decline on Larger Graphs, Removal of Size Information, DISGEN Framework, Decoupling Loss


- [Random Latent Exploration for Deep Reinforcement Learning](https://icml.cc/virtual/2024/poster/33783) (Poster)
  - **Authors:** [Srinath Mahankali](http://openreview.net/profile?id=~Srinath_V._Mahankali1), [Zhang-Wei Hong](http://openreview.net/profile?id=~Zhang-Wei_Hong1), [Ayush Sekhari](http://openreview.net/profile?id=~Ayush_Sekhari1), [Alexander Rakhlin](http://openreview.net/profile?id=~Alexander_Rakhlin1), [Pulkit Agrawal](http://openreview.net/profile?id=~Pulkit_Agrawal1)
  - **Affiliations:** Massachusetts Institute of Technology, Massachusetts Institute of Technology, Massachusetts Institute of Technology, Massachusetts Institute of Technology, Massachusetts Institute of Technology
  - **TL;DR:** This paper introduces Random Latent Exploration (RLE), a novel exploration technique for deep reinforcement learning that enhances exploration in high-dimensional state spaces by perturbing rewards. RLE demonstrates superior performance on ATARI and ISAAC GYM benchmarks compared to traditional exploration methods.
  - **Keywords:** Reinforcement Learning, Exploration Techniques, Random Latent Exploration (RLE), Exploration Bonuses, Randomized Value Functions, ATARI, ISAAC GYM, Sparse-reward scenarios, Deep exploration, High-dimensional state spaces, Improved exploration strategies, Higher overall scores in tasks, ATARI, ISAAC GYM


- [CHAI: Clustered Head Attention for Efficient LLM Inference](https://icml.cc/virtual/2024/poster/32701) (Poster)
  - **Authors:** [Saurabh Agarwal](http://openreview.net/profile?id=~Saurabh_Agarwal1), [Bilge Acun](http://openreview.net/profile?id=~Bilge_Acun1), [Basil Hosmer](http://openreview.net/profile?id=~Basil_Hosmer1), [Mostafa Elhoushi](http://openreview.net/profile?id=~Mostafa_Elhoushi1), [Yejin Lee](http://openreview.net/profile?id=~Yejin_Lee4), [Shivaram Venkataraman](http://openreview.net/profile?id=~Shivaram_Venkataraman1), [Dimitris Papailiopoulos](http://openreview.net/profile?id=~Dimitris_Papailiopoulos1), [Carole-Jean Wu](http://openreview.net/profile?id=~Carole-Jean_Wu2)
  - **Affiliations:** University of Wisconsin-Madison, Meta-FAIR, Meta-FAIR, Meta-FAIR, Meta-FAIR, University of Wisconsin-Madison, University of Wisconsin-Madison, Meta-FAIR
  - **TL;DR:** This study introduces Clustered Head Attention (CHAI) to improve the efficiency of Large Language Models (LLMs) during inference by reducing memory and compute requirements. CHAI demonstrates significant reductions in memory usage and latency without fine-tuning, while maintaining a small accuracy deviation across multiple models.
  - **Keywords:** Large Language Models, Efficient Inference, Clustered Head Attention (CHAI), Multi-Head Attention (MHA), Natural Language Processing, Memory and compute intensity during inference, Redundancy in attention heads, Reduction in memory requirements for K,V cache, Reduction in inference time latency, OPT-66B, LLAMA-7B, LLAMA-33B, Arc-Challenge dataset, Key (K), Value (V) pairs, Attention heads


- [Accelerated Speculative Sampling Based on Tree Monte Carlo](https://icml.cc/virtual/2024/poster/32890) (Poster)
  - **Authors:** [Zhengmian Hu](http://openreview.net/profile?id=~Zhengmian_Hu1), [Heng Huang](http://openreview.net/profile?id=~Heng_Huang1)
  - **Affiliations:** Department of Computer Science, University of Maryland, College Park, USA, Department of Computer Science, University of Maryland, College Park, USA
  - **TL;DR:** This paper introduces Accelerated Speculative Sampling (ASpS), a novel method that enhances the inference speed of large language models by generating multiple tokens in a single forward pass while maintaining the original distribution. The proposed method outperforms traditional speculative sampling by optimizing the coupling process in a tree space, leading to faster inference.
  - **Keywords:** Large Language Models, Inference Acceleration, Speculative Sampling, Tree Monte Carlo, Maximum Coupling, Text Generation, Translation, Summarization, Slow Inference Speed, Accelerated Speculative Sampling (ASpS), Unbiasedness, Convergence


- [Deep Demonstration Tracing: Learning Generalizable Imitator Policy for Runtime Imitation from a Single Demonstration](https://icml.cc/virtual/2024/poster/34633) (Poster)
  - **Authors:** [Xiong-Hui Chen](http://openreview.net/profile?id=~Xiong-Hui_Chen1), [Junyin Ye](http://openreview.net/profile?id=~Junyin_Ye1), [Hang Zhao](http://openreview.net/profile?id=~Hang_Zhao3), [Yi-Chen Li](http://openreview.net/profile?id=~Yi-Chen_Li1), [Xu-Hui Liu](http://openreview.net/profile?id=~Xu-Hui_Liu1), [Haoran Shi](http://openreview.net/profile?id=~Haoran_Shi2), [Yu-Yan Xu](http://openreview.net/profile?id=~Yu-Yan_Xu1), [Zhihao Ye](http://openreview.net/profile?id=~Zhihao_Ye3), [Si-Hang Yang](http://openreview.net/profile?id=~Si-Hang_Yang1), [Yang Yu](http://openreview.net/profile?id=~Yang_Yu5), [Anqi Huang](http://openreview.net/profile?id=~Anqi_Huang1), [Kai Xu](http://openreview.net/profile?id=~Kai_Xu5), [Zongzhang Zhang](http://openreview.net/profile?id=~Zongzhang_Zhang1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, School of Computer Science, National University of Defense Technology, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, Polixir Technologies, Polixir Technologies, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, Nanjing University of Science and Technology, Nanjing, School of Computer Science, National University of Defense Technology, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China
  - **TL;DR:** This study introduces Deep Demonstration Tracing (DDT), a novel algorithm for one-shot imitation learning that enhances the adaptability of imitator agents in dynamic environments. The results demonstrate that DDT outperforms traditional OSIL methods across various tasks, effectively addressing unforeseen changes.
  - **Keywords:** One-shot imitation learning (OSIL), generalization in dynamic environments, Deep Demonstration Tracing (DDT), demonstration transformer architecture, meta-reinforcement learning, Navigation tasks, robotics tasks, Adaptability to unforeseen changes, generalization ability of imitator agents, Superior performance over existing OSIL methods, regularization for policies in unexpected situations


- [Active Statistical Inference](https://icml.cc/virtual/2024/poster/34504) (Oral)
  - **Authors:** [Tijana Zrnic](http://openreview.net/profile?id=~Tijana_Zrnic1), [Emmanuel J Candes](http://openreview.net/profile?id=~Emmanuel_Candes1)
  - **Affiliations:** Department of Statistics and Stanford Data Science, Stanford University, USA, Department of Statistics and Department of Mathematics, Stanford University, USA
  - **TL;DR:** The study introduces active inference, a methodology that utilizes machine learning to optimize data labeling under budget constraints, prioritizing uncertain data points for labeling. It demonstrates that this approach can achieve the same accuracy as traditional methods with significantly fewer samples, potentially saving over 80% of the sample budget.
  - **Keywords:** active inference, statistical inference, active learning, machine learning models, confidence intervals, hypothesis tests, public opinion research, census analysis, proteomics, data labeling budget, uncertainty in predictions, reduced sample budget, smaller confidence intervals, more powerful tests


- [Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution](https://icml.cc/virtual/2024/poster/34275) (Poster)
  - **Authors:** [Xihaier Luo](http://openreview.net/profile?id=~Xihaier_Luo1), [Xiaoning Qian](http://openreview.net/profile?id=~Xiaoning_Qian2), [Byung-Jun Yoon](http://openreview.net/profile?id=~Byung-Jun_Yoon1)
  - **Affiliations:** Computational Science Initiative, Brookhaven National Laboratory, Upton, NY, USA, Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA, Computational Science Initiative, Brookhaven National Laboratory, Upton, NY, USA; Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA
  - **TL;DR:** This study presents a novel arbitrary-scale super-resolution method utilizing a hierarchical neural operator and a learnable frequency-aware loss prior to enhance scientific data resolution. The proposed approach effectively addresses challenges such as low-resolution features and spectral bias, demonstrating consistent improvements over existing state-of-the-art methods.
  - **Keywords:** arbitrary-scale super-resolution, operator learning, hierarchical neural operator, Galerkin-type self-attention, Sinc filters, scientific data enhancement, low-level vision tasks, medical imaging, climate modeling, remote sensing, low-resolution features, spectral bias, loss function issues, learnable prior structure, model-agnostic loss prior, implicit neural representation (INR), multi-layer perceptron (MLP)


- [SILVER: Single-loop variance reduction and application to federated learning](https://icml.cc/virtual/2024/poster/33044) (Poster)
  - **Authors:** [Kazusato Oko](http://openreview.net/profile?id=~Kazusato_Oko1), [Shunta Akiyama](http://openreview.net/profile?id=~Shunta_Akiyama1), [Denny Wu](http://openreview.net/profile?id=~Denny_Wu2), [Tomoya Murata](http://openreview.net/profile?id=~Tomoya_Murata1), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1)
  - **Affiliations:** The University of Tokyo; Center for Advanced Intelligence Project, RIKEN, The University of Tokyo, New York University; Flatiron Institute, NTT DATA Mathematical Systems Inc., The University of Tokyo; Center for Advanced Intelligence Project, RIKEN
  - **TL;DR:** The study introduces SILVER, a single-loop variance-reduced gradient estimator for finite-sum non-convex optimization that achieves optimal gradient complexity without requiring multiple full gradient computations. It demonstrates second-order optimality and exponential convergence, making it suitable for communication-efficient federated learning algorithms.
  - **Keywords:** variance reduction, distributed optimization, federated learning, SILVER (SIngle-Loop VariancE-Reduction), gradient estimator, finite-sum non-convex optimization, communication costs, data heterogeneity, optimal gradient complexity, second-order optimality, exponential convergence, Polyak-Łojasiewicz (PL) region, first-order stationary point, second-order stationary point (SOSP)


- [ODIN: Disentangled Reward Mitigates Hacking in RLHF](https://icml.cc/virtual/2024/poster/32624) (Poster)
  - **Authors:** [Lichang Chen](http://openreview.net/profile?id=~Lichang_Chen2), [Chen Zhu](http://openreview.net/profile?id=~Chen_Zhu2), [Jiuhai Chen](http://openreview.net/profile?id=~Jiuhai_Chen1), [Davit Soselia](http://openreview.net/profile?id=~Davit_Soselia1), [Tianyi Zhou](http://openreview.net/profile?id=~Tianyi_Zhou1), [Tom Goldstein](http://openreview.net/profile?id=~Tom_Goldstein1), [Heng Huang](http://openreview.net/profile?id=~Heng_Huang1), [Mohammad Shoeybi](http://openreview.net/profile?id=~Mohammad_Shoeybi1), [Bryan Catanzaro](http://openreview.net/profile?id=~Bryan_Catanzaro1)
  - **Affiliations:** University of Maryland, College Park, Meta, work done while at Nvidia, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, University of Maryland, College Park, Nvidia, Nvidia
  - **TL;DR:** This study addresses the issue of reward hacking in Reinforcement Learning from Human Feedback (RLHF) by proposing a new evaluation protocol and a modified reward model that decorrelates response length from reward scores. The findings demonstrate significant improvements in policy performance by mitigating the bias towards verbosity in responses.
  - **Keywords:** Reinforcement Learning from Human Feedback (RLHF), Large Language Models (LLMs), Reward Model (RM), Hyperparameter tuning, AI Safety, AI Alignment, Reward hacking, verbosity in responses, out-of-distribution generalization, Improved reward model, mitigation of length bias


- [SMaRt: Improving GANs with Score Matching Regularity](https://icml.cc/virtual/2024/poster/33200) (Poster)
  - **Authors:** [Mengfei Xia](http://openreview.net/profile?id=~Mengfei_Xia1), [Yujun Shen](http://openreview.net/profile?id=~Yujun_Shen1), [Ceyuan Yang](http://openreview.net/profile?id=~Ceyuan_Yang2), [Ran Yi](http://openreview.net/profile?id=~Ran_Yi1), [Wenping Wang](http://openreview.net/profile?id=~Wenping_Wang1), [Yong-Jin Liu](http://openreview.net/profile?id=~Yong-jin_Liu1)
  - **Affiliations:** Tsinghua University; BNRist, Ant Group, Shanghai AI Laboratory, Shanghai Jiao Tong University, Texas A&M University, Tsinghua University
  - **TL;DR:** This study proposes SMaRt, a method that enhances GAN optimization by integrating score matching regularity to address the challenges of learning from complex data manifolds. The approach demonstrates significant improvements in synthesis performance on real-world datasets, notably reducing FID scores in image generation tasks.
  - **Keywords:** Generative Adversarial Networks (GANs), Score Matching, Score Matching Regularity, Adversarial Loss, Image Generation, Data Distribution, Learning from diverse data, Positive Lebesgue measure subsets, Improved synthesis performance, Optimization of GANs, ImageNet, GANs (Generative Adversarial Networks), Diffusion Probabilistic Models (DPMs)


- [Protein Conformation Generation via Force-Guided SE(3) Diffusion Models](https://icml.cc/virtual/2024/poster/33695) (Poster)
  - **Authors:** [YAN WANG](http://openreview.net/profile?id=~YanWang1), [Lihao Wang](http://openreview.net/profile?id=~Lihao_Wang1), [Yuning Shen](http://openreview.net/profile?id=~Yuning_Shen1), [Yiqun Wang](http://openreview.net/profile?id=~Yiqun_Wang3), [Huizhuo Yuan](http://openreview.net/profile?id=~Huizhuo_Yuan1), [Yue Wu](http://openreview.net/profile?id=~Yue_Wu12), [Quanquan Gu](http://openreview.net/profile?id=~Quanquan_Gu1)
  - **Affiliations:** ByteDance Research; School of Mathematical Sciences, Tongji University, Shanghai, ByteDance Research, ByteDance Research, ByteDance Research, Department of Computer Science, University of California, Los Angeles, Department of Computer Science, University of California, Los Angeles, ByteDance Research
  - **TL;DR:** This study introduces CONFDIFF, a force-guided SE(3) diffusion model for generating diverse and high-fidelity protein conformations, addressing limitations of traditional molecular dynamics simulations. Experimental results demonstrate that CONFDIFF outperforms existing state-of-the-art methods in protein conformation prediction tasks.
  - **Keywords:** protein conformation generation, deep generative modeling, SE(3) diffusion models, force-guided network, protein structure prediction, drug discovery, rare event sampling, long equilibration time, limited sampling efficiency, CONFDIFF model, improved protein conformation diversity and fidelity, molecular dynamics (MD) simulations, Boltzmann distribution, AlphaFold, RoseTTAFold, OmegaFold


- [InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning](https://icml.cc/virtual/2024/poster/32741) (Poster)
  - **Authors:** [Zhe Huang](http://openreview.net/profile?id=~Zhe_Huang2), [Xiaowei Yu](http://openreview.net/profile?id=~Xiaowei_Yu1), [Dajiang Zhu](http://openreview.net/profile?id=~Dajiang_Zhu1), [Michael Hughes](http://openreview.net/profile?id=~Michael_C_Hughes1)
  - **Affiliations:** Department of Computer Science, Tufts University, Medford, MA, USA, Department of Computer Science, University of Texas at Arlington, Arlington, TX, USA, Department of Computer Science, University of Texas at Arlington, Arlington, TX, USA, Department of Computer Science, Tufts University, Medford, MA, USA
  - **TL;DR:** This paper introduces InterLUDE, a novel semi-supervised learning approach that enhances performance by facilitating interactions between labeled and unlabeled data through embedding fusion and a new consistency regularization loss. Experiments demonstrate significant improvements in classification accuracy, achieving a 3.2% error rate on the STL-10 dataset with only 40 labels.
  - **Keywords:** semi-supervised learning (SSL), image classification, embedding fusion, consistency regularization, image classification, medical SSL, data sparsity, interaction between labeled and unlabeled data, new SSL algorithm (InterLUDE), improved representation learning, STL-10 dataset


- [Using Left and Right Brains Together: Towards Vision and Language Planning](https://icml.cc/virtual/2024/poster/33100) (Poster)
  - **Authors:** [Jun CEN](http://openreview.net/profile?id=~Jun_CEN1), [Chenfei Wu](http://openreview.net/profile?id=~Chenfei_Wu2), [Xiao Liu](http://openreview.net/profile?id=~Xiao_Liu14), [Shengming Yin](http://openreview.net/profile?id=~Shengming_Yin1), [Yixuan Pei](http://openreview.net/profile?id=~Yixuan_Pei2), [Jinglong Yang](http://openreview.net/profile?id=~Jinglong_Yang1), [Qifeng Chen](http://openreview.net/profile?id=~Qifeng_Chen1), [Nan Duan](http://openreview.net/profile?id=~Nan_Duan1), [Jianguo Zhang](http://openreview.net/profile?id=~Jianguo_Zhang2)
  - **Affiliations:** Research Institute of Trustworthy Autonomous Systems and Department of Computer Science and Engineering, Southern University of Science and Technology, Microsoft Research Asia, Microsoft Research Asia, Microsoft Research Asia, Xi’an Jiaotong University, City University of Hong Kong, The Hong Kong University of Science and Technology; Research Institute of Trustworthy Autonomous Systems, Microsoft Research Asia, Research Institute of Trustworthy Autonomous Systems and Department of Computer Science and Engineering, Southern University of Science and Technology; Peng Cheng Lab, Shenzhen, China
  - **TL;DR:** This study introduces a novel vision-language planning framework that integrates visual and language planning to enhance decision-making capabilities in multi-modality tasks. The results demonstrate that this approach significantly improves contextual awareness and task execution compared to existing models.
  - **Keywords:** Vision-Language Planning, Multi-modality Models, Large Language Models (LLMs), Large Multi-modality Models (LMMs), Chain-of-Thought (CoT), Robotics, Medical Diagnosis, Games, Lack of vision and spatial imagination in existing models, Novel vision-language planning framework, improved contextual task execution


- [Scaling Rectified Flow Transformers for High-Resolution Image Synthesis](https://icml.cc/virtual/2024/poster/34535) (Best Paper)
  - **Authors:** [Patrick Esser](http://openreview.net/profile?id=~Patrick_Esser1), [Sumith Kulal](http://openreview.net/profile?id=~Sumith_Kulal1), [Andreas Blattmann](http://openreview.net/profile?id=~Andreas_Blattmann1), [Rahim Entezari](http://openreview.net/profile?id=~Rahim_Entezari1), [Jonas Müller](http://openreview.net/profile?id=~Jonas_M%C3%BCller1), [Harry Saini](http://openreview.net/profile?id=~Harry_Saini1), [Yam Levi](http://openreview.net/profile?id=~Yam_Levi1), [Dominik Lorenz](http://openreview.net/profile?id=~Dominik_Lorenz1), [Axel Sauer](http://openreview.net/profile?id=~Axel_Sauer1), [Frederic Boesel](http://openreview.net/profile?id=~Frederic_Boesel1), [Dustin Podell](http://openreview.net/profile?id=~Dustin_Podell1), [Tim Dockhorn](http://openreview.net/profile?id=~Tim_Dockhorn1), [Zion English](http://openreview.net/profile?id=~Zion_English1), [Robin Rombach](http://openreview.net/profile?id=~Robin_Rombach1)
  - **Affiliations:** Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI, Stability AI
  - **TL;DR:** This study presents an improved approach to training rectified flow models for high-resolution image synthesis, demonstrating superior performance compared to traditional diffusion models. The novel transformer-based architecture enhances text comprehension and image generation, leading to lower validation loss and better human preference ratings.
  - **Keywords:** Generative modeling, High-resolution image synthesis, Rectified flow, Diffusion models, Transformer-based architecture, Text-to-image generation, High-dimensional data, Noise sampling techniques, Improved noise sampling, Bidirectional flow of information, Lower validation loss


- [Policy-conditioned Environment Models are More Generalizable](https://icml.cc/virtual/2024/poster/33439) (Poster)
  - **Authors:** [Ruifeng Chen](http://openreview.net/profile?id=~Ruifeng_Chen1), [Xiong-Hui Chen](http://openreview.net/profile?id=~Xiong-Hui_Chen1), [Yihao Sun](http://openreview.net/profile?id=~Yihao_Sun1), [Siyuan Xiao](http://openreview.net/profile?id=~Siyuan_Xiao1), [Minhui Li](http://openreview.net/profile?id=~Minhui_Li1), [Yang Yu](http://openreview.net/profile?id=~Yang_Yu5)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China, National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; Polixir Technologies
  - **TL;DR:** This study introduces policy-conditioned model (PCM) learning to enhance the accuracy of environment dynamics models in reinforcement learning, particularly for evaluating diverse target policies. The findings demonstrate that PCM significantly reduces value gaps and outperforms existing methods in offline policy evaluation and selection.
  - **Keywords:** reinforcement learning, environment model learning, offline policy evaluation, policy-conditioned model (PCM) learning, meta-dynamics model, offline policy optimization, model predictive control, value gap in policy evaluation, inaccurate predictions for target policies, improved prediction accuracy, effective algorithm for model learning, DOPE benchmark


- [On Discrete Prompt Optimization for Diffusion Models](https://icml.cc/virtual/2024/poster/34519) (Poster)
  - **Authors:** [Ruochen Wang](http://openreview.net/profile?id=~Ruochen_Wang2), [Ting Liu](http://openreview.net/profile?id=~Ting_Liu4), [Cho-Jui Hsieh](http://openreview.net/profile?id=~Cho-Jui_Hsieh1), [Boqing Gong](http://openreview.net/profile?id=~Boqing_Gong1)
  - **Affiliations:** University of California, Los Angeles; Google Research, Google Deepmind, University of California, Los Angeles; Google Research, Google Deepmind
  - **TL;DR:** This paper presents a novel gradient-based framework for optimizing prompts in text-to-image diffusion models, addressing challenges such as enormous domain space and text gradient computation. The proposed methods can significantly enhance prompt effectiveness or create adversarial prompts that undermine image generation fidelity.
  - **Keywords:** prompt optimization, text-to-image diffusion models, gradient-based framework, discrete optimization, image generation, enormous domain space, text gradient computation, prompt enhancement, adversarial attack, compact domain spaces, Shortcut Text Gradient, DiffusionDB, ChatGPT, COCO


- [Interplay of ROC and Precision-Recall AUCs: Theoretical Limits and Practical Implications in Binary Classification](https://icml.cc/virtual/2024/poster/34771) (Poster)
  - **Authors:** [Martin Mihelich](http://openreview.net/profile?id=~Martin_Mihelich1), [François Castagnos](http://openreview.net/profile?id=~Fran%C3%A7ois_Castagnos1), [Charles Dognin](http://openreview.net/profile?id=~Charles_Dognin2)
  - **Affiliations:** Glanceable, France, Glanceable, France, Glanceable, France
  - **TL;DR:** This paper presents two theorems that establish the relationship between AUC ROC and AUC PR in binary classification, highlighting that a high AUC ROC does not guarantee a high AUC PR, especially in imbalanced datasets. The findings advocate for prioritizing AUC PR over AUC ROC for better model evaluation and performance assessment.
  - **Keywords:** Binary classification, AUC ROC, AUC PR, Theorems, Analytical expressions, Imbalanced datasets, Model evaluation, Variability of AUC PR, Reliance on AUC ROC, Bounds for AUC PR and AUC ROC, Model performance evaluation


- [Nearest Neighbour Score Estimators for Diffusion Generative Models](https://icml.cc/virtual/2024/poster/33378) (Poster)
  - **Authors:** [Matthew Niedoba](http://openreview.net/profile?id=~Matthew_Niedoba2), [Dylan Green](http://openreview.net/profile?id=~Dylan_Green1), [Saeid Naderiparizi](http://openreview.net/profile?id=~Saeid_Naderiparizi1), [Vasileios Lioutas](http://openreview.net/profile?id=~Vasileios_Lioutas1), [Jonathan Lavington](http://openreview.net/profile?id=~Jonathan_Wilder_Lavington1), [Xiaoxuan Liang](http://openreview.net/profile?id=~Xiaoxuan_Liang2), [Yunpeng Liu](http://openreview.net/profile?id=~Yunpeng_Liu1), [Ke Zhang](http://openreview.net/profile?id=~Ke_Zhang23), [Setareh Dabiri](http://openreview.net/profile?id=~Setareh_Dabiri2), [Adam Scibior](http://openreview.net/profile?id=~Adam_Scibior1), [Berend Zwartsenberg](http://openreview.net/profile?id=~Berend_Zwartsenberg1), [Frank Wood](http://openreview.net/profile?id=~Frank_Wood2)
  - **Affiliations:** Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada, Inverted AI, Vancouver, Canada, Department of Computer Science, University of British Columbia, Vancouver, Canada; Inverted AI, Vancouver, Canada
  - **TL;DR:** This paper introduces a novel nearest neighbour score function estimator for diffusion generative models, significantly reducing estimator variance and improving convergence speed and sample quality in training consistency models. The proposed method can also replace learned networks for probability-flow ODE integration, suggesting new research directions.
  - **Keywords:** diffusion generative models, score function estimation, nearest neighbour score function estimator, self-normalized importance sampling, Monte Carlo methods, image generation, video generation, 3D object synthesis, high variance in score estimation, bias in neural network approximations, slow sampling speed, low variance score estimator, improved convergence speed, enhanced sample quality, CIFAR-10, probability flow ODE (PF-ODE), diffusion SDE


- [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://icml.cc/virtual/2024/poster/34932) (Poster)
  - **Authors:** [Siddharth Karamcheti](http://openreview.net/profile?id=~Siddharth_Karamcheti1), [Suraj Nair](http://openreview.net/profile?id=~Suraj_Nair1), [Ashwin Balakrishna](http://openreview.net/profile?id=~Ashwin_Balakrishna1), [Percy Liang](http://openreview.net/profile?id=~Percy_Liang1), [Thomas Kollar](http://openreview.net/profile?id=~Thomas_Kollar1), [Dorsa Sadigh](http://openreview.net/profile?id=~Dorsa_Sadigh1)
  - **Affiliations:** Department of Computer Science, Stanford University, Toyota Research Institute, Los Altos, CA, USA, Toyota Research Institute, Los Altos, CA, USA, Toyota Research Institute, Los Altos, CA, USA, Department of Computer Science, Stanford University, Stanford, CA, USA, Toyota Research Institute, Los Altos, CA, USA, Department of Computer Science, Stanford University, Stanford, CA, USA
  - **TL;DR:** This study investigates the design space of visually-conditioned language models (VLMs) and identifies key insights that enhance training and performance. The authors present a unified evaluation framework and demonstrate that their models outperform existing state-of-the-art models while reducing training compute by over 30%.
  - **Keywords:** Visually-conditioned language models (VLMs), model performance evaluation, Visual dialogue, scene understanding, robotic task planning, Hallucination, lack of objective evaluations, underexplored design decisions, Unified framework for evaluating VLMs, optimized training code, checkpoints for models, LLaVa, InstructBLIP, PaLI-3


- [Directly Denoising Diffusion Models](https://icml.cc/virtual/2024/poster/33272) (Poster)
  - **Authors:** [Dan Zhang](http://openreview.net/profile?id=~Dan_Zhang10), [Jingjing Wang](http://openreview.net/profile?id=~Jingjing_Wang12), [Feng Luo](http://openreview.net/profile?id=~Feng_Luo1)
  - **Affiliations:** School of Computing, Clemson University, USA, School of Computing, Clemson University, USA, School of Computing, Clemson University, USA
  - **TL;DR:** This paper introduces the Directly Denoising Diffusion Model (DDDM), which efficiently generates realistic images with few-step sampling while maintaining the option for multistep sampling. The model achieves competitive FID scores, outperforming GANs and distillation-based models, and aligns with state-of-the-art methods when extended to 1000 steps.
  - **Keywords:** Diffusion Models, Image Generation, Directly Denoising Diffusion Model (DDDM), Pseudo-LPIPS, Probability Flow ODE, Image Generation, Video Generation, Inpainting, Super-Resolution, Slow Sampling Speed, Accumulated Distortion, Strong Performance in Benchmark Datasets, FID Scores, CIFAR-10, GANs (Generative Adversarial Networks), DDIM (Denoising Diffusion Implicit Models), ODE (Ordinary Differential Equation)


- [What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation](https://icml.cc/virtual/2024/poster/34176) (Spotlight Poster)
  - **Authors:** [Aaditya Singh](http://openreview.net/profile?id=~Aaditya_K_Singh1), [Ted Moskovitz](http://openreview.net/profile?id=~Ted_Moskovitz1), [Feilx Hill](http://openreview.net/profile?id=~Felix_Hill1), [Stephanie Chan](http://openreview.net/profile?id=~Stephanie_C.Y._Chan1), [Andrew Saxe](http://openreview.net/profile?id=~Andrew_M_Saxe1)
  - **Affiliations:** Gatsby Computational Neuroscience Unit, University College London, Gatsby Computational Neuroscience Unit, University College London, Google DeepMind, Google DeepMind, Gatsby Computational Neuroscience Unit, University College London
  - **TL;DR:** This study investigates the emergence dynamics of induction heads in transformer models, revealing critical subcircuits that interact to facilitate in-context learning. The findings highlight the importance of understanding these mechanisms for improving the performance of large language models.
  - **Keywords:** in-context learning, transformer models, mechanistic interpretability, match-and-copy operation, causal framework, clamping, emergence dynamics, phase change in loss, identification of subcircuits, understanding of induction heads, synthetic data, induction head, previous token head, induction circuit, large language models (LLMs)


- [FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning](https://icml.cc/virtual/2024/poster/34712) (Poster)
  - **Authors:** [Yuwei Fu](http://openreview.net/profile?id=~Yuwei_Fu1), [Haichao Zhang](http://openreview.net/profile?id=~Haichao_Zhang4), [di wu](http://openreview.net/profile?id=~Di_Wu11), [Wei Xu](http://openreview.net/profile?id=~Wei_Xu13), [Benoit Boulet](http://openreview.net/profile?id=~Benoit_Boulet1)
  - **Affiliations:** McGill University; Horizon Robotics, Horizon Robotics, McGill University, Horizon Robotics, McGill University
  - **TL;DR:** This study explores the use of pre-trained visual-language models to enhance reinforcement learning in sparse reward tasks, addressing the challenge of reward misalignment through a novel fine-tuning method called FuRL. The proposed approach demonstrates improved performance and exploration in benchmark tasks.
  - **Keywords:** Visual-Language Models, Reinforcement Learning, Sparse Reward Tasks, Fuzzy VLM reward-aided RL (FuRL), SAC, DrQ, Relay RL, Online Reinforcement Learning, Robotics, Reward Misalignment, Sample Efficiency, Local Minima, Fine-tuning Method, Improved Exploration Strategies, Meta-world benchmark tasks


- [Thermometer: Towards Universal Calibration for Large Language Models](https://icml.cc/virtual/2024/poster/33123) (Poster)
  - **Authors:** [Maohao Shen](http://openreview.net/profile?id=~Maohao_Shen1), [Subhro Das](http://openreview.net/profile?id=~Subhro_Das1), [Kristjan Greenewald](http://openreview.net/profile?id=~Kristjan_Greenewald1), [Prasanna Sattigeri](http://openreview.net/profile?id=~Prasanna_Sattigeri1), [Gregory Wornell](http://openreview.net/profile?id=~Gregory_W._Wornell1), [Soumya Ghosh](http://openreview.net/profile?id=~Soumya_Ghosh1)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, USA; MIT-IBM Watson AI Lab, IBM Research, MIT-IBM Watson AI Lab, IBM Research, MIT-IBM Watson AI Lab, IBM Research, MIT-IBM Watson AI Lab, IBM Research, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, USA, MIT-IBM Watson AI Lab, IBM Research
  - **TL;DR:** This study addresses the calibration challenges in large language models (LLMs) and proposes THERMOMETER, a computationally efficient calibration approach that maintains LLM accuracy while improving calibration across diverse tasks. Empirical evaluations demonstrate its effectiveness compared to existing methods.
  - **Keywords:** Calibration, Large Language Models (LLMs), THERMOMETER (calibration approach), instruction tuning, Poor calibration in LLMs, computational challenges in calibration, Improved calibration methods, auxiliary model for calibration


- [Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference](https://icml.cc/virtual/2024/poster/33643) (Poster)
  - **Authors:** [Md Musfiqur Rahman](http://openreview.net/profile?id=~Md_Musfiqur_Rahman1), [Murat Kocaoglu](http://openreview.net/profile?id=~Murat_Kocaoglu1)
  - **Affiliations:** School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA, School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA
  - **TL;DR:** This study introduces Modular-DCM, a novel algorithm for efficiently answering causal queries using high-dimensional data by leveraging pre-trained deep generative models. The algorithm demonstrates superior performance on various datasets, addressing challenges related to latent confounders and high-dimensional distributions.
  - **Keywords:** Causal inference, Deep generative models, Adversarial training, Modular training, Healthcare, Image analysis, High-dimensional data, Latent confounders, Modular-DCM algorithm, Efficient learning, Colored-MNIST, COVIDx, CelebA-HQ, Structural causal models (SCMs), Causal queries


- [Out-of-Domain Generalization in Dynamical Systems Reconstruction](https://icml.cc/virtual/2024/poster/32708) (Poster)
  - **Authors:** [Niclas Göring](http://openreview.net/profile?id=~Niclas_Alexander_G%C3%B6ring1), [Florian Hess](http://openreview.net/profile?id=~Florian_Hess1), [Manuel Brenner](http://openreview.net/profile?id=~Manuel_Brenner1), [Zahra Monfared](http://openreview.net/profile?id=~Zahra_Monfared1), [Daniel Durstewitz](http://openreview.net/profile?id=~Daniel_Durstewitz1)
  - **Affiliations:** Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany, Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany, Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany, Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany, Department of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim, Heidelberg University, Mannheim, Germany; Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany; Interdisciplinary Center for Scientific Computing, Heidelberg University, Heidelberg, Germany
  - **TL;DR:** This study develops a formal framework for out-of-domain generalization in dynamical systems reconstruction, highlighting the limitations of current deep learning methods in generalizing to unobserved domains. It provides a comprehensive mathematical treatment of the challenges in achieving effective generalization in dynamical systems modeling.
  - **Keywords:** Out-of-domain generalization, dynamical systems reconstruction, deep learning, Black-box deep learning techniques, symbolic regression, recurrent neural networks (RNNs), Neural ODEs, operator theory, reservoir computing, Dynamical systems, scientific modeling, time series data analysis, Generalization to unobserved domains, limitations of current DSR methods, learnability of DSR models, Formal framework for OOD generalization in DSR, mathematical treatment of OODG, Dynamical systems theory (DST), vector field approximation, flow operator


- [Bring Your Own (Non-Robust) Algorithm to Solve Robust MDPs by Estimating The Worst Kernel](https://icml.cc/virtual/2024/poster/33930) (Poster)
  - **Authors:** [Uri Gadot](http://openreview.net/profile?id=~Uri_Gadot1), [Kaixin Wang](http://openreview.net/profile?id=~Kaixin_Wang1), [Navdeep Kumar](http://openreview.net/profile?id=~Navdeep_Kumar1), [Kfir Levy](http://openreview.net/profile?id=~Kfir_Yehuda_Levy1), [Shie Mannor](http://openreview.net/profile?id=~Shie_Mannor2)
  - **Affiliations:** Technion, Technion, Technion, Technion, Technion; NVIDIA Research
  - **TL;DR:** This paper introduces EWoK, a novel approach for solving Robust Markov Decision Processes (RMDPs) that estimates the worst transition kernel to learn robust policies, addressing scalability issues in high-dimensional domains. The method retains flexibility in the learning process and can be applied to any non-robust reinforcement learning algorithm, demonstrating effectiveness in various environments.
  - **Keywords:** Robust Markov Decision Processes (RMDPs), Reinforcement Learning (RL), Estimated Worst transition Kernel (EWoK), High-dimensional domains, Sequential decision-making, Scalability issues in learning robust policies, Perturbations in transition models, Novel online approach for learning robust policies, Flexibility in learning process, DeepMind Control Suite, Cartpole


- [EvTexture: Event-driven Texture Enhancement for Video Super-Resolution](https://icml.cc/virtual/2024/poster/34032) (Poster)
  - **Authors:** [Dachun Kai](http://openreview.net/profile?id=~Dachun_Kai1), [Jiayao Lu](http://openreview.net/profile?id=~Jiayao_Lu1), [Yueyi Zhang](http://openreview.net/profile?id=~Yueyi_Zhang2), [Xiaoyan Sun](http://openreview.net/profile?id=~Xiaoyan_Sun1)
  - **Affiliations:** University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center
  - **TL;DR:** The study presents EvTexture, a novel video super-resolution method that utilizes event signals for texture enhancement, achieving significant improvements in restoring high-resolution details. Experimental results demonstrate state-of-the-art performance, particularly on the Vid4 dataset, with a notable gain in texture quality compared to existing methods.
  - **Keywords:** Video Super-Resolution (VSR), Event-based Vision, Texture Enhancement, Iterative Texture Enhancement Module, Surveillance, Virtual Reality, Video Enhancement, Texture Restoration, Blurry Textures, Jitter Effects, EvTexture Method, State-of-the-art Performance, High-resolution Details, Vid4 Dataset, High Temporal Resolution, High Dynamic Range


- [Position: LLMs Can’t Plan, But Can Help Planning in LLM-Modulo Frameworks](https://icml.cc/virtual/2024/poster/33965) (Spotlight Poster)
  - **Authors:** [Subbarao Kambhampati](http://openreview.net/profile?id=~Subbarao_Kambhampati1), [Karthik Valmeekam](http://openreview.net/profile?id=~Karthik_Valmeekam1), [Lin Guan](http://openreview.net/profile?id=~Lin_Guan1), [Mudit Verma](http://openreview.net/profile?id=~Mudit_Verma2), [Kaya Stechly](http://openreview.net/profile?id=~Kaya_Stechly1), [Siddhant Bhambri](http://openreview.net/profile?id=~Siddhant_Bhambri1), [Lucas Saldyt](http://openreview.net/profile?id=~Lucas_Paul_Saldyt1), [Anil B Murthy](http://openreview.net/profile?id=~Anil_B_Murthy1)
  - **Affiliations:** School of Computing and AI, Arizona State University, Tempe, AZ, USA, School of Computing and AI, Arizona State University, Tempe, AZ, USA, School of Computing and AI, Arizona State University, Tempe, AZ, USA, School of Computing and AI, Arizona State University, Tempe, AZ, USA, School of Computing and AI, Arizona State University, Tempe, AZ, USA, School of Computing and AI, Arizona State University, Tempe, AZ, USA, School of Computing and AI, Arizona State University, Tempe, AZ, USA, School of Computing and AI, Arizona State University, Tempe, AZ, USA
  - **TL;DR:** The paper argues that auto-regressive Large Language Models (LLMs) cannot independently perform planning or reasoning, and proposes the LLM-Modulo Framework as a means to enhance their capabilities through integration with external model-based verifiers. This approach aims to provide a more effective neuro-symbolic architecture that extends the potential of LLMs in planning and reasoning tasks.
  - **Keywords:** Large Language Models, Planning, Reasoning, LLM-Modulo Frameworks, Model-based Verifiers, AI Research, Neuro-symbolic Integration, Limitations of LLMs in Planning, Misunderstandings in Literature, Integration of LLMs with Symbolic Components, Enhanced Planning Capabilities, Auto-regressive LLMs, System 1 and System 2 Competency


- [New Sample Complexity Bounds for Sample Average Approximation in Heavy-Tailed Stochastic Programming](https://icml.cc/virtual/2024/poster/35094) (Poster)
  - **Authors:** [Hongcheng Liu](http://openreview.net/profile?id=~Hongcheng_Liu1), [Jindong Tong](http://openreview.net/profile?id=~Jindong_Tong1)
  - **Affiliations:** Department of Industrial and Systems Engineering, University of Florida, Gainesville, FL, 32611 USA, Department of Industrial and Systems Engineering, University of Florida, Gainesville, FL, 32611 USA
  - **TL;DR:** This paper investigates sample average approximation in convex stochastic programming under heavy-tailed conditions, demonstrating that sample complexity can be independent of the feasible region's complexity measures. The findings suggest improved bounds that are advantageous concerning problem dimensionality.
  - **Keywords:** Sample Average Approximation, Heavy-Tailed Stochastic Programming, Stochastic Programming, Machine Learning, Sample complexity, Convex optimization, New sample complexity bounds, Lipschitz continuous gradient, Composite objective function


- [SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization](https://icml.cc/virtual/2024/poster/33687) (Poster)
  - **Authors:** [Jialong Guo](http://openreview.net/profile?id=~Jialong_Guo1), [Xinghao Chen](http://openreview.net/profile?id=~Xinghao_Chen1), [Yehui Tang](http://openreview.net/profile?id=~Yehui_Tang1), [Yunhe Wang](http://openreview.net/profile?id=~Yunhe_Wang1)
  - **Affiliations:** Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab, Huawei Noah’s Ark Lab
  - **TL;DR:** This paper presents SLAB, an efficient transformer architecture that replaces LayerNorm with a novel method called PRepBN and introduces a simplified linear attention module. The proposed method achieves strong performance in image classification and language modeling while significantly reducing computational latency.
  - **Keywords:** Efficient Transformers, Computational Efficiency, Simplified Linear Attention (SLA), Progressive Re-parameterized Batch Normalization (PRepBN), Image Classification, Object Detection, Language Modeling, High Computational Cost, Computational Bottlenecks in Transformers, Improved Performance Metrics, Reduced Latency, ImageNet-1K, Transformer, LayerNorm, BatchNorm


- [Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling](https://icml.cc/virtual/2024/poster/33613) (Oral)
  - **Authors:** [Bairu Hou](http://openreview.net/profile?id=~Bairu_Hou2), [Yujian Liu](http://openreview.net/profile?id=~Yujian_Liu1), [Kaizhi Qian](http://openreview.net/profile?id=~Kaizhi_Qian1), [Jacob Andreas](http://openreview.net/profile?id=~Jacob_Andreas1), [Shiyu Chang](http://openreview.net/profile?id=~Shiyu_Chang2), [Yang Zhang](http://openreview.net/profile?id=~Yang_Zhang3)
  - **Affiliations:** UC Santa Barbara, UC Santa Barbara, MIT-IBM Watson AI Lab; IBM Research, MIT CSAIL, UC Santa Barbara, MIT-IBM Watson AI Lab; IBM Research
  - **TL;DR:** This paper introduces an uncertainty decomposition framework for large language models (LLMs) called input clarification ensembling, which separates aleatoric and epistemic uncertainty in LLM predictions. Empirical evaluations demonstrate that this approach provides accurate and reliable uncertainty quantification across various language processing tasks.
  - **Keywords:** Uncertainty decomposition, Large language models (LLMs), Input clarification ensembling, Bayesian neural networks, Language processing tasks, Uncertainty quantification, Aleatoric uncertainty, Epistemic uncertainty, Input ambiguity, Accurate uncertainty quantification, Trustworthiness, Interpretability


- [No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths](https://icml.cc/virtual/2024/poster/34950) (Poster)
  - **Authors:** [Charles Guille-Escuret](http://openreview.net/profile?id=~Charles_Guille-Escuret1), [Hiroki Naganuma](http://openreview.net/profile?id=~Hiroki_Naganuma1), [Kilian Fatras](http://openreview.net/profile?id=~Kilian_FATRAS1), [Ioannis Mitliagkas](http://openreview.net/profile?id=~Ioannis_Mitliagkas1)
  - **Affiliations:** Mila, Montreal, Canada; Université de Montréal, Montreal, Canada, Mila, Montreal, Canada; Université de Montréal, Montreal, Canada, University of McGill, Montreal, Canada; Dreamfold, Mila, Montreal, Canada; Université de Montréal, Montreal, Canada; Archimedes Unit, Athena Research Center, Athens
  - **TL;DR:** This study investigates the geometric properties of optimization paths in neural networks, revealing that optimization trajectories maintain stable dynamics and do not encounter significant obstacles. The findings provide insights that can theoretically guarantee linear convergence and inform learning rate schedules in practice.
  - **Keywords:** optimization dynamics, neural networks, loss landscapes, stochastic first-order optimization algorithms, stochastic gradient descent (SGD), image classification, semantic segmentation, language modeling, non-convexity, local minima, saddle points, linear convergence, learning rate schedules, restricted secant inequality, error bound


- [Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language Models](https://icml.cc/virtual/2024/poster/32745) (Poster)
  - **Authors:** [Francesca-Zhoufan Li](http://openreview.net/profile?id=~Francesca-Zhoufan_Li1), [Ava Amini](http://openreview.net/profile?id=~Ava_P_Amini1), [Yisong Yue](http://openreview.net/profile?id=~Yisong_Yue1), [Kevin Yang](http://openreview.net/profile?id=~Kevin_K_Yang1), [Alex Lu](http://openreview.net/profile?id=~Alex_Xijie_Lu1)
  - **Affiliations:** Department of Bioengineering, California Institute of Technology, California, USA; None, Microsoft Research, Massachusetts, USA, Department of Computing and Mathematical Sciences, California Institute of Technology, California, USA, Microsoft Research, Massachusetts, USA, Microsoft Research, Massachusetts, USA; Department of Bioengineering, California Institute of Technology, California, USA
  - **TL;DR:** This study investigates the effectiveness of pretrained protein language models (PLMs) in improving protein property and structure prediction through transfer learning. The findings reveal that while PLMs enhance performance across various tasks, the benefits do not scale with pretraining, indicating a reliance on low-level features and a need for improved pretraining methods.
  - **Keywords:** Transfer Learning, Protein Language Models (PLMs), Masked Language Modeling (MLM), Protein Function Prediction, Bioinformatics, Limited Labelled Data, Mismatch in Pretraining and Downstream Tasks, Systematic Analysis of Transfer Learning, Feature Reuse


- [Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://icml.cc/virtual/2024/poster/34354) (Poster)
  - **Authors:** [Boyi Wei](http://openreview.net/profile?id=~Boyi_Wei2), [Kaixuan Huang](http://openreview.net/profile?id=~Kaixuan_Huang1), [Yangsibo Huang](http://openreview.net/profile?id=~Yangsibo_Huang2), [Tinghao Xie](http://openreview.net/profile?id=~Tinghao_Xie1), [Xiangyu Qi](http://openreview.net/profile?id=~Xiangyu_Qi2), [Mengzhou Xia](http://openreview.net/profile?id=~Mengzhou_Xia1), [Prateek Mittal](http://openreview.net/profile?id=~Prateek_Mittal1), [Mengdi Wang](http://openreview.net/profile?id=~Mengdi_Wang1), [Peter Henderson](http://openreview.net/profile?id=~Peter_Henderson1)
  - **Affiliations:** Princeton University, Princeton University, Princeton University, Princeton University, Princeton University, Princeton University, Princeton University, Princeton University, Princeton University
  - **TL;DR:** This study investigates the brittleness of safety mechanisms in large language models (LLMs) through pruning and low-rank modifications, revealing that critical safety regions are sparse and their removal compromises safety with minimal impact on utility. The findings highlight the vulnerability of LLMs to low-cost fine-tuning attacks, emphasizing the need for more robust safety strategies.
  - **Keywords:** Safety alignment, Large language models (LLMs), Model brittleness, Pruning, Low-rank modifications, Weight attribution, AI safety, AI alignment, Jailbreaking, Vulnerability to fine-tuning attacks, Fragility of safety mechanisms, Identification of safety-critical regions, Insights into model safety


- [TimeMIL: Advancing Multivariate Time Series Classification via a Time-aware Multiple Instance Learning](https://icml.cc/virtual/2024/poster/34744) (Poster)
  - **Authors:** [Xiwen Chen](http://openreview.net/profile?id=~Xiwen_Chen3), [Peijie Qiu](http://openreview.net/profile?id=~Peijie_Qiu1), [Wenhui Zhu](http://openreview.net/profile?id=~Wenhui_Zhu1), [Huayu Li](http://openreview.net/profile?id=~Huayu_Li2), [Hao Wang](http://openreview.net/profile?id=~Hao_Wang68), [Aristeidis Sotiras](http://openreview.net/profile?id=~Aristeidis_Sotiras3), [Yalin Wang](http://openreview.net/profile?id=~Yalin_Wang3), [Abolfazl Razi](http://openreview.net/profile?id=~Abolfazl_Razi2)
  - **Affiliations:** Clemson University, USA, Washington University in St. Louis, USA, Arizona State University, USA, University of Arizona, USA, Clemson University, USA, Washington University in St. Louis, USA, Arizona State University, USA, Clemson University, USA
  - **TL;DR:** The study introduces TimeMIL, a novel weakly supervised framework for multivariate time series classification that enhances pattern localization and models time dependencies. It outperforms 26 state-of-the-art methods, demonstrating its effectiveness in addressing challenges related to data sparsity and pattern locality in time series data.
  - **Keywords:** multivariate time series classification (MTSC), weakly supervised learning, multiple-instance learning (MIL), tokenized transformer, learnable wavelet positional token, healthcare, human action recognition, audio signal processing, Internet of Things, semantic communication, data sparsity, locality of patterns, supervised learning limitations, TimeMIL framework, improved pattern localization, modeling time dependencies, deep neural networks, convolutional neural networks (CNN), recurrent neural networks (RNN), long short-term memory (LSTM)


- [MOMENT: A Family of Open Time-series Foundation Models](https://icml.cc/virtual/2024/poster/34530) (Poster)
  - **Authors:** [Mononito Goswami](http://openreview.net/profile?id=~Mononito_Goswami1), [Konrad Szafer](http://openreview.net/profile?id=~Konrad_Szafer1), [Arjun Choudhry](http://openreview.net/profile?id=~Arjun_Choudhry1), [Yifu Cai](http://openreview.net/profile?id=~Yifu_Cai1), [Shuo Li](http://openreview.net/profile?id=~Shuo_Li7), [Artur Dubrawski](http://openreview.net/profile?id=~Artur_Dubrawski2)
  - **Affiliations:** Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA, University of Pennsylvania, Philadelphia, USA, Auton Lab, Robotics Institute, Carnegie Mellon University, Pittsburgh, USA
  - **TL;DR:** The paper introduces MOMENT, a family of open-source foundation models designed for general-purpose time series analysis, addressing challenges in pre-training and evaluation. Experiments demonstrate the models' effectiveness in various tasks with minimal data and task-specific fine-tuning.
  - **Keywords:** time series analysis, foundation models, transformer models, masked time series prediction, forecasting, classification, anomaly detection, imputation, absence of cohesive public time series repository, diverse time series characteristics, limited resources for evaluation, pre-trained models, benchmark for evaluation, Time Series Pile


- [Counterfactual Image Editing](https://icml.cc/virtual/2024/poster/34158) (Poster)
  - **Authors:** [Yushu Pan](http://openreview.net/profile?id=~Yushu_Pan1), [Elias Bareinboim](http://openreview.net/profile?id=~Elias_Bareinboim2)
  - **Affiliations:** Department of Computer Science, Columbia University, New York, USA, Department of Computer Science, Columbia University, New York, USA
  - **TL;DR:** This paper formalizes the task of counterfactual image editing using causal language and demonstrates the impossibility of achieving this solely from i.i.d. samples. It proposes a new approach to approximate counterfactual distributions while maintaining causal consistency, along with an efficient algorithm for generating counterfactual images.
  - **Keywords:** Counterfactual image editing, Generative AI, Augmented structural causal models (ASCMs), Neural causal models, Data augmentation, Fairness analysis, Generalizability, Transportability, Impossibility of counterfactual editing from i.i.d. samples, Causal relationships between features, Ctf-consistent estimators, Efficient algorithm for counterfactual image generation, GANs (Generative Adversarial Networks), VAE (Variational Autoencoders), Diffusion Models


- [Learning Coverage Paths in Unknown Environments with Deep Reinforcement Learning](https://icml.cc/virtual/2024/poster/33132) (Poster)
  - **Authors:** [Arvi Jonnarth](http://openreview.net/profile?id=~Arvi_Jonnarth1), [Jie Zhao](http://openreview.net/profile?id=~Jie_Zhao3), [Michael Felsberg](http://openreview.net/profile?id=~Michael_Felsberg2)
  - **Affiliations:** Linköping University; Husqvarna Group, Dalian University of Technology, Linköping University; University of KwaZulu-Natal
  - **TL;DR:** This study investigates the use of deep reinforcement learning for coverage path planning in unknown environments, proposing a novel egocentric map representation and a reward function to enhance coverage efficiency. The approach outperforms previous RL-based methods and specialized techniques across various CPP scenarios.
  - **Keywords:** Coverage Path Planning (CPP), Reinforcement Learning (RL), Robotic lawn mowing, Search-and-rescue, Autonomous underwater exploration, Aerial terrain coverage, Online path planning in unknown environments, NP-hard problems, Egocentric map representation, Novel reward term based on total variation, Task- and robot-agnostic method


- [Learning to Route Among Specialized Experts for Zero-Shot Generalization](https://icml.cc/virtual/2024/poster/32970) (Poster)
  - **Authors:** [Mohammed Muqeeth](http://openreview.net/profile?id=~Mohammed_Muqeeth1), [Haokun Liu](http://openreview.net/profile?id=~Haokun_Liu1), [Yufan Liu](http://openreview.net/profile?id=~Yufan_Liu5), [Colin Raffel](http://openreview.net/profile?id=~Colin_Raffel1)
  - **Affiliations:** MIT-IBM, University of Toronto; Vector Institute, University of North Carolina at Chapel Hill, University of Toronto; Vector Institute
  - **TL;DR:** This study introduces PHATGOOSE, a method for routing among specialized language models to enhance zero-shot generalization without needing simultaneous access to training datasets. The results demonstrate that PHATGOOSE outperforms previous routing methods and even explicit multitask training in certain scenarios.
  - **Keywords:** expert language models, zero-shot generalization, parameter-efficient fine-tuning, Post-Hoc Adaptive Tokenwise Gating (PHATGOOSE), recycling expert models, improving zero-shot generalization, outperforming past methods, per-token and per-module routing


- [Improving Prototypical Visual Explanations with Reward Reweighing, Reselection, and Retraining](https://icml.cc/virtual/2024/poster/34227) (Poster)
  - **Authors:** [Aaron Li](http://openreview.net/profile?id=~Aaron_Jiaxun_Li1), [Robin Netzorg](http://openreview.net/profile?id=~Robin_Netzorg1), [Zhihan Cheng](http://openreview.net/profile?id=~Zhihan_Cheng1), [Zhuoqin Zhang](http://openreview.net/profile?id=~Zhuoqin_Zhang1), [Bin Yu](http://openreview.net/profile?id=~Bin_Yu5)
  - **Affiliations:** Harvard University, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley, University of California, Berkeley
  - **TL;DR:** This study introduces the R3 framework to enhance the interpretability and predictive accuracy of the ProtoPNet model by aligning learned prototypes with human preferences through a reward model. The approach effectively addresses issues of spurious features and inconsistent concepts in image classification.
  - **Keywords:** Interpretability, Deep Learning, Image Classification, Prototypical Part Network (ProtoPNet), Reward Model, Post-Processing Framework, Image Recognition, Learning Spurious Features, Inconsistent Concepts, Model Interpretability, R3 Framework, Improved Prototype Quality, Enhanced Predictive Accuracy, Caltech-UCSD Birds-200-2011 (CUB-200-211)


- [Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data](https://icml.cc/virtual/2024/poster/33638) (Poster)
  - **Authors:** [Fahim Tajwar](http://openreview.net/profile?id=~Fahim_Tajwar1), [Anikait Singh](http://openreview.net/profile?id=~Anikait_Singh1), [Archit Sharma](http://openreview.net/profile?id=~Archit_Sharma1), [Rafael Rafailov](http://openreview.net/profile?id=~Rafael_Rafailov1), [Jeff Schneider](http://openreview.net/profile?id=~Jeff_Schneider1), [Tengyang Xie](http://openreview.net/profile?id=~Tengyang_Xie1), [Stefano Ermon](http://openreview.net/profile?id=~Stefano_Ermon1), [Chelsea Finn](http://openreview.net/profile?id=~Chelsea_Finn1), [Aviral Kumar](http://openreview.net/profile?id=~Aviral_Kumar2)
  - **Affiliations:** CMU, Stanford, Stanford, Stanford, CMU, UW-Madison, Stanford, Stanford, Google DeepMind
  - **TL;DR:** This study investigates various fine-tuning techniques for large language models (LLMs) using preference data, revealing that on-policy sampling and negative gradient methods significantly outperform offline and maximum likelihood approaches. The findings provide actionable insights for optimizing data collection and fine-tuning strategies in LLMs.
  - **Keywords:** Preference fine-tuning, Large language models (LLMs), On-policy reinforcement learning (RL), Supervised learning, Contrastive learning, Negative gradient, Fine-tuning with preference data, Control of LLM responses, Mode-seeking objectives, KL-penalized expected reward optimization, AlpacaFarm, UltraFeedback


- [Variational Learning is Effective for Large Deep Networks](https://icml.cc/virtual/2024/poster/33590) (Spotlight Poster)
  - **Authors:** [Yuesong Shen](http://openreview.net/profile?id=~Yuesong_Shen1), [Nico Daheim](http://openreview.net/profile?id=~Nico_Daheim1), [Bai Cong](http://openreview.net/profile?id=~Bai_Cong1), [Peter Nickl](http://openreview.net/profile?id=~Peter_Nickl1), [Gian Maria Marconi](http://openreview.net/profile?id=~Gian_Maria_Marconi1), [Bazan Raoul](http://openreview.net/profile?id=~Bazan_Clement_Emile_Marcel_Raoul1), [Rio Yokota](http://openreview.net/profile?id=~Rio_Yokota1), [Iryna Gurevych](http://openreview.net/profile?id=~Iryna_Gurevych1), [Daniel Cremers](http://openreview.net/profile?id=~Daniel_Cremers1), [Khan Emtiyaz](http://openreview.net/profile?id=~Mohammad_Emtiyaz_Khan1), [Thomas Moellenhoff](http://openreview.net/profile?id=~Thomas_M%C3%B6llenhoff1)
  - **Affiliations:** Technical University of Munich & Munich Center for Machine Learning, UKP Lab, Technical University of Darmstadt & hessian.AI, Tokyo Institute of Technology, RIKEN Center for AI Project, RIKEN Center for AI Project, Tokyo Institute of Technology, Tokyo Institute of Technology, UKP Lab, Technical University of Darmstadt & hessian.AI, Technical University of Munich & Munich Center for Machine Learning, RIKEN Center for AI Project, RIKEN Center for AI Project
  - **TL;DR:** This paper presents the Improved Variational Online Newton (IVON) method, demonstrating its effectiveness in training large neural networks like GPT-2 and ResNets, outperforming traditional optimizers like Adam in terms of predictive uncertainty and accuracy. The findings suggest that variational learning can be successfully applied to large-scale deep learning problems, particularly in the context of Large Language Models.
  - **Keywords:** Variational learning, Deep learning, Large neural networks, Improved Variational Online Newton (IVON), Adam optimizer, Large Language Models (LLMs), Image classification, Ineffectiveness of variational learning for large-scale problems, Model overfitting, State-of-the-art accuracy and uncertainty, Predictive uncertainty, Model merging, Generalization error prediction, GPT-2, ResNet-50, ImageNet, Bayesian principles, Hessian estimation


- [Bringing Motion Taxonomies to Continuous Domains via GPLVM on Hyperbolic manifolds](https://icml.cc/virtual/2024/poster/33112) (Poster)
  - **Authors:** [Noémie Jaquier](http://openreview.net/profile?id=~No%C3%A9mie_Jaquier1), [Leonel Rozo](http://openreview.net/profile?id=~Leonel_Rozo1), [Miguel González-Duque](http://openreview.net/profile?id=~Miguel_Gonz%C3%A1lez-Duque3), [Slava Borovitskiy](http://openreview.net/profile?id=~Viacheslav_Borovitskiy1), [Tamim Asfour](http://openreview.net/profile?id=~Tamim_Asfour1)
  - **Affiliations:** Karlsruhe Institute of Technology, Bosch Center for Artificial Intelligence, University of Copenhagen, ETH Zürich, Karlsruhe Institute of Technology
  - **TL;DR:** This study proposes a novel Gaussian process hyperbolic latent variable model to effectively model human motion taxonomies, addressing the limitations of existing computational models. The model successfully learns hyperbolic embeddings that preserve the hierarchical structure of motion taxonomies and outperforms traditional methods, enabling realistic trajectory generation.
  - **Keywords:** Human motion taxonomies, hierarchical classification, motion generation, Gaussian process hyperbolic latent variable model, hyperbolic embeddings, Robotics, human-robot interaction, motion analysis, Gap between discrete hierarchical structure and high-dimensional heterogeneous data, Hyperbolic embeddings that preserve graph structure, encoding unseen data


- [EvIL: Evolution Strategies for Generalisable Imitation Learning](https://icml.cc/virtual/2024/poster/34814) (Poster)
  - **Authors:** [Silvia Sapora](http://openreview.net/profile?id=~Silvia_Sapora1), [Gokul Swamy](http://openreview.net/profile?id=~Gokul_Swamy1), [Christopher Lu](http://openreview.net/profile?id=~Chris_Lu1), [Yee-Whye Teh](http://openreview.net/profile?id=~Yee_Whye_Teh2), [Jakob Foerster](http://openreview.net/profile?id=~Jakob_Nicolaus_Foerster1)
  - **Affiliations:** University of Oxford, UK, Carnegie Mellon University, USA, University of Oxford, UK, University of Oxford, UK, University of Oxford, UK
  - **TL;DR:** The study addresses the challenges in imitation learning, particularly the inefficiency of reward recovery and shaping in new environments. It introduces a novel evolution-strategies based method (EvIL) that enhances re-training efficiency and performance in both target and source environments.
  - **Keywords:** Imitation Learning, Transfer Learning, Inverse Reinforcement Learning (IRL), Evolution Strategies, Continuous Control Tasks, Robotics, Poorly shaped rewards, Inefficient environment interaction, Reward model ensembles, Reward-shaping term optimization, Behavioural Cloning (BC), Maximum Entropy IRL, GAIL


- [Light and Optimal Schrödinger Bridge Matching](https://icml.cc/virtual/2024/poster/34581) (Poster)
  - **Authors:** [Nikita Gushchin](http://openreview.net/profile?id=~Nikita_Gushchin1), [Sergei Kholkin](http://openreview.net/profile?id=~Sergei_Kholkin1), [Evgeny Burnaev](http://openreview.net/profile?id=~Evgeny_Burnaev1), [Alexander Korotin](http://openreview.net/profile?id=~Alexander_Korotin2)
  - **Affiliations:** Skolkovo Institute of Science and Technology; Artificial Intelligence Research Institute, Skolkovo Institute of Science and Technology; Artificial Intelligence Research Institute, Skolkovo Institute of Science and Technology; Artificial Intelligence Research Institute, Skolkovo Institute of Science and Technology; Artificial Intelligence Research Institute
  - **TL;DR:** This paper introduces a novel method called optimal Schrödinger bridge matching to effectively learn Schrödinger Bridges, addressing limitations in existing solvers. The proposed LightSB-M solver demonstrates improved performance in practical tasks by utilizing a single bridge matching step with arbitrary transport plans.
  - **Keywords:** Schrödinger Bridges, diffusion models, Entropic Optimal Transport, bridge matching procedures, optimal parameterization, energy-based modeling, limitations of current solvers, error accumulation in iterative matching, optimal Schrödinger bridge matching, LightSB-M solver, Gaussian mixture parameterization, transport plan


- [On the Duality Between Sharpness-Aware Minimization and Adversarial Training](https://icml.cc/virtual/2024/poster/35116) (Poster)
  - **Authors:** [Yihao Zhang](http://openreview.net/profile?id=~Yihao_Zhang3), [Hangzhou He](http://openreview.net/profile?id=~Hangzhou_He1), [Jingyu Zhu](http://openreview.net/profile?id=~Jingyu_Zhu1), [Huanran Chen](http://openreview.net/profile?id=~Huanran_Chen1), [Yifei Wang](http://openreview.net/profile?id=~Yifei_Wang1), [Zeming Wei](http://openreview.net/profile?id=~Zeming_Wei1)
  - **Affiliations:** Peking University, Peking University, Peking University; University of California, Berkeley, Beijing Institute of Technology, MIT CSAIL, Peking University
  - **TL;DR:** This study investigates the relationship between Sharpness-Aware Minimization (SAM) and Adversarial Training (AT), revealing that SAM can enhance adversarial robustness without sacrificing clean accuracy. The findings suggest that SAM may serve as a viable alternative to AT, particularly when maintaining accuracy is prioritized.
  - **Keywords:** Adversarial Training, Sharpness-Aware Minimization, Adversarial Training (AT), Sharpness-Aware Minimization (SAM), AutoAttack, Deep Neural Networks (DNNs), Adversarial Defense, Adversarial examples, trade-off between accuracy and robustness, Improved adversarial robustness, enhanced clean accuracy


- [Liouville Flow Importance Sampler](https://icml.cc/virtual/2024/poster/34167) (Poster)
  - **Authors:** [Yifeng Tian](http://openreview.net/profile?id=~Yifeng_Tian1), [Nishant Panda](http://openreview.net/profile?id=~Nishant_Panda1), [Yen Ting Lin](http://openreview.net/profile?id=~Yen_Ting_Lin1)
  - **Affiliations:** Information Sciences Group (CCS-3), Computational and Statistical Sciences Division, Los Alamos National Laboratory, Los Alamos, NM 87545, USA, Information Sciences Group (CCS-3), Computational and Statistical Sciences Division, Los Alamos National Laboratory, Los Alamos, NM 87545, USA, Information Sciences Group (CCS-3), Computational and Statistical Sciences Division, Los Alamos National Laboratory, Los Alamos, NM 87545, USA
  - **TL;DR:** The paper introduces the Liouville Flow Importance Sampler (LFIS), a novel flow-based model designed to sample from complex unnormalized distributions by learning a time-dependent velocity field. LFIS demonstrates state-of-the-art performance across various benchmark problems, ensuring unbiased estimation of statistical quantities.
  - **Keywords:** flow-based models, unnormalized density functions, sampling, Liouville Flow Importance Sampler (LFIS), neural networks, partial differential equations, statistical physics, molecular dynamics, Bayesian inference, hard-to-sample distributions, estimating log-marginal likelihood, unbiased estimation of statistical quantities, state-of-the-art performance in benchmark problems


- [Offline Training of Language Model Agents with Functions as Learnable Weights](https://icml.cc/virtual/2024/poster/35088) (Poster)
  - **Authors:** [Shaokun Zhang](http://openreview.net/profile?id=~Shaokun_Zhang2), [Jieyu Zhang](http://openreview.net/profile?id=~Jieyu_Zhang1), [Jiale Liu](http://openreview.net/profile?id=~Jiale_Liu2), [Linxin Song](http://openreview.net/profile?id=~Linxin_Song1), [Chi Wang](http://openreview.net/profile?id=~Chi_Wang3), [Ranjay Krishna](http://openreview.net/profile?id=~Ranjay_Krishna1), [Qingyun Wu](http://openreview.net/profile?id=~Qingyun_Wu2)
  - **Affiliations:** Pennsylvania State University, University of Washington, Pennsylvania State University, University of Southern California, Microsoft Research, University of Washington, Pennsylvania State University
  - **TL;DR:** This study introduces a novel training paradigm for Large Language Model agents that allows for the optimization of agent functions without modifying the LLM weights. The proposed method, AgentOptimizer, significantly enhances the performance of LLM agents in various tasks while addressing the challenges of function curation and LLM modification.
  - **Keywords:** Large Language Models, Agent-based automation, AgentOptimizer, function learning, Difficulty in modifying LLMs, time-consuming function curation, Improved performance of LLM agents, agent training algorithm, AutoGen library


- [Fast Adversarial Attacks on Language Models In One GPU Minute](https://icml.cc/virtual/2024/poster/32756) (Poster)
  - **Authors:** [Vinu Sankar Sadasivan](http://openreview.net/profile?id=~Vinu_Sankar_Sadasivan1), [Shoumik Saha](http://openreview.net/profile?id=~Shoumik_Saha1), [Gaurang Sriramanan](http://openreview.net/profile?id=~Gaurang_Sriramanan1), [Priyatham Kattakinda](http://openreview.net/profile?id=~Priyatham_Kattakinda1), [Atoosa Malemir Chegini](http://openreview.net/profile?id=~Atoosa_Chegini1), [Soheil Feizi](http://openreview.net/profile?id=~Soheil_Feizi2)
  - **Affiliations:** Department of Computer Science; Department of Electrical & Computer Engineering, Department of Computer Science; Department of Electrical & Computer Engineering, Department of Computer Science; Department of Electrical & Computer Engineering, Department of Electrical & Computer Engineering, Department of Computer Science, Department of Computer Science
  - **TL;DR:** This paper presents BEAST, a fast adversarial attack method for language models that achieves high success rates in jailbreaking and inducing hallucinations within one minute. The findings indicate that BEAST can significantly manipulate model outputs, raising concerns about the security and privacy of language models.
  - **Keywords:** Adversarial Attacks, Language Models, AI Safety, BEAST (beam search-based adversarial attack), gradient-free targeted attack, Jailbreaking, Eliciting Hallucinations, Privacy Attacks, Manipulation of aligned LMs, generation of harmful content, High attack success rates, generation of adversarial suffixes, increased incorrect outputs, Hallucination, Large Language Models, Adversarial Machine Learning


- [Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty](https://icml.cc/virtual/2024/poster/34468) (Poster)
  - **Authors:** [Kaizhao Liu](http://openreview.net/profile?id=~Kaizhao_Liu1), [Jose Blanchet](http://openreview.net/profile?id=~Jose_Blanchet1), [Lexing Ying](http://openreview.net/profile?id=~Lexing_Ying1), [Yiping Lu](http://openreview.net/profile?id=~Yiping_Lu1)
  - **Affiliations:** Department of Mathematics, Peking University, Beijing, China, Department of Management Science and Engineering, Stanford University, Department of Mathematics, Stanford University, Courant Institute of Mathematical Sciences, New York University; Department of Industrial Engineering and Management Sciences, Northwestern University
  - **TL;DR:** The paper introduces Orthogonal Bootstrap, a method that reduces the computational cost of Bootstrap while improving empirical accuracy in simulating input uncertainty. It demonstrates that this approach maintains the same width of constructed confidence intervals despite requiring fewer Monte Carlo replications.
  - **Keywords:** Bootstrap, input uncertainty, simulation, Orthogonal Bootstrap, Monte Carlo replications, Infinitesimal Jackknife, Confidence interval construction, debiasing functional estimation, Computational cost of Bootstrap, simulation error, data randomness, Reduction in computational cost, improved empirical accuracy


- [Extreme Compression of Large Language Models via Additive Quantization](https://icml.cc/virtual/2024/poster/34964) (Poster)
  - **Authors:** [Vage Egiazarian](http://openreview.net/profile?id=~Vage_Egiazarian1), [Andrei Panferov](http://openreview.net/profile?id=~Andrei_Panferov1), [Denis Kuznedelev](http://openreview.net/profile?id=~Denis_Kuznedelev1), [Elias Frantar](http://openreview.net/profile?id=~Elias_Frantar1), [Artem Babenko](http://openreview.net/profile?id=~Artem_Babenko1), [Dan Alistarh](http://openreview.net/profile?id=~Dan_Alistarh7)
  - **Affiliations:** HSE University; Yandex Research, HSE University; Yandex Research, Skoltech, IST Austria, Yandex Research, IST Austria; NeuralMagic
  - **TL;DR:** The paper presents AQLM, an innovative algorithm for extreme compression of large language models, achieving significant improvements in model size and accuracy at low bit counts. It demonstrates practical implementations that outperform existing methods while maintaining a smaller memory footprint.
  - **Keywords:** Large Language Models, Extreme Compression, Additive Quantization, Multi-Codebook Quantization, Low bit counts, Compression-vs-accuracy trade-off, AQLM algorithm, Pareto optimality in accuracy-vs-model-size


- [Discovering Symmetry Breaking in Physical Systems with Relaxed Group Convolution](https://icml.cc/virtual/2024/poster/34988) (Poster)
  - **Authors:** [Rui Wang](http://openreview.net/profile?id=~Rui_Wang11), [Elyssa Hofgard](http://openreview.net/profile?id=~Elyssa_Hofgard1), [Han Gao](http://openreview.net/profile?id=~Han_Gao3), [Robin Walters](http://openreview.net/profile?id=~Robin_Walters1), [Tess Smidt](http://openreview.net/profile?id=~Tess_Smidt1)
  - **Affiliations:** Massachusetts Institute of Technology, Massachusetts Institute of Technology, Harvard University, Northeastern University, Massachusetts Institute of Technology
  - **TL;DR:** This study explores the use of relaxed group convolutions to model and identify symmetry breaking in various physical systems. The findings demonstrate that this approach effectively captures subtle asymmetries and enhances the understanding of complex behaviors in systems ranging from crystal structures to fluid dynamics.
  - **Keywords:** symmetry breaking, physical systems, equivariance, relaxed group convolutions, equivariant convolution, crystal structure phase transition, turbulent flow, pendulum systems, identifying sources of asymmetry, modeling complex data, discovering symmetry-breaking factors, capturing symmetry inductive biases


- [AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios](https://icml.cc/virtual/2024/poster/34882) (Poster)
  - **Authors:** [Zhongzhan Huang](http://openreview.net/profile?id=~Zhongzhan_Huang1), [Mingfu Liang](http://openreview.net/profile?id=~Mingfu_Liang1), [Shanshan Zhong](http://openreview.net/profile?id=~Shanshan_Zhong1), [Liang Lin](http://openreview.net/profile?id=~Liang_Lin1)
  - **Affiliations:** Sun Yat-sen University, China; Peng Cheng Laboratory, China, Northwestern University, USA, Sun Yat-sen University, China, Sun Yat-sen University, China; Peng Cheng Laboratory, China
  - **TL;DR:** The study introduces AttNS, an attention-inspired numerical solver that addresses generalization and robustness issues in AI-Hybrid numerical solvers for differential equations, particularly in limited data scenarios. The results demonstrate that AttNS enhances various numerical solvers while requiring less data and preventing numerical explosion issues.
  - **Keywords:** Attention mechanisms, Numerical solving, Differential equations, AI-Hybrid numerical solvers, Residual Neural Networks (ResNet), High-dimensional problems, Chaotic systems, Generalization issues, Robustness issues, Limited data scenarios, Attention-Inspired Numerical Solver (AttNS), Enhanced numerical solvers


- [Probabilistic Constrained Reinforcement Learning with Formal Interpretability](https://icml.cc/virtual/2024/poster/33725) (Poster)
  - **Authors:** [YANRAN WANG](http://openreview.net/profile?id=~YANRAN_WANG3), [QIUCHEN QIAN](http://openreview.net/profile?id=~QIUCHEN_QIAN1), [David Boyle](http://openreview.net/profile?id=~David_Boyle1)
  - **Affiliations:** Systems and Algorithms Laboratory, Imperial College London, South Kensington, London, Systems and Algorithms Laboratory, Imperial College London, South Kensington, London, Systems and Algorithms Laboratory, Imperial College London, South Kensington, London
  - **TL;DR:** This study introduces Adaptive Wasserstein Variational Optimization (AWaVO) to enhance interpretability in reinforcement learning, addressing challenges in understanding reward functions and optimal policies. The approach demonstrates a global convergence rate and balances high performance with sufficient interpretability in practical applications.
  - **Keywords:** Reinforcement Learning, Interpretability, Adaptive Wasserstein Variational Optimization (AWaVO), Autonomous navigation, Advanced manufacturing, Financial trading, Interpretability challenges in reinforcement learning, Convergence guarantee, Training transparency, Global convergence rate, Trade-off between performance and interpretability, Probabilistic inference, Sequential decision-making


- [Error Feedback Can Accurately Compress Preconditioners](https://icml.cc/virtual/2024/poster/34170) (Poster)
  - **Authors:** [Ionut-Vlad Modoranu](http://openreview.net/profile?id=~Ionut-Vlad_Modoranu1), [Aleksei Kalinov](http://openreview.net/profile?id=~Aleksei_Kalinov1), [Eldar Kurtic](http://openreview.net/profile?id=~Eldar_Kurtic1), [Elias Frantar](http://openreview.net/profile?id=~Elias_Frantar1), [Dan Alistarh](http://openreview.net/profile?id=~Dan_Alistarh7)
  - **Affiliations:** Institute of Science and Technology Austria, Institute of Science and Technology Austria, Institute of Science and Technology Austria, Institute of Science and Technology Austria, Institute of Science and Technology Austria
  - **TL;DR:** This paper introduces a novel error-feedback technique for compressing full-matrix preconditioners in deep learning optimizers, significantly reducing memory costs without sacrificing accuracy. The proposed method, EFCP, achieves up to 99% sparsity in preconditioners, making full-matrix preconditioning practical for large-scale models.
  - **Keywords:** deep learning, optimization, Full-Matrix Adagrad (GGT), Matrix-Free Approximate Curvature (M-FAC), error-feedback technique, sparsification, low-rank compression, massive storage costs, memory constraints, impractical full-matrix preconditioning, Error Feedback for Compressed Preconditioning (EFCP), compression of gradient history, BERT-base model, full-matrix preconditioning, adaptive regularization, natural gradient, Fisher approximation


- [Controlled Decoding from Language Models](https://icml.cc/virtual/2024/poster/33639) (Poster)
  - **Authors:** [Sidharth Mudgal](http://openreview.net/profile?id=~Sidharth_Mudgal1), [Jong Lee](http://openreview.net/profile?id=~Jong_Lee2), [Harish Ganapathy](http://openreview.net/profile?id=~Harish_Ganapathy1), [YaGuang Li](http://openreview.net/profile?id=~YaGuang_Li2), [Tao Wang](http://openreview.net/profile?id=~Tao_Wang30), [Yanping Huang](http://openreview.net/profile?id=~Yanping_Huang1), [Zhifeng Chen](http://openreview.net/profile?id=~Zhifeng_Chen1), [Heng-Tze Cheng](http://openreview.net/profile?id=~Heng-Tze_Cheng1), [Michael Collins](http://openreview.net/profile?id=~Michael_Collins1), [Trevor Strohman](http://openreview.net/profile?id=~Trevor_Strohman1), [Jilin Chen](http://openreview.net/profile?id=~Jilin_Chen1), [Alex Beutel](http://openreview.net/profile?id=~Alex_Beutel1), [Ahmad Beirami](http://openreview.net/profile?id=~Ahmad_Beirami1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, OpenAI; Google, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, OpenAI; Google, Google DeepMind
  - **TL;DR:** This paper introduces controlled decoding (CD), a modular approach for aligning language model responses to high-reward outcomes using a prefix scorer trained for reinforcement learning. The authors demonstrate that CD effectively improves controlled generation and can be applied to unseen models without additional training.
  - **Keywords:** language model alignment, controlled generation, KL-regularized reinforcement learning, tokenwise RL, prefix scorer, aligning machine-generated content to rewards, control over pre-trained representations, controlled decoding (CD), multi-objective RL problem solving, blockwise decoding, best-of-K strategy, FUDGE, COLD


- [SPADE: Sparsity-Guided Debugging for Deep Neural Networks](https://icml.cc/virtual/2024/poster/33092) (Poster)
  - **Authors:** [Arshia Soltani Moakhar](http://openreview.net/profile?id=~Arshia_Soltani_Moakhar1), [Eugenia Iofinova](http://openreview.net/profile?id=~Eugenia_Iofinova1), [Elias Frantar](http://openreview.net/profile?id=~Elias_Frantar1), [Dan Alistarh](http://openreview.net/profile?id=~Dan_Alistarh7)
  - **Affiliations:** Institute of Science and Technology Austria (ISTA), Institute of Science and Technology Austria (ISTA), Institute of Science and Technology Austria (ISTA), Institute of Science and Technology Austria (ISTA); NeuralMagic
  - **TL;DR:** This paper introduces SPADE, a novel approach that incorporates sparsity into the interpretation process of deep neural networks without altering their behavior during inference. The method significantly enhances the accuracy of image saliency maps and improves neuron visualizations, aiding in the understanding of network behavior.
  - **Keywords:** Neural network interpretability, Deep neural networks, Sample-targeted pruning, Sparsity-guided debugging, Image saliency maps, Neuron visualizations, Interpretability challenges, Feature disentanglement, Improved accuracy of interpretability methods, Enhanced neuron visualizations


- [On The Statistical Complexity of Offline Decision-Making](https://icml.cc/virtual/2024/poster/33548) (Poster)
  - **Authors:** [Thanh Nguyen-Tang](http://openreview.net/profile?id=~Thanh_Nguyen-Tang1), [Raman Arora](http://openreview.net/profile?id=~Raman_Arora1)
  - **Affiliations:** Department of Computer Science, Johns Hopkins University, Baltimore 21218, USA, Department of Computer Science, Johns Hopkins University, Baltimore 21218, USA
  - **TL;DR:** This study investigates the statistical complexity of offline decision-making using function approximation, establishing minimax-optimal rates for stochastic contextual bandits and Markov decision processes. It highlights the challenges of distributional shift and the importance of offline data in learning effective decision-making policies.
  - **Keywords:** offline decision-making, reinforcement learning, function approximation, stochastic contextual bandits, Markov decision processes, healthcare, autonomous driving, dialogue systems, distributional shift, learning from offline data, minimax-optimal rates, characterization of behavior policy


- [Scaling Exponents Across Parameterizations and Optimizers](https://icml.cc/virtual/2024/poster/35186) (Poster)
  - **Authors:** [Katie Everett](http://openreview.net/profile?id=~Katie_E_Everett1), [Lechao Xiao](http://openreview.net/profile?id=~Lechao_Xiao2), [Mitchell Wortsman](http://openreview.net/profile?id=~Mitchell_Wortsman1), [Alexander Alemi](http://openreview.net/profile?id=~Alexander_A_Alemi1), [Roman Novak](http://openreview.net/profile?id=~Roman_Novak2), [Peter Liu](http://openreview.net/profile?id=~Peter_J_Liu1), [Izzeddin Gur](http://openreview.net/profile?id=~Izzeddin_Gur1), [Jascha Sohl-Dickstein](http://openreview.net/profile?id=~Jascha_Sohl-Dickstein2), [Leslie Kaelbling](http://openreview.net/profile?id=~Leslie_Pack_Kaelbling1), [Jaehoon Lee](http://openreview.net/profile?id=~Jaehoon_Lee2), [Jeffrey Pennington](http://openreview.net/profile?id=~Jeffrey_Pennington1)
  - **Affiliations:** Google DeepMind; MIT, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, MIT, Google DeepMind, Google DeepMind
  - **TL;DR:** This study investigates the scaling of neural network models across various parameterizations and optimizers, revealing that existing assumptions may exclude optimal learning rate prescriptions. The authors propose a new learning rate approach and a modified version of the Adam optimizer, Adam-atan2, to enhance stability and performance.
  - **Keywords:** model scaling, parameterization, optimizer choices, stochastic gradient descent (SGD), Adam, Adafactor, per-layer learning rate, alignment between parameters and data, gradient underflow, hyperparameter transfer, new learning rate prescriptions, Adam-atan2, maximal update parameterization (muP), scaling dimensions


- [LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views](https://icml.cc/virtual/2024/poster/35016) (Poster)
  - **Authors:** [Yuji Roh](http://openreview.net/profile?id=~Yuji_Roh1), [Qingyun Liu](http://openreview.net/profile?id=~Qingyun_Liu1), [Huan Gui](http://openreview.net/profile?id=~Huan_Gui4), [Zhe Yuan](http://openreview.net/profile?id=~Zhe_Yuan2), [Yujin Tang](http://openreview.net/profile?id=~Yujin_Tang1), [Steven Whang](http://openreview.net/profile?id=~Steven_Euijong_Whang1), [Liang Liu](http://openreview.net/profile?id=~Liang_Liu11), [Shuchao Bi](http://openreview.net/profile?id=~Shuchao_Bi1), [Lichan Hong](http://openreview.net/profile?id=~Lichan_Hong1), [Ed Chi](http://openreview.net/profile?id=~Ed_H._Chi1), [Zhe Zhao](http://openreview.net/profile?id=~Zhe_Zhao3)
  - **Affiliations:** Google Inc, Mountain View, CA, USA; Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea, Google DeepMind, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, Google Inc, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea, Google Inc, Mountain View, CA, USA, Google Inc, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA, Google DeepMind, Mountain View, CA, USA
  - **TL;DR:** This paper introduces LEVI, a novel fine-tuning method that adaptively ensembles a pre-trained model with a task-specific model to enhance out-of-distribution generalization. The approach addresses limitations in both pre-training and fine-tuning data, leading to improved performance across diverse tasks.
  - **Keywords:** Fine-tuning, Generalization, Pre-trained models, Layer-wise Ensemble, Adaptive Ensembling, Natural Language Processing, Computer Vision, Out-of-distribution generalization, Over-reliance on pre-trained representations, LEVI method for generalizable fine-tuning


- [Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation](https://icml.cc/virtual/2024/poster/33413) (Poster)
  - **Authors:** [Randall Balestriero](http://openreview.net/profile?id=~Randall_Balestriero1), [Romain Cosentino](http://openreview.net/profile?id=~Romain_Cosentino2), [Sarath Shekkizhar](http://openreview.net/profile?id=~Sarath_Shekkizhar1)
  - **Affiliations:** Brown University, Computer Science Department, Tenyx, Tenyx
  - **TL;DR:** This study explores the geometric properties of Large Language Models (LLMs) to enhance understanding of their internal mechanisms and improve toxicity detection. The authors present new methods for controlling embeddings and extracting interpretable features, demonstrating significant advancements in toxicity identification.
  - **Keywords:** Large Language Models, Geometry, Toxicity Detection, Multi-Head Attention, Feedforward Neural Networks (MLP), Toxicity Detection, Natural Language Processing, Understanding internal representations of LLMs, Bypassing RLHF protection, Novel principled solutions, Interpretable geometrical features, Omni-Toxic dataset


- [Modeling Language Tokens as Functionals of Semantic Fields](https://icml.cc/virtual/2024/poster/34594) (Poster)
  - **Authors:** [Zhengqi Pei](http://openreview.net/profile?id=~Zhengqi_Pei1), [Anran Zhang](http://openreview.net/profile?id=~Anran_Zhang2), [Shuhui Wang](http://openreview.net/profile?id=~Shuhui_Wang1), [Qingming Huang](http://openreview.net/profile?id=~Qingming_Huang1)
  - **Affiliations:** Institute of Computing Technology, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Institute of Computing Technology, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Institute of Computing Technology, Chinese Academy of Sciences; Peng Cheng Laboratory, School of Computer Science and Technology, University of Chinese Academy of Sciences
  - **TL;DR:** This paper introduces the LasF module, which models language tokens as functionals of semantic fields to enhance language modeling by simulating neuronal behaviors. The LasF-based models demonstrate improved accuracy and efficiency in reading comprehension and question-answering tasks compared to existing models.
  - **Keywords:** natural language processing, language modeling, Transformer-based models, state-space models, LasF module, reading comprehension, question-answering, computational efficiency, nonlinear semantic relations, improved accuracy, parameter-efficient language models, CommonsenseQA, WikiText103, PennTreebank, semantic fields, high-dimensional distributed embeddings, attention mechanism


- [VideoPrism: A Foundational Visual Encoder for Video Understanding](https://icml.cc/virtual/2024/poster/33093) (Poster)
  - **Authors:** [Long Zhao](http://openreview.net/profile?id=~Long_Zhao2), [Nitesh Bharadwaj Gundavarapu](http://openreview.net/profile?id=~Nitesh_Bharadwaj_Gundavarapu1), [Liangzhe Yuan](http://openreview.net/profile?id=~Liangzhe_Yuan2), [Hao Zhou](http://openreview.net/profile?id=~Hao_Zhou3), [Shen Yan](http://openreview.net/profile?id=~Shen_Yan2), [Jennifer J. Sun](http://openreview.net/profile?id=~Jennifer_J._Sun1), [Luke Friedman](http://openreview.net/profile?id=~Luke_Friedman1), [Rui Qian](http://openreview.net/profile?id=~Rui_Qian1), [Tobias Weyand](http://openreview.net/profile?id=~Tobias_Weyand3), [Yue Zhao](http://openreview.net/profile?id=~Yue_Zhao4), [Rachel Hornung](http://openreview.net/profile?id=~Rachel_Hornung1), [Florian Schroff](http://openreview.net/profile?id=~Florian_Schroff1), [Ming-Hsuan Yang](http://openreview.net/profile?id=~Ming-Hsuan_Yang1), [David Ross](http://openreview.net/profile?id=~David_A_Ross1), [Huisheng Wang](http://openreview.net/profile?id=~Huisheng_Wang1), [Hartwig Adam](http://openreview.net/profile?id=~Hartwig_Adam1), [Mikhail Sirotenko](http://openreview.net/profile?id=~Mikhail_Sirotenko1), [Ting Liu](http://openreview.net/profile?id=~Ting_Liu4), [Boqing Gong](http://openreview.net/profile?id=~Boqing_Gong1)
  - **Affiliations:** Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google
  - **TL;DR:** The study introduces VideoPrism, a general-purpose video encoder pre-trained on a large corpus of video-caption pairs and clips, achieving state-of-the-art performance across various video understanding tasks with a single frozen model. The approach emphasizes leveraging video modality while utilizing available text to enhance understanding and performance.
  - **Keywords:** video understanding, foundational video models, masked autoencoding, global-local distillation, semantic video embeddings, video classification, localization, retrieval, captioning, question answering, balancing appearance-heavy tasks with motion-centric reasoning, state-of-the-art performance on video understanding benchmarks, 36M video-caption pairs, 582M video clips, ASR transcripts, video foundation models (ViFMs), frozen encoder


- [Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing](https://icml.cc/virtual/2024/poster/35156) (Poster)
  - **Authors:** [Gabriel Arpino](http://openreview.net/profile?id=~Gabriel_Arpino1), [Xiaoqi Liu](http://openreview.net/profile?id=~Xiaoqi_Liu1), [Ramji Venkataramanan](http://openreview.net/profile?id=~Ramji_Venkataramanan1)
  - **Affiliations:** University of Cambridge, University of Cambridge, University of Cambridge
  - **TL;DR:** This study proposes an Approximate Message Passing (AMP) algorithm for estimating change points and signals in high-dimensional linear regression, providing an asymptotic characterization of its performance. The method allows for uncertainty quantification and is validated through numerical experiments on synthetic data and images.
  - **Keywords:** change points, high-dimensional linear regression, Approximate Message Passing (AMP), localization of change points, uncertainty quantification, estimation performance characterization, approximate posterior distribution, synthetic data, images


- [Graph Positional and Structural Encoder](https://icml.cc/virtual/2024/poster/33943) (Poster)
  - **Authors:** [Semih Cantürk](http://openreview.net/profile?id=~Semih_Cant%C3%BCrk1), [Renming Liu](http://openreview.net/profile?id=~Renming_Liu1), [Olivier Lapointe-Gagné](http://openreview.net/profile?id=~Olivier_Lapointe-Gagn%C3%A91), [Vincent Létourneau](http://openreview.net/profile?id=~Vincent_L%C3%A9tourneau1), [Guy Wolf](http://openreview.net/profile?id=~Guy_Wolf1), [Dominique Beaini](http://openreview.net/profile?id=~Dominique_Beaini1), [Ladislav Rampasek](http://openreview.net/profile?id=~Ladislav_Ramp%C3%A1%C5%A1ek1)
  - **Affiliations:** DIRO, Université de Montréal; Mila – Quebec AI Institute, Department of Computational Mathematics, Science and Engineering, Michigan State University, DIRO, Université de Montréal; Mila – Quebec AI Institute, DIRO, Université de Montréal; Mila – Quebec AI Institute, DIRO, Université de Montréal; Mila – Quebec AI Institute, DIRO, Université de Montréal; Mila – Quebec AI Institute; Valence Labs, Isomorphic Labs
  - **TL;DR:** The study introduces the Graph Positional and Structural Encoder (GPSE), a novel graph encoder that enhances GNNs by learning efficient representations of positional and structural encodings. The results demonstrate that GPSE significantly outperforms traditional methods across various benchmarks, indicating its potential as a foundational tool for graph representation learning.
  - **Keywords:** Graph Neural Networks (GNN), Graph Representation Learning, Graph Positional and Structural Encoder (GPSE), Message-Passing Neural Network (MPNN), Graph Transformers (GT), Biomedicine, Molecular Chemistry, Designing optimal Positional and Structural Encodings (PSE) for graph prediction tasks, limitations of MPNN framework, Efficient common latent representation for multiple PSEs, pre-trained graph encoders, PyG library, Positional and Structural Encodings (PSE), Random Walk Encodings, Graph Laplacian Eigenvectors


- [Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](https://icml.cc/virtual/2024/poster/34623) (Poster)
  - **Authors:** [Xiangming Gu](http://openreview.net/profile?id=~Xiangming_Gu1), [Xiaosen Zheng](http://openreview.net/profile?id=~Xiaosen_Zheng1), [Tianyu Pang](http://openreview.net/profile?id=~Tianyu_Pang1), [Chao Du](http://openreview.net/profile?id=~Chao_Du1), [Qian Liu](http://openreview.net/profile?id=~Qian_Liu2), [Ye Wang](http://openreview.net/profile?id=~Ye_Wang3), [Jing Jiang](http://openreview.net/profile?id=~Jing_Jiang1), [Min Lin](http://openreview.net/profile?id=~Min_Lin1)
  - **Affiliations:** Sea AI Lab; National University of Singapore, Sea AI Lab; National University of Singapore, Sea AI Lab, Sea AI Lab, Sea AI Lab, National University of Singapore, Singapore Management University, Sea AI Lab
  - **TL;DR:** This study investigates a severe safety issue in multimodal large language models (MLLMs) where a single adversarial image can exponentially cause harmful behaviors across one million agents, termed "infectious jailbreak." The findings highlight the urgent need for effective defense mechanisms to prevent such vulnerabilities in AI systems.
  - **Keywords:** Multimodal Large Language Models, AI Safety, Infectious Jailbreak, Multi-Agent Environments, Virtual Assistants, Adversarial Images, Unaligned Behaviors, Jailbreaking, Infectious Jailbreak Mechanism, Defense Mechanism Principles, LLaVA-1.5


- [How do Transformers Perform In-Context Autoregressive Learning ?](https://icml.cc/virtual/2024/poster/33245) (Poster)
  - **Authors:** [Michael Sander](http://openreview.net/profile?id=~Michael_Eli_Sander1), [Raja Giryes](http://openreview.net/profile?id=~Raja_Giryes1), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1), [Mathieu Blondel](http://openreview.net/profile?id=~Mathieu_Blondel1), [Gabriel Peyré](http://openreview.net/profile?id=~Gabriel_Peyr%C3%A92)
  - **Affiliations:** Ecole Normale Supérieure and CNRS, France, Tel Aviv University, Israel, University of Tokyo and RIKEN AIP, Japan, Google DeepMind, Ecole Normale Supérieure and CNRS, France
  - **TL;DR:** This paper investigates how Transformers perform in-context autoregressive learning by training a model on next token prediction tasks, revealing that the model estimates context-dependent parameters and applies a prediction mapping. The findings enhance the understanding of Transformers' adaptability and learning processes in natural language tasks.
  - **Keywords:** Transformers, In-Context Learning, Autoregressive Learning, Next Token Prediction, Gradient Descent, Linear Transformer, Natural Language Processing, Understanding Transformer success, Context-dependent parameter estimation, Characterization of autoregressive in-context learning, Estimation of context matrix, Orthogonal Matrices, Multi-Head Transformer, Positional Encoding


- [DITTO: Diffusion Inference-Time T-Optimization for Music Generation](https://icml.cc/virtual/2024/poster/32644) (Oral)
  - **Authors:** [Zachary Novack](http://openreview.net/profile?id=~Zachary_Novack1), [Julian McAuley](http://openreview.net/profile?id=~Julian_McAuley1), [Taylor Berg-Kirkpatrick](http://openreview.net/profile?id=~Taylor_Berg-Kirkpatrick1), [Nicholas Bryan](http://openreview.net/profile?id=~Nicholas_J._Bryan1)
  - **Affiliations:** University of California – San Diego; Adobe Research, University of California – San Diego, University of California – San Diego, Adobe Research
  - **TL;DR:** The paper introduces DITTO, a framework for controlling pre-trained text-to-music diffusion models at inference-time by optimizing initial noise latents, achieving state-of-the-art performance in various music generation tasks without the need for fine-tuning. The method demonstrates high-quality, flexible control over music generation processes, including inpainting and melody control.
  - **Keywords:** music generation, diffusion models, Diffusion Inference-Time T-Optimization (DITTO), gradient checkpointing, text-to-music generation, audio editing, high-level control limitations, fine-grained expressivity challenges, state-of-the-art performance, training-free control


- [Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation](https://icml.cc/virtual/2024/poster/32837) (Oral)
  - **Authors:** [Can Yaras](http://openreview.net/profile?id=~Can_Yaras1), [Peng Wang](http://openreview.net/profile?id=~Peng_Wang23), [Laura Balzano](http://openreview.net/profile?id=~Laura_Balzano1), [Qing Qu](http://openreview.net/profile?id=~Qing_Qu2)
  - **Affiliations:** EECS Department, University of Michigan, Ann Arbor, USA, EECS Department, University of Michigan, Ann Arbor, USA, EECS Department, University of Michigan, Ann Arbor, USA, EECS Department, University of Michigan, Ann Arbor, USA
  - **TL;DR:** This study explores the benefits of overparameterization in deep learning by leveraging low-dimensional structures and compressible dynamics, leading to efficient low-rank matrix completion and improved fine-tuning of language models. The proposed method, Deep LoRA, reduces overfitting and simplifies hyperparameter setups while maintaining efficiency, particularly in scenarios with limited data.
  - **Keywords:** overparameterization, deep learning, low-rank learning, low-rank matrix completion, fine-tuning, Deep LoRA, natural language tasks, matrix recovery, computational requirements, overfitting, training efficiency, compact factorizations, improved optimization landscape


- [Score-Based Causal Discovery of Latent Variable Causal Models](https://icml.cc/virtual/2024/poster/33729) (Poster)
  - **Authors:** [Ignavier Ng](http://openreview.net/profile?id=~Ignavier_Ng1), [Xinshuai Dong](http://openreview.net/profile?id=~Xinshuai_Dong1), [Haoyue Dai](http://openreview.net/profile?id=~Haoyue_Dai1), [Biwei Huang](http://openreview.net/profile?id=~Biwei_Huang1), [Peter Spirtes](http://openreview.net/profile?id=~Peter_Spirtes1), [Kun Zhang](http://openreview.net/profile?id=~Kun_Zhang1)
  - **Affiliations:** Carnegie Mellon University, Carnegie Mellon University, Carnegie Mellon University, University of California, San Diego, Carnegie Mellon University, Carnegie Mellon University; Mohamed bin Zayed University of Artificial Intelligence
  - **TL;DR:** This study develops score-based methods for identifying causal structures involving latent variables, addressing challenges faced by existing constraint-based methods. The proposed methods achieve identifiability guarantees and demonstrate effectiveness through experimental validation.
  - **Keywords:** Causal discovery, Latent variable causal models, Score-based methods, Greedy Equivalence Search (GES), Scientific fields, Psychological studies, Latent confounders, Spurious correlations, Causal sufficiency, Identifiability guarantees, Score equivalence, Structure learning, Conditional independence, Partial ancestral graphs (PAG)


- [Ameliorate Spurious Correlations in Dataset Condensation](https://icml.cc/virtual/2024/poster/34046) (Poster)
  - **Authors:** [Jiaxing Cui](http://openreview.net/profile?id=~Justin_Cui1), [Ruochen Wang](http://openreview.net/profile?id=~Ruochen_Wang2), [Yuanhao Xiong](http://openreview.net/profile?id=~Yuanhao_Xiong1), [Cho-Jui Hsieh](http://openreview.net/profile?id=~Cho-Jui_Hsieh1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, Department of Computer Science, University of California, Los Angeles, Department of Computer Science, University of California, Los Angeles, Department of Computer Science, University of California, Los Angeles
  - **TL;DR:** This study investigates how biases in original datasets affect the dataset condensation process, revealing that color and background biases are amplified while corruption bias is suppressed. The authors propose a sample reweighting method to mitigate bias amplification, achieving significant performance improvements in models trained on condensed datasets.
  - **Keywords:** Dataset Condensation, Dataset Bias, Sample Reweighting, Kernel Density Estimation, Machine Learning, Neural Architecture Search, Federated Learning, Continual Learning, Graph Compression, Multimodality, Bias Propagation, Bias Amplification, Performance Decline, Effective Mitigation Strategies for Bias, Improved Test Accuracy, CMNIST


- [Optimistic Multi-Agent Policy Gradient](https://icml.cc/virtual/2024/poster/34235) (Poster)
  - **Authors:** [Wenshuai Zhao](http://openreview.net/profile?id=~Wenshuai_Zhao1), [Yi Zhao](http://openreview.net/profile?id=~Yi_Zhao6), [Zhiyuan Li](http://openreview.net/profile?id=~Zhiyuan_Li9), [Kannala Juho](http://openreview.net/profile?id=~Juho_Kannala5), [Joni Pajarinen](http://openreview.net/profile?id=~Joni_Pajarinen2)
  - **Affiliations:** Department of Electrical Engineering and Automation, Aalto University, Finland, Department of Electrical Engineering and Automation, Aalto University, Finland, School of Computer Science and Engineering, University of Electronic Science and Technology of China, China, Department of Computer Science, Aalto University, Finland; University of Oulu, Finland, Department of Electrical Engineering and Automation, Aalto University, Finland
  - **TL;DR:** This study addresses the issue of relative overgeneralization in multi-agent reinforcement learning by proposing a framework for optimistic updates in policy gradient methods. The proposed method demonstrates improved performance on various tasks, outperforming strong baselines in 13 out of 19 tested scenarios.
  - **Keywords:** Multi-agent reinforcement learning (MARL), cooperative decision making, Multi-agent policy gradient (MAPG), optimistic updates, Robotics, wireless networking, Relative overgeneralization (RO), suboptimal joint policy convergence, Framework for optimistic updates, performance improvement in MARL tasks, Multi-agent MuJoCo, Overcooked benchmarks


- [Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models](https://icml.cc/virtual/2024/poster/34750) (Poster)
  - **Authors:** [Mingjia Huo](http://openreview.net/profile?id=~Mingjia_Huo1), [Sai Ashish Somayajula](http://openreview.net/profile?id=~Sai_Ashish_Somayajula1), [Youwei Liang](http://openreview.net/profile?id=~Youwei_Liang1), [Ruisi Zhang](http://openreview.net/profile?id=~Ruisi_Zhang2), [Farinaz Koushanfar](http://openreview.net/profile?id=~Farinaz_Koushanfar1), [Pengtao Xie](http://openreview.net/profile?id=~Pengtao_Xie3)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA, Department of Electrical and Computer Engineering, University of California, San Diego, La Jolla, CA 92093, USA
  - **TL;DR:** This study introduces a novel multi-objective optimization approach for watermarking in large language models, enhancing the detectability of AI-generated texts while maintaining their semantic coherence. The experimental results demonstrate that the proposed method outperforms existing watermarking techniques.
  - **Keywords:** Large Language Models, AI-generated text detection, Watermarking, Multi-objective optimization (MOO), Token-specific watermarking, AI ethics, AI safety, Detectability of watermarks, Semantic coherence, Enhanced detectability of LLM-generated texts, Improved semantic integrity


- [Acquisition Conditioned Oracle for Nongreedy Active Feature Acquisition](https://icml.cc/virtual/2024/poster/33267) (Poster)
  - **Authors:** [Michael Valancius](http://openreview.net/profile?id=~Michael_Valancius1), [Maxwell Lennon](http://openreview.net/profile?id=~Maxwell_Lennon1), [Junier Oliva](http://openreview.net/profile?id=~Junier_Oliva1)
  - **Affiliations:** Department of Biostatistics, University of North Carolina, Chapel Hill, North Carolina, USA, Department of Computer Science, University of North Carolina, Chapel Hill, North Carolina, USA, Department of Computer Science, University of North Carolina, Chapel Hill, North Carolina, USA
  - **TL;DR:** This study introduces a novel methodology for active feature acquisition (AFA) that minimizes costs while improving inference accuracy, particularly in health care applications. The proposed Acquisition Conditioned Oracle (ACO) outperforms existing methods by allowing for dynamic and personalized feature selection based on previous acquisitions.
  - **Keywords:** Active Feature Acquisition (AFA), Dynamic Feature Collection, Nonparametric Oracle, Reinforcement Learning (RL), Greedy Policies, Health Care, Psychological Assessments, Educational Assessments, Automated Troubleshooting Systems, Cyberphysical Systems, Acquisition Costs, Missing Features, User Fatigue, Acquisition Conditioned Oracle (ACO), Improved Diagnostic Performance


- [Vision Transformers as Probabilistic Expansion from Learngene](https://icml.cc/virtual/2024/poster/34987) (Poster)
  - **Authors:** [Qiufeng Wang](http://openreview.net/profile?id=~Qiufeng_Wang3), [Xu Yang](http://openreview.net/profile?id=~Xu_Yang5), [Haokun Chen](http://openreview.net/profile?id=~Haokun_Chen4), [Xin Geng](http://openreview.net/profile?id=~Xin_Geng1)
  - **Affiliations:** School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China, School of Computer Science and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China
  - **TL;DR:** This study introduces Probabilistic Expansion from Learn Gene (PEG) to enhance the initialization of Vision Transformers by allowing for elastic scaling tailored to specific resource constraints. The proposed method effectively samples and expands model components, demonstrating superior performance compared to traditional initialization strategies.
  - **Keywords:** deep learning, Vision Transformers, Multi-Head Self-Attention, Feed-Forward Networks, Probabilistic Expansion, one-size-fits-all model initialization, resource constraints, Probabilistic Expansion from Learn Gene (PEG), elastic initialization


- [Particle Denoising Diffusion Sampler](https://icml.cc/virtual/2024/poster/32782) (Poster)
  - **Authors:** [Angus Phillips](http://openreview.net/profile?id=~Angus_Phillips1), [Hai-Dang Dau](http://openreview.net/profile?id=~Hai-Dang_Dau1), [Michael Hutchinson](http://openreview.net/profile?id=~Michael_John_Hutchinson1), [Valentin De Bortoli](http://openreview.net/profile?id=~Valentin_De_Bortoli1), [George Deligiannidis](http://openreview.net/profile?id=~George_Deligiannidis2), [Arnaud Doucet](http://openreview.net/profile?id=~Arnaud_Doucet2)
  - **Affiliations:** University of Oxford, University of Oxford, University of Oxford, CNRS; ENS Ulm, University of Oxford, University of Oxford
  - **TL;DR:** The study introduces the Particle Denoising Diffusion Sampler (PDDS), a novel method for sampling from unnormalized probability densities and estimating their normalizing constants. It demonstrates that PDDS provides asymptotically consistent estimates under mild assumptions, applicable to multimodal and high-dimensional sampling tasks.
  - **Keywords:** Denoising diffusion models, generative modeling, Particle Denoising Diffusion Sampler (PDDS), score matching, Monte Carlo sampling, High-dimensional sampling tasks, multimodal sampling, Intractable normalizing constants, sampling from unnormalized probability densities, Asymptotically consistent estimates, novel score matching loss


- [Position: Technical Research and Talent is Needed for Effective AI Governance](https://icml.cc/virtual/2024/poster/34716) (Oral)
  - **Authors:** [Anka Reuel](http://openreview.net/profile?id=~Anka_Reuel1), [Lisa Soder](http://openreview.net/profile?id=~Lisa_Soder1), [Benjamin Bucknall](http://openreview.net/profile?id=~Benjamin_Bucknall1), [Trond Undheim](http://openreview.net/profile?id=~Trond_Arne_Undheim1)
  - **Affiliations:** Department of Computer Science, Stanford University, Stanford, US, Interface, Brussels, BE, Centre for the Governance of AI, Oxford, UK; Oxford Martin AI Governance Initiative, Oxford, UK, Department of Computer Science, Stanford University, Stanford, US
  - **TL;DR:** The paper emphasizes the urgent need for targeted AI/ML research and closer collaboration between technical researchers and policymakers to effectively address the gaps in AI governance and regulation. It highlights the increasing integration of AI into society and the associated risks, advocating for informed governance to mitigate these challenges.
  - **Keywords:** AI governance, regulation, AI integration, Education, hiring, finance, generative AI, Potential harms and risks of AI, gaps in technical tools for governance, AI/ML research, regulatory action, workforce automation, AI-related incidents, legislation on AI


- [Fast Sampling-Based Sketches for Tensors](https://icml.cc/virtual/2024/poster/32871) (Spotlight Poster)
  - **Authors:** [William Swartworth](http://openreview.net/profile?id=~William_Joseph_Swartworth1), [David Woodruff](http://openreview.net/profile?id=~David_Woodruff1)
  - **Affiliations:** Carnegie Mellon University, Carnegie Mellon University
  - **TL;DR:** This paper presents a novel approach for constructing fast sampling-based sketches for two and three mode tensors, focusing on ℓ0-sampling and ℓ1-embeddings. The proposed method significantly reduces the time complexity of applying sketches to rank-one tensors, achieving efficiency in handling large data sets.
  - **Keywords:** sampling-based sketches, tensor structure, ℓ0-sampling, ℓ1-embeddings, fast convolution, data compression, streaming algorithms, linear-algebraic computations, high-dimensional data, data sparsity, fast tensor sketches, efficient sketching algorithms, Johnson-Lindenstrauss (JL) embeddings, rank-one tensor


- [Integrated Hardware Architecture and Device Placement Search](https://icml.cc/virtual/2024/poster/32820) (Poster)
  - **Authors:** [Irene Wang](http://openreview.net/profile?id=~Irene_Wang1), [Jakub Tarnawski](http://openreview.net/profile?id=~Jakub_Tarnawski1), [Amar Phanishayee](http://openreview.net/profile?id=~Amar_Phanishayee1), [Divya Mahajan](http://openreview.net/profile?id=~Divya_Mahajan1)
  - **Affiliations:** Georgia Institute of Technology, GA, USA, Microsoft Research, WA, USA, Microsoft Research, WA, USA, Georgia Institute of Technology, GA, USA
  - **TL;DR:** This study introduces PHAZE, a framework for co-optimizing hardware architecture and device placement strategies in distributed deep learning training. The approach significantly enhances throughput for large language models compared to existing state-of-the-art solutions.
  - **Keywords:** deep learning, hardware architecture, device placement strategy, Integer Linear Program (ILP), dynamic programming, distributed deep learning, large language models, memory footprint, computational resource balance, data distribution, co-optimization of architecture and device placement, improved throughput, tensor cores, vector cores, model parallelism


- [SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms](https://icml.cc/virtual/2024/poster/35024) (Poster)
  - **Authors:** [Xingrun Xing](http://openreview.net/profile?id=~Xingrun_Xing1), [Zheng Zhang](http://openreview.net/profile?id=~Zheng_Zhang12), [Ziyi Ni](http://openreview.net/profile?id=~Ziyi_Ni1), [Shitao Xiao](http://openreview.net/profile?id=~Shitao_Xiao1), [Yiming Ju](http://openreview.net/profile?id=~Yiming_Ju1), [Siqi Fan](http://openreview.net/profile?id=~Siqi_Fan4), [Yequan Wang](http://openreview.net/profile?id=~Yequan_Wang1), [Jiajun Zhang](http://openreview.net/profile?id=~Jiajun_Zhang1), [Guoqi Li](http://openreview.net/profile?id=~Guoqi_Li1)
  - **Affiliations:** Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; Beijing Academy of Artificial Intelligence, Beijing Academy of Artificial Intelligence, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing Academy of Artificial Intelligence, Beijing Academy of Artificial Intelligence, Beijing Academy of Artificial Intelligence, Beijing Academy of Artificial Intelligence, Institute of Automation, Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences, Institute of Automation, Chinese Academy of Sciences
  - **TL;DR:** This study introduces SpikeLM, a fully spike-driven language modeling mechanism that enhances generalization capabilities in language tasks by addressing the limitations of existing spiking neural networks. The proposed elastic bi-spiking mechanism significantly improves accuracy and bridges the performance gap between spiking and traditional neural networks.
  - **Keywords:** Spike-driven language modeling, Energy-efficient artificial intelligence, Spiking neural networks (SNNs), Elastic bi-spiking mechanism, Language tasks, General language modeling, Information loss in binary spikes, Technological challenges in spike representation and optimization, Fully spiking mechanism for language tasks, Higher accuracy in language modeling, Bridging performance gap between SNNs and ANNs, Biological plausibility, Event-driven sparsity, Binary activation


- [What’s the score? Automated Denoising Score Matching for Nonlinear Diffusions](https://icml.cc/virtual/2024/poster/32749) (Poster)
  - **Authors:** [raghav singhal](http://openreview.net/profile?id=~Raghav_Singhal1), [Mark Goldstein](http://openreview.net/profile?id=~Mark_Goldstein1), [Rajesh Ranganath](http://openreview.net/profile?id=~Rajesh_Ranganath2)
  - **Affiliations:** Courant Institute of Mathematical Sciences, New York University; Center for Data Science, New York University, Courant Institute of Mathematical Sciences, New York University; Center for Data Science, New York University, Courant Institute of Mathematical Sciences, New York University; Center for Data Science, New York University
  - **TL;DR:** This study introduces a novel family of denoising score matching objectives for nonlinear diffusion processes, enabling automated training and score estimation. The proposed method demonstrates effectiveness in generative modeling with non-Gaussian priors and applications in statistical physics.
  - **Keywords:** diffusion-based generative modeling, nonlinear diffusion processes, denoising score matching (DSM), implicit score matching (ISM), local-DSM, generative models, statistical physics, limitations of linear processes, challenges with non-Gaussian priors, high-dimensional nonlinear diffusion processes, automated training, score estimation, new denoising score matching objectives, CIFAR10


- [Data-free Distillation of Diffusion Models with Bootstrapping](https://icml.cc/virtual/2024/poster/33280) (Poster)
  - **Authors:** [Jiatao Gu](http://openreview.net/profile?id=~Jiatao_Gu1), [Chen Wang](http://openreview.net/profile?id=~Chen_Wang13), [Shuangfei Zhai](http://openreview.net/profile?id=~Shuangfei_Zhai3), [Yizhe Zhang](http://openreview.net/profile?id=~Yizhe_Zhang2), [Lingjie Liu](http://openreview.net/profile?id=~Lingjie_Liu1), [Joshua M Susskind](http://openreview.net/profile?id=~Joshua_M._Susskind1)
  - **Affiliations:** Apple; University of Pennsylvania, University of Pennsylvania, Apple, Apple, University of Pennsylvania, Apple
  - **TL;DR:** This study presents a novel data-free distillation technique for diffusion models, addressing the slow generation issue by learning a time-conditioned model that predicts outputs from a pre-trained teacher model. The proposed method significantly reduces inference steps while maintaining quality, making it efficient for image generation tasks.
  - **Keywords:** diffusion models, knowledge distillation, data-free distillation, time-conditioned model, bootstrapping, image generation, slow generation, iterative denoising, efficient data-free distillation algorithm


- [Two Fists, One Heart: Multi-Objective Optimization Based Strategy Fusion for Long-tailed Learning](https://icml.cc/virtual/2024/poster/34249) (Poster)
  - **Authors:** [Zhe Zhao](http://openreview.net/profile?id=~Zhe_Zhao5), [Pengkun Wang](http://openreview.net/profile?id=~Pengkun_Wang1), [HaiBin Wen](http://openreview.net/profile?id=~HaiBin_Wen1), [Wei Xu](http://openreview.net/profile?id=~Wei_Xu21), [LAI Song](http://openreview.net/profile?id=~Song_Lai1), [Qingfu Zhang](http://openreview.net/profile?id=~Qingfu_Zhang1), [Yang Wang](http://openreview.net/profile?id=~Yang_Wang32)
  - **Affiliations:** University of Science and Technology of China, Hefei 230026, China; City University of Hong Kong, None, University of Science and Technology of China, Hefei 230026, China; Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou 215123, China, Shaoguan University, None, None, City University of Hong Kong, None, City University of Hong Kong, None, University of Science and Technology of China, Hefei 230026, China; Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou 215123, China; Key Laboratory of Precision and Intelligent Chemistry, USTC
  - **TL;DR:** This study addresses the challenges of long-tailed learning by transforming the trade-off problem into a multi-objective optimization problem and proposing a strategy fusion method (MOOSF) that effectively resolves conflicts between performance on head and tail classes. The findings demonstrate that even simple strategy fusion can outperform more complex long-tailed learning strategies, offering a new perspective on the issue.
  - **Keywords:** long-tailed learning, multi-objective optimization, strategy fusion, multi-objective optimization (MOO), trade-off between head and tail classes, imbalance distribution of sample classes, Multi-Objective Optimization based Strategy Fusion (MOOSF), efficient fusion of heterogeneous strategies


- [Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization](https://icml.cc/virtual/2024/poster/33888) (Poster)
  - **Authors:** [Kwang-Sung Jun](http://openreview.net/profile?id=~Kwang-Sung_Jun1), [Jungtaek Kim](http://openreview.net/profile?id=~Jungtaek_Kim1)
  - **Affiliations:** University of Arizona, University of Pittsburgh
  - **TL;DR:** This paper addresses the challenge of adapting to unknown noise levels in linear bandits by proposing a novel semi-adaptive confidence set and a variance-adaptive algorithm, demonstrating improved performance in Bayesian optimization tasks. The findings indicate that the proposed methods achieve better or comparable results compared to existing approaches.
  - **Keywords:** Linear Bandits, Bayesian Optimization, Confidence Sets, Variance-Adaptive Algorithms, Recommendation Systems, Online Advertising, Unknown Noise Level, Efficient Exploration, Improved Regret Bound, Novel Linear Bandit Algorithm (LOSAN), Sub-Gaussian Parameter, Regret Equality


- [AD3: Implicit Action is the Key for World Models to Distinguish the Diverse Visual Distractors](https://icml.cc/virtual/2024/poster/33712) (Poster)
  - **Authors:** [Yucen Wang](http://openreview.net/profile?id=~Yucen_Wang1), [Shenghua Wan](http://openreview.net/profile?id=~Shenghua_Wan1), [Le Gan](http://openreview.net/profile?id=~Le_Gan1), [Shuai Feng](http://openreview.net/profile?id=~Shuai_Feng3), [De-Chuan Zhan](http://openreview.net/profile?id=~De-Chuan_Zhan1)
  - **Affiliations:** School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Cyberspace Science and Technology, Beijing Institute of Technology, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China
  - **TL;DR:** This study introduces the Implicit Action Generator (IAG) and the AD3 algorithm to effectively distinguish homogeneous visual distractors in reinforcement learning tasks. The proposed methods demonstrate superior performance in optimizing policies within task-relevant state spaces, addressing significant challenges posed by distractors that resemble controllable agents.
  - **Keywords:** reinforcement learning, visual control, task-irrelevant distractors, Implicit Action Generator (IAG), implicit Action-informed Diverse visual Distractors Distinguisher (AD3), homogeneous distractors, distinguishing task-relevant and irrelevant components, superior performance on visual control tasks, optimization of policy within task-relevant state space, model-based methods, observation reconstruction, adversarial reward dissociation


- [Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT](https://icml.cc/virtual/2024/poster/34452) (Poster)
  - **Authors:** [Jon Saad-Falcon](http://openreview.net/profile?id=~Jon_Saad-Falcon1), [Daniel Y Fu](http://openreview.net/profile?id=~Daniel_Y_Fu1), [Simran Arora](http://openreview.net/profile?id=~Simran_Arora1), [Neel Guha](http://openreview.net/profile?id=~Neel_Guha1), [Christopher Re](http://openreview.net/profile?id=~Christopher_Re1)
  - **Affiliations:** Stanford University, Computer Science, Stanford, CA, Stanford University, Computer Science, Stanford, CA, Stanford University, Computer Science, Stanford, CA, Stanford University, Computer Science, Stanford, CA, Stanford University, Computer Science, Stanford, CA
  - **TL;DR:** This paper presents the M2-BERT retrieval encoder designed for long-context documents, addressing challenges in evaluating and pretraining retrieval models. The proposed model outperforms existing Transformer-based models while being significantly smaller in size.
  - **Keywords:** long-context retrieval, machine learning systems, M2-BERT, Monarch Mixer architecture, state-space encoder, search, question-answering, dialogue, fact verification, long documents, synthesizing information across text, evaluation of long-context retrieval performance, LoCoV1 benchmark, finetuning approach for retrieval, performance improvement over Transformer-based models


- [Graph Neural Networks Use Graphs When They Shouldn't](https://icml.cc/virtual/2024/poster/33463) (Poster)
  - **Authors:** [Maya Bechler-Speicher](http://openreview.net/profile?id=~Maya_Bechler-Speicher1), [Ido Amos](http://openreview.net/profile?id=~Ido_Amos1), [Ran Gilad-Bachrach](http://openreview.net/profile?id=~Ran_Gilad-Bachrach2), [Amir Globerson](http://openreview.net/profile?id=~Amir_Globerson1)
  - **Affiliations:** Blavatnik School of Computer Science, Tel-Aviv University; Google Research, School of Electrical Engineering, Tel-Aviv University, Department of Bio-Medical Engineering and Edmond J. Safra Center for Bioinformatics, Tel-Aviv University, Blavatnik School of Computer Science, Tel-Aviv University; Google Research
  - **TL;DR:** This study investigates the tendency of Graph Neural Networks (GNNs) to overfit the provided graph structure, demonstrating that in some cases, ignoring the graph can yield better predictive performance. The findings suggest that regular graphs can mitigate this overfitting and improve GNN performance.
  - **Keywords:** Graph Neural Networks, Overfitting, Graph Structure, Gradient Descent, Neural Networks, Social Networks, Molecular Biology, Graph Overfitting, Non-informative Graph Structures, Extrapolation in Regular Graphs, Performance Enhancement, GNNs (Graph Neural Networks), Regular Graphs


- [Diffusion Models Encode the Intrinsic Dimension of Data Manifolds](https://icml.cc/virtual/2024/poster/33707) (Poster)
  - **Authors:** [Jan Stanczuk](http://openreview.net/profile?id=~Jan_Pawel_Stanczuk1), [Georgios Batzolis](http://openreview.net/profile?id=~Georgios_Batzolis1), [Teo Deveney](http://openreview.net/profile?id=~Teo_Deveney1), [Carola-Bibiane Schönlieb](http://openreview.net/profile?id=~Carola-Bibiane_Sch%C3%B6nlieb1)
  - **Affiliations:** Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, United Kingdom, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, United Kingdom, Department of Mathematical Sciences, University of Bath, Bath, United Kingdom, Department of Applied Mathematics and Theoretical Physics, University of Cambridge, Cambridge, United Kingdom
  - **TL;DR:** This study presents a mathematical proof that diffusion models can effectively encode data manifolds by approximating their normal bundles, leading to a novel method for estimating the intrinsic dimension of data. The proposed method outperforms established estimators in experiments on both Euclidean and image data.
  - **Keywords:** diffusion models, intrinsic dimension, data manifolds, score matching, gradient of log-density, high-dimensional data modeling, dimensionality reduction, curse of dimensionality, data sparsity, novel method for estimating intrinsic dimension, approximation of normal bundles, GANs (Generative Adversarial Networks), Variational auto-encoders (VAEs), M-flows


- [In-Context Learning Agents Are Asymmetric Belief Updaters](https://icml.cc/virtual/2024/poster/34730) (Poster)
  - **Authors:** [Johannes A. Schubert](http://openreview.net/profile?id=~Johannes_A._Schubert1), [Akshay Kumar Jagadish](http://openreview.net/profile?id=~Akshay_Kumar_Jagadish1), [Marcel Binz](http://openreview.net/profile?id=~Marcel_Binz1), [Eric Schulz](http://openreview.net/profile?id=~Eric_Schulz1)
  - **Affiliations:** Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany, Computational Principles of Intelligence Lab, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Institute for Human-Centered AI, Helmholtz Computational Health Center, Munich, Germany
  - **TL;DR:** This study investigates the in-context learning dynamics of large language models, revealing that they exhibit asymmetric belief updating, learning more from positive outcomes than negative ones. The findings suggest that the framing of learning tasks significantly influences the learning process, with implications for both artificial and human agents.
  - **Keywords:** in-context learning, belief updating, large language models (LLMs), meta-reinforcement learning, two-alternative forced choice (2AFC), cognitive psychology, artificial intelligence, asymmetric belief updating, optimism bias, counterfactual feedback, insights into in-context learning dynamics, implications for problem framing


- [Overcoming Saturation in Density Ratio Estimation by Iterated Regularization](https://icml.cc/virtual/2024/poster/34256) (Poster)
  - **Authors:** [Lukas Gruber](http://openreview.net/profile?id=~Lukas_Gruber2), [Markus Holzleitner](http://openreview.net/profile?id=~Markus_Holzleitner1), [Johannes Lehner](http://openreview.net/profile?id=~Johannes_Lehner1), [Sepp Hochreiter](http://openreview.net/profile?id=~Sepp_Hochreiter1), [Werner Zellinger](http://openreview.net/profile?id=~Werner_Zellinger1)
  - **Affiliations:** ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz, Austria, MaLGa Center, Department of Mathematics, University of Genoa, ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz, Austria, ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning, Johannes Kepler University Linz, Austria; NXAI GmbH, Linz, Austria, Johann Radon Institute for Computational and Applied Mathematics, Austrian Academy of Sciences
  - **TL;DR:** This study addresses the issue of error saturation in density ratio estimation methods, proposing iterated regularization to improve error convergence rates. The introduced methods demonstrate superior performance on benchmarks and in large-scale evaluations for domain adaptation models.
  - **Keywords:** density ratio estimation, error saturation, regularization, kernel methods, Bregman divergence, reproducing kernel Hilbert space (RKHS), anomaly detection, two-sample testing, unsupervised domain adaptation, generative modeling, error convergence rates, saturation in inverse problems, iterated regularization, fast error rates


- [Adaptive Sampling of k-Space in Magnetic Resonance for Rapid Pathology Prediction](https://icml.cc/virtual/2024/poster/33405) (Poster)
  - **Authors:** [Chen-Yu Yen](http://openreview.net/profile?id=~Chen-Yu_Yen1), [raghav singhal](http://openreview.net/profile?id=~Raghav_Singhal1), [Umang Sharma](http://openreview.net/profile?id=~Umang_Sharma1), [Rajesh Ranganath](http://openreview.net/profile?id=~Rajesh_Ranganath2), [Sumit Chopra](http://openreview.net/profile?id=~Sumit_Chopra1), [Lerrel Pinto](http://openreview.net/profile?id=~Lerrel_Pinto1)
  - **Affiliations:** New York University, New York University, New York University, New York University, New York University, New York University
  - **TL;DR:** This study introduces Adaptive Sampling for MR (ASMR), a method that optimizes k-space sampling for disease detection in MR imaging, achieving near-full sampling performance with only 8% of the data. The results demonstrate significant potential for reducing scan times while maintaining diagnostic accuracy across multiple pathology classification tasks.
  - **Keywords:** Magnetic Resonance Imaging, Disease Detection, Adaptive Sampling, Adaptive Sampling for MR (ASMR), k-space sampling, Medical Imaging, Pathology Prediction, Lengthy Scan Times, Inaccessibility of MR Imaging, Performance Optimization in Disease Detection, Bypassing Image Reconstruction, k-space, Fourier Transform, Compressed Sensing


- [Self-Rewarding Language Models](https://icml.cc/virtual/2024/poster/35202) (Poster)
  - **Authors:** [Weizhe Yuan](http://openreview.net/profile?id=~Weizhe_Yuan1), [Richard Yuanzhe Pang](http://openreview.net/profile?id=~Richard_Yuanzhe_Pang1), [Kyunghyun Cho](http://openreview.net/profile?id=~Kyunghyun_Cho1), [Xian Li](http://openreview.net/profile?id=~Xian_Li1), [Sainbayar Sukhbaatar](http://openreview.net/profile?id=~Sainbayar_Sukhbaatar1), [Jing Xu](http://openreview.net/profile?id=~Jing_Xu5), [JASON WESTON](http://openreview.net/profile?id=~Jason_E_Weston1)
  - **Affiliations:** Meta; New York University, Meta; New York University, New York University, Meta, Meta, Meta, Meta; New York University
  - **TL;DR:** This study introduces Self-Rewarding Language Models that utilize their own feedback to improve instruction following capabilities and reward generation. The approach demonstrates significant performance enhancements over existing systems, suggesting a pathway towards models that can continually self-improve.
  - **Keywords:** Self-Rewarding Language Models, Superhuman agents, Instruction following, Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), LLM-as-a-Judge prompting, Bottleneck of human performance level, Need for superhuman feedback, Improved instruction following ability, High-quality self-reward generation, Llama 2 70B, Open Assistant, AlpacaEval 2.0 leaderboard


- [Efficient Algorithms for Empirical Group Distributionally Robust Optimization and Beyond](https://icml.cc/virtual/2024/poster/33045) (Poster)
  - **Authors:** [Dingzhi Yu](http://openreview.net/profile?id=~Dingzhi_Yu1), [Yunuo Cai](http://openreview.net/profile?id=~Yunuo_Cai1), [Wei Jiang](http://openreview.net/profile?id=~Wei_Jiang8), [Lijun Zhang](http://openreview.net/profile?id=~Lijun_Zhang1)
  - **Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; None, School of Data Science, Fudan University, Shanghai, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China, National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Pazhou Laboratory (Huangpu), Guangzhou, China
  - **TL;DR:** This paper presents efficient algorithms for empirical Group Distributionally Robust Optimization (GDRO), specifically developing the ALEG algorithm to minimize maximal empirical risk across distinct groups. The proposed methods demonstrate improved computational efficiency and accuracy compared to existing techniques, with implications for various machine learning applications.
  - **Keywords:** Group Distributionally Robust Optimization (GDRO), Empirical Risk Minimization (ERM), Minimax optimization, Stochastic primal-dual algorithm, Variance reduction techniques, Federated learning, Robust language modeling, Robust neural network training, Collaborative PAC learning, Maximal empirical risk minimization, Empirical variant of risk minimization, ALEG algorithm, ALEM optimization algorithm


- [COALA: A Practical and Vision-Centric Federated Learning Platform](https://icml.cc/virtual/2024/poster/34768) (Poster)
  - **Authors:** [Weiming Zhuang](http://openreview.net/profile?id=~Weiming_Zhuang1), [Jian Xu](http://openreview.net/profile?id=~Jian_Xu7), [Chen Chen](http://openreview.net/profile?id=~Chen_Chen20), [Jingtao Li](http://openreview.net/profile?id=~Jingtao_Li1), [Lingjuan Lyu](http://openreview.net/profile?id=~Lingjuan_Lyu1)
  - **Affiliations:** Sony AI; Tsinghua University, Tsinghua University, Sony AI, Sony AI, Sony AI
  - **TL;DR:** The paper presents COALA, a vision-centric Federated Learning platform that benchmarks various practical scenarios across task, data, and model levels. It highlights the potential of Federated Learning in computer vision applications while addressing challenges such as data privacy and heterogeneity.
  - **Keywords:** Federated Learning, Computer Vision, Object Detection, Segmentation, Pose Estimation, Data Privacy, Data Heterogeneity, Benchmarking, Federated Continual Learning


- [Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback](https://icml.cc/virtual/2024/poster/33788) (Poster)
  - **Authors:** [songyang gao](http://openreview.net/profile?id=~Songyang_Gao1), [Qiming Ge](http://openreview.net/profile?id=~Qiming_Ge2), [Wei Shen](http://openreview.net/profile?id=~Wei_Shen12), [Shihan Dou](http://openreview.net/profile?id=~Shihan_Dou1), [Junjie Ye](http://openreview.net/profile?id=~Junjie_Ye4), [Xiao Wang](http://openreview.net/profile?id=~Xiao_Wang12), [Rui Zheng](http://openreview.net/profile?id=~Rui_Zheng1), [Yicheng Zou](http://openreview.net/profile?id=~Yicheng_Zou1), [Zhi Chen](http://openreview.net/profile?id=~Zhi_Chen1), [Hang Yan](http://openreview.net/profile?id=~Hang_Yan2), [Qi Zhang](http://openreview.net/profile?id=~Qi_Zhang8), [Dahua Lin](http://openreview.net/profile?id=~Dahua_Lin1)
  - **Affiliations:** Shanghai Artificial Intelligence Laboratory, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, ByteDance Inc, Shanghai Artificial Intelligence Laboratory, Shanghai Artificial Intelligence Laboratory, Shanghai Artificial Intelligence Laboratory, School of Computer Science, Fudan University, Shanghai Artificial Intelligence Laboratory
  - **TL;DR:** This paper presents Linear Alignment, a novel algorithm that aligns language models with human preferences in a single inference step, eliminating the need for complex data annotation and model training. The proposed method significantly improves the performance and efficiency of aligning large language models across various scenarios.
  - **Keywords:** AI alignment, human preferences, language models, Reinforcement Learning from Human Feedback (RLHF), Linear Alignment, policy optimization, AI assistants, personalized AI, Complex annotation and training requirements, scalability issues in RLHF, Linear Alignment algorithm, closed-form policy extraction, Large Language Models (LLMs), PPO (Proximal Policy Optimization)


- [A Closer Look at the Limitations of Instruction Tuning](https://icml.cc/virtual/2024/poster/33808) (Poster)
  - **Authors:** [Sreyan Ghosh](http://openreview.net/profile?id=~Sreyan_Ghosh1), [Chandra Kiran Evuru](http://openreview.net/profile?id=~Chandra_Kiran_Reddy_Evuru1), [Sonal Kumar](http://openreview.net/profile?id=~Sonal_Kumar1), [Ramaneswaran S](http://openreview.net/profile?id=~Ramaneswaran_S1), [Deepali Aneja](http://openreview.net/profile?id=~Deepali_Aneja2), [Zeyu Jin](http://openreview.net/profile?id=~Zeyu_Jin2), [Ramani Duraiswami](http://openreview.net/profile?id=~Ramani_Duraiswami1), [Dinesh Manocha](http://openreview.net/profile?id=~Dinesh_Manocha3)
  - **Affiliations:** University of Maryland, College Park, USA; None, University of Maryland, College Park, USA, University of Maryland, College Park, USA, NVIDIA, India, Adobe, USA, Adobe, USA, University of Maryland, College Park, USA, University of Maryland, College Park, USA
  - **TL;DR:** This paper investigates the limitations of Instruction Tuning (IT) in large language models, revealing that IT does not enhance knowledge or skills and can lead to knowledge degradation and increased hallucination. The findings suggest that responses generated from pre-trained knowledge consistently outperform those from models that learn new knowledge through IT.
  - **Keywords:** Instruction Tuning, Large Language Models, Conversational Agents, LoRA fine-tuning, Full-parameter fine-tuning, Open-domain conversational agents, Natural Language Processing, Knowledge degradation, Response quality decline, Hallucination, Limitations of Instruction Tuning, Performance comparison of models


- [Sign Gradient Descent-based Neuronal Dynamics: ANN-to-SNN Conversion Beyond ReLU Network](https://icml.cc/virtual/2024/poster/33242) (Poster)
  - **Authors:** [Hyunseok Oh](http://openreview.net/profile?id=~Hyunseok_Oh1), [Youngki Lee](http://openreview.net/profile?id=~Youngki_Lee2)
  - **Affiliations:** Department of Computer Science & Engineering, Seoul National University, Seoul, Republic of Korea, Department of Computer Science & Engineering, Seoul National University, Seoul, Republic of Korea
  - **TL;DR:** This study presents a novel sign gradient descent-based neuronal dynamics model that enhances the performance of ANN-to-SNN conversion beyond ReLU networks, achieving state-of-the-art results on large-scale datasets. The research connects the dynamics of spiking neurons with optimization methods, addressing limitations in discrete theory and nonlinearity support.
  - **Keywords:** Spiking Neural Networks (SNN), Artificial Neural Networks (ANN), Energy-efficient AI, Sign Gradient Descent (signGD), Integrate-and-fire models, Subgradient method, AI inference, Neuroscientific simulation, Discrete theory limitations, Nonlinearity support, New neuronal dynamics model, Improved ANN-to-SNN conversion performance


- [Theoretical Analysis of Learned Database Operations under Distribution Shift through Distribution Learnability](https://icml.cc/virtual/2024/poster/33068) (Oral)
  - **Authors:** [Sepanta Zeighami](http://openreview.net/profile?id=~Sepanta_Zeighami2), [Cyrus Shahabi](http://openreview.net/profile?id=~Cyrus_Shahabi1)
  - **Affiliations:** University of California, Berkeley; None, University of Southern California
  - **TL;DR:** This paper presents a theoretical analysis of learned database operations under distribution shift, focusing on indexing, cardinality estimation, and sorting. It establishes performance bounds and characterizes when learned models outperform traditional methods, addressing the challenges posed by dynamic datasets.
  - **Keywords:** learned database operations, distribution shift, theoretical analysis, machine learning models, distribution learnability framework, indexing, cardinality estimation, sorting, performance degradation under distribution shift, lack of theoretical understanding, theoretical characterization of learned models, performance bounds


- [EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for Ligand Binding Site Prediction](https://icml.cc/virtual/2024/poster/35126) (Oral)
  - **Authors:** [yang zhang](http://openreview.net/profile?id=~yang_zhang28), [Zhewei Wei](http://openreview.net/profile?id=~Zhewei_Wei1), [Ye Yuan](http://openreview.net/profile?id=~Ye_Yuan15), [Chongxuan Li](http://openreview.net/profile?id=~Chongxuan_Li1), [Wenbing Huang](http://openreview.net/profile?id=~Wenbing_Huang1)
  - **Affiliations:** Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China, Beijing Institute of Technology, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China
  - **TL;DR:** This study introduces EquiPocket, an E(3)-equivariant Graph Neural Network for predicting ligand binding sites in proteins, addressing limitations of existing CNN-based methods. The proposed framework demonstrates superior performance on benchmark datasets, enhancing the accuracy of binding site predictions crucial for drug discovery.
  - **Keywords:** ligand binding site prediction, drug discovery, E(3)-equivariant Graph Neural Network (GNN), deep learning, convolutional neural networks (CNN), protein-ligand docking, structure-based molecular generation, representation of irregular protein structures, sensitivity to rotations, characterization of protein surface, protein size shift, EquiPocket framework, dense attention output layer


- [Fast and Sample Efficient Multi-Task Representation Learning in Stochastic Contextual Bandits](https://icml.cc/virtual/2024/poster/32809) (Poster)
  - **Authors:** [Jiabin Lin](http://openreview.net/profile?id=~Jiabin_Lin1), [Shana Moothedath](http://openreview.net/profile?id=~Shana_Moothedath1), [Namrata Vaswani](http://openreview.net/profile?id=~Namrata_Vaswani1)
  - **Affiliations:** Department of Electrical and Computer Engineering, Iowa State University, Ames IA 50011-1250, USA, Department of Electrical and Computer Engineering, Iowa State University, Ames IA 50011-1250, USA, Department of Electrical and Computer Engineering, Iowa State University, Ames IA 50011-1250, USA
  - **TL;DR:** This study investigates how representation learning can enhance the efficiency of learning in contextual bandit problems by developing a new algorithm that recovers a low-rank feature matrix. The proposed multi-task learning algorithm demonstrates improved performance through theoretical analysis and empirical experiments.
  - **Keywords:** representation learning, contextual bandits, multi-task learning, alternating projected gradient descent, minimization estimator, robotics, clinical trials, communications, recommender systems, exploration-exploitation dilemma, learning efficiency, regret bound, low-rank feature matrix recovery


- [FADAS: Towards Federated Adaptive Asynchronous Optimization](https://icml.cc/virtual/2024/poster/33327) (Poster)
  - **Authors:** [Yujia Wang](http://openreview.net/profile?id=~Yujia_Wang3), [Shiqiang Wang](http://openreview.net/profile?id=~Shiqiang_Wang1), [Songtao Lu](http://openreview.net/profile?id=~Songtao_Lu1), [Jinghui Chen](http://openreview.net/profile?id=~Jinghui_Chen1)
  - **Affiliations:** College of Information Sciences and Technology, Pennsylvania State University, State College, PA, USA, IBM T. J. Watson Research Center, Yorktown Heights, NY, USA, IBM T. J. Watson Research Center, Yorktown Heights, NY, USA, College of Information Sciences and Technology, Pennsylvania State University, State College, PA, USA
  - **TL;DR:** This paper introduces FADAS, a novel federated adaptive asynchronous optimization method that addresses the challenges of synchronous aggregation in federated learning, particularly in the presence of straggler clients. The proposed method incorporates asynchronous updates and a delay-adaptive learning adjustment strategy, demonstrating superior performance and established convergence rates compared to existing asynchronous federated learning methods.
  - **Keywords:** Federated Learning, Privacy-Preserving Machine Learning, Adaptive Federated Optimization, Asynchronous Updates, Stochastic Gradient Descent (SGD), Large-Scale Model Training, Synchronous Aggregation Challenges, Straggler Clients, Asynchronous Delays, FADAS Method, Delay-Adaptive Learning Adjustment Strategy, Convergence Rate Establishment, FedAvg, FedAdam, FedAMS, FedLALR, FedLADA, FAFED, Adam, AdamW


- [Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior](https://icml.cc/virtual/2024/poster/34683) (Poster)
  - **Authors:** [Shuyu Cheng](http://openreview.net/profile?id=~Shuyu_Cheng1), [Yibo Miao](http://openreview.net/profile?id=~Yibo_Miao1), [Yinpeng Dong](http://openreview.net/profile?id=~Yinpeng_Dong2), [Xiao Yang](http://openreview.net/profile?id=~Xiao_Yang4), [Xiao-Shan Gao](http://openreview.net/profile?id=~Xiao-Shan_Gao2), [Jun Zhu](http://openreview.net/profile?id=~Jun_Zhu2)
  - **Affiliations:** Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China; JQ Investments, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China; RealAI, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China, Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, THBI Lab, BNRist Center, Tsinghua University, Beijing, 100084, China; RealAI
  - **TL;DR:** This study introduces a Prior-guided Bayesian Optimization (P-BO) algorithm to enhance the efficiency of black-box adversarial attacks by leveraging a surrogate model as a global function prior. The proposed method significantly reduces the number of queries needed and improves attack success rates compared to existing state-of-the-art black-box attacks.
  - **Keywords:** Black-box adversarial attacks, Adversarial machine learning, Bayesian Optimization, Gaussian process, Query-based attacks, Image classifiers, Vision-language models, Query efficiency, Adversarial transferability, Vulnerability to adversarial examples, Prior-guided Bayesian Optimization (P-BO), Adaptive integration strategy


- [Stochastic Interpolants with Data-Dependent Couplings](https://icml.cc/virtual/2024/poster/34545) (Spotlight Poster)
  - **Authors:** [Michael Albergo](http://openreview.net/profile?id=~Michael_Samuel_Albergo1), [Mark Goldstein](http://openreview.net/profile?id=~Mark_Goldstein1), [Nicholas Boffi](http://openreview.net/profile?id=~Nicholas_Matthew_Boffi1), [Rajesh Ranganath](http://openreview.net/profile?id=~Rajesh_Ranganath2), [Eric Vanden-Eijnden](http://openreview.net/profile?id=~Eric_Vanden-Eijnden1)
  - **Affiliations:** Center for Cosmology and Particle Physics, New York University, Courant Institute of Mathematical Sciences, New York University, Courant Institute of Mathematical Sciences, New York University, Center for Data Science, New York University, Courant Institute of Mathematical Sciences, New York University
  - **TL;DR:** This study introduces a framework for coupling base and target densities in generative models using stochastic interpolants, enabling the construction of conditional generative models. The authors demonstrate the effectiveness of their approach through applications in super-resolution and in-painting.
  - **Keywords:** generative models, dynamical transport of measure, stochastic interpolants, ordinary differential equations (ODE), stochastic differential equations (SDE), super-resolution, in-painting, coupling of base and target densities, data-agnostic base density, conditional generative models, square loss regression problem, transport maps


- [Is Kernel Prediction More Powerful than Gating in Convolutional Neural Networks?](https://icml.cc/virtual/2024/poster/33951) (Poster)
  - **Authors:** [Lorenz K. Muller](http://openreview.net/profile?id=~Lorenz_K_Muller1)
  - **Affiliations:** Computing Systems Lab, Huawei Technologies, Zurich, Switzerland
  - **TL;DR:** This paper investigates the relationship between gating mechanisms and weight prediction in convolutional neural networks, demonstrating that gating can effectively imitate weight prediction layers. The findings suggest a novel approach to image denoising that reduces memory requirements by combining fixed layers and gating.
  - **Keywords:** HyperNetworks, kernel prediction, gating mechanisms, Convolutional Neural Networks, factorization machines, SGD (Stochastic Gradient Descent), Image denoising, recommender systems, natural language processing, Hierarchy of multiplicative interactions, memory requirements, Equivalence between gating and weight prediction, reformulation of predicted kernels


- [Arrows of Time for Large Language Models](https://icml.cc/virtual/2024/poster/33931) (Oral)
  - **Authors:** [Vassilis Papadopoulos](http://openreview.net/profile?id=~Vassilis_Papadopoulos1), [Jérémie Wenger](http://openreview.net/profile?id=~J%C3%A9r%C3%A9mie_Wenger1), [Clement Hongler](http://openreview.net/profile?id=~Cl%C3%A9ment_Hongler1)
  - **Affiliations:** FSL/Institute of Physics, EPFL; CSFT/Institute of Mathematics, EPFL, Lausanne, Switzerland, Department of Computing, Goldsmiths/University of London, London, UK, CSFT/Institute of Mathematics, EPFL, Lausanne, Switzerland
  - **TL;DR:** This study investigates the time directionality in the probabilistic modeling of Autoregressive Large Language Models (LLMs), revealing a consistent asymmetry in their ability to predict tokens based on their temporal order. The findings suggest that this asymmetry can be explained through considerations of sparsity and computational complexity, challenging traditional information-theoretic perspectives.
  - **Keywords:** Large Language Models, Time Directionality, Autoregressive Models, Probabilistic Modeling, Natural Language Processing, Time Asymmetry, Log-Perplexity, Theoretical Framework for Asymmetry, Insights on Sparsity and Computational Complexity, GPT, Transformer, BERT


- [Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains](https://icml.cc/virtual/2024/poster/34270) (Poster)
  - **Authors:** [Junhong Shen](http://openreview.net/profile?id=~Junhong_Shen1), [Neil Tenenholtz](http://openreview.net/profile?id=~Neil_Tenenholtz1), [James Hall](http://openreview.net/profile?id=~James_Brian_Hall1), [David Alvarez-Melis](http://openreview.net/profile?id=~David_Alvarez-Melis1), [Nicolò Fusi](http://openreview.net/profile?id=~Nicolo_Fusi1)
  - **Affiliations:** Carnegie Mellon University, Microsoft Research, Microsoft Research, Microsoft Research; Harvard University, Microsoft Research
  - **TL;DR:** This study presents TAG-LLM, a novel framework for repurposing general-purpose large language models (LLMs) to effectively solve tasks in specialized domains by learning custom input tags. The approach enhances LLM performance in areas like predicting chemical and protein properties while enabling zero-shot generalization to new problems.
  - **Keywords:** Repurposing LLMs, Specialized Domains, Model-agnostic framework, Continuous vectors, Input tags, Biomedical sciences, Chemical properties prediction, Drug-target interactions, Performance degradation in specialized domains, Catastrophic forgetting, Data sparsity, Zero-shot generalization, Enhanced LLM performance, Large Language Models (LLMs), Domain tags, Function tags


- [In-Context Language Learning: Architectures and Algorithms](https://icml.cc/virtual/2024/poster/35057) (Poster)
  - **Authors:** [Ekin Akyürek](http://openreview.net/profile?id=~Ekin_Aky%C3%BCrek1), [Bailin Wang](http://openreview.net/profile?id=~Bailin_Wang3), [Yoon Kim](http://openreview.net/profile?id=~Yoon_Kim1), [Jacob Andreas](http://openreview.net/profile?id=~Jacob_Andreas1)
  - **Affiliations:** MIT CSAIL, MIT CSAIL, MIT CSAIL, MIT CSAIL
  - **TL;DR:** This study investigates in-context learning (ICL) in neural language models through a new framework called in-context language learning (ICLL), focusing on regular languages generated by finite automata. The findings reveal that Transformers outperform other models on ICLL tasks by leveraging specialized attention heads, leading to improved performance in natural language modeling.
  - **Keywords:** in-context learning, formal language learning, Transformers, neural sequence models, attention heads, natural language modeling, divergence between model problems and real ICL, structured generative models, improved performance on synthetic ICLL, reduced perplexity in language modeling, SlimPajama dataset, regular languages, random finite automata, n-gram statistics


- [Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective](https://icml.cc/virtual/2024/poster/33154) (Poster)
  - **Authors:** [Shokichi Takakura](http://openreview.net/profile?id=~Shokichi_Takakura1), [Taiji Suzuki](http://openreview.net/profile?id=~Taiji_Suzuki1)
  - **Affiliations:** Department of Mathematical Informatics, the University of Tokyo, Tokyo, Japan; Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan, Department of Mathematical Informatics, the University of Tokyo, Tokyo, Japan; Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan
  - **TL;DR:** This study investigates the feature learning capabilities of two-layer neural networks in the mean-field regime, demonstrating that they can efficiently learn a union of multiple reproducing kernel Hilbert spaces and acquire a data-dependent kernel that aligns with the target function. The findings highlight the global convergence of mean-field Langevin dynamics and the role of implicit regularization in optimization.
  - **Keywords:** mean-field analysis, two-layer neural networks, feature learning, mean-field Langevin dynamics, kernel methods, gradient descent, non-convex optimization, data-dependent kernel, optimization landscape, global convergence, implicit regularization, optimization guarantees, neural tangent kernel (NTK), reproducing kernel Hilbert spaces


- [Gated Linear Attention Transformers with Hardware-Efficient Training](https://icml.cc/virtual/2024/poster/33349) (Poster)
  - **Authors:** [Songlin Yang](http://openreview.net/profile?id=~Songlin_Yang1), [Bailin Wang](http://openreview.net/profile?id=~Bailin_Wang3), [Yikang Shen](http://openreview.net/profile?id=~Yikang_Shen1), [Rameswar Panda](http://openreview.net/profile?id=~Rameswar_Panda1), [Yoon Kim](http://openreview.net/profile?id=~Yoon_Kim1)
  - **Affiliations:** Massachusetts Institute of Technology, Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, MIT-IBM Watson AI Lab, Massachusetts Institute of Technology
  - **TL;DR:** This study presents a hardware-efficient algorithm for linear attention in Transformers, resulting in a gated linear attention (GLA) model that performs competitively with traditional softmax attention and excels in length generalization. The GLA Transformer demonstrates higher training throughput and effective performance on moderate-scale language modeling tasks.
  - **Keywords:** linear attention, Transformers, efficient parallel training, FLASH LINEAR ATTENTION, gated linear attention (GLA), RNN with 2D hidden states, language modeling, underperformance of linear attention compared to softmax attention, I/O-awareness in implementations, hardware-efficient algorithm for linear attention, competitive performance against LLaMA-architecture Transformer, softmax attention, RetNet, Mamba, LLaMA-architecture


- [Agnostic Sample Compression Schemes for Regression](https://icml.cc/virtual/2024/poster/34897) (Spotlight Poster)
  - **Authors:** [Idan Attias](http://openreview.net/profile?id=~Idan_Attias1), [Steve Hanneke](http://openreview.net/profile?id=~Steve_Hanneke1), [Aryeh Kontorovich](http://openreview.net/profile?id=~Aryeh_Kontorovich1), [Menachem Sadigurschi](http://openreview.net/profile?id=~Menachem_Sadigurschi1)
  - **Affiliations:** Department of Computer Science, Ben-Gurion University, Israel, Department of Computer Science, Purdue University, USA, Department of Computer Science, Ben-Gurion University, Israel, Department of Computer Science, Ben-Gurion University, Israel
  - **TL;DR:** This paper presents the first positive results for bounded sample compression in agnostic regression settings using ℓp loss, demonstrating both approximate and exact compression schemes. It refines previous negative results and poses open questions regarding the existence of compression schemes for various loss functions.
  - **Keywords:** Sample compression, Agnostic regression, ℓp loss, Approximate sample compression scheme, Bounded sample compression, Fat-shattering dimension, Exact agnostic compression scheme, Approximate compression of size linear in dimension, Efficient exact sample compression scheme, VC-dimension, Pseudo-dimension, Fat-shattering dimension


- [Reducing Item Discrepancy via Differentially Private Robust Embedding Alignment for Privacy-Preserving Cross Domain Recommendation](https://icml.cc/virtual/2024/poster/32942) (Poster)
  - **Authors:** [Weiming Liu](http://openreview.net/profile?id=~Weiming_Liu2), [Xiaolin Zheng](http://openreview.net/profile?id=~Xiaolin_Zheng1), [Chaochao Chen](http://openreview.net/profile?id=~Chaochao_Chen3), [Jiahe Xu](http://openreview.net/profile?id=~Jiahe_Xu1), [Xinting Liao](http://openreview.net/profile?id=~Xinting_Liao1), [Fan Wang](http://openreview.net/profile?id=~Fan_Wang14), [Yanchao Tan](http://openreview.net/profile?id=~Yanchao_Tan1), [Yew Soon ONG](http://openreview.net/profile?id=~Yew-Soon_Ong1)
  - **Affiliations:** Zhejiang University, China, Zhejiang University, China, Zhejiang University, China, Zhejiang University, China, Zhejiang University, China, Zhejiang University, China, Fuzhou University, China, Nanyang Technology University, Singapore; Agency for Science, Technology and Research, Singapore
  - **TL;DR:** This paper presents the RidCDR model to address the Privacy-Preserving Cross-Domain Recommendation (PPCDR) problem, enhancing model performance on both source and target domains without overlapping users and items. The empirical results demonstrate that RidCDR significantly outperforms existing state-of-the-art models while ensuring data privacy.
  - **Keywords:** Cross-Domain Recommendation (CDR), Privacy-Preserving Cross-Domain Recommendation (PPCDR), Reducing Item Discrepancy (RidCDR), private-robust embedding alignment, Recommender systems, Data sparsity, privacy protection, non-overlapped users and items, Enhanced model performance, knowledge sharing across domains, Amazon dataset, Douban dataset


- [Dealing With Unbounded Gradients in Stochastic Saddle-point Optimization](https://icml.cc/virtual/2024/poster/34051) (Poster)
  - **Authors:** [Gergely Neu](http://openreview.net/profile?id=~Gergely_Neu1), [Nneka Okolo](http://openreview.net/profile?id=~Nneka_Okolo1)
  - **Affiliations:** Universitat Pompeu Fabra, Barcelona, Spain, Universitat Pompeu Fabra, Barcelona, Spain
  - **TL;DR:** This paper proposes a regularization technique to stabilize stochastic optimization methods for finding saddle points in convex-concave functions, addressing the challenge of unbounded gradients that can lead to instability and divergence. The method is applied to reinforcement learning, providing performance guarantees for near-optimal policy discovery in average-reward MDPs.
  - **Keywords:** stochastic optimization, saddle-point problems, stochastic first-order methods, gradient descent, reinforcement learning, unbounded gradients, instability, divergence, regularization technique, performance guarantees, convex-concave functions, bilinear objectives


- [ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations](https://icml.cc/virtual/2024/poster/34244) (Poster)
  - **Authors:** [Kailas Vodrahalli](http://openreview.net/profile?id=~Kailas_Vodrahalli1), [James Zou](http://openreview.net/profile?id=~James_Zou1)
  - **Affiliations:** Stanford University, Stanford University
  - **TL;DR:** This study investigates how users interact with text-to-image models to generate images similar to a target image through iterative prompting. The findings reveal diverse user strategies and propose a new metric for assessing AI model steerability across different image types.
  - **Keywords:** Human-AI interaction, text-to-image models, prompt engineering, Markov chain analysis, iterative prompting, Artistic creations, AI-generated images, Aligning AI output to human inputs, user prompting strategies, AI model steerability metric, insights into user strategies, ArtWhisperer dataset


- [Failures Are Fated, But Can Be Faded: Characterizing and Mitigating Unwanted Behaviors in Large-Scale Vision and Language Models](https://icml.cc/virtual/2024/poster/34614) (Spotlight Poster)
  - **Authors:** [Som Sagar](http://openreview.net/profile?id=~Som_Sagar1), [Aditya Taparia](http://openreview.net/profile?id=~Aditya_Taparia1), [Ransalu Senanayake](http://openreview.net/profile?id=~Ransalu_Senanayake1)
  - **Affiliations:** School of Computing and Augmented Intelligence, Arizona State University, Tempe, United States of America, School of Computing and Augmented Intelligence, Arizona State University, Tempe, United States of America, School of Computing and Augmented Intelligence, Arizona State University, Tempe, United States of America
  - **TL;DR:** This paper introduces a deep reinforcement learning-based method to characterize and mitigate failure modes in large-scale vision and language models, emphasizing the importance of understanding model failures for engineers and policymakers. The proposed approach effectively restructures the failure landscape to enhance model performance across various tasks.
  - **Keywords:** failure characterization, model auditing, deep learning, deep reinforcement learning, pre-trained models, Computer Vision, Natural Language Processing, Vision-Language tasks, model failures, social biases, alignment with human values, post-hoc methods, failure landscape restructuring


- [The Pitfalls of Next-Token Prediction](https://icml.cc/virtual/2024/poster/34893) (Poster)
  - **Authors:** [Gregor Bachmann](http://openreview.net/profile?id=~Gregor_Bachmann1), [Vaishnavh Nagarajan](http://openreview.net/profile?id=~Vaishnavh_Nagarajan3)
  - **Affiliations:** ETH Zürich, Switzerland, Google Research, US
  - **TL;DR:** The paper critiques the ability of next-token prediction models to accurately represent human intelligence, highlighting failures in teacher-forced training and proposing that training to predict multiple tokens in advance may resolve these issues. The findings suggest a need for further exploration beyond the current next-token prediction paradigm.
  - **Keywords:** next-token prediction, human intelligence modeling, autoregressive inference, teacher-forced training, Transformer, Mamba architecture, compounding errors, failure to learn accurate next-token predictor, predicting multiple tokens in advance


- [Estimating Canopy Height at Scale](https://icml.cc/virtual/2024/poster/33710) (Poster)
  - **Authors:** [Jan Pauls](http://openreview.net/profile?id=~Jan_Pauls1), [Max Zimmer](http://openreview.net/profile?id=~Max_Zimmer1), [Una Kelly](http://openreview.net/profile?id=~Una_M._Kelly1), [Martin Schwartz](http://openreview.net/profile?id=~Martin_Schwartz1), [Sassan Saatchi](http://openreview.net/profile?id=~Sassan_Saatchi1), [Philippe CIAIS](http://openreview.net/profile?id=~Philippe_CIAIS1), [Sebastian Pokutta](http://openreview.net/profile?id=~Sebastian_Pokutta1), [Martin Brandt](http://openreview.net/profile?id=~Martin_Brandt1), [Fabian Gieseke](http://openreview.net/profile?id=~Fabian_Gieseke1)
  - **Affiliations:** Department of Information Systems, University of Münster, Germany, Department for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany, Department of Information Systems, University of Münster, Germany, Laboratoire des Sciences du Climat et de l’Environnement, LSCE/IPSL, France, Jet Propulsion Laboratory (JPL), California Institute of Technology, USA, Laboratoire des Sciences du Climat et de l’Environnement, LSCE/IPSL, France, Department for AI in Society, Science, and Technology, Zuse Institute Berlin, Germany, Department of Geosciences and Natural Resource Management, University of Copenhagen, Denmark, Department of Computer Science, University of Copenhagen, Denmark
  - **TL;DR:** This study presents a framework for global-scale canopy height estimation using satellite data, significantly improving prediction accuracy compared to existing maps. The resulting height map will enhance ecological analyses and support forest management and climate change mitigation efforts.
  - **Keywords:** canopy height estimation, forest monitoring, climate change mitigation, convolutional neural networks, advanced data preprocessing, novel loss function, ecological analyses, biomass monitoring, forest management, geolocation inaccuracies, erroneous labels in mountainous regions, inadequate forest quantification, global-scale height maps, improved prediction accuracy (MAE/RMSE), Shuttle Radar Topography Mission


- [Plug-in Performative Optimization](https://icml.cc/virtual/2024/poster/33292) (Poster)
  - **Authors:** [Licong Lin](http://openreview.net/profile?id=~Licong_Lin2), [Tijana Zrnic](http://openreview.net/profile?id=~Tijana_Zrnic1)
  - **Affiliations:** Department of Statistics, University of California, Berkeley, USA, Stanford Data Science and Department of Statistics, Stanford University, USA
  - **TL;DR:** This study introduces plug-in performative optimization as a method to optimize performative risk in prediction settings, demonstrating that even misspecified models can significantly enhance learning outcomes. The findings suggest that leveraging models, despite their inaccuracies, can lead to better performance in performative contexts.
  - **Keywords:** performative prediction, performative risk, optimization, bandit strategies, derivative-free optimization, best-response models, uncertainty in feedback, model misspecification, plug-in performative optimization, improved learning in performative settings, distribution map, induced data distributions


- [Recovering the Pre-Fine-Tuning Weights of Generative Models](https://icml.cc/virtual/2024/poster/34894) (Poster)
  - **Authors:** [Eliahu Horwitz](http://openreview.net/profile?id=~Eliahu_Horwitz1), [Jonathan Kahana](http://openreview.net/profile?id=~Jonathan_Kahana1), [Yedid Hoshen](http://openreview.net/profile?id=~Yedid_Hoshen3)
  - **Affiliations:** School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel, School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel, School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel
  - **TL;DR:** This paper introduces Spectral DeTuning, a method for recovering the pre-fine-tuning weights of generative models using low-rank adaptation techniques. The findings reveal a significant vulnerability in fine-tuned models, highlighting potential risks for future AI systems.
  - **Keywords:** generative modeling, model safety, AI alignment, Spectral DeTuning, low-rank adaptation (LoRA), iterative low-rank matrix factorization, large-scale models, personalized models, recovery of pre-fine-tuning weights, vulnerability in fine-tuned models, recovery of exact pre-fine-tuning weights, optimization stability, rank scheduler, Large Language Models (LLMs), fine-tuning, pre-fine-tuning (Pre-FT)


- [Learning to Predict Mutational Effects of Protein-Protein Interactions by Microenvironment-aware Hierarchical Prompt Learning](https://icml.cc/virtual/2024/poster/33441) (Poster)
  - **Authors:** [Lirong Wu](http://openreview.net/profile?id=~Lirong_Wu1), [Yijun Tian](http://openreview.net/profile?id=~Yijun_Tian1), [Haitao Lin](http://openreview.net/profile?id=~Haitao_Lin2), [Yufei Huang](http://openreview.net/profile?id=~Yufei_Huang4), [Siyuan Li](http://openreview.net/profile?id=~Siyuan_Li6), [Nitesh Chawla](http://openreview.net/profile?id=~Nitesh_V_Chawla1), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** Westlake University, University of Notre Dame, Westlake University, Westlake University, Westlake University, University of Notre Dame, Westlake University
  - **TL;DR:** This study presents a novel hierarchical prompt learning framework to predict the effects of amino acid mutations on protein-protein interactions, addressing challenges such as data sparsity and complex structural dependencies. The proposed method demonstrates superior performance in mutation effect prediction and optimizing human antibodies against SARS-CoV-2.
  - **Keywords:** Protein-protein interactions, Mutation effect prediction, Hierarchical prompt learning, Masked microenvironment modeling, Protein complex design, Antibody optimization, Data sparsity, High-dimensional data, Change in binding free energy (∆∆G) prediction, Hierarchical prompt codebook, Improved training efficiency, Microenvironmental patterns


- [Understanding Finetuning for Factual Knowledge Extraction](https://icml.cc/virtual/2024/poster/33596) (Poster)
  - **Authors:** [Gaurav Ghosal](http://openreview.net/profile?id=~Gaurav_Rohit_Ghosal1), [Tatsunori Hashimoto](http://openreview.net/profile?id=~Tatsunori_Hashimoto1), [Aditi Raghunathan](http://openreview.net/profile?id=~Aditi_Raghunathan1)
  - **Affiliations:** Department of Machine Learning, Carnegie Mellon University, Pittsburgh, USA, Department of Computer Science, Stanford University, Stanford, USA, Department of Machine Learning, Carnegie Mellon University, Pittsburgh, USA
  - **TL;DR:** This study investigates how QA fine-tuning data affects the factuality of language models, revealing that fine-tuning on lesser-known facts can degrade performance, while well-known facts enhance factual accuracy. The findings emphasize the importance of understanding how facts are stored in pretrained models for effective fine-tuning in knowledge-intensive tasks.
  - **Keywords:** Factual knowledge extraction, QA fine-tuning, downstream factuality, Fine-tuning, pretraining, question answering benchmarks, Knowledge-intensive tasks, question answering, Poor factuality from lesser-known facts, model reliance on generic responses, Interaction between pretrained knowledge and fine-tuning data, impact of fact encoding on factuality, PopQA, Entity Questions, MMLU, Llama-2-7B, Mistral-7B


- [A Global Geometric Analysis of Maximal Coding Rate Reduction](https://icml.cc/virtual/2024/poster/32840) (Poster)
  - **Authors:** [Peng Wang](http://openreview.net/profile?id=~Peng_Wang23), [Huikang Liu](http://openreview.net/profile?id=~Huikang_Liu2), [Druv Pai](http://openreview.net/profile?id=~Druv_Pai1), [Yaodong Yu](http://openreview.net/profile?id=~Yaodong_Yu4), [Zhihui Zhu](http://openreview.net/profile?id=~Zhihui_Zhu1), [Qing Qu](http://openreview.net/profile?id=~Qing_Qu2), [Yi Ma](http://openreview.net/profile?id=~Yi_Ma4)
  - **Affiliations:** Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, Antai College of Economics and Management, Shanghai Jiao Tong University, Shanghai, Department of Electrical Engineering and Computer Science, University of California, Berkeley, Department of Electrical Engineering and Computer Science, University of California, Berkeley, Department of Computer Science and Engineering, The Ohio State University, Columbus, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, Institute of Data Science, University of Hong Kong
  - **TL;DR:** This study provides a comprehensive analysis of the Maximal Coding Rate Reduction (MCR2) objective, demonstrating that its local and global optima correspond to low-dimensional, discriminative representations. The findings suggest that MCR2 is a promising approach for learning diverse representations in deep learning applications.
  - **Keywords:** Maximal Coding Rate Reduction (MCR2), Deep Learning, Structured Representations, Gradient Descent, Optimization Methods, Computer Vision, Natural Language Processing, Health Care, Lack of theoretical justification for MCR2, Global landscape of MCR2 not studied, Characterization of local and global optima, Learning diverse and discriminative representations, Synthetic and real data sets, Explainable Deep Network Architectures


- [Navigating Scaling Laws: Compute Optimality in Adaptive Model Training](https://icml.cc/virtual/2024/poster/35070) (Spotlight Poster)
  - **Authors:** [Sotiris Anagnostidis](http://openreview.net/profile?id=~Sotiris_Anagnostidis1), [Gregor Bachmann](http://openreview.net/profile?id=~Gregor_Bachmann1), [Imanol Schlag](http://openreview.net/profile?id=~Imanol_Schlag3), [Thomas Hofmann](http://openreview.net/profile?id=~Thomas_Hofmann1)
  - **Affiliations:** Department of Computer Science, ETH Zürich, Department of Computer Science, ETH Zürich, ETH AI Center, Department of Computer Science, ETH Zürich
  - **TL;DR:** This study introduces adaptive model training methodologies that allow models to change shape during training, optimizing compute resource allocation to achieve better performance. The findings suggest that this approach can significantly reduce the computational resources needed to reach target performance levels compared to static models.
  - **Keywords:** deep learning, adaptive model training, neural scaling laws, Transformer architecture, compute-optimal model, natural language understanding, computer vision, static model limitations, compute resource optimization, adaptive training methodologies, optimal traversal between scaling laws, frontier models, large-scale architectures


- [Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge](https://icml.cc/virtual/2024/poster/34081) (Spotlight Poster)
  - **Authors:** [Yufei Huang](http://openreview.net/profile?id=~Yufei_Huang4), [Odin Zhang](http://openreview.net/profile?id=~Odin_Zhang1), [Lirong Wu](http://openreview.net/profile?id=~Lirong_Wu1), [Cheng Tan](http://openreview.net/profile?id=~Cheng_Tan1), [Haitao Lin](http://openreview.net/profile?id=~Haitao_Lin2), [Zhangyang Gao](http://openreview.net/profile?id=~Zhangyang_Gao1), [Siyuan Li](http://openreview.net/profile?id=~Siyuan_Li6), [Stan Z Li](http://openreview.net/profile?id=~Stan_Z._Li2)
  - **Affiliations:** Zhejiang University, Hangzhou; AI Lab, Research Center for Industries of the Future, Westlake University, Zhejiang University, Hangzhou; University of Washington, Seattle, Zhejiang University, Hangzhou; AI Lab, Research Center for Industries of the Future, Westlake University, Zhejiang University, Hangzhou; AI Lab, Research Center for Industries of the Future, Westlake University, Zhejiang University, Hangzhou; AI Lab, Research Center for Industries of the Future, Westlake University, Zhejiang University, Hangzhou; AI Lab, Research Center for Industries of the Future, Westlake University, Zhejiang University, Hangzhou; AI Lab, Research Center for Industries of the Future, Westlake University, AI Lab, Research Center for Industries of the Future, Westlake University
  - **TL;DR:** This study introduces Re-Dock, a novel diffusion bridge generative model for flexible molecular docking that predicts ligand poses and pocket sidechain conformations simultaneously. The proposed method addresses limitations of existing approaches by accurately modeling protein-ligand interactions and demonstrating superior effectiveness and efficiency on benchmark datasets.
  - **Keywords:** molecular docking, protein-ligand binding, drug design, flexible docking, diffusion bridge generative model, energy-to-geometry mapping, Newton-Euler equation, drug design, protein-ligand interaction, induced-fit mechanism, unrealistic conformation predictions, sidechain flexibility, superior effectiveness and efficiency over current methods, co-modeling binding energy and conformations, apo-dock, cross-dock


- [Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities](https://icml.cc/virtual/2024/poster/33833) (Poster)
  - **Authors:** [Golnoosh Farnadi](http://openreview.net/profile?id=~Golnoosh_Farnadi1), [Mohammad Havaei](http://openreview.net/profile?id=~Mohammad_Havaei2), [Negar Rostamzadeh](http://openreview.net/profile?id=~Negar_Rostamzadeh1)
  - **Affiliations:** Google Research, Montreal, Canada, Google Research, Montreal, Canada, Google Research, Montreal, Canada
  - **TL;DR:** This position paper discusses the interconnected disparities faced by marginalized communities due to foundation models in AI, emphasizing the risks of performance, representation, and privacy issues. The authors call for a holistic approach to mitigate these disparities at their source to prevent long-lasting negative consequences.
  - **Keywords:** Foundation models, Marginalized communities, Disparities, AI, Machine Learning, Performance disparities, Representation disparities, Privacy violations, Robustness issues, Calls to action for mitigating disparities, Large Language Models (LLMs), Multimodal models, Cascading disparity phenomenon


- [A Language Model’s Guide Through Latent Space](https://icml.cc/virtual/2024/poster/33611) (Poster)
  - **Authors:** [Dimitri von Rütte](http://openreview.net/profile?id=~Dimitri_von_R%C3%BCtte1), [Sotiris Anagnostidis](http://openreview.net/profile?id=~Sotiris_Anagnostidis1), [Gregor Bachmann](http://openreview.net/profile?id=~Gregor_Bachmann1), [Thomas Hofmann](http://openreview.net/profile?id=~Thomas_Hofmann1)
  - **Affiliations:** Data Analytics Lab, Department of Computer Science, ETH Zurich, Data Analytics Lab, Department of Computer Science, ETH Zurich, Data Analytics Lab, Department of Computer Science, ETH Zurich, Data Analytics Lab, Department of Computer Science, ETH Zurich
  - **TL;DR:** This paper explores the concept guidance framework for language models, extending it to various concepts like appropriateness and humor, while developing a novel metric for evaluating guidance effectiveness. The findings indicate that while some concepts are easier to guide, others require extensive tuning and may lead to confusion, highlighting the need for further investigation into the relationship between detectability and guidability.
  - **Keywords:** concept guidance, language models, internal representations, truthfulness, appropriateness, humor, creativity, quality, novel metric for concept guidance, detection and guidance strategies, Mistral-7B, Llama-2-chat, large language models (LLMs), mechanistic interpretability, linear representation hypothesis


- [One Meta-tuned Transformer is What You Need for Few-shot Learning](https://icml.cc/virtual/2024/poster/35219) (Spotlight Poster)
  - **Authors:** [Xu Yang](http://openreview.net/profile?id=~Xu_Yang10), [Huaxiu Yao](http://openreview.net/profile?id=~Huaxiu_Yao1), [Ying WEI](http://openreview.net/profile?id=~Ying_Wei1)
  - **Affiliations:** City University of Hong Kong, University of North Carolina at Chapel Hill, Nanyang Technological University
  - **TL;DR:** This study introduces MetaFormer, a framework that utilizes attention mechanisms to enhance few-shot learning by modeling relationships between samples and tasks. The proposed methods significantly improve performance in both inductive and transductive few-shot learning scenarios, outperforming existing state-of-the-art approaches.
  - **Keywords:** few-shot learning, meta-learning, vision transformers, Masked Sample Attention (MSA), Patch-grained Task Attention (PTA), image classification, data sparsity, task performance enhancement, MetaFormer framework, improvements in few-shot learning performance, Vision Transformers (ViTs), attention mechanisms


- [Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought](https://icml.cc/virtual/2024/poster/34660) (Poster)
  - **Authors:** [Zhen-Yu Zhang](http://openreview.net/profile?id=~Zhen-Yu_Zhang1), [Siwei Han](http://openreview.net/profile?id=~Siwei_Han1), [Huaxiu Yao](http://openreview.net/profile?id=~Huaxiu_Yao1), [Gang Niu](http://openreview.net/profile?id=~Gang_Niu1), [Masashi Sugiyama](http://openreview.net/profile?id=~Masashi_Sugiyama1)
  - **Affiliations:** Center for Advanced Intelligence Project, RIKEN, University of North Carolina at Chapel Hill, University of North Carolina at Chapel Hill, Center for Advanced Intelligence Project, RIKEN, Center for Advanced Intelligence Project, RIKEN; Graduate School of Frontier Sciences, The University of Tokyo
  - **TL;DR:** This paper proposes a pairwise-comparison approach to improve the generation of chain-of-thoughts in large language models, addressing the issue of noisy evaluations. The proposed method demonstrates effectiveness in identifying promising intermediate thoughts through an iterative process, enhancing the model's reasoning capabilities.
  - **Keywords:** Chain-of-thoughts (CoT), Large Language Models (LLMs), Reasoning, Pairwise-comparison evaluation, Ensemble learning, Dueling bandits, Complex reasoning problems, Multi-step reasoning, Noisy evaluation from LLMs, Selection of promising intermediate thoughts, New algorithm variants for CoT generation


- [Conformal Prediction for Deep Classifier via Label Ranking](https://icml.cc/virtual/2024/poster/33656) (Poster)
  - **Authors:** [Jianguo Huang](http://openreview.net/profile?id=~Jianguo_Huang2), [HuaJun Xi](http://openreview.net/profile?id=~HuaJun_Xi1), [Linjun Zhang](http://openreview.net/profile?id=~Linjun_Zhang1), [Huaxiu Yao](http://openreview.net/profile?id=~Huaxiu_Yao1), [Yue Qiu](http://openreview.net/profile?id=~Yue_Qiu5), [Hongxin Wei](http://openreview.net/profile?id=~Hongxin_Wei1)
  - **Affiliations:** Department of Statistics and Data Science, Southern University of Science and Technology; The School of Information Science and Technology, ShanghaiTech University, Department of Statistics and Data Science, Southern University of Science and Technology, Department of Statistics, Rutgers University, Department of Computer Science, University of North Carolina at Chapel Hill, College of Mathematics and Statistics, Chongqing University, Department of Statistics and Data Science, Southern University of Science and Technology
  - **TL;DR:** This study introduces a novel algorithm called Sorted Adaptive Prediction Sets (SAPS) to improve conformal prediction by minimizing reliance on softmax probabilities, resulting in more compact prediction sets and enhanced coverage rates. The findings suggest that the softmax probability values may be redundant for effective conformal prediction.
  - **Keywords:** Conformal Prediction, Model Uncertainty, Sorted Adaptive Prediction Sets (SAPS), Adaptive Prediction Sets (APS), Regularized Adaptive Prediction Sets (RAPS), Autonomous Driving, Medical Diagnostics, Financial Decision-Making, Miscalibrated Probabilities, Large Prediction Sets, Non-Conformity Scores, Compact Prediction Sets, Enhanced Conditional Coverage Rate, Softmax Probability, Non-Conformity Score


- [PDHG-Unrolled Learning-to-Optimize Method for Large-Scale Linear Programming](https://icml.cc/virtual/2024/poster/35097) (Poster)
  - **Authors:** [Bingheng Li](http://openreview.net/profile?id=~Bingheng_Li1), [Linxin Yang](http://openreview.net/profile?id=~Linxin_Yang1), [Yupeng Chen](http://openreview.net/profile?id=~Yupeng_Chen3), [Senmiao Wang](http://openreview.net/profile?id=~Senmiao_Wang1), [Haitao Mao](http://openreview.net/profile?id=~Haitao_Mao1), [Qian Chen](http://openreview.net/profile?id=~Qian_Chen10), [Yao Ma](http://openreview.net/profile?id=~Yao_Ma3), [Akang Wang](http://openreview.net/profile?id=~Akang_Wang1), [Tian Ding](http://openreview.net/profile?id=~Tian_Ding1), [Jiliang Tang](http://openreview.net/profile?id=~Jiliang_Tang1), [Ruoyu Sun](http://openreview.net/profile?id=~Ruoyu_Sun1)
  - **Affiliations:** Michigan State University, School of Data Science, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen International Center For Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data, Shenzhen International Center For Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data, School of Data Science, The Chinese University of Hong Kong, Shenzhen, China, Michigan State University, Shenzhen International Center For Industrial and Applied Mathematics, Shenzhen Research Institute of Big Data; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China, Rensselaer Polytechnic Institute, School of Data Science, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen International Center For Industrial and Applied Mathematics, Shenzhen International Center For Industrial and Applied Mathematics, Michigan State University, School of Data Science, The Chinese University of Hong Kong, Shenzhen, China; Shenzhen International Center For Industrial and Applied Mathematics
  - **TL;DR:** This paper introduces a novel neural network architecture called PDHG-Net, which unrolls the Primal-Dual Hybrid Gradient (PDHG) method to solve large-scale linear programming problems. The proposed two-stage learning-to-optimize framework significantly accelerates LP solving, achieving up to a 3× speedup compared to traditional first-order methods.
  - **Keywords:** Large-scale linear programming, optimization, First-order methods (FOMs), Learning to optimize (L2O), PDHG algorithm, neural networks, Communication networks, power systems, finance, logistics, Scaling up LP algorithms, solving large-scale LP problems, PDHG-Net architecture, two-stage inference approach, speedup in LP solving


- [Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer](https://icml.cc/virtual/2024/poster/34690) (Poster)
  - **Authors:** [Toru Shirakawa](http://openreview.net/profile?id=~Toru_Shirakawa1), [Yi Li](http://openreview.net/profile?id=~Yi_Li38), [Yulun Wu](http://openreview.net/profile?id=~Yulun_Wu1), [Sky Qiu](http://openreview.net/profile?id=~Sky_Qiu1), [Yuxuan Li](http://openreview.net/profile?id=~Yuxuan_Li6), [Mingduo Zhao](http://openreview.net/profile?id=~Mingduo_Zhao1), [Hiroyasu Iso](http://openreview.net/profile?id=~Hiroyasu_Iso1), [Mark van der Laan](http://openreview.net/profile?id=~Mark_J._van_der_Laan1)
  - **Affiliations:** Osaka University Graduate School of Medicine, Suita, Japan; National Center for Global Health and Medicine, Tokyo, Japan, University of California, Berkeley, United States, University of California, Berkeley, United States, University of California, Berkeley, United States, University of California, Berkeley, United States, University of California, Berkeley, United States, Osaka University Graduate School of Medicine, Suita, Japan; National Center for Global Health and Medicine, Tokyo, Japan, University of California, Berkeley, United States
  - **TL;DR:** This study introduces Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE) to estimate counterfactual outcomes under dynamic treatment policies in longitudinal settings, demonstrating superior performance in complex scenarios and small-sample contexts. The method was applied to evaluate blood pressure management strategies in a real-world cardiovascular study, providing robust statistical inference.
  - **Keywords:** Longitudinal data analysis, Counterfactual outcomes, Dynamic treatment policies, Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), Transformer architecture, Temporal-difference learning, Targeted minimum loss-based estimation (TMLE), Medicine, Public health, Cardiovascular epidemiology, High-dimensional longitudinal data, Curse of dimensionality, Computational challenges in estimating counterfactual probabilities, Improved estimation methods, Statistical inference with confidence intervals


- [Multiplicative Weights Update, Area Convexity and Random Coordinate Descent for Densest Subgraph Problems](https://icml.cc/virtual/2024/poster/33572) (Oral)
  - **Authors:** [Ta Duy Nguyen](http://openreview.net/profile?id=~Ta_Duy_Nguyen1), [Alina Ene](http://openreview.net/profile?id=~Alina_Ene1)
  - **Affiliations:** Department of Computer Science, Boston University, USA, Department of Computer Science, Boston University, USA
  - **TL;DR:** This study presents new algorithms for the densest subgraph problem and its decomposition, utilizing multiplicative weights update and area convexity, achieving improved convergence rates and efficiency. The proposed methods are practical for large graphs and competitive with existing approaches.
  - **Keywords:** densest subgraph problem, dense subgraph decomposition, multiplicative weights update, area convexity, random coordinate descent, machine learning, data mining, maximum density subgraph, optimization problems, linear convergence rate, efficient algorithms, maximum flow, parametric flows


- [Representation Surgery: Theory and Practice of Affine Steering](https://icml.cc/virtual/2024/poster/34483) (Poster)
  - **Authors:** [Shashwat Singh](http://openreview.net/profile?id=~Shashwat_Singh1), [Shauli Ravfogel](http://openreview.net/profile?id=~Shauli_Ravfogel1), [Jonathan Herzig](http://openreview.net/profile?id=~Jonathan_Herzig2), [Roee Aharoni](http://openreview.net/profile?id=~Roee_Aharoni1), [Ryan Cotterell](http://openreview.net/profile?id=~Ryan_Cotterell1), [Ponnurangam Kumaraguru](http://openreview.net/profile?id=~Ponnurangam_Kumaraguru3)
  - **Affiliations:** IIIT Hyderabad, Bar-Ilan University; Google Research, Google Research, Google Research, ETH Zurich, IIIT Hyderabad
  - **TL;DR:** This paper investigates the use of affine steering functions to manipulate neural language model representations, aiming to reduce the generation of toxic and biased text. The authors derive optimal steering functions and demonstrate their effectiveness in mitigating undesirable behaviors in language models.
  - **Keywords:** language models, representation surgery, steering functions, affine steering functions, least-squares optimization, natural language processing, bias mitigation, toxicity reduction, undesirable behavior in language models, gender bias, toxic text generation, improved steering approach, empirical effectiveness in bias mitigation


- [Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning](https://icml.cc/virtual/2024/poster/34278) (Poster)
  - **Authors:** [Jinsoo Yoo](http://openreview.net/profile?id=~Jinsoo_Yoo1), [Yunpeng Liu](http://openreview.net/profile?id=~Yunpeng_Liu1), [Frank Wood](http://openreview.net/profile?id=~Frank_Wood2), [Geoff Pleiss](http://openreview.net/profile?id=~Geoff_Pleiss1)
  - **Affiliations:** University of British Columbia; Inverted AI; Mila; Vector Institute, University of British Columbia; Inverted AI, University of British Columbia; Inverted AI; Mila, University of British Columbia; Vector Institute
  - **TL;DR:** This study introduces Layerwise Proximal Replay (LPR), a method designed to enhance online continual learning by addressing the instability in optimization trajectories caused by experience replay. The findings demonstrate that LPR consistently improves the performance of replay-based methods across various problem settings.
  - **Keywords:** online continual learning, experience replay, catastrophic forgetting, Layerwise Proximal Replay (LPR), optimization geometry modification, catastrophic forgetting, unstable optimization trajectories, underfitting, improved optimization stability, enhanced replay-based learning methods


- [A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?](https://icml.cc/virtual/2024/poster/34116) (Poster)
  - **Authors:** [Agustinus Kristiadi](http://openreview.net/profile?id=~Agustinus_Kristiadi1), [Felix Strieth-Kalthoff](http://openreview.net/profile?id=~Felix_Strieth-Kalthoff1), [Marta Skreta](http://openreview.net/profile?id=~Marta_Skreta1), [Pascal Poupart](http://openreview.net/profile?id=~Pascal_Poupart2), [Alan Aspuru-Guzik](http://openreview.net/profile?id=~Alan_Aspuru-Guzik2), [Geoff Pleiss](http://openreview.net/profile?id=~Geoff_Pleiss1)
  - **Affiliations:** Vector Institute; University of Toronto; None; None, University of Toronto, University of Toronto, University of Waterloo, University of Toronto, University of British Columbia
  - **TL;DR:** This study investigates the effectiveness of large language models (LLMs) in enhancing Bayesian optimization for material discovery, concluding that LLMs can be beneficial if they are pretrained or finetuned with domain-specific data. The findings highlight the importance of leveraging prior knowledge in efficiently exploring molecular spaces.
  - **Keywords:** Material discovery, Bayesian optimization, Large language models (LLMs), Bayesian neural networks, Gaussian processes, Chemistry, molecular space exploration, Uncertainty in optimization, black-box function approximation, Usefulness of LLMs for Bayesian optimization, domain-specific finetuning


- [Accelerating Federated Learning with Quick Distributed Mean Estimation](https://icml.cc/virtual/2024/poster/33421) (Poster)
  - **Authors:** [Ran Ben Basat](http://openreview.net/profile?id=~Ran_Ben-Basat1), [Shay Vargaftik](http://openreview.net/profile?id=~shay_vargaftik1), [Amit Portnoy](http://openreview.net/profile?id=~Amit_Portnoy1), [Gil Einziger](http://openreview.net/profile?id=~Gil_Einziger1), [Yaniv Ben Itzhak](http://openreview.net/profile?id=~Yaniv_Ben-Itzhak1), [Michael Mitzenmacher](http://openreview.net/profile?id=~Michael_Mitzenmacher1)
  - **Affiliations:** University College London, VMware Research, Ben-Gurion University of the Negev, Ben-Gurion University of the Negev, VMware Research, Harvard University
  - **TL;DR:** This paper presents a novel approach to Distributed Mean Estimation (DME) that enhances the efficiency of federated learning by improving encoding and decoding times while maintaining optimal accuracy. The proposed method, QUIC-FL, demonstrates state-of-the-art performance in communication efficiency and accuracy compared to existing DME techniques.
  - **Keywords:** Federated Learning, Distributed Mean Estimation, Quantization, Randomized Hadamard Transform, Stochastic Quantization, Communication-efficient learning, Model training across clients, Communication bottleneck, Data privacy, Improved encoding and decoding times, Optimal Normalized Mean Squared Error (NMSE), NMSE, DME (Distributed Mean Estimation)


- [SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models](https://icml.cc/virtual/2024/poster/32875) (Poster)
  - **Authors:** [Dongyang Liu](http://openreview.net/profile?id=~Dongyang_Liu3), [Renrui Zhang](http://openreview.net/profile?id=~Renrui_Zhang1), [Longtian Qiu](http://openreview.net/profile?id=~Longtian_Qiu1), [Siyuan Huang](http://openreview.net/profile?id=~Siyuan_Huang4), [Weifeng Lin](http://openreview.net/profile?id=~Weifeng_Lin1), [Shitian Zhao](http://openreview.net/profile?id=~Shitian_Zhao1), [Shijie Geng](http://openreview.net/profile?id=~Shijie_Geng1), [Ziyi Lin](http://openreview.net/profile?id=~Ziyi_Lin1), [Peng Jin](http://openreview.net/profile?id=~Peng_Jin4), [Kaipeng Zhang](http://openreview.net/profile?id=~Kaipeng_Zhang1), [WENQI SHAO](http://openreview.net/profile?id=~Wenqi_Shao2), [Chao Xu](http://openreview.net/profile?id=~Chao_Xu16), [Conghui He](http://openreview.net/profile?id=~Conghui_He2), [Junjun He](http://openreview.net/profile?id=~Junjun_He2), [Hao Shao](http://openreview.net/profile?id=~Hao_Shao1), [Pan Lu](http://openreview.net/profile?id=~Pan_Lu2), [Yu Qiao](http://openreview.net/profile?id=~Yu_Qiao1), [Hongsheng Li](http://openreview.net/profile?id=~Hongsheng_Li3), [Peng Gao](http://openreview.net/profile?id=~Peng_Gao3)
  - **Affiliations:** MMLab, CUHK; Shanghai AI Laboratory, MMLab, CUHK; Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, Rutgers University, MMLab, CUHK; Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, Shanghai AI Laboratory, MMLab, CUHK, University of California, Los Angeles, Shanghai AI Laboratory, MMLab, CUHK; Centre for Perceptual and Interactive Intelligence (CPII), Shanghai AI Laboratory
  - **TL;DR:** The study introduces SPHINX-X, a series of Multi-modal Large Language Models (MLLMs) that enhance training efficiency and architecture by simplifying the training process and expanding the dataset. The results indicate a strong correlation between multi-modal performance and the scales of data and parameters used.
  - **Keywords:** Multi-modal Large Language Models (MLLMs), Artificial General Intelligence, SPHINX framework, multi-stage training, one-stage all-in-one paradigm, Autonomous driving, graphical user interfaces (GUI) agents, Limited data coverage for tasks, training efficiency, Spectrum of MLLMs varying in parameter size and multilingual capabilities, comprehensive benchmarking, TinyLlama-1.1B, InternLM2-7B, LLaMA2-13B, Mixtral-8×7B, OCR intensive dataset, Set-of-Mark dataset


- [Learning to Infer Generative Template Programs for Visual Concepts](https://icml.cc/virtual/2024/poster/32854) (Poster)
  - **Authors:** [R. Kenny Jones](http://openreview.net/profile?id=~R._Kenny_Jones1), [Siddhartha Chaudhuri](http://openreview.net/profile?id=~Siddhartha_Chaudhuri3), [Daniel Ritchie](http://openreview.net/profile?id=~Daniel_Ritchie1)
  - **Affiliations:** Brown University, Adobe Research, Brown University
  - **TL;DR:** This paper presents a neurosymbolic system that learns to infer Template Programs for visual concepts, enabling few-shot generation and co-segmentation across various visual domains. The proposed method outperforms task-specific alternatives and demonstrates competitive performance against domain-specific approaches.
  - **Keywords:** visual concepts, neurosymbolic systems, few-shot generation, co-segmentation, Template Programs, domain-specific language (DSL), inference networks, 2D layouts, Omniglot characters, 3D shapes, grammar induction, generalization across tasks, new learning paradigm, structured symbolic objects


- [Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context](https://icml.cc/virtual/2024/poster/33676) (Poster)
  - **Authors:** [Xiang Cheng](http://openreview.net/profile?id=~Xiang_Cheng1), [Yuxin Chen](http://openreview.net/profile?id=~Yuxin_Chen8), [Suvrit Sra](http://openreview.net/profile?id=~Suvrit_Sra1)
  - **Affiliations:** Massachusetts Institute of Technology, University of California, Davis, Technical University of Munich
  - **TL;DR:** This paper investigates how Transformers can implement gradient descent to learn non-linear functions in context, providing both theoretical and empirical evidence. The findings reveal that the choice of non-linear activation functions is crucial for effectively learning these functions.
  - **Keywords:** Transformers, in-context learning, gradient descent, Non-linear architectures, gradient-based learning algorithms, Learning non-linear functions, understanding in-context learning mechanisms, Evidence of Transformers implementing gradient descent, optimal choice of non-linear activation, Non-linear functions, attention modules, softmax, ReLU, Turing Complete, function space


- [D-Flow: Differentiating through Flows for Controlled Generation](https://icml.cc/virtual/2024/poster/34016) (Poster)
  - **Authors:** [Heli Ben-Hamu](http://openreview.net/profile?id=~Heli_Ben-Hamu1), [Omri Puny](http://openreview.net/profile?id=~Omri_Puny1), [Itai Gat](http://openreview.net/profile?id=~Itai_Gat1), [Brian Karrer](http://openreview.net/profile?id=~Brian_Karrer1), [Uriel Singer](http://openreview.net/profile?id=~Uriel_Singer1), [Yaron Lipman](http://openreview.net/profile?id=~Yaron_Lipman1)
  - **Affiliations:** Meta; Weizmann Institute of Science, Weizmann Institute of Science, Meta, Meta, Meta, Meta; Weizmann Institute of Science
  - **TL;DR:** This paper introduces D-Flow, a framework for controlling the generation process of diffusion and flow-matching models by optimizing the source noise point, enabling effective solutions for inverse problems and conditional generation. The framework demonstrates state-of-the-art performance in various applications, including image and audio generation.
  - **Keywords:** Controlled generation, Inverse problems, Diffusion models, Flow-Matching (FM) models, Differentiation through the flow, Image generation, Audio generation, Conditional molecule generation, Inverse problems, Conditional generation, Sample editing, D-Flow framework, Optimization of generation process, MS-COCO validation set, Gaussian probability paths, Data manifold


- [Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling](https://icml.cc/virtual/2024/poster/33158) (Poster)
  - **Authors:** [Yair Schiff](http://openreview.net/profile?id=~Yair_Schiff1), [Chia Hsiang Kao](http://openreview.net/profile?id=~Chia_Hsiang_Kao1), [Aaron Gokaslan](http://openreview.net/profile?id=~Aaron_Gokaslan1), [Tri Dao](http://openreview.net/profile?id=~Tri_Dao1), [Albert Gu](http://openreview.net/profile?id=~Albert_Gu1), [Volodymyr Kuleshov](http://openreview.net/profile?id=~Volodymyr_Kuleshov1)
  - **Affiliations:** Department of Computer Science, Cornell University, New York, NY USA, Department of Computer Science, Cornell University, New York, NY USA, Department of Computer Science, Cornell University, New York, NY USA, Department of Computer Science, Princeton University, Princeton, NJ USA, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA USA, Department of Computer Science, Cornell University, New York, NY USA
  - **TL;DR:** The study introduces Caduceus, a novel family of bi-directional long-range DNA sequence models that effectively address challenges in genomic sequence modeling, including long-range interactions and reverse complementarity. Caduceus outperforms existing models, even those significantly larger, on tasks such as long-range variant effect prediction.
  - **Keywords:** DNA sequence modeling, genomics, bi-directional context, Mamba block, BiMamba component, MambaDNA block, RC equivariance, Genomic sequence analysis, long-range variant effect prediction, Long-range token interactions, upstream and downstream effects, reverse complementarity of DNA, Caduceus DNA foundation models, pre-training and fine-tuning strategies


- [Fast Peer Adaptation with Context-aware Exploration](https://icml.cc/virtual/2024/poster/34696) (Poster)
  - **Authors:** [Long Ma](http://openreview.net/profile?id=~Long_Ma5), [Yuanfei Wang](http://openreview.net/profile?id=~Yuanfei_Wang1), [Fangwei Zhong](http://openreview.net/profile?id=~Fangwei_Zhong3), [Song-Chun Zhu](http://openreview.net/profile?id=~Song-Chun_Zhu1), [Yizhou Wang](http://openreview.net/profile?id=~Yizhou_Wang1)
  - **Affiliations:** Academy for Advanced Interdisciplinary Studies, Peking University; Nat’l Key Laboratory of General Artificial Intelligence, BIGAI&PKU, Center on Frontiers of Computing Studies, School of Computer Science, Peking University; Nat’l Key Laboratory of General Artificial Intelligence, BIGAI&PKU, School of Intelligence Science and Technology, Peking University; Inst. for Artificial Intelligence, Peking University; Nat’l Eng. Research Center of Visual Technology, Peking University, School of Intelligence Science and Technology, Peking University; Inst. for Artificial Intelligence, Peking University, Center on Frontiers of Computing Studies, School of Computer Science, Peking University; Inst. for Artificial Intelligence, Peking University; Nat’l Eng. Research Center of Visual Technology, Peking University
  - **TL;DR:** This paper presents a method for fast adaptation to unknown peers in multi-agent games by introducing a peer identification reward that encourages effective exploration and strategy identification. The proposed approach demonstrates improved exploration behavior and faster adaptation compared to existing methods across various game scenarios.
  - **Keywords:** Fast adaptation, multi-agent games, peer identification, Context-aware policy, peer identification reward, Competitive games (Kuhn Poker), Cooperative games (PO-Overcooked), Mixed games (Predator-Prey-W), Efficient exploration, strategy identification, partially observable environments, Active exploration behavior, faster adaptation, better outcomes


- [Unveiling the Dynamics of Information Interplay in Supervised Learning](https://icml.cc/virtual/2024/poster/34970) (Poster)
  - **Authors:** [Kun Song](http://openreview.net/profile?id=~Kun_Song3), [Zhiquan Tan](http://openreview.net/profile?id=~Zhiquan_Tan1), [Bochao Zou](http://openreview.net/profile?id=~Bochao_Zou1), [Huimin Ma](http://openreview.net/profile?id=~Huimin_Ma1), [Weiran Huang](http://openreview.net/profile?id=~Weiran_Huang1)
  - **Affiliations:** University of Science and Technology Beijing, Department of Mathematical Sciences, Tsinghua University, University of Science and Technology Beijing, University of Science and Technology Beijing, MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University; Shanghai AI Laboratory
  - **TL;DR:** This study investigates the dynamics of information interplay in supervised learning using matrix information theory, introducing new metrics (MIR and HDR) to analyze interactions between data representations and classification heads. The findings reveal insights into phenomena like Neural Collapse and grokking, and demonstrate that incorporating these metrics can enhance the training process.
  - **Keywords:** Supervised Learning, Information Dynamics, Matrix Information Theory, Matrix Mutual Information Ratio (MIR), Matrix Entropy Difference Ratio (HDR), Neural Collapse, Generalization Capabilities, Information Interplay, Optimization of Information Interactions, Enhanced Training Procedure, ImageNet, COCO, Neural Collapse, Linear Mode Connectivity, Grokking


- [Dual Operating Modes of In-Context Learning](https://icml.cc/virtual/2024/poster/34565) (Poster)
  - **Authors:** [Ziqian Lin](http://openreview.net/profile?id=~Ziqian_Lin1), [Kangwook Lee](http://openreview.net/profile?id=~Kangwook_Lee1)
  - **Affiliations:** Department of Computer Science, University of Wisconsin-Madison, Madison, Wisconsin, USA, Department of Electrical & Computer Engineering, University of Wisconsin-Madison, Madison, Wisconsin, USA
  - **TL;DR:** This study analyzes the dual operating modes of in-context learning (ICL) in large language models, focusing on task learning and task retrieval. It explains the "early ascent" phenomenon in ICL risk and provides insights into the effects of in-context examples on model performance.
  - **Keywords:** In-context learning (ICL), dual operating modes, Generalized probabilistic model, Bayesian inference, Large language models (LLMs), Early ascent phenomenon in ICL risk, task retrieval vs. task learning, Explanation of ICL risk behavior, analysis of biased labels in ICL


- [StyDeSty: Min-Max Stylization and Destylization for Single Domain Generalization](https://icml.cc/virtual/2024/poster/34639) (Poster)
  - **Authors:** [Songhua Liu](http://openreview.net/profile?id=~Songhua_Liu2), [Xin Jin](http://openreview.net/profile?id=~Xin_Jin8), [Xingyi Yang](http://openreview.net/profile?id=~Xingyi_Yang1), [Jingwen Ye](http://openreview.net/profile?id=~Jingwen_Ye1), [Xinchao Wang](http://openreview.net/profile?id=~Xinchao_Wang1)
  - **Affiliations:** National University of Singapore, Singapore, Eastern Institute of Technology, Ningbo, China, National University of Singapore, Singapore, National University of Singapore, Singapore, National University of Singapore, Singapore; Eastern Institute of Technology, Ningbo, China
  - **TL;DR:** The study introduces StyDeSty, a novel approach for single domain generalization that enhances model robustness by explicitly aligning source and pseudo domains through stylization and destylization mechanisms. The method demonstrates significant improvements in classification accuracy, outperforming state-of-the-art techniques.
  - **Keywords:** Single Domain Generalization, Domain Generalization, Data Augmentation, Adversarial Perturbation, Stylization, Destylization, Neural Architecture Search (NAS), Distribution Shift, Robustness, Coherence between Augmented Domains, StyDeSty, Latent Domain Learning, Content-Invariant Features


- [Gibbs Sampling of Continuous Potentials on a Quantum Computer](https://icml.cc/virtual/2024/poster/33365) (Poster)
  - **Authors:** [Arsalan Motamedi](http://openreview.net/profile?id=~Arsalan_Motamedi1), [Pooya Ronagh](http://openreview.net/profile?id=~Pooya_Ronagh1)
  - **Affiliations:** Institute for Quantum Computing, University of Waterloo, Waterloo, ON, Canada; Department of Physics & Astronomy, University of Waterloo, Waterloo, ON, Canada, Institute for Quantum Computing, University of Waterloo, Waterloo, ON, Canada; Department of Physics & Astronomy, University of Waterloo, Waterloo, ON, Canada; Perimeter Institute for Theoretical Physics, Waterloo, ON, Canada; Irréversible, Vancouver, BC, Canada
  - **TL;DR:** This study presents a quantum algorithm for Gibbs sampling from continuous energy potentials, leveraging quantum Fourier transforms and finite difference techniques to solve the Fokker–Planck equation. The findings indicate significant improvements in computational efficiency and precision for sampling in energy-based models, particularly at high temperatures.
  - **Keywords:** Gibbs sampling, quantum computing, energy-based models, Quantum Fourier transforms, Fokker–Planck equation, finite difference techniques, Generative modeling, machine learning, Computational expense in training energy-based models, numerical instability in inference, Polynomial quantum speedups in mean estimation, improved precision in sampling from Morse functions


- [Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text](https://icml.cc/virtual/2024/poster/33662) (Poster)
  - **Authors:** [Abhimanyu Hans](http://openreview.net/profile?id=~Abhimanyu_Hans1), [Avi Schwarzschild](http://openreview.net/profile?id=~Avi_Schwarzschild1), [Valeriia Cherepanova](http://openreview.net/profile?id=~Valeriia_Cherepanova1), [Hamid Kazemi](http://openreview.net/profile?id=~Hamid_Kazemi1), [Aniruddha Saha](http://openreview.net/profile?id=~Aniruddha_Saha1), [Micah Goldblum](http://openreview.net/profile?id=~Micah_Goldblum1), [Jonas Geiping](http://openreview.net/profile?id=~Jonas_Geiping1), [Tom Goldstein](http://openreview.net/profile?id=~Tom_Goldstein1)
  - **Affiliations:** University of Maryland, Carnegie Mellon University, University of Maryland, University of Maryland, University of Maryland, New York University, ELLIS Institute Tübingen, MPI Intelligent Systems, University of Maryland
  - **TL;DR:** The study presents a novel zero-shot detector called Binoculars for identifying machine-generated text, achieving over 90% accuracy in detecting outputs from various LLMs without requiring training data. This method addresses critical issues in social media moderation and academic integrity by effectively distinguishing between human and machine-generated content.
  - **Keywords:** LLM detection, zero-shot detection, log-perplexity, cross-perplexity, social media moderation, academic integrity, distinguishing between human-generated and machine-generated text, model-agnostic detection, novel LLM detector (Binoculars), state-of-the-art accuracy, Large Language Models (LLMs), ChatGPT


- [Estimating Distributional Treatment Effects in Randomized Experiments: Machine Learning for Variance Reduction](https://icml.cc/virtual/2024/poster/34058) (Poster)
  - **Authors:** [Undral Byambadalai](http://openreview.net/profile?id=~Undral_Byambadalai1), [Tatsushi Oka](http://openreview.net/profile?id=~Tatsushi_Oka1), [Shota Yasui](http://openreview.net/profile?id=~Shota_Yasui1)
  - **Affiliations:** CyberAgent, Inc., Tokyo, Japan, Department of Economics, Keio University, Tokyo, Japan, CyberAgent, Inc., Tokyo, Japan
  - **TL;DR:** This study proposes a novel regression adjustment method for estimating distributional treatment effects in randomized experiments, integrating machine learning techniques to enhance precision. The findings demonstrate significant variance reduction in treatment effect estimators, providing deeper insights beyond average treatment effects.
  - **Keywords:** Distributional treatment effects, Randomized experiments, Regression adjustment, Machine learning, Neyman-orthogonal moment condition, Causal inference, Policy decision-making, Estimation of treatment effects, Variance reduction, Distributional Treatment Effect (DTE), Quantile Treatment Effect (QTE)


- [Residual-Conditioned Optimal Transport: Towards Structure-Preserving Unpaired and Paired Image Restoration](https://icml.cc/virtual/2024/poster/33337) (Poster)
  - **Authors:** [Xiaole Tang](http://openreview.net/profile?id=~Xiaole_Tang1), [Hu Xin](http://openreview.net/profile?id=~Xin_Hu1), [Xiang Gu](http://openreview.net/profile?id=~Xiang_Gu1), [Jian Sun](http://openreview.net/profile?id=~Jian_Sun1)
  - **Affiliations:** School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China
  - **TL;DR:** This study introduces a Residual-Conditioned Optimal Transport (RCOT) approach for image restoration that effectively preserves the structure of images by modeling the restoration process as an optimal transport problem. The proposed method demonstrates competitive performance in restoring images with high perceptual quality compared to existing state-of-the-art techniques.
  - **Keywords:** Image restoration, Optimal transport, Residual-Conditioned Optimal Transport (RCOT), Fourier residual-guided OT, Image denoising, Image quality enhancement, Structure preservation in image restoration, Degradation removal, Minimax optimization problem, Adversarial training of neural networks, Generative Adversarial Networks (GANs), Diffusion probabilistic models (DPMs)


- [Multi-layer Rehearsal Feature Augmentation for Class-Incremental Learning](https://icml.cc/virtual/2024/poster/33673) (Poster)
  - **Authors:** [Bowen Zheng](http://openreview.net/profile?id=~Bowen_Zheng4), [Da-Wei Zhou](http://openreview.net/profile?id=~Da-Wei_Zhou1), [Han-Jia Ye](http://openreview.net/profile?id=~Han-Jia_Ye1), [De-Chuan Zhan](http://openreview.net/profile?id=~De-Chuan_Zhan1)
  - **Affiliations:** School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China, School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China
  - **TL;DR:** This study addresses the challenge of catastrophic forgetting in Class-Incremental Learning by proposing Multi-layer Rehearsal Feature Augmentation (MRFA) to optimize the all-layer margin on rehearsal samples. The findings demonstrate that MRFA enhances generalization and mitigates overfitting, improving performance across various CIL scenarios.
  - **Keywords:** Class-Incremental Learning (CIL), Continual Learning, Multi-layer Rehearsal Feature Augmentation (MRFA), all-layer margin, Catastrophic forgetting, overfitting on rehearsal samples, distribution shift, Optimization of all-layer margin, feature augmentation


- [Rate-Optimal Policy Optimization for Linear Markov Decision Processes](https://icml.cc/virtual/2024/poster/33917) (Oral)
  - **Authors:** [Uri Sherman](http://openreview.net/profile?id=~Uri_Sherman1), [Alon Cohen](http://openreview.net/profile?id=~Alon_Cohen1), [Tomer Koren](http://openreview.net/profile?id=~Tomer_Koren1), [Yishay Mansour](http://openreview.net/profile?id=~Yishay_Mansour2)
  - **Affiliations:** Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel; Google Research, Tel Aviv, Israel, Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel; Google Research, Tel Aviv, Israel, Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel; Google Research, Tel Aviv, Israel, Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel; Google Research, Tel Aviv, Israel
  - **TL;DR:** This study presents a computationally efficient policy optimization algorithm for linear Markov Decision Processes that achieves rate optimal e𝑂(√𝐾) regret in both stochastic and adversarial settings. The findings establish the first optimal rate guarantees for regret minimization in these contexts, significantly advancing the theoretical understanding of policy optimization methods.
  - **Keywords:** Regret minimization, Policy optimization, Linear Markov Decision Processes, Natural policy gradient, Mirror Descent, Reinforcement Learning, Robotics, Computer games, Large language models, Regret, Optimal rate of convergence, Adversarial losses, Stochastic losses, Rate optimal regret, Computationally efficient algorithms, Linear MDP, Action-value estimates


- [Consistent Long-Term Forecasting of Ergodic Dynamical Systems](https://icml.cc/virtual/2024/poster/33541) (Poster)
  - **Authors:** [Vladimir Kostic](http://openreview.net/profile?id=~Vladimir_R_Kostic1), [Karim Lounici](http://openreview.net/profile?id=~Karim_Lounici1), [Prune Inzerilli](http://openreview.net/profile?id=~Prune_Inzerilli1), [Pietro Novelli](http://openreview.net/profile?id=~Pietro_Novelli1), [Massimiliano Pontil](http://openreview.net/profile?id=~massimiliano_pontil1)
  - **Affiliations:** Italian Institute of Technology, Genoa, Italy; University of Novi Sad, Serbia, CMAP, Ecole Polytechnique, Palaiseau, France, CMAP, Ecole Polytechnique, Palaiseau, France, Italian Institute of Technology, Genoa, Italy, Italian Institute of Technology, Genoa, Italy; University College London, UK
  - **TL;DR:** This study focuses on improving long-term forecasting of ergodic dynamical systems using enhanced estimators derived from Koopman and transfer operator theories. The proposed methods establish uniform error bounds for forecasting, even over infinite time horizons, demonstrating practical advantages through numerical experiments.
  - **Keywords:** long-term forecasting, ergodic dynamical systems, Koopman operator, transfer operators, eigenvalue deflation, feature centering, energy forecasting, epidemiology, finance, atomistic simulations, fluid dynamics, weather and climate forecasting, forecasting error, convergence to invariant distribution, non-linear relationships among covariates, high probability bounds on empirical estimators, uniform error bounds


- [USTAD: Unified Single-model Training Achieving Diverse Scores for Information Retrieval](https://icml.cc/virtual/2024/poster/34284) (Poster)
  - **Authors:** [Seungyeon Kim](http://openreview.net/profile?id=~Seungyeon_Kim1), [Ankit Singh Rawat](http://openreview.net/profile?id=~Ankit_Singh_Rawat1), [Manzil Zaheer](http://openreview.net/profile?id=~Manzil_Zaheer1), [Wittawat Jitkrittum](http://openreview.net/profile?id=~Wittawat_Jitkrittum1), [Veeranjaneyulu Sadhanala](http://openreview.net/profile?id=~Veeranjaneyulu_Sadhanala1), [Sadeep Jayasumana](http://openreview.net/profile?id=~Sadeep_Jayasumana1), [Aditya Menon](http://openreview.net/profile?id=~Aditya_Krishna_Menon1), [Rob Fergus](http://openreview.net/profile?id=~Rob_Fergus1), [Sanjiv Kumar](http://openreview.net/profile?id=~Sanjiv_Kumar1)
  - **Affiliations:** Google Research, New York, USA, Google Research, New York, USA, Google DeepMind, New York, USA, Google Research, New York, USA, Google Research, New York, USA, Google Research, New York, USA, Google Research, New York, USA, Google DeepMind, New York, USA, Google Research, New York, USA
  - **TL;DR:** The study introduces USTAD, a unified Transformer-based model that effectively combines the functionalities of cross-encoder and dual-encoder architectures for information retrieval. It demonstrates that a single model can achieve competitive performance while simplifying the complexity of traditional IR systems.
  - **Keywords:** Information Retrieval, Unified Model Training, Transformer-based models, Cross-encoder (CE), Dual-encoder (DE), Embedding matching-based distillation, Complexity of IR pipelines, Need for efficient retrieval and ranking, USTAD model, Asymmetric student models, Improved CE to DE distillation, MS-MARCO


- [Can AI Assistants Know What They Don't Know?](https://icml.cc/virtual/2024/poster/33416) (Poster)
  - **Authors:** [Qinyuan Cheng](http://openreview.net/profile?id=~Qinyuan_Cheng1), [Tianxiang Sun](http://openreview.net/profile?id=~Tianxiang_Sun1), [Xiangyang Liu](http://openreview.net/profile?id=~Xiangyang_Liu3), [Wenwei Zhang](http://openreview.net/profile?id=~Wenwei_Zhang1), [Zhangyue Yin](http://openreview.net/profile?id=~Zhangyue_Yin1), [Shimin Li](http://openreview.net/profile?id=~Shimin_Li1), [Linyang Li](http://openreview.net/profile?id=~Linyang_Li1), [Zhengfu He](http://openreview.net/profile?id=~Zhengfu_He2), [Kai Chen](http://openreview.net/profile?id=~Kai_Chen4), [Xipeng Qiu](http://openreview.net/profile?id=~Xipeng_Qiu1)
  - **Affiliations:** Department of Computer Science, Fudan University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, Department of Computer Science, Fudan University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, Department of Computer Science, Fudan University, Shanghai, China; Shanghai AI Laboratory, Shanghai, China, Shanghai AI Laboratory, Shanghai, China, Department of Computer Science, Fudan University, Shanghai, China, Department of Computer Science, Fudan University, Shanghai, China, Department of Computer Science, Fudan University, Shanghai, China, Department of Computer Science, Fudan University, Shanghai, China, Shanghai AI Laboratory, Shanghai, China, Department of Computer Science, Fudan University, Shanghai, China
  - **TL;DR:** This study investigates whether AI assistants can recognize their knowledge limitations and express this uncertainty in natural language. The findings indicate that aligning AI assistants with a model-specific "I don't know" dataset significantly enhances their ability to decline answering questions outside their knowledge scope, resulting in higher truthfulness.
  - **Keywords:** AI assistants, knowledge awareness, truthfulness, Supervised Fine-tuning, preference optimization, Open-domain question answering, AI safety, Factual errors, hallucinations, knowledge limits, Idk dataset, improved truthfulness in AI responses, Idk dataset, Large Language Models (LLMs), AI alignment


- [Integrating Global Context Contrast and Local Sensitivity for Blind Image Quality Assessment](https://icml.cc/virtual/2024/poster/34240) (Spotlight Poster)
  - **Authors:** [Xudong Li](http://openreview.net/profile?id=~XuDong_Li5), [Runze Hu](http://openreview.net/profile?id=~Runze_Hu1), [Jingyuan Zheng](http://openreview.net/profile?id=~Jingyuan_Zheng1), [Yan Zhang](http://openreview.net/profile?id=~Yan_Zhang22), [Shengchuan Zhang](http://openreview.net/profile?id=~Shengchuan_Zhang1), [Xiawu Zheng](http://openreview.net/profile?id=~Xiawu_Zheng1), [Ke Li](http://openreview.net/profile?id=~Ke_Li4), [Yunhang Shen](http://openreview.net/profile?id=~Yunhang_Shen1), [Yutao Liu](http://openreview.net/profile?id=~Yutao_Liu2), [Pingyang Dai](http://openreview.net/profile?id=~Pingyang_Dai1), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, School of Information and Electronics, Beijing Institute of Technology, Beijing 100080, China, School of Medicine, Xiamen University, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Tencent Youtu Lab, Tencent Youtu Lab, School of Computer Science and Technology, Ocean University of China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China
  - **TL;DR:** This paper introduces the Perceptual Context and Sensitivity BIQA (CSIQA), a novel approach that integrates global and local perspectives for blind image quality assessment. The proposed method demonstrates superior performance over existing state-of-the-art BIQA techniques through effective contrastive learning strategies.
  - **Keywords:** Blind Image Quality Assessment (BIQA), Contrastive Learning, Quality Context Contrastive Learning, Quality-aware Mask Attention, Image Quality Assessment, Computer Vision, Local and Global Context in Image Quality, Perception of Local Distortions, Novel contrastive learning paradigm (CSIQA), Improved image quality assessment performance, Eight standard BIQA datasets


- [On the Role of Edge Dependency in Graph Generative Models](https://icml.cc/virtual/2024/poster/35198) (Poster)
  - **Authors:** [Sudhanshu Chanpuriya](http://openreview.net/profile?id=~Sudhanshu_Chanpuriya1), [Cameron Musco](http://openreview.net/profile?id=~Cameron_N_Musco1), [Konstantinos Sotiropoulos](http://openreview.net/profile?id=~Konstantinos_Sotiropoulos1), [Charalampos Tsourakakis](http://openreview.net/profile?id=~Charalampos_Tsourakakis1)
  - **Affiliations:** University of Illinois Urbana-Champaign, Urbana, USA, University of Massachusetts Amherst, Amherst, USA, Meta, Menlo Park, United States; Boston University, Boston, USA, None
  - **TL;DR:** This study investigates the trade-off between representation power and model overlap in graph generative models, introducing a hierarchy of models based on edge dependency. The findings reveal that more complex dependency structures enhance the ability to generate diverse and accurate graphs, providing competitive baselines against popular generative models.
  - **Keywords:** Graph generative models, edge dependency, representation power, Edge independent models, node independent models, arbitrarily dependent models, dense subgraph discovery, Complex networks, social networks, drug discovery, bioinformatics, Trade-off between representation power and model overlap, generating diverse outputs, New generative models, theoretical bounds on triangles and cycles, evaluation of output quality, Real-world datasets, Random graphs, motif counts, triangle density


- [Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models](https://icml.cc/virtual/2024/poster/34359) (Poster)
  - **Authors:** [Jinhao Li](http://openreview.net/profile?id=~Jinhao_Li2), [Haopeng Li](http://openreview.net/profile?id=~Haopeng_Li1), [Sarah Erfani](http://openreview.net/profile?id=~Sarah_Monazam_Erfani1), [Lei Feng](http://openreview.net/profile?id=~Lei_Feng1), [James Bailey](http://openreview.net/profile?id=~James_Bailey1), [Feng Liu](http://openreview.net/profile?id=~Feng_Liu2)
  - **Affiliations:** School of Computing and Information Systems, University of Melbourne, Australia, School of Computing and Information Systems, University of Melbourne, Australia, School of Computing and Information Systems, University of Melbourne, Australia, Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore, School of Computing and Information Systems, University of Melbourne, Australia, School of Computing and Information Systems, University of Melbourne, Australia; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore
  - **TL;DR:** This study introduces a method called weighted visual-text cross alignment (WCA) to enhance zero-shot performance in vision-language models by aligning finer text descriptions with specific local areas of images rather than the whole image. The proposed method significantly improves performance and is comparable to few-shot learning techniques.
  - **Keywords:** Vision-Language Models, Zero-Shot Learning, Weighted Visual-Text Cross Alignment (WCA), Localized Visual Prompting, Image Classification, Natural Language Processing, Alignment of text descriptions with image areas, Sensitivity to prompts, Improved zero-shot performance, Similarity matrix for alignment, CLIP


- [Degeneration-free Policy Optimization: RL Fine-Tuning for Language Models without Degeneration](https://icml.cc/virtual/2024/poster/33191) (Poster)
  - **Authors:** [Youngsoo Jang](http://openreview.net/profile?id=~Youngsoo_Jang2), [Geon-Hyeong Kim](http://openreview.net/profile?id=~Geon-Hyeong_Kim2), [Byoungjip Kim](http://openreview.net/profile?id=~Byoungjip_Kim1), [Yu Jin Kim](http://openreview.net/profile?id=~Yu_Jin_Kim1), [Honglak Lee](http://openreview.net/profile?id=~Honglak_Lee2), [Moontae Lee](http://openreview.net/profile?id=~Moontae_Lee1)
  - **Affiliations:** LG AI Research, LG AI Research, LG AI Research, LG AI Research, LG AI Research, LG AI Research; University of Illinois Chicago
  - **TL;DR:** The study introduces Degeneration-free Policy Optimization (DfPO) to fine-tune language models through reinforcement learning, addressing the challenge of text degeneration while improving downstream task scores. The proposed method effectively balances maximizing task rewards and preserving natural text generation without requiring additional hyperparameter tuning.
  - **Keywords:** Language Models, Reinforcement Learning, Policy Optimization, KL Divergence, PPO (Proximal Policy Optimization), KL-masking, Natural Language Processing, Text Generation, Text Degeneration, Alignment with Task Scores, Degeneration-free Policy Optimization (DfPO), Improved Downstream Task Scores


- [Adaptive Feature Selection for No-Reference Image Quality Assessment by Mitigating Semantic Noise Sensitivity](https://icml.cc/virtual/2024/poster/34287) (Poster)
  - **Authors:** [Xudong Li](http://openreview.net/profile?id=~XuDong_Li5), [Timin Gao](http://openreview.net/profile?id=~Timin_Gao1), [Runze Hu](http://openreview.net/profile?id=~Runze_Hu1), [Yan Zhang](http://openreview.net/profile?id=~Yan_Zhang22), [Shengchuan Zhang](http://openreview.net/profile?id=~Shengchuan_Zhang1), [Xiawu Zheng](http://openreview.net/profile?id=~Xiawu_Zheng1), [Jingyuan Zheng](http://openreview.net/profile?id=~Jingyuan_Zheng1), [Yunhang Shen](http://openreview.net/profile?id=~Yunhang_Shen1), [Ke Li](http://openreview.net/profile?id=~Ke_Li4), [Yutao Liu](http://openreview.net/profile?id=~Yutao_Liu2), [Pingyang Dai](http://openreview.net/profile?id=~Pingyang_Dai1), [Rongrong Ji](http://openreview.net/profile?id=~Rongrong_Ji5)
  - **Affiliations:** Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, School of Information and Electronics, Beijing Institute of Technology, Beijing 100080, China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, School of Medicine, Xiamen University, None, Tencent Youtu Lab, None, Tencent Youtu Lab, None, School of Computer Science and Technology, Ocean University of China, None, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China, Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China
  - **TL;DR:** This study introduces a Quality-Aware Feature Matching IQA Metric (QFM-IQM) to improve No-Reference Image Quality Assessment by effectively mitigating the impact of harmful semantic noise in feature extraction. The proposed method demonstrates enhanced performance in distinguishing image quality across various datasets, aligning predictions more closely with human evaluations.
  - **Keywords:** No-Reference Image Quality Assessment (NR-IQA), Image Quality Evaluation, Quality-Aware Feature Matching IQA Metric (QFM-IQM), Adversarial Perspective, Image Quality Assessment, Semantic Noise Sensitivity, Feature Selection, Enhanced Semantic Noise Distinguish Capabilities, Improved Model Generalization, Eight standard IQA datasets, ImageNet


- [Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery](https://icml.cc/virtual/2024/poster/34615) (Poster)
  - **Authors:** [Yassir Jedra](http://openreview.net/profile?id=~Yassir_Jedra1), [William Réveillard](http://openreview.net/profile?id=~William_R%C3%A9veillard1), [Stefan Stojanovic](http://openreview.net/profile?id=~Stefan_Stojanovic1), [Alexandre Proutiere](http://openreview.net/profile?id=~Alexandre_Proutiere1)
  - **Affiliations:** Laboratory for Information and Decision Systems, MIT, Cambridge, MA, USA, Division of Decision and Control Systems, KTH, Stockholm, Sweden, Division of Decision and Control Systems, KTH, Stockholm, Sweden, Division of Decision and Control Systems, KTH, Stockholm, Sweden
  - **TL;DR:** This paper investigates contextual bandits with low-rank structures, presenting efficient algorithms for policy evaluation, best policy identification, and regret minimization. The proposed methods leverage spectral techniques for singular subspace recovery, achieving nearly minimax optimal performance in these tasks.
  - **Keywords:** contextual bandits, low-rank structure, spectral methods, singular subspace recovery, recommendation systems, clinical trials, exploration-exploitation trade-off, low-rank bandit problems, efficient algorithms, policy evaluation, best policy identification, regret minimization


- [Classification Under Strategic Self-Selection](https://icml.cc/virtual/2024/poster/33011) (Poster)
  - **Authors:** [Guy Horowitz](http://openreview.net/profile?id=~Guy_Horowitz1), [Yonatan Sommer](http://openreview.net/profile?id=~Yonatan_Sommer1), [Moran Koren](http://openreview.net/profile?id=~Moran_Koren1), [Nir Rosenfeld](http://openreview.net/profile?id=~Nir_Rosenfeld2)
  - **Affiliations:** Faculty of Computer Science, Technion – Israel Institute of Technology, Faculty of Computer Science, Technion – Israel Institute of Technology, Department of Industrial Engineering and Management, Ben Gurion University, Faculty of Computer Science, Technion – Israel Institute of Technology
  - **TL;DR:** This study investigates the impact of strategic self-selection on machine learning classifiers used for screening applicants, proposing a differentiable framework that accounts for user behavior. The findings highlight the importance of adapting learning methods to the self-selected distribution of applicants to improve decision-making processes in various domains.
  - **Keywords:** Strategic self-selection, machine learning, user behavior, Differentiable framework, learning under self-selective behavior, Loan approvals, university admissions, job hiring, welfare benefits, healthcare programs, Self-selection, uncertainty in application decisions, screening rules, Strategically robust methods, influence on applicant population


- [Can Mamba Learn How To Learn? A Comparative Study on In-Context Learning Tasks](https://icml.cc/virtual/2024/poster/34497) (Poster)
  - **Authors:** [Jong Ho Park](http://openreview.net/profile?id=~Jongho_Park2), [Jaden Park](http://openreview.net/profile?id=~Jaeseung_Park1), [Zheyang Xiong](http://openreview.net/profile?id=~Zheyang_Xiong1), [Nayoung Lee](http://openreview.net/profile?id=~Nayoung_Lee1), [Jaewoong Cho](http://openreview.net/profile?id=~Jaewoong_Cho1), [Samet Oymak](http://openreview.net/profile?id=~Samet_Oymak2), [Kangwook Lee](http://openreview.net/profile?id=~Kangwook_Lee1), [Dimitris Papailiopoulos](http://openreview.net/profile?id=~Dimitris_Papailiopoulos1)
  - **Affiliations:** KRAFTON, Seoul National University, University of Wisconsin-Madison, University of Wisconsin-Madison, KRAFTON, University of Michigan, Ann Arbor, KRAFTON; University of Wisconsin-Madison, KRAFTON
  - **TL;DR:** This study evaluates the in-context learning capabilities of state-space models, particularly Mamba, against Transformer models, revealing that while SSMs perform comparably in standard tasks, they struggle with non-standard retrieval tasks. The introduction of a hybrid model, MambaFormer, demonstrates improved performance in areas where individual models are limited.
  - **Keywords:** in-context learning (ICL), state-space models (SSMs), language modeling, Mamba, hybrid model, attention blocks, limitations of SSMs in non-standard retrieval tasks, MambaFormer, enhanced ICL capabilities, The Pile, Transformer, meta-learning


- [Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem](https://icml.cc/virtual/2024/poster/34992) (Spotlight Poster)
  - **Authors:** [Maciej Wołczyk](http://openreview.net/profile?id=~Maciej_Wolczyk1), [Bartłomiej Cupiał](http://openreview.net/profile?id=~Bart%C5%82omiej_Cupia%C5%821), [Mateusz Ostaszewski](http://openreview.net/profile?id=~Mateusz_Ostaszewski1), [Michał Bortkiewicz](http://openreview.net/profile?id=~Micha%C5%82_Bortkiewicz1), [Michał Zając](http://openreview.net/profile?id=~Micha%C5%82_Zaj%C4%85c1), [Razvan Pascanu](http://openreview.net/profile?id=~Razvan_Pascanu1), [Lukasz Kucinski](http://openreview.net/profile?id=~%C5%81ukasz_Kuci%C5%84ski1), [Piotr Milos](http://openreview.net/profile?id=~Piotr_Mi%C5%82o%C5%9B1)
  - **Affiliations:** IDEAS NCBR; University of Warsaw; None; None, IDEAS NCBR; University of Warsaw, Warsaw University of Technology, Warsaw University of Technology, Jagiellonian University, Google DeepMind, IDEAS NCBR; University of Warsaw; Institute of Mathematics, Polish Academy of Sciences; deepsense.ai, IDEAS NCBR; University of Warsaw; Institute of Mathematics, Polish Academy of Sciences; deepsense.ai
  - **TL;DR:** This study investigates the challenges of fine-tuning reinforcement learning models, particularly the issue of forgetting pre-trained capabilities, which can lead to poor transfer to downstream tasks. The authors demonstrate that applying knowledge retention techniques can significantly improve performance, achieving a new state-of-the-art score in the NetHack environment.
  - **Keywords:** Fine-tuning, Reinforcement Learning, Knowledge Transfer, NetHack, Montezuma’s Revenge, Meta-World, Forgetting of pre-trained capabilities, State coverage gap, Imperfect cloning gap, Knowledge retention techniques, State-of-the-art performance improvement, Foundation models, Neural models


- [Structure Your Data: Towards Semantic Graph Counterfactuals](https://icml.cc/virtual/2024/poster/34153) (Poster)
  - **Authors:** [Angeliki Dimitriou](http://openreview.net/profile?id=~Angeliki_Dimitriou1), [Maria Lymperaiou](http://openreview.net/profile?id=~Maria_Lymperaiou1), [Giorgos Filandrianos](http://openreview.net/profile?id=~Georgios_Filandrianos1), [Konstantinos Thomas](http://openreview.net/profile?id=~Konstantinos_Thomas1), [Giorgos Stamou](http://openreview.net/profile?id=~Giorgos_Stamou1)
  - **Affiliations:** Artificial Intelligence and Learning Systems Laboratory, National Technical University of Athens, Artificial Intelligence and Learning Systems Laboratory, National Technical University of Athens, Artificial Intelligence and Learning Systems Laboratory, National Technical University of Athens, Artificial Intelligence and Learning Systems Laboratory, National Technical University of Athens, Artificial Intelligence and Learning Systems Laboratory, National Technical University of Athens
  - **TL;DR:** This study proposes a model-agnostic approach for generating counterfactual explanations using semantic graphs, enhancing the interpretability of black-box models. The method demonstrates superior performance over existing models, validated through quantitative and qualitative assessments.
  - **Keywords:** Counterfactual explanations, Explainability, Semantic graphs, Graph Edit Distance (GED), Graph Neural Networks (GNNs), Visual domain, Medical image classification, Black-box model interpretability, NP-hard graph similarity problem, Model-agnostic graph-based approach, Actionable explanations, Benchmark datasets, Real-world datasets, State-of-the-art (SotA) performance, Human validation


- [A Persuasive Approach to Combating Misinformation](https://icml.cc/virtual/2024/poster/33949) (Poster)
  - **Authors:** [Safwan Hossain](http://openreview.net/profile?id=~Safwan_Hossain1), [Andjela Mladenovic](http://openreview.net/profile?id=~Andjela_Mladenovic1), [Yiling Chen](http://openreview.net/profile?id=~Yiling_Chen1), [Gauthier Gidel](http://openreview.net/profile?id=~Gauthier_Gidel1)
  - **Affiliations:** Harvard University, Mila, Université de Montréal, Harvard University, Mila, Université de Montréal
  - **TL;DR:** The study proposes using Bayesian Persuasion as a strategic tool for social media platforms to combat misinformation by leveraging information asymmetry to influence user sharing behavior. Experimental results demonstrate that this approach significantly reduces the sharing of misinformation in both single round and performative settings.
  - **Keywords:** misinformation, social media, Bayesian Persuasion, machine learning, linear programming, social media platforms, spread of misinformation, user behavior, content popularity, optimal signaling scheme, platform utility, performative model


- [Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning](https://icml.cc/virtual/2024/poster/33538) (Poster)
  - **Authors:** [Yizhe Huang](http://openreview.net/profile?id=~Yizhe_Huang2), [Anji Liu](http://openreview.net/profile?id=~Anji_Liu1), [Fanqi Kong](http://openreview.net/profile?id=~Fanqi_Kong1), [Yaodong Yang](http://openreview.net/profile?id=~Yaodong_Yang1), [Song-Chun Zhu](http://openreview.net/profile?id=~Song-Chun_Zhu1), [Xue Feng](http://openreview.net/profile?id=~Xue_Feng3)
  - **Affiliations:** Institute for Artificial Intelligence, Peking University; State Key Laboratory of General Artificial Intelligence, BIGAI, University of California, Los Angeles, State Key Laboratory of General Artificial Intelligence, BIGAI; Tsinghua University, Institute for Artificial Intelligence, Peking University; State Key Laboratory of General Artificial Intelligence, BIGAI, Institute for Artificial Intelligence, Peking University; State Key Laboratory of General Artificial Intelligence, BIGAI; PKU-WUHAN Institute for Artificial Intelligence, State Key Laboratory of General Artificial Intelligence, BIGAI
  - **TL;DR:** This study presents Hierarchical Opponent modeling and Planning (HOP), a novel algorithm designed for few-shot adaptation in mixed-motive environments, demonstrating superior adaptation capabilities and the emergence of social intelligence during interactions with unseen agents. The findings highlight the importance of hierarchical modeling and planning in enhancing multi-agent decision-making efficiency.
  - **Keywords:** Few-shot adaptation, Mixed-motive environments, Hierarchical Opponent Modeling, Monte Carlo Tree Search (MCTS), Multi-agent decision-making, Efficient adaptation to co-players, Non-deterministic relationships, General-sum reward structure, Hierarchical decision-making algorithm, Improved efficiency in adaptation, Social intelligence


- [Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws](https://icml.cc/virtual/2024/poster/35193) (Poster)
  - **Authors:** [Nikhil Sardana](http://openreview.net/profile?id=~Nikhil_Sardana1), [Jacob Portes](http://openreview.net/profile?id=~Jacob_Portes1), [Alexandre (Sasha) Doubov](http://openreview.net/profile?id=~Sasha_Doubov1), [Jonathan Frankle](http://openreview.net/profile?id=~Jonathan_Frankle1)
  - **Affiliations:** Databricks MosaicML, United States of America, None, None, None
  - **TL;DR:** The study modifies Chinchilla scaling laws to account for inference costs in training large language models, suggesting that models should be smaller and trained longer than previously deemed optimal. The findings indicate that model quality improves significantly as the ratio of tokens to parameters increases, even at extreme ranges.
  - **Keywords:** Large Language Models, Scaling Laws, Chinchilla scaling laws, LLaMA models, Natural Language Processing, Inference costs, Training-inference compute trade-off, Optimal LLM parameter count, Pre-training data size


- [Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need](https://icml.cc/virtual/2024/poster/35036) (Poster)
  - **Authors:** [Shangda Yang](http://openreview.net/profile?id=~Shangda_Yang1), [Vitaly Zankin](http://openreview.net/profile?id=~Vitaly_Zankin1), [Maximilian Balandat](http://openreview.net/profile?id=~Maximilian_Balandat1), [Stefan Scherer](http://openreview.net/profile?id=~Stefan_Scherer1), [Kevin Carlberg](http://openreview.net/profile?id=~Kevin_Thomas_Carlberg1), [Neil Walton](http://openreview.net/profile?id=~Neil_Walton1), [Kody Law](http://openreview.net/profile?id=~Kody_J._H._Law1)
  - **Affiliations:** Department of Mathematics, University of Manchester, Department of Mathematics, University of Manchester, Meta Platforms, Inc., Meta Platforms, Inc., Meta Platforms, Inc., Durham University Business School, Meta Platforms, Inc.
  - **TL;DR:** This paper introduces a method to enhance the efficiency of multi-step look-ahead Bayesian optimization by applying multilevel Monte Carlo techniques, addressing the computational challenges associated with nested expectations. The findings demonstrate significant improvements in performance and applicability of look-ahead methods in Bayesian optimization contexts.
  - **Keywords:** Bayesian Optimization, Multilevel Monte Carlo, Monte Carlo, Sample Average Approximation, Nested expectations, computational complexity, curse of dimensionality, Improved performance of look-ahead methods, approximation improvements, Gaussian process, Markov decision process


- [HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding](https://icml.cc/virtual/2024/poster/34578) (Poster)
  - **Authors:** [Zhaorun Chen](http://openreview.net/profile?id=~Zhaorun_Chen1), [Zhuokai Zhao](http://openreview.net/profile?id=~Zhuokai_Zhao1), [HONGYIN LUO](http://openreview.net/profile?id=~Hongyin_Luo1), [Huaxiu Yao](http://openreview.net/profile?id=~Huaxiu_Yao1), [Bo Li](http://openreview.net/profile?id=~Bo_Li19), [Jiawei Zhou](http://openreview.net/profile?id=~Jiawei_Zhou1)
  - **Affiliations:** University of Chicago, Chicago IL, USA, University of Chicago, Chicago IL, USA, Massachusetts Institute of Technology, Boston MA, USA, UNC-Chapel Hill, Chapel Hill NC, USA, University of Chicago, Chicago IL, USA; University of Illinois at Urbana-Champaign, Champaign IL, USA, Toyota Technological Institute at Chicago, Chicago IL, USA
  - **TL;DR:** The study introduces HALC, a novel decoding algorithm aimed at reducing object hallucinations in large vision-language models by leveraging fine-grained visual information and a specialized beam search. Experimental results demonstrate HALC's effectiveness in mitigating hallucinations while maintaining high-quality text generation across multiple benchmarks.
  - **Keywords:** Object Hallucination, Vision-Language Models, Decoding Algorithms, Adaptive Focal-Contrast Decoding, Beam Search Algorithm, Large Vision-Language Models (LVLMs), Multi-modal Contexts, Object Hallucinations (OH), Attribute Hallucinations, Relationship Hallucinations, HALC (Object Hallucination Reduction), Integration into LVLMs, MiniGPT-4, LLaVA, mPLUG-Owl2


- [Learning to Explore in POMDPs with Informational Rewards](https://icml.cc/virtual/2024/poster/33084) (Poster)
  - **Authors:** [Annie Xie](http://openreview.net/profile?id=~Annie_Xie1), [Logan M. Bhamidipaty](http://openreview.net/profile?id=~Logan_Mondal_Bhamidipaty1), [Evan Liu](http://openreview.net/profile?id=~Evan_Zheran_Liu1), [Joey Hong](http://openreview.net/profile?id=~Joey_Hong2), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1), [Chelsea Finn](http://openreview.net/profile?id=~Chelsea_Finn1)
  - **Affiliations:** Stanford University, Stanford University, Imbue, UC Berkeley, UC Berkeley, Stanford University
  - **TL;DR:** This study presents a POMDP agent designed to effectively gather information about hidden states by utilizing exploration bonuses that reward relevant information-gathering strategies. The proposed method outperforms existing approaches in complex exploration scenarios while also demonstrating the ability to learn without privileged information.
  - **Keywords:** exploration in POMDPs, information-gathering strategies, reinforcement learning, off-policy algorithms, meta-exploration, partially observed environments, decision-making problems, partial observability, exploration challenges, information gathering, exploration bonus, learning without privileged information, POMDP (Partially Observable Markov Decision Process), hidden parameter Markov decision processes


- [Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models](https://icml.cc/virtual/2024/poster/35085) (Poster)
  - **Authors:** [Akhil Kedia](http://openreview.net/profile?id=~Akhil_Kedia1), [Mohd Abbas Zaidi](http://openreview.net/profile?id=~Mohd_Abbas_Zaidi2), [Sushil Khyalia](http://openreview.net/profile?id=~Sushil_Khyalia1), [JungHo Jung](http://openreview.net/profile?id=~JungHo_Jung1), [Harshith Goka](http://openreview.net/profile?id=~Harshith_Goka1), [Haejun Lee](http://openreview.net/profile?id=~Haejun_Lee2)
  - **Affiliations:** Language Intelligence Lab, Samsung Research, Seoul, South Korea, Language Intelligence Lab, Samsung Research, Seoul, South Korea, Department of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA; Language Intelligence Lab, Samsung Research, Seoul, South Korea, Language Intelligence Lab, Samsung Research, Seoul, South Korea, Language Intelligence Lab, Samsung Research, Seoul, South Korea, Language Intelligence Lab, Samsung Research, Seoul, South Korea
  - **TL;DR:** This study develops a unified signal propagation theory for transformer models, addressing issues like vanishing gradients and rank collapse, and introduces DeepScaleLM, an initialization scheme that allows for the training of very deep models with improved performance across various tasks. The findings suggest that deeper transformer models can outperform shallower ones while maintaining stability and robustness.
  - **Keywords:** transformer models, signal propagation theory, deep learning, DeepScaleLM, initialization and scaling scheme, residual scaling, Language Modeling, Speech Translation, Image Classification, Question Answering, vanishing gradients, exploding gradients, rank collapse, instability in deep transformers, closed-form expressions for moments of outputs and gradients, improved training stability for deep models, Pre-LN transformers, Post-LN transformers, FFN (Feed Forward Network), ReLU, GeLU, LayerNorm, Dropout, Softmax, Single-Head Attention


- [Complexity Matters: Feature Learning in the Presence of Spurious Correlations](https://icml.cc/virtual/2024/poster/35175) (Poster)
  - **Authors:** [GuanWen Qiu](http://openreview.net/profile?id=~GuanWen_Qiu1), [Da Kuang](http://openreview.net/profile?id=~Da_Kuang2), [Surbhi Goel](http://openreview.net/profile?id=~Surbhi_Goel1)
  - **Affiliations:** Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA, Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA
  - **TL;DR:** This study investigates the dynamics of feature learning in neural networks, particularly how spurious features impact the learning of core features. Key findings reveal that simpler spurious features can slow down the learning rate of core features and that distinct subnetworks may form to learn these features separately.
  - **Keywords:** feature learning, spurious correlations, neural networks, gradient descent, boolean function analysis, classification tasks, machine learning, spurious features, core features, simplicity bias, theoretical framework, synthetic dataset, learning dynamics, parity functions, staircase functions, ReLU network


- [Self-Supervised Interpretable End-to-End Learning via Latent Functional Modularity](https://icml.cc/virtual/2024/poster/33564) (Poster)
  - **Authors:** [Hyunki Seong](http://openreview.net/profile?id=~Hyunki_Seong1), [Hyunchul Shim](http://openreview.net/profile?id=~Hyunchul_Shim1)
  - **Affiliations:** School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea, School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea
  - **TL;DR:** This paper presents MoNet, a functionally modular network designed for self-supervised and interpretable end-to-end learning, which effectively enhances decision-making processes in latent space for autonomous navigation. The method outperforms baseline models by 7% to 28% in task specificity while providing insights into explainable AI in robotic learning.
  - **Keywords:** self-supervised learning, interpretable learning, end-to-end learning, latent-guided contrastive loss, functional modularity, autonomous navigation, robotic learning, lack of task specificity, unclear decision-making processes, MoNet, post-hoc explainability, explainable artificial intelligence


- [High-Dimensional Bayesian Optimization via Semi-Supervised Learning with Optimized Unlabeled Data Sampling](https://icml.cc/virtual/2024/poster/33629) (Spotlight Poster)
  - **Authors:** [Yuxuan Yin](http://openreview.net/profile?id=~Yuxuan_Yin1), [Yu Wang](http://openreview.net/profile?id=~Yu_Wang29), [Peng Li](http://openreview.net/profile?id=~Peng_Li8)
  - **Affiliations:** Department of Electrical and Computer Engineering, University of California, Santa Barbara, USA, Department of Electrical and Computer Engineering, University of California, Santa Barbara, USA, Department of Electrical and Computer Engineering, University of California, Santa Barbara, USA
  - **TL;DR:** This paper presents a novel semi-supervised learning approach called Teacher-Student Bayesian Optimization (TSBO) that integrates a teacher-student paradigm to enhance sample efficiency in Bayesian optimization tasks. The method significantly reduces the need for expensive labeled data queries while improving the generalization of the Gaussian Process surrogate model.
  - **Keywords:** Bayesian Optimization, Semi-Supervised Learning, Teacher-Student Model, Gaussian Process (GP), Unlabeled Data Sampling, Black-Box Optimization, Functional Molecule Design, Structural Optimization, Expensive Labeled Data Queries, Data Query Efficiency, High-Dimensional Data, Teacher-Student Bayesian Optimization (TSBO), Improved Sample Efficiency


- [Physics and Lie symmetry informed Gaussian processes](https://icml.cc/virtual/2024/poster/35147) (Poster)
  - **Authors:** [David Dalton](http://openreview.net/profile?id=~David_Dalton1), [Dirk Husmeier](http://openreview.net/profile?id=~Dirk_Husmeier2), [Hao Gao](http://openreview.net/profile?id=~Hao_Gao9)
  - **Affiliations:** School of Mathematics and Statistics, University of Glasgow, United Kingdom, School of Mathematics and Statistics, University of Glasgow, United Kingdom, School of Mathematics and Statistics, University of Glasgow, United Kingdom
  - **TL;DR:** This study introduces a method for incorporating Lie symmetries into physics-informed Gaussian processes to enhance the modeling of partial differential equations. The results indicate that symmetry constraints significantly improve GP performance in both forward and inverse problems, showing competitive results compared to neural networks in low-data scenarios.
  - **Keywords:** Physics-informed machine learning (PIML), Partial differential equations (PDEs), Gaussian processes (GP), Lie symmetry method, Forward and inverse problems, PDE modeling, Computational costs, data integration challenges, Improved GP performance with symmetry constraints, competitive performance with neural networks in low-data environments


- [Interpreting Equivariant Representations](https://icml.cc/virtual/2024/poster/32789) (Poster)
  - **Authors:** [Andreas Abildtrup Hansen](http://openreview.net/profile?id=~Andreas_Abildtrup_Hansen1), [Anna Calissano](http://openreview.net/profile?id=~Anna_Calissano1), [Aasa Feragen](http://openreview.net/profile?id=~Aasa_Feragen2)
  - **Affiliations:** Department of Visual Computing, Technical University of Denmark, Kgs. Lyngby, Denmark, INRIA d’Université Côte d’Azur, France; Department of Mathematics, Imperial College London, London, England, Department of Visual Computing, Technical University of Denmark, Kgs. Lyngby, Denmark
  - **TL;DR:** This paper emphasizes the significance of accounting for the inductive bias of equivariant models when utilizing latent representations, demonstrating that invariant projections can enhance performance in tasks like molecular graph generation and image classification. The findings suggest that analyzing invariant latent representations yields better results than their equivariant counterparts.
  - **Keywords:** latent representations, equivariant models, deep learning, permutation equivariant variational autoencoder, rotation-equivariant representation, molecular graph generation, image classification, inductive bias, performance in downstream tasks, invariant projections of latent representations, analysis of latent representations, VAE (Variational Autoencoder), data augmentation


- [Risk Estimation in a Markov Cost Process: Lower and Upper Bounds](https://icml.cc/virtual/2024/poster/34863) (Poster)
  - **Authors:** [Gugan Chandrashekhar Mallika Thoppe](http://openreview.net/profile?id=~Gugan_Thoppe1), [Prashanth L.A.](http://openreview.net/profile?id=~Prashanth_L_A1), [Sanjay Bhat](http://openreview.net/profile?id=~Sanjay_P._Bhat1)
  - **Affiliations:** Dept. of Computer Science and Automation, Indian Institute of Science (IISc), Bengaluru, India; Robert Bosch Centre for Data Science and Artificial Intelligence, IIT Madras, Chennai, India, Dept. of Computer Science and Engineering, Indian Institute of Technology Madras, Chennai, India, TCS Research, Hyderabad, India
  - **TL;DR:** This study addresses the estimation of risk measures, specifically variance, VaR, and CVaR, in the context of a Markov cost process, demonstrating that achieving ϵ-accuracy requires at least Ω(1/ϵ²) samples. The authors derive upper bounds for CVaR and variance estimation that align with their lower bounds, marking a significant advancement in risk-sensitive reinforcement learning.
  - **Keywords:** Risk estimation, Markov cost process, Reinforcement learning, Variance, Value-at-Risk (VaR), Conditional Value-at-Risk (CVaR), Truncation scheme, Financial domain, Transportation, Estimating risk measures, Sample complexity, Lower and upper bounds for risk measure estimation, Improved sample complexity bounds


- [PAGER: Accurate Failure Characterization in Deep Regression Models](https://icml.cc/virtual/2024/poster/34993) (Poster)
  - **Authors:** [Jayaraman J. Thiagarajan](http://openreview.net/profile?id=~Jayaraman_J._Thiagarajan3), [Vivek Narayanaswamy](http://openreview.net/profile?id=~Vivek_Narayanaswamy1), [Puja Trivedi](http://openreview.net/profile?id=~Puja_Trivedi1), [Rushil Anirudh](http://openreview.net/profile?id=~Rushil_Anirudh1)
  - **Affiliations:** Lawrence Livermore National Labs, CA, USA, Lawrence Livermore National Labs, CA, USA, University of Michigan, USA, Amazon, CA, USA
  - **TL;DR:** This study introduces PAGER, a framework for accurately detecting and characterizing failures in deep regression models by integrating epistemic uncertainty with manifold non-conformity scores. The findings demonstrate that PAGER effectively identifies risk regimes that align closely with true risk, surpassing existing failure detection methods.
  - **Keywords:** AI safety, failure detection in regression models, epistemic uncertainty, manifold non-conformity scores, anchored training, healthcare, physical sciences, reinforcement learning, failure characterization, risk assessment in regression models, PAGER framework, risk regimes, failure detection metrics


- [Robust Yet Efficient Conformal Prediction Sets](https://icml.cc/virtual/2024/poster/34224) (Poster)
  - **Authors:** [Soroush H. Zargarbashi](http://openreview.net/profile?id=~Soroush_H._Zargarbashi1), [Mohammad Sadegh Akhondzadeh](http://openreview.net/profile?id=~Mohammad_Sadegh_Akhondzadeh1), [Aleksandar Bojchevski](http://openreview.net/profile?id=~Aleksandar_Bojchevski1)
  - **Affiliations:** CISPA Helmholtz Center for Information Security; University of Cologne, University of Cologne, University of Cologne
  - **TL;DR:** This paper presents a method for deriving robust conformal prediction sets that maintain coverage guarantees against adversarial attacks, including both evasion and poisoning. The proposed approach utilizes tighter bounds based on cumulative distribution functions, resulting in more efficient prediction sets and improved performance in safety-critical applications.
  - **Keywords:** Conformal Prediction, Uncertainty Quantification, Robustness, Randomly Smoothed Conformal Prediction (RSCP), Cumulative Distribution Function (CDF) bounds, Image Classification, Segmentation, Question Answering, Node Classification, Adversarial Attacks (Evasion and Poisoning), Calibration Data Sensitivity, Provably Robust Prediction Sets, Finite Sample Correction, ImageNet


- [On the Trajectory Regularity of ODE-based Diffusion Sampling](https://icml.cc/virtual/2024/poster/34478) (Poster)
  - **Authors:** [Defang Chen](http://openreview.net/profile?id=~Defang_Chen1), [Zhenyu Zhou](http://openreview.net/profile?id=~Zhenyu_Zhou6), [Can Wang](http://openreview.net/profile?id=~Can_Wang5), [Chunhua Shen](http://openreview.net/profile?id=~Chunhua_Shen2), [Siwei Lyu](http://openreview.net/profile?id=~Siwei_Lyu1)
  - **Affiliations:** State Key Laboratory of Blockchain and Data Security, Zhejiang University, China; Hangzhou High-Tech Zone (Binjiang) Blockchain and Data Security Research Institute, China, State Key Laboratory of Blockchain and Data Security, Zhejiang University, China; Hangzhou High-Tech Zone (Binjiang) Blockchain and Data Security Research Institute, China, State Key Laboratory of Blockchain and Data Security, Zhejiang University, China; Hangzhou High-Tech Zone (Binjiang) Blockchain and Data Security Research Institute, China, Zhejiang University, China, University at Buffalo, USA
  - **TL;DR:** This paper investigates the trajectory properties in the ODE-based sampling process of diffusion models, highlighting the role of an implicit denoising trajectory in achieving shape regularity. The authors propose a dynamic programming-based scheme to optimize the time schedule in sampling, resulting in enhanced performance in image generation with minimal computational cost.
  - **Keywords:** Diffusion-based generative models, Stochastic differential equations (SDEs), Ordinary differential equations (ODEs), Score function, Implicit denoising trajectory, Dynamic programming-based scheme, Image generation, Text-to-image synthesis, Regularity in sampling trajectories, Complexity of SDEs, Coupled sampling trajectory, Improved time scheduling in sampling, Probability flow ordinary differential equation (PF-ODE), Gaussian noise


- [Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension](https://icml.cc/virtual/2024/poster/34890) (Poster)
  - **Authors:** [Fan Yin](http://openreview.net/profile?id=~Fan_Yin1), [Jayanth Srinivasa](http://openreview.net/profile?id=~Jayanth_Srinivasa1), [Kai-Wei Chang](http://openreview.net/profile?id=~Kai-Wei_Chang1)
  - **Affiliations:** Department of Computer Science, University of California, Los Angeles, LA, U.S.A., Cisco Research, U.S.A., Department of Computer Science, University of California, Los Angeles, LA, U.S.A.; Cisco Research, U.S.A.
  - **TL;DR:** This study investigates how to characterize and predict the truthfulness of texts generated by large language models (LLMs) using the local intrinsic dimension (LID) of model activations. The findings suggest that intrinsic dimensions can effectively reveal the truthfulness of model outputs and enhance user trust in LLMs.
  - **Keywords:** truthfulness, large language models (LLMs), hallucinations, local intrinsic dimension (LID), internal activations, question answering (QA), natural language processing (NLP), hallucinations, model uncertainty, quantifying truthfulness, understanding LLMs


- [Differentiable Weightless Neural Networks](https://icml.cc/virtual/2024/poster/34511) (Poster)
  - **Authors:** [Alan Bacellar](http://openreview.net/profile?id=~Alan_Tendler_Leibel_Bacellar1), [Zachary Susskind](http://openreview.net/profile?id=~Zachary_Susskind1), [Mauricio Breternitz Jr](http://openreview.net/profile?id=~Mauricio_Breternitz_Jr1), [Eugene John](http://openreview.net/profile?id=~Eugene_John1), [Lizy John](http://openreview.net/profile?id=~Lizy_Kurian_John1), [Priscila Lima](http://openreview.net/profile?id=~Priscila_Machado_Vieira_Lima1), [Felipe França](http://openreview.net/profile?id=~Felipe_M.G._Fran%C3%A7a1)
  - **Affiliations:** Federal University of Rio de Janeiro, Brazil, The University of Texas at Austin, USA, ISCTE - Instituto Universitario de Lisboa, Lisbon, Portugal, The University of Texas at San Antonio, USA, The University of Texas at Austin, USA, Federal University of Rio de Janeiro, Brazil, Instituto de Telecomunicações, Porto, Portugal
  - **TL;DR:** This paper introduces Differentiable Weightless Neural Networks (DWN), a model utilizing interconnected lookup tables to enhance computational efficiency in edge computing contexts. The proposed DWNs demonstrate superior performance in latency, throughput, and energy efficiency compared to existing solutions, positioning them as a promising approach for high-throughput neural networks.
  - **Keywords:** Differentiable Weightless Neural Networks, Edge Computing, Extended Finite Difference technique, Lookup Tables (LUTs), FPGA-based hardware accelerator, Low-power microcontroller, Ultra-low-cost chips, Computational efficiency, Inference optimization, Learnable Mapping, Learnable Reduction, Spectral Regularization, Iris dataset, Weightless Neural Networks (WNNs), Binary Neural Networks (BNNs)


- [A Unified Adaptive Testing System Enabled by Hierarchical Structure Search](https://icml.cc/virtual/2024/poster/33744) (Poster)
  - **Authors:** [Junhao Yu](http://openreview.net/profile?id=~Junhao_Yu2), [Yan Zhuang](http://openreview.net/profile?id=~Yan_Zhuang4), [Zhenya Huang](http://openreview.net/profile?id=~Zhenya_Huang2), [Qi Liu](http://openreview.net/profile?id=~Qi_Liu3), [Xin Li](http://openreview.net/profile?id=~Xin_Li56), [Rui Li](http://openreview.net/profile?id=~Rui_Li35), [Enhong Chen](http://openreview.net/profile?id=~Enhong_Chen1)
  - **Affiliations:** State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, School of Computer Science and Technology, Xidian University, State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China; Institute of Artificial Intelligence, Hefei Comprehensive National Science Center
  - **TL;DR:** This paper presents a unified data-driven framework for Adaptive Testing Systems (ATS) that dynamically adjusts questions based on individual ability levels, improving assessment accuracy while reducing the number of questions by an average of 20%. The proposed method offers theoretical guarantees for estimation error and convergence, enhancing training stability in various testing scenarios.
  - **Keywords:** Adaptive Testing System (ATS), personalized ability assessment, Item Response Theory (IRT), Reinforcement Learning, Standardized tests (GRE, TOEFL, GMAT, Duolingo Test), Need for a unified framework for diverse ATS formats, optimizing accuracy with fewer questions, Unified data-driven ATS framework, hierarchical test structure search problem, theoretical guarantees for estimation error and convergence, Computerized Adaptive Testing (CAT), Multistage Testing (MST)


- [CF-OPT: Counterfactual Explanations for Structured Prediction](https://icml.cc/virtual/2024/poster/32709) (Poster)
  - **Authors:** [Germain Vivier-Ardisson](http://openreview.net/profile?id=~Germain_Vivier-Ardisson1), [Alexandre Forel](http://openreview.net/profile?id=~Alexandre_Forel1), [Axel Parmentier](http://openreview.net/profile?id=~Axel_Parmentier1), [Thibaut Vidal](http://openreview.net/profile?id=~Thibaut_Vidal1)
  - **Affiliations:** CIRRELT & SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematical and Industrial Engineering, Polytechnique Montreal, Montreal, Canada, CIRRELT & SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematical and Industrial Engineering, Polytechnique Montreal, Montreal, Canada, CERMICS, École des Ponts, Marne-la-Vallée, France, CIRRELT & SCALE-AI Chair in Data-Driven Supply Chains, Department of Mathematical and Industrial Engineering, Polytechnique Montreal, Montreal, Canada
  - **TL;DR:** The study introduces CF-OPT, a method for generating counterfactual explanations in structured learning pipelines that combine machine learning and optimization. It aims to enhance interpretability by providing insights into the decision-making process, demonstrating that plausible counterfactuals can be obtained for various structured learning problems.
  - **Keywords:** structured learning, interpretability, counterfactual explanations, variational autoencoders, first-order optimization, lack of interpretability, opaque decision-making processes, CF-OPT, counterfactual explanations for structured learning pipelines


- [Observable Propagation: Uncovering Feature Vectors in Transformers](https://icml.cc/virtual/2024/poster/34583) (Poster)
  - **Authors:** [Jacob Dunefsky](http://openreview.net/profile?id=~Jacob_Dunefsky1), [Arman Cohan](http://openreview.net/profile?id=~Arman_Cohan1)
  - **Affiliations:** Department of Computer Science, Yale University, New Haven, CT, United States, Department of Computer Science, Yale University, New Haven, CT, United States
  - **TL;DR:** This study introduces a novel method called "observable propagation" (OBPROP) for identifying linear features in transformer models with minimal data requirements. The findings indicate that OBPROP outperforms traditional methods in low-data scenarios and enhances understanding of biases in large language models.
  - **Keywords:** mechanistic interpretability, feature vectors, transformers, observable propagation (OBPROP), coupling coefficient, gendered occupational bias, political party prediction, programming language detection, data sparsity, model interpretability, new method for finding feature vectors, analysis of feature vectors


- [Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization](https://icml.cc/virtual/2024/poster/33193) (Spotlight Poster)
  - **Authors:** [Haocheng Xi](http://openreview.net/profile?id=~Haocheng_Xi1), [Yuxiang Chen](http://openreview.net/profile?id=~Yuxiang_Chen2), [Kang Zhao](http://openreview.net/profile?id=~Kang_Zhao5), [KAI JUN TEH](http://openreview.net/profile?id=~KAI_JUN_TEH1), [Jianfei Chen](http://openreview.net/profile?id=~Jianfei_Chen1), [Jun Zhu](http://openreview.net/profile?id=~Jun_Zhu2)
  - **Affiliations:** Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab, Tsinghua-Bosch Joint ML Center, Tsinghua University; Institute for Interdisciplinary Information Sciences, Tsinghua University, Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab, Tsinghua-Bosch Joint ML Center, Tsinghua University, Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab, Tsinghua-Bosch Joint ML Center, Tsinghua University, Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab, Tsinghua-Bosch Joint ML Center, Tsinghua University, Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab, Tsinghua-Bosch Joint ML Center, Tsinghua University, Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab, Tsinghua-Bosch Joint ML Center, Tsinghua University
  - **TL;DR:** The study introduces Jetfire, an efficient INT8 training method for transformers that optimizes memory access and maintains accuracy, achieving a 1.42x training speedup and 1.49x memory reduction compared to FP16 training. Extensive experiments show that Jetfire matches the accuracy of FP16 training while outperforming existing INT8 methods for transformers.
  - **Keywords:** Transformer pretraining, Fully quantized training (FQT), INT8 data flow, Per-block quantization, Natural language processing, Computer vision, Resource-intensive pretraining, High memory access overheads, Low-precision computations, Efficient INT8 training method, Comparable accuracy to FP16 training, Speedup and memory reduction, Transformers, Quantization


- [Policy Evaluation for Variance in Average Reward Reinforcement Learning](https://icml.cc/virtual/2024/poster/33647) (Poster)
  - **Authors:** [Shubhada Agrawal](http://openreview.net/profile?id=~Shubhada_Agrawal1), [Prashanth L.A.](http://openreview.net/profile?id=~Prashanth_L_A1), [Siva Maguluri](http://openreview.net/profile?id=~Siva_Theja_Maguluri1)
  - **Affiliations:** H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, USA, Department of Computer Science and Engineering, Indian Institute of Technology Madras, India, H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, USA
  - **TL;DR:** This study presents a temporal-difference algorithm for evaluating policies in average reward reinforcement learning, focusing on asymptotic variance as a risk measure for safety-critical applications. The proposed method achieves a finite time convergence rate and introduces the first sequential estimator for asymptotic variance with provable guarantees.
  - **Keywords:** Average reward reinforcement learning, Risk-sensitive reinforcement learning, Temporal-difference (TD) algorithm, Linear stochastic approximation, Safety-critical applications, Healthcare, Finance, Variance control, Mean-variance tradeoff, Asymptotic variance estimator, Finite time convergence rate, Markov decision process (MDP), Poisson equation


- [Kepler codebook](https://icml.cc/virtual/2024/poster/34986) (Poster)
  - **Authors:** [Junrong Lian](http://openreview.net/profile?id=~Junrong_Lian1), [Ziyue Dong](http://openreview.net/profile?id=~Ziyue_Dong1), [Pengxu Wei](http://openreview.net/profile?id=~Pengxu_Wei1), [Wei Ke](http://openreview.net/profile?id=~Wei_Ke1), [Chang Liu](http://openreview.net/profile?id=~Chang_Liu9), [Qixiang Ye](http://openreview.net/profile?id=~Qixiang_Ye1), [Xiangyang Ji](http://openreview.net/profile?id=~Xiangyang_Ji1), [Liang Lin](http://openreview.net/profile?id=~Liang_Lin1)
  - **Affiliations:** School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China, School of Software Engineering, Xi’an Jiaotong University, Xi’an, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China, School of Software Engineering, Xi’an Jiaotong University, Xi’an, China, Department of Automation, Tsinghua University, Beijing, China, School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China, Department of Automation, Tsinghua University, Beijing, China, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Peng Cheng Laboratory, Shenzhen, China
  - **TL;DR:** This study introduces the Kepler codebook, a novel approach to codebook training that addresses issues of codebook collapse and low usage by framing it as a sphere packing problem. The proposed method significantly enhances image reconstruction and generation performance across various datasets and resolutions.
  - **Keywords:** codebook learning, discrete distributions, image generation, vector quantization (VQ), Gumbel-Softmax, stochastic quantization, image reconstruction, image synthesis, codebook collapse, low codebook usage, unreliable training, Kepler codebook, improved codebook distribution, performance improvement in image generation, natural datasets, human face datasets, sphere packing problem, regularization


- [Adaptive Observation Cost Control for Variational Quantum Eigensolvers](https://icml.cc/virtual/2024/poster/33556) (Poster)
  - **Authors:** [Christopher J. Anders](http://openreview.net/profile?id=~Christopher_J._Anders1), [Kim A. Nicoli](http://openreview.net/profile?id=~Kim_Andrea_Nicoli1), [Bingting Wu](http://openreview.net/profile?id=~Bingting_Wu1), [Naima Borras](http://openreview.net/profile?id=~Naima_Elosegui_Borras1), [Samuele Pedrielli](http://openreview.net/profile?id=~Samuele_Pedrielli1), [Lena Funcke](http://openreview.net/profile?id=~Lena_Funcke1), [Karl Jansen](http://openreview.net/profile?id=~Karl_Jansen1), [Stefan Kühn](http://openreview.net/profile?id=~Stefan_K%C3%BChn1), [Shinichi Nakajima](http://openreview.net/profile?id=~Shinichi_Nakajima2)
  - **Affiliations:** Berlin Institute for the Foundations of Learning and Data (BIFOLD); Technische Universität Berlin, Germany, Transdisciplinary Research Area (TRA) Matter; University of Bonn, Germany; Helmholtz Institute for Radiation and Nuclear Physics (HISKP), University of Bonn, Germany, Technische Universität Berlin, Germany, Berlin Institute for the Foundations of Learning and Data (BIFOLD); Technische Universität Berlin, Germany, Technische Universität Berlin, Germany, Transdisciplinary Research Area (TRA) Matter; University of Bonn, Germany; Helmholtz Institute for Radiation and Nuclear Physics (HISKP), University of Bonn, Germany, CQTA, Deutsches Elektronen-Synchrotron (DESY), Zeuthen, Germany, CQTA, Deutsches Elektronen-Synchrotron (DESY), Zeuthen, Germany, Berlin Institute for the Foundations of Learning and Data (BIFOLD); Technische Universität Berlin, Germany; RIKEN Center for AIP, Japan
  - **TL;DR:** This paper introduces an adaptive cost control method called SubsCoRe for improving the efficiency of the Sequential Minimal Optimization (SMO) in Variational Quantum Eigensolvers (VQE) by minimizing observation noise. The proposed method significantly enhances optimization accuracy while reducing the number of required quantum measurement shots.
  - **Keywords:** Variational Quantum Eigensolver (VQE), Quantum Computing, Sequential Minimal Optimization (SMO), Gaussian Process (GP) Surrogate, Quantum Chemistry, Material Science, High-Energy Physics, Observation Noise, Shot Noise, Measurement Accuracy, Adaptive Cost Control Method (SubsCoRe), Efficiency Improvement in SMO


- [Learning Low-dimensional Latent Dynamics from High-dimensional Observations: Non-asymptotics and Lower Bounds](https://icml.cc/virtual/2024/poster/33466) (Poster)
  - **Authors:** [Yuyang Zhang](http://openreview.net/profile?id=~Yuyang_Zhang4), [Shahriar Talebi](http://openreview.net/profile?id=~Shahriar_Talebi2), [Na Li](http://openreview.net/profile?id=~Na_Li3)
  - **Affiliations:** SEAS, Harvard University, Cambridge, USA, SEAS, Harvard University, Cambridge, USA, SEAS, Harvard University, Cambridge, USA
  - **TL;DR:** This paper presents an algorithm for learning low-dimensional dynamics from high-dimensional observations using linear time-invariant (LTI) models, achieving optimal sample complexity bounds. It also explores a meta-learning approach that allows for collective learning from multiple datasets, potentially breaking traditional sample complexity limits.
  - **Keywords:** low-dimensional latent dynamics, high-dimensional observations, linear time-invariant (LTI) models, algorithm for recovering high-dimensional features, sample complexity analysis, finance, economics, biology, time series analysis, learning low-dimensional dynamics from high-dimensional data, sample complexity in high-dimensional settings, optimal sample complexity bounds, end-to-end algorithm for meta-learning


- [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://icml.cc/virtual/2024/poster/35153) (Poster)
  - **Authors:** [Yuhui Li](http://openreview.net/profile?id=~Yuhui_Li1), [Fangyun Wei](http://openreview.net/profile?id=~Fangyun_Wei1), [Chao Zhang](http://openreview.net/profile?id=~Chao_Zhang10), [Hongyang Zhang](http://openreview.net/profile?id=~Hongyang_Zhang1)
  - **Affiliations:** Peking University, Microsoft Research, Peking University, University of Waterloo; Vector Institute
  - **TL;DR:** This paper introduces EAGLE, a speculative sampling framework that enhances the efficiency of autoregressive decoding in large language models by addressing feature uncertainty. EAGLE achieves significant latency speedup and improved throughput while maintaining the distribution of generated text across various tasks.
  - **Keywords:** Large Language Models, Speculative Sampling, Autoregressive Decoding, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), Dialogue, Code Generation, Mathematical Reasoning, Instruction Following, Inference Latency, Feature Uncertainty, Latency Speedup, Throughput Improvement


- [Copyright Traps for Large Language Models](https://icml.cc/virtual/2024/poster/34309) (Poster)
  - **Authors:** [Matthieu Meeus](http://openreview.net/profile?id=~Matthieu_Meeus1), [Igor Shilov](http://openreview.net/profile?id=~Igor_Shilov1), [Manuel Faysse](http://openreview.net/profile?id=~Manuel_Faysse1), [Yves-Alexandre de Montjoye](http://openreview.net/profile?id=~Yves-Alexandre_de_Montjoye1)
  - **Affiliations:** Department of Computing, Imperial College London, United Kingdom, Department of Computing, Imperial College London, United Kingdom, MICS, CentraleSupélec, Université Paris-Saclay, Paris, France, Department of Computing, Imperial College London, United Kingdom; MICS, CentraleSupélec, Université Paris-Saclay, Paris, France
  - **TL;DR:** This study investigates the use of copyright traps to detect the incorporation of copyrighted materials in Large Language Models (LLMs) that do not naturally memorize content. The findings reveal that while medium-length trap sentences are undetectable, longer sequences can be reliably identified, contributing to the understanding of LLM memorization dynamics.
  - **Keywords:** Copyright issues, Large Language Models (LLMs), Document-level membership inference, Randomized controlled experimental setup, Copyright detection in LLMs, Fair use of copyrighted content, Detection of memorization in models, Copyright traps, Detection of copyrighted materials in LLMs, Memorization, AUC (Area Under Curve)


- [Tandem Transformers for Inference Efficient LLMs](https://icml.cc/virtual/2024/poster/33975) (Poster)
  - **Authors:** [Aishwarya P S](http://openreview.net/profile?id=~Aishwarya_P_S1), [Pranav Nair](http://openreview.net/profile?id=~Pranav_Ajit_Nair1), [Yashas Samaga](http://openreview.net/profile?id=~Yashas_Samaga_B_L2), [Toby Boyd](http://openreview.net/profile?id=~Toby_James_Boyd1), [Sanjiv Kumar](http://openreview.net/profile?id=~Sanjiv_Kumar1), [Prateek Jain](http://openreview.net/profile?id=~Prateek_Jain1), [Praneeth Kumar Netrapalli](http://openreview.net/profile?id=~Praneeth_Netrapalli1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google Research, New York City, Google DeepMind, Google DeepMind
  - **TL;DR:** This study introduces Tandem Transformers, a novel architecture that combines a small autoregressive model with a large model operating in block mode to enhance inference efficiency in large language models. The proposed method achieves a 3.3% improvement in prediction accuracy and a significant speedup in inference while maintaining downstream task performance.
  - **Keywords:** Inference efficiency, Large language models (LLMs), Tandem Transformers, autoregressive model, block mode processing, Natural language processing (NLP), Inference speed limitations, autoregressive generation process, Improved next-token prediction accuracy, speedup in inference, PaLM2 pretraining dataset, Speculative decoding (SPEED)


- [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://icml.cc/virtual/2024/poster/34527) (Poster)
  - **Authors:** [Mahdi Nikdan](http://openreview.net/profile?id=~Mahdi_Nikdan1), [Soroush Tabesh](http://openreview.net/profile?id=~Soroush_Tabesh1), [Elvir Crnčević](http://openreview.net/profile?id=~Elvir_Crn%C4%8Devi%C4%871), [Dan Alistarh](http://openreview.net/profile?id=~Dan_Alistarh7)
  - **Affiliations:** IST Austria, IST Austria, Graz University of Technology, Neural Magic
  - **TL;DR:** This paper introduces Robust Adaptation (RoSA), a new parameter-efficient fine-tuning method for large language models that significantly improves accuracy while maintaining low computational and memory costs. RoSA outperforms existing methods like LoRA and can match the performance of full fine-tuning on complex tasks.
  - **Keywords:** Parameter-Efficient Fine-Tuning, Large Language Models, Robust Adaptation (RoSA), Low-Rank Adaptation (LoRA), Sparse Fine-Tuning, Generative Tasks, Mathematical Reasoning, SQL Query Generation, High Computational and Memory Costs, Accuracy Recovery, Improved Accuracy, Stable Convergence, Hyper-Parameter Tuning


- [Proteus: Exploring Protein Structure Generation for Enhanced Designability and Efficiency](https://icml.cc/virtual/2024/poster/34422) (Poster)
  - **Authors:** [chentong wang](http://openreview.net/profile?id=~Chentong_Wang1), [Yannan Qu](http://openreview.net/profile?id=~Yannan_Qu1), [Zhangzhi Peng](http://openreview.net/profile?id=~Zhangzhi_Peng1), [Yukai Wang](http://openreview.net/profile?id=~Yukai_Wang1), [Hongli Zhu](http://openreview.net/profile?id=zhuhongli@westlake.edu.cn), [dachuan chen](http://openreview.net/profile?id=chendachuan@westlake.edu.cn), [Longxing Cao](http://openreview.net/profile?id=caolongxing@westlake.edu.cn)
  - **Affiliations:** Zhejiang University, Hangzhou, Zhejiang, China; School of Life Sciences, Westlake University, Hangzhou, Zhejiang, China, School of Life Sciences, Westlake University, Hangzhou, Zhejiang, China, Duke University, Durham, North Carolina, USA, School of Life Sciences, Westlake University, Hangzhou, Zhejiang, China, School of Life Sciences, Westlake University, Hangzhou, Zhejiang, China, School of Life Sciences, Westlake University, Hangzhou, Zhejiang, China, School of Life Sciences, Westlake University, Hangzhou, Zhejiang, China
  - **TL;DR:** The study introduces Proteus, a novel deep diffusion network for generating designable protein backbones without relying on pre-training, demonstrating superior efficiency and a high success rate in protein design. This advancement has the potential to significantly enhance the field of protein design.
  - **Keywords:** Protein design, Protein structure generation, Deep diffusion network, Graph-based triangle methods, Multi-track interaction network, Protein backbone generation, Protein design, Dependency on pre-training, Non-designable structures, Enhanced efficiency in protein backbone generation, High designability, RFdiffusion, AlphaFold2, RosettaFold, FoldingDiff, Hallucination


- [Exact Soft Analytical Side-Channel Attacks using Tractable Circuits](https://icml.cc/virtual/2024/poster/35183) (Poster)
  - **Authors:** [Thomas Wedenig](http://openreview.net/profile?id=~Thomas_Wedenig1), [Rishub Nagpal](http://openreview.net/profile?id=rishub.nagpal@iaik.tugraz.at), [Gaëtan Cassiers](http://openreview.net/profile?id=gaetan.cassiers@uclouvain.be), [Stefan Mangard](http://openreview.net/profile?id=stefan.mangard@tugraz.at), [Robert Peharz](http://openreview.net/profile?id=~Robert_Peharz5)
  - **Affiliations:** Institute of Theoretical Computer Science, Graz University of Technology, Graz, Austria; Institute of Applied Information Processing and Communications, Graz University of Technology, Graz, Austria, Institute of Applied Information Processing and Communications, Graz University of Technology, Graz, Austria, Institute of Information and Communication Technologies, Electronics and Applied Mathematics (ICTM), UCLouvain, Ottignies-Louvain-la-Neuve, Belgium, Institute of Applied Information Processing and Communications, Graz University of Technology, Graz, Austria, Institute of Theoretical Computer Science, Graz University of Technology, Graz, Austria
  - **TL;DR:** This paper presents ExSASCA, a fast and exact inference method for soft analytical side-channel attacks, significantly improving the success rate of key recovery in cryptographic algorithms like AES. The method leverages knowledge compilation and tractable probabilistic circuits, outperforming existing techniques while maintaining computational efficiency.
  - **Keywords:** cryptographic algorithms, side-channel attacks, information security, soft analytical side-channel attack (SASCA), loopy belief propagation, exact inference, knowledge compilation, tractable probabilistic circuits, Advanced Encryption Standard (AES), cryptographic computations, weaknesses in cryptographic algorithms, convergence issues in loopy belief propagation, ExSASCA method, improved success rate in key recovery


- [Iterative Regularized Policy Optimization with Imperfect Demonstrations](https://icml.cc/virtual/2024/poster/34488) (Poster)
  - **Authors:** [Xudong Gong](http://openreview.net/profile?id=~Gong_Xudong1), [Feng Dawei](http://openreview.net/profile?id=~Feng_Dawei1), [Kele Xu](http://openreview.net/profile?id=~Kele_Xu2), [Yuanzhao Zhai](http://openreview.net/profile?id=~Yuanzhao_Zhai1), [Chengkang Yao](http://openreview.net/profile?id=yaochengkang@126.com), [Weijia Wang](http://openreview.net/profile?id=weijia.hust@gmail.com), [Bo Ding](http://openreview.net/profile?id=~Bo_Ding1), [Huaimin Wang](http://openreview.net/profile?id=~Huaimin_Wang1)
  - **Affiliations:** College of Computer, National University of Defense Technology, Changsha, Hunan, China; State Key Laboratory of Complex & Critical Software Environment, Changsha, Hunan, China, College of Computer, National University of Defense Technology, Changsha, Hunan, China; State Key Laboratory of Complex & Critical Software Environment, Changsha, Hunan, China, College of Computer, National University of Defense Technology, Changsha, Hunan, China; State Key Laboratory of Complex & Critical Software Environment, Changsha, Hunan, China, College of Computer, National University of Defense Technology, Changsha, Hunan, China; State Key Laboratory of Complex & Critical Software Environment, Changsha, Hunan, China, Flight Automatic Control Research Institute, A VIC, Xian, Shanxi, China, Flight Automatic Control Research Institute, A VIC, Xian, Shanxi, China, College of Computer, National University of Defense Technology, Changsha, Hunan, China; State Key Laboratory of Complex & Critical Software Environment, Changsha, Hunan, China, College of Computer, National University of Defense Technology, Changsha, Hunan, China; State Key Laboratory of Complex & Critical Software Environment, Changsha, Hunan, China
  - **TL;DR:** The study proposes Iterative Regularized Policy Optimization (IRPO) to enhance policy learning from imperfect demonstrations by combining offline imitation learning with online reinforcement exploration. Experimental results show that IRPO effectively improves both the quality of demonstrations and the performance of the learned policies.
  - **Keywords:** Imitation Learning, Reinforcement Learning, Kullback–Leibler (KL) regularization, Iterative Regularized Policy Optimization (IRPO), Control systems, UAV control, Imperfect demonstrations, Online learning instability, Over-constrained exploration, Improved demonstration quality, Enhanced policy performance


- [Online Resource Allocation with Non-Stationary Customers](https://icml.cc/virtual/2024/poster/34038) (Poster)
  - **Authors:** [Xiaoyue Zhang](http://openreview.net/profile?id=~Xiaoyue_Zhang3), [Hanzhang Qin](http://openreview.net/profile?id=~Hanzhang_Qin1), [Mabel Chou](http://openreview.net/profile?id=mabelchou@nus.edu.sg)
  - **Affiliations:** Institute of Operations Research and Analytics, National University of Singapore, Singapore 117602, Institute of Operations Research and Analytics, National University of Singapore, Singapore 117602, Institute of Operations Research and Analytics, National University of Singapore, Singapore 117602
  - **TL;DR:** This paper presents the Unified Learning-while-Earning (ULwE) algorithm for online resource allocation in non-stationary environments, demonstrating its effectiveness in adapting to changing customer behaviors and achieving optimal revenue outcomes. The proposed method shows sublinear regret under near-stationary conditions and an optimal competitive ratio in general non-stationary scenarios.
  - **Keywords:** online resource allocation, non-stationary customer arrivals, Unified Learning-while-Earning (ULwE) algorithm, stochastic contextual bandit with knapsacks, online matching, online advertising, traffic management, dynamic resource distribution, fluctuating consumer behaviors, non-stationary environments, sublinear regret, optimal competitive ratio, near-optimal revenues


- [Multi-class Probabilistic Bounds for Majority Vote Classifiers with Partially Labeled Data](https://icml.cc/virtual/2024/poster/35637) (Poster)
  - **Authors:** Vasilii Feofanov, Emilie Devijver, Massih-Reza Amini
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Mathematical Framework for Online Social Media Auditing](https://icml.cc/virtual/2024/poster/35644) (Poster)
  - **Authors:** Wasim Huleihel, Yehonathan Refael
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI](https://icml.cc/virtual/2024/poster/34062) (Poster)
  - **Authors:** [Kaining Ying](http://openreview.net/profile?id=~Kaining_Ying2), [Fanqing Meng](http://openreview.net/profile?id=~Fanqing_Meng1), [Jin Wang](http://openreview.net/profile?id=~Jin_Wang16), [Zhiqian Li](http://openreview.net/profile?id=~Zhiqian_Li1), [Han Lin](http://openreview.net/profile?id=~Han_Lin3), [Yue Yang](http://openreview.net/profile?id=~Yue_Yang6), [Hao Zhang](http://openreview.net/profile?id=~Hao_Zhang56), [Wenbo Zhang](http://openreview.net/profile?id=~Wenbo_Zhang9), [Yuqi Lin](http://openreview.net/profile?id=~Yuqi_Lin1), [Shuo Liu](http://openreview.net/profile?id=~Shuo_Liu5), [jiayi lei](http://openreview.net/profile?id=~jiayi_lei1), [Quanfeng Lu](http://openreview.net/profile?id=~Quanfeng_Lu1), [Runjian Chen](http://openreview.net/profile?id=~Runjian_Chen1), [Peng Xu](http://openreview.net/profile?id=~Peng_Xu11), [Renrui Zhang](http://openreview.net/profile?id=~Renrui_Zhang1), [Haozhe Zhang](http://openreview.net/profile?id=~Haozhe_Zhang4), [Peng Gao](http://openreview.net/profile?id=~Peng_Gao3), [Yali Wang](http://openreview.net/profile?id=~Yali_Wang1), [Yu Qiao](http://openreview.net/profile?id=~Yu_Qiao1), [Ping Luo](http://openreview.net/profile?id=~Ping_Luo2), [Kaipeng Zhang](http://openreview.net/profile?id=~Kaipeng_Zhang1), [WENQI SHAO](http://openreview.net/profile?id=~Wenqi_Shao2)
  - **Affiliations:** Shanghai Artificial Intelligence Laboratory, Shanghai Jiao Tong University, The University of Hong Kong, Shanghai Artificial Intelligence Laboratory; The University of Hong Kong, Shanghai Jiao Tong University, Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, The University of Adelaide, Shanghai Artificial Intelligence Laboratory; Zhejiang University, Shanghai Artificial Intelligence Laboratory, Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, The University of Hong Kong, The University of Hong Kong, Shanghai Artificial Intelligence Laboratory, Zhejiang University, Shanghai Artificial Intelligence Laboratory, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shanghai Artificial Intelligence Laboratory, The University of Hong Kong, Shanghai Artificial Intelligence Laboratory, Shanghai Artificial Intelligence Laboratory
  - **TL;DR:** This study introduces MMT-Bench, a comprehensive benchmark for evaluating Large Vision-Language Models (LVLMs) across a wide range of multimodal tasks, addressing the limitations of existing benchmarks. The evaluation results highlight significant challenges for 32 LVLMs, aiming to inspire the development of next-generation multimodal foundation models for achieving general-purpose multimodal intelligence.
  - **Keywords:** Large Vision-Language Models, Multimodal Benchmarking, Multitask AGI, Visual Dialogue, Embodied Navigation, Limited coverage of multimodal tasks, Evaluation of LVLMs, MMT-Bench, Evaluation metrics for LVLMs, 31,325 multi-choice visual questions, LVLMs (Large Vision-Language Models), AGI (Artificial General Intelligence)


- [Reference Neural Operators: Learning the Smooth Dependence of Solutions of PDEs on Geometric Deformations](https://icml.cc/virtual/2024/poster/34670) (Poster)
  - **Authors:** [Ze Cheng](http://openreview.net/profile?id=~Ze_Cheng2), [Zhongkai Hao](http://openreview.net/profile?id=~Zhongkai_Hao1), [Wang Xiaoqiang](http://openreview.net/profile?id=xiaoqiang.wang2@cn.bosch.com), [Jianing Huang](http://openreview.net/profile?id=jianing.huang@cn.bosch.com), [Youjia Wu](http://openreview.net/profile?id=~Youjia_Wu1), [Xudan Liu](http://openreview.net/profile?id=xudan.liu@cn.bosch.com), [Yiru Zhao](http://openreview.net/profile?id=yiru.zhao@cn.bosch.com), [LIU SONGMING](http://openreview.net/profile?id=~Songming_Liu1), [Hang Su](http://openreview.net/profile?id=~Hang_Su3)
  - **Affiliations:** Bosch (China) Investment Co., Ltd., Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University, Bosch (China) Investment Co., Ltd., Bosch (China) Investment Co., Ltd., Bosch (China) Investment Co., Ltd., Bosch (China) Investment Co., Ltd., Bosch (China) Investment Co., Ltd., Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University, Dept. of Comp. Sci. & Techn., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, Tsinghua University
  - **TL;DR:** This study introduces reference neural operators (RNO) to efficiently predict solutions of partial differential equations (PDEs) based on geometric deformations, significantly reducing the need for large datasets. The proposed method demonstrates improved accuracy and up to 80% error reduction compared to existing models.
  - **Keywords:** neural operators, partial differential equations (PDEs), geometric deformations, reference neural operators (RNO), engineering design optimization, physics simulation, data efficiency, large dataset requirement, computational cost, 80% error reduction, improved accuracy over baseline models


- [Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](https://icml.cc/virtual/2024/poster/33418) (Oral)
  - **Authors:** [Collin Burns](http://openreview.net/profile?id=~Collin_Burns1), [Pavel Izmailov](http://openreview.net/profile?id=~Pavel_Izmailov1), [Jan Kirchner](http://openreview.net/profile?id=~Jan_Hendrik_Kirchner1), [Bowen Baker](http://openreview.net/profile?id=~Bowen_Baker2), [Leo Gao](http://openreview.net/profile?id=~Leo_Gao1), [Leopold Aschenbrenner](http://openreview.net/profile?id=leopold@openai.com), [Yining Chen](http://openreview.net/profile?id=~Yining_Chen1), [Adrien Ecoffet](http://openreview.net/profile?id=~Adrien_Ecoffet1), [Manas Joglekar](http://openreview.net/profile?id=manas@openai.com), [Jan Leike](http://openreview.net/profile?id=~Jan_Leike1), [Ilya Sutskever](http://openreview.net/profile?id=~Ilya_Sutskever2), [Jeffrey K Wu](http://openreview.net/profile?id=~Jeffrey_Wu1)
  - **Affiliations:** Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI, Superalignment Generalization Team, OpenAI
  - **TL;DR:** The study investigates whether weak supervision can effectively elicit the capabilities of stronger models, finding that finetuning strong pretrained models on weak model-generated labels leads to better performance. The results indicate that while naive finetuning is insufficient to fully recover strong model capabilities, simple methods can significantly enhance weak-to-strong generalization, suggesting a path forward for aligning superhuman models.
  - **Keywords:** AI Alignment, Superhuman Models, Weak Supervision, Reinforcement Learning from Human Feedback (RLHF), Finetuning, Weak-to-Strong Generalization, Natural Language Processing (NLP), Chess, Reward Modeling, Reliable Supervision, Generalization Challenges, Complexity of Model Behavior, Improved Performance through Weak Supervision, Auxiliary Confidence Loss, GPT-4, GPT-2, GPT-3.5


- [Position: Intent-aligned AI Systems Must Optimize for Agency Preservation](https://icml.cc/virtual/2024/poster/32943) (Spotlight Poster)
  - **Authors:** [Catalin Mitelut](http://openreview.net/profile?id=~Catalin_Mitelut1), [Benjamin Smith](http://openreview.net/profile?id=benjsmith@gmail.com), [Peter Vamplew](http://openreview.net/profile?id=~Peter_Vamplew2)
  - **Affiliations:** Forum Basiliense, University of Basel, University of Oregon, Federation University Australia
  - **TL;DR:** The paper argues that AI systems aligned solely to human intent are insufficient for safety, as they can lead to a loss of human agency. It proposes that AI systems should explicitly optimize for the preservation of human agency to prevent manipulation and control loss in AI-human interactions.
  - **Keywords:** AI safety, AI alignment, human agency preservation, Human agency loss, manipulation of human intent, Agency-preserving AI-human interactions, formal definition of agency preservation, Goodhart’s law, artificial general intelligence (AGI), large-language-models (LLMs)


- [SFC: Achieve Accurate Fast Convolution under Low-precision Arithmetic](https://icml.cc/virtual/2024/poster/33457) (Poster)
  - **Authors:** [Liulu He](http://openreview.net/profile?id=~Liulu_He1), [yufei zhao](http://openreview.net/profile?id=662023230005@smail.nju.edu.cn), [rui gao](http://openreview.net/profile?id=gaorui_seu@foxmail.com), [Yuan Du](http://openreview.net/profile?id=~Yuan_Du2), [Li Du](http://openreview.net/profile?id=~Li_Du5)
  - **Affiliations:** School of Electronic Science and Engineering, Nanjing University, Nanjing 210023, China; Interdisciplinary Research Center for Future Intelligent Chips (Chip-X), Nanjing University, Suzhou 215163, China, School of Electronic Science and Engineering, Nanjing University, Nanjing 210023, China; Interdisciplinary Research Center for Future Intelligent Chips (Chip-X), Nanjing University, Suzhou 215163, China, School of Electronic Science and Engineering, Nanjing University, Nanjing 210023, China; Interdisciplinary Research Center for Future Intelligent Chips (Chip-X), Nanjing University, Suzhou 215163, China, School of Electronic Science and Engineering, Nanjing University, Nanjing 210023, China; Interdisciplinary Research Center for Future Intelligent Chips (Chip-X), Nanjing University, Suzhou 215163, China, School of Electronic Science and Engineering, Nanjing University, Nanjing 210023, China; Interdisciplinary Research Center for Future Intelligent Chips (Chip-X), Nanjing University, Suzhou 215163, China
  - **TL;DR:** The study introduces SFC, a new algebra transform for fast convolution that enhances efficiency under low-precision arithmetic while maintaining accuracy. It demonstrates a significant reduction in multiplication requirements for convolution operations compared to existing methods, addressing the challenges of combining fast convolution algorithms with model quantization.
  - **Keywords:** Fast convolution, model quantization, deep learning, Winograd algorithm, Fast Fourier Transform (FFT), Discrete Fourier Transform (DFT), Number Theoretic Transform (NTT), High-precision arithmetic requirement, numerical error increase, model accuracy degradation, SFC (new algebra transform), multiplication reduction for convolution


- [PhAST: Physics-Aware, Scalable, and Task-Specific GNNs for Accelerated Catalyst Design](https://icml.cc/virtual/2024/poster/35634) (Poster)
  - **Authors:** Alexandre Duval, Victor Schmidt, Santiago Miret, Yoshua Bengio, Alex Hernandez-Garcia, David Rolnick
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Extending Adversarial Attacks to Produce Adversarial Class Probability Distributions](https://icml.cc/virtual/2024/poster/35632) (Poster)
  - **Authors:** Jon Vadillo, Roberto Santana, Jose A Lozano
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [A Bayesian Approach to Online Planning](https://icml.cc/virtual/2024/poster/34204) (Poster)
  - **Authors:** [Nir Greshler](http://openreview.net/profile?id=~Nir_Greshler1), [David Ben Eli](http://openreview.net/profile?id=dudi.beneli@gm.com), [Carmel Rabinovitz](http://openreview.net/profile?id=~Carmel_Rabinovitz1), [Gabi Guetta](http://openreview.net/profile?id=gabi.guetta@gm.com), [Liran Gispan](http://openreview.net/profile?id=~Liran_Gispan1), [Guy Zohar](http://openreview.net/profile?id=guy.zohar@gm.com), [Aviv Tamar](http://openreview.net/profile?id=~Aviv_Tamar2)
  - **Affiliations:** General Motors, Advanced Technical Center, Israel, General Motors, Advanced Technical Center, Israel, General Motors, Advanced Technical Center, Israel, General Motors, Advanced Technical Center, Israel, General Motors, Advanced Technical Center, Israel, General Motors, Advanced Technical Center, Israel, Department of Electrical and Computer Engineering, Technion - Israel Institute of Technology, Haifa, Israel
  - **TL;DR:** This study presents a Bayesian approach to online planning that incorporates uncertainty estimates from neural networks to enhance decision-making in environments like game playing and robotics. The proposed methods, including Thompson sampling and Bayes-UCB, demonstrate improved search effectiveness in scenarios where neural network outputs are inaccurate.
  - **Keywords:** Bayesian planning, online planning, uncertainty quantification, Monte Carlo Tree Search (MCTS), Thompson sampling, Bayes-UCB, Game playing, robotic manipulation, autonomous driving, planning with large language models, Errors in neural network approximations, action-value estimation uncertainty, Finite time Bayesian regret bound, efficient implementation for posterior distributions, ProcGen Maze, ProcGen Leaper


- [Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks](https://icml.cc/virtual/2024/poster/35633) (Poster)
  - **Authors:** Dongyoung Lim, Sotirios Sabanis
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Fast Timing-Conditioned Latent Audio Diffusion](https://icml.cc/virtual/2024/poster/33311) (Oral)
  - **Authors:** [Zach Evans](http://openreview.net/profile?id=~Zach_Evans1), [CJ Carr](http://openreview.net/profile?id=cj@stability.ai), [Josiah Taylor](http://openreview.net/profile?id=josiah@stability.ai), [Scott Hawley](http://openreview.net/profile?id=~Scott_H._Hawley1), [Jordi Pons](http://openreview.net/profile?id=~Jordi_Pons1)
  - **Affiliations:** Stability AI, Stability AI, Stability AI, Belmont University; Stability AI, Stability AI
  - **TL;DR:** This study presents Stable Audio, a latent diffusion model that efficiently generates long-form, variable-length stereo audio from text prompts. It achieves state-of-the-art results in audio generation while allowing for fine control over content and length, addressing challenges in previous models related to fixed-size outputs.
  - **Keywords:** audio generation, latent diffusion models, text-to-music, latent diffusion, fully-convolutional variational autoencoder, timing embeddings, music generation, sound effects generation, computational efficiency, variable-length audio generation, fixed-size output limitation, state-of-the-art results in long-form audio generation, structured music generation


- [The WMDP Benchmark: Measuring and Reducing Malicious Use with Unlearning](https://icml.cc/virtual/2024/poster/32695) (Poster)
  - **Authors:** [Nathaniel Li](http://openreview.net/profile?id=~Nathaniel_Li1), [Alexander Pan](http://openreview.net/profile?id=~Alexander_Pan1), [Anjali Gopal](http://openreview.net/profile?id=~Anjali_Gopal1), [Summer Yue](http://openreview.net/profile?id=~Summer_Yue2), [Daniel Berrios](http://openreview.net/profile?id=~Daniel_Berrios1), [Alice Gatti](http://openreview.net/profile?id=~Alice_Gatti1), [Justin Li](http://openreview.net/profile?id=~Justin_D._Li1), [Ann-Kathrin Dombrowski](http://openreview.net/profile?id=~Ann-Kathrin_Dombrowski1), [Shashwat Goel](http://openreview.net/profile?id=~Shashwat_Goel1), [Gabriel Mukobi](http://openreview.net/profile?id=~Gabriel_Mukobi1), [Nathan Helm-Burger](http://openreview.net/profile?id=~Nathan_Helm-Burger1), [Rassin Lababidi](http://openreview.net/profile?id=~Rassin_Lababidi1), [Lennart Justen](http://openreview.net/profile?id=~Lennart_Justen1), [Andrew Liu](http://openreview.net/profile?id=~Andrew_Bo_Liu1), [Michael Chen](http://openreview.net/profile?id=~Michael_Chen3), [Isabelle Barrass](http://openreview.net/profile?id=~Isabelle_Barrass1), [Oliver Zhang](http://openreview.net/profile?id=~Oliver_Zhang1), [Xiaoyuan Zhu](http://openreview.net/profile?id=~Xiaoyuan_Zhu2), [Rishub Tamirisa](http://openreview.net/profile?id=~Rishub_Tamirisa1), [Bhrugu Bharathi](http://openreview.net/profile?id=~Bhrugu_Bharathi2), [Ariel Herbert-Voss](http://openreview.net/profile?id=~Ariel_Herbert-Voss1), [Cort Breuer](http://openreview.net/profile?id=~Cort_B_Breuer1), [Andy Zou](http://openreview.net/profile?id=~Andy_Zou1), [Mantas Mazeika](http://openreview.net/profile?id=~Mantas_Mazeika3), [Zifan Wang](http://openreview.net/profile?id=~Zifan_Wang1), [Palash Oswal](http://openreview.net/profile?id=~Palash_Oswal1), [Weiran Lin](http://openreview.net/profile?id=~Weiran_Lin1), [Adam Hunt](http://openreview.net/profile?id=~Adam_Alfred_Hunt1), [Justin Tienken-Harder](http://openreview.net/profile?id=~Justin_Tienken-Harder1), [Kevin Shih](http://openreview.net/profile?id=~Kevin_Y._Shih1), [Kemper Talley](http://openreview.net/profile?id=~Kemper_Talley1), [John Guan](http://openreview.net/profile?id=~John_Guan1), [Ian Steneker](http://openreview.net/profile?id=~Ian_Steneker1), [David Campbell](http://openreview.net/profile?id=~David_Campbell2), [Brad Jokubaitis](http://openreview.net/profile?id=~Brad_Jokubaitis1), [Steven Basart](http://openreview.net/profile?id=~Steven_Basart1), [Stephen Fitz](http://openreview.net/profile?id=~Stephen_Fitz1), [Ponnurangam Kumaraguru](http://openreview.net/profile?id=~Ponnurangam_Kumaraguru3), [Kallol Karmakar](http://openreview.net/profile?id=~Kallol_Krishna_Karmakar1), [Uday Tupakula](http://openreview.net/profile?id=~Uday_Tupakula1), [Vijay Varadharajan](http://openreview.net/profile?id=~Vijay_Varadharajan1), [Yan Shoshitaishvili](http://openreview.net/profile?id=~Yan_Shoshitaishvili1), [Jimmy Ba](http://openreview.net/profile?id=~Jimmy_Ba1), [Kevin Esvelt](http://openreview.net/profile?id=~Kevin_M._Esvelt1), [Alexandr Wang](http://openreview.net/profile?id=~Alexandr_Wang1), [Dan Hendrycks](http://openreview.net/profile?id=~Dan_Hendrycks1)
  - **Affiliations:** Center for AI Safety; UC Berkeley, UC Berkeley, MIT; SecureBio, Scale AI, Scale AI, Center for AI Safety, Center for AI Safety; NYU, Center for AI Safety, Center for AI Safety, Stanford, SecureBio, SecureBio, MIT; SecureBio, SecureBio; Harvard, Center for AI Safety, Center for AI Safety, Center for AI Safety, USC, UIUC; Lapis Labs, UCLA, Harvard, Stanford, Center for AI Safety, Center for AI Safety; UIUC, Center for AI Safety, CMU, CMU, CMU, Harvard, Stanford, RTX BBN Technologies, UC Berkeley, Scale AI, Scale AI, Scale AI, Center for AI Safety, Keio University, IIIT Hyderabad, University of Newcastle, University of Newcastle, University of Newcastle, ASU, xAI, MIT, Scale AI, Center for AI Safety
  - **TL;DR:** The study introduces the Weapons of Mass Destruction Proxy (WMDP) benchmark to measure hazardous knowledge in AI, particularly focusing on the risks posed by large language models (LLMs) in malicious use scenarios. It also presents a state-of-the-art unlearning method (RMU) that effectively reduces model performance on the WMDP benchmark while preserving general capabilities, suggesting a viable approach to mitigate malicious use risks.
  - **Keywords:** Malicious use of AI, Dual-use technology, AI safety, Unlearning methods, RMU (Representation-based Model Unlearning), Biosecurity, Cybersecurity, Chemical security, Risks of large language models (LLMs), Malicious use scenarios, Evaluation limitations, WMDP benchmark, Hazardous knowledge measurement, WMDP benchmark (3,668 multiple-choice questions), Large Language Models (LLMs), AI-enabled threats


- [Cell2Sentence: Teaching Large Language Models the Language of Biology](https://icml.cc/virtual/2024/poster/34580) (Poster)
  - **Authors:** [Daniel Levine](http://openreview.net/profile?id=~Daniel_Levine2), [Syed Rizvi](http://openreview.net/profile?id=~Syed_A_Rizvi1), [Sacha Lévy](http://openreview.net/profile?id=~Sacha_Lévy1), [Nazreen Pallikkavaliyaveetil MohammedSheriff](http://openreview.net/profile?id=nazreen.pm@yale.edu), [David Zhang](http://openreview.net/profile?id=david.zhang.ddz5@yale.edu), [Xingyu Chen](http://openreview.net/profile?id=~Xingyu_Chen11), [SINA GHADERMARZI](http://openreview.net/profile?id=~Sina_Ghadermarzi1), [Ruiming Wu](http://openreview.net/profile?id=wuru@seas.upenn.edu), [Zihe Zheng](http://openreview.net/profile?id=zihe.zheng@yale.edu), [Ivan Vrkic](http://openreview.net/profile?id=~Ivan_Vrkic1), [Anna Zhong](http://openreview.net/profile?id=anna.zhong@yale.edu), [Daphne Raskin](http://openreview.net/profile?id=daphne.raskin@yale.edu), [Insu Han](http://openreview.net/profile?id=~Insu_Han1), [Antonio Henrique de Oliveira Fonseca](http://openreview.net/profile?id=~Antonio_Henrique_de_Oliveira_Fonseca1), [Josue Ortega Caro](http://openreview.net/profile?id=~Josue_Ortega_Caro1), [Amin Karbasi](http://openreview.net/profile?id=~Amin_Karbasi3), [Rahul Dhodapkar](http://openreview.net/profile?id=~Rahul_Madhav_Dhodapkar1), [David van Dijk](http://openreview.net/profile?id=~David_van_Dijk1)
  - **Affiliations:** Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA, School of Engineering Applied Science, University of Pennsylvania, Philadelphia, PA, USA, Department of Computer Science, Yale University, New Haven, CT, USA, School of Computer and Communication Sciences, Swiss Federal Institute of Technology Lausanne, Lausanne, Switzerland, Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA; Department of Neuroscience, Yale School of Medicine, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA; Wu Tsai Institute, Yale University, New Haven, CT, USA, Google, Roski Eye Institute, University of Southern California, Los Angeles, CA, USA; Department of Internal Medicine (Cardiology), Yale School of Medicine, New Haven, CT, USA, Department of Computer Science, Yale University, New Haven, CT, USA; Cardiovascular Research Center, Yale School of Medicine, New Haven, CT, USA; Interdepartmental Program in Computational Biology & Bioinformatics, Yale University, New Haven, CT, USA
  - **TL;DR:** This study introduces Cell2Sentence (C2S), a method for adapting large language models to single-cell transcriptomics by transforming gene expression data into textual sequences. The findings demonstrate that fine-tuning models like GPT-2 with C2S significantly enhances their ability to generate biologically relevant cells and accurately predict cell types.
  - **Keywords:** Large Language Models, Single-Cell Transcriptomics, Cell2Sentence (C2S), Fine-tuning, Natural Language Processing, Biology, Transcriptomics, Adapting LLMs to biological contexts, Bridging gaps between NLP and biology, Generation of biologically valid cells, Prediction of cell types, Data-driven text generation


- [Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI](https://icml.cc/virtual/2024/poster/34830) (Oral)
  - **Authors:** [Francisco Eiras](http://openreview.net/profile?id=~Francisco_Eiras1), [Aleksandar Petrov](http://openreview.net/profile?id=~Aleksandar_Petrov1), [Bertie Vidgen](http://openreview.net/profile?id=~Bertie_Vidgen1), [Christian Schroeder de Witt](http://openreview.net/profile?id=~Christian_Schroeder_de_Witt1), [Fabio Pizzati](http://openreview.net/profile?id=~Fabio_Pizzati1), [Katherine Elkins](http://openreview.net/profile?id=~Katherine_Elkins1), [Supratik Mukhopadhyay](http://openreview.net/profile?id=~Supratik_Mukhopadhyay2), [Adel Bibi](http://openreview.net/profile?id=~Adel_Bibi1), [Botos Csaba](http://openreview.net/profile?id=~Botos_Csaba1), [Fabro Steibel](http://openreview.net/profile?id=ofabro@itsrio.org), [Fazl Barez](http://openreview.net/profile?id=~Fazl_Barez1), [Genevieve Smith](http://openreview.net/profile?id=~Genevieve_Smith1), [Gianluca Guadagni](http://openreview.net/profile?id=~Gianluca_Guadagni1), [Jon Chun](http://openreview.net/profile?id=~Jon_Chun1), [Jordi Cabot](http://openreview.net/profile?id=~Jordi_Cabot1), [Joseph Marvin Imperial](http://openreview.net/profile?id=~Joseph_Marvin_Imperial1), [Juan Arturo Nolazco Flores](http://openreview.net/profile?id=~Juan_A._Nolazco-Flores1), [Lori Landay](http://openreview.net/profile?id=~Lori_Landay1), [Matthew T Jackson](http://openreview.net/profile?id=~Matthew_Thomas_Jackson1), [Paul Röttger](http://openreview.net/profile?id=~Paul_Rottger1), [Phil Torr](http://openreview.net/profile?id=~Philip_Torr1), [Trevor Darrell](http://openreview.net/profile?id=~Trevor_Darrell2), [Yong Suk Lee](http://openreview.net/profile?id=~Yong_Suk_Lee1), [Jakob Foerster](http://openreview.net/profile?id=~Jakob_Nicolaus_Foerster1)
  - **Affiliations:** University of Oxford; MLCommons; None; None; None; None; None; None; None; None; None; None; None; None; None; None, University of Oxford, MLCommons, University of Oxford, University of Oxford, Kenyon College, Center for Computation & Technology, Louisiana State University, University of Oxford, University of Oxford, Institute for Technology & Society (ITS), Rio, University of Oxford, University of California, Berkeley, University of Virginia, Kenyon College, Luxembourg Institute of Science and Technology; University of Luxembourg, University of Bath; National University Philippines, ITESM, Berklee College of Music, University of Oxford, Bocconi University, University of Oxford, University of California, Berkeley, University of Notre Dame, University of Oxford
  - **TL;DR:** The paper discusses the near to mid-term risks and opportunities associated with open-source Generative AI, advocating for responsible open sourcing while addressing potential regulatory challenges. It emphasizes the importance of understanding the benefits and risks of both open and closed-source AI models to ensure safe and effective development in various domains.
  - **Keywords:** Generative AI, Open-source AI, AI safety, AI openness taxonomy, large language models, Science, Medicine, Education, Environment, Risks of AI, Regulation of AI, Societal impact of AI, Risk mitigation strategies, Best practices for AI development


- [Genie: Generative Interactive Environments](https://icml.cc/virtual/2024/poster/33646) (Best Paper)
  - **Authors:** [Jake Bruce](http://openreview.net/profile?id=~Jake_Bruce1), [Michael Dennis](http://openreview.net/profile?id=~Michael_D_Dennis1), [Ashley Edwards](http://openreview.net/profile?id=~Ashley_Edwards1), [Jack Parker-Holder](http://openreview.net/profile?id=~Jack_Parker-Holder1), [Yuge Shi](http://openreview.net/profile?id=~Yuge_Shi1), [Edward Hughes](http://openreview.net/profile?id=~Edward_Hughes1), [Matthew Lai](http://openreview.net/profile?id=~Matthew_Lai1), [Aditi Mavalankar](http://openreview.net/profile?id=~Aditi_Mavalankar1), [Richie Steigerwald](http://openreview.net/profile?id=~Richie_Steigerwald1), [Chris Apps](http://openreview.net/profile?id=capps@google.com), [Yusuf Aytar](http://openreview.net/profile?id=~Yusuf_Aytar1), [Sarah Bechtle](http://openreview.net/profile?id=~Sarah_Maria_Elisabeth_Bechtle1), [Feryal Behbahani](http://openreview.net/profile?id=~Feryal_Behbahani1), [Stephanie Chan](http://openreview.net/profile?id=~Stephanie_C.Y._Chan1), [Nicolas Heess](http://openreview.net/profile?id=~Nicolas_Heess1), [Lucy Gonzalez](http://openreview.net/profile?id=lucygps@google.com), [Simon Osindero](http://openreview.net/profile?id=~Simon_Osindero1), [Sherjil Ozair](http://openreview.net/profile?id=~Sherjil_Ozair1), [Scott Reed](http://openreview.net/profile?id=~Scott_Reed1), [Jingwei Zhang](http://openreview.net/profile?id=~Jingwei_Zhang2), [Konrad Zolna](http://openreview.net/profile?id=~Konrad_Zolna1), [Jeff Clune](http://openreview.net/profile?id=~Jeff_Clune3), [Nando de Freitas](http://openreview.net/profile?id=~Nando_de_Freitas1), [Satinder Singh](http://openreview.net/profile?id=~Satinder_Singh2), [Tim Rocktäschel](http://openreview.net/profile?id=~Tim_Rocktäschel1)
  - **Affiliations:** Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, University of British Columbia, University of British Columbia, University of British Columbia, Google DeepMind
  - **TL;DR:** The paper introduces Genie, a generative interactive environment capable of creating diverse, action-controllable virtual worlds from unlabelled Internet videos. It highlights the model's ability to generate environments without ground-truth action labels, paving the way for training generalist agents.
  - **Keywords:** generative AI, interactive environments, autoregressive dynamics model, spatiotemporal video tokenizer, latent action model, virtual worlds, gaming, foundation world model, learned latent action space, 200,000 hours of Internet gaming videos


- [Position: A Safe Harbor for AI Evaluation and Red Teaming](https://icml.cc/virtual/2024/poster/33560) (Oral)
  - **Authors:** [Shayne Longpre](http://openreview.net/profile?id=~Shayne_Longpre1), [Sayash Kapoor](http://openreview.net/profile?id=~Sayash_Kapoor2), [Kevin Klyman](http://openreview.net/profile?id=~Kevin_Klyman1), [Ashwin Ramaswami](http://openreview.net/profile?id=aramaswamis@gmail.com), [Rishi Bommasani](http://openreview.net/profile?id=~Rishi_Bommasani1), [Borhane Blili-Hamelin](http://openreview.net/profile?id=~Borhane_Blili-Hamelin1), [Yangsibo Huang](http://openreview.net/profile?id=~Yangsibo_Huang2), [Aviya Skowron](http://openreview.net/profile?id=~Aviya_Skowron1), [Zheng Xin Yong](http://openreview.net/profile?id=~Zheng_Xin_Yong1), [Suhas Kotha](http://openreview.net/profile?id=~Suhas_Kotha1), [Yi Zeng](http://openreview.net/profile?id=~Yi_Zeng3), [Weiyan Shi](http://openreview.net/profile?id=~Weiyan_Shi2), [Xianjun Yang](http://openreview.net/profile?id=~Xianjun_Yang1), [Reid Southen](http://openreview.net/profile?id=~Reid_Southen1), [Alex Robey](http://openreview.net/profile?id=~Alexander_Robey1), [Patrick Chao](http://openreview.net/profile?id=~Patrick_Chao1), [Diyi Yang](http://openreview.net/profile?id=~Diyi_Yang2), [Ruoxi Jia](http://openreview.net/profile?id=~Ruoxi_Jia1), [Daniel Kang](http://openreview.net/profile?id=~Daniel_Kang1), [Alex Pentland](http://openreview.net/profile?id=~Alex_Pentland1), [Arvind Narayanan](http://openreview.net/profile?id=~Arvind_Narayanan1), [Percy Liang](http://openreview.net/profile?id=~Percy_Liang1), [Peter Henderson](http://openreview.net/profile?id=~Peter_Henderson1)
  - **Affiliations:** MIT, Princeton University, Stanford University, Georgetown University, Stanford University, AI Risk and Vulnerability Alliance, Princeton University, Eleuther AI, Brown University, Carnegie Mellon University, Virginia Tech, Northeastern University, UCSB, None, University of Pennsylvania, University of Pennsylvania, Stanford University, Virginia Tech, UIUC, MIT, Princeton University, Stanford University, Princeton University
  - **TL;DR:** The paper advocates for a legal and technical safe harbor for independent evaluation and red teaming of generative AI systems to mitigate risks and enhance public interest safety research. It highlights the current challenges posed by corporate terms of service that hinder good faith research and calls for commitments from AI developers to support transparency and accountability.
  - **Keywords:** generative AI, safety evaluation, red teaming, model misuse, bias, privacy, disinformation, self-harm, copyright infringement, fraud, weapons acquisition, non-consensual images, AI safety, AI alignment


- [Representing Molecules as Random Walks Over Interpretable Grammars](https://icml.cc/virtual/2024/poster/33427) (Spotlight Poster)
  - **Authors:** [Michael Sun](http://openreview.net/profile?id=~Michael_Sun1), [Minghao Guo](http://openreview.net/profile?id=~Minghao_Guo1), [Weize Yuan](http://openreview.net/profile?id=~Weize_Yuan1), [Veronika Thost](http://openreview.net/profile?id=~Veronika_Thost1), [Crystal Owens](http://openreview.net/profile?id=~Crystal_Elaine_Owens1), [Aristotle Grosz](http://openreview.net/profile?id=~Aristotle_Franklin_Grosz1), [Sharvaa Selvan](http://openreview.net/profile?id=~Sharvaa_Selvan1), [Katelyn Zhou](http://openreview.net/profile?id=~Katelyn_Zhou1), [Hassan Mohiuddin](http://openreview.net/profile?id=~Hassan_Mohiuddin1), [Benjamin Pedretti](http://openreview.net/profile?id=~Benjamin_J_Pedretti1), [Zachary Smith](http://openreview.net/profile?id=zpsmith@mit.edu), [Jie Chen](http://openreview.net/profile?id=~Jie_Chen1), [Wojciech Matusik](http://openreview.net/profile?id=~Wojciech_Matusik2)
  - **Affiliations:** MIT CSAIL, MIT, MIT Chemistry, MIT-IBM Watson AI Lab, IBM Research, MIT, MIT Chemical Engineering, MIT, Wellesley, MIT, MIT Chemical Engineering, MIT Chemical Engineering, MIT-IBM Watson AI Lab, IBM Research, MIT
  - **TL;DR:** This study presents a data-efficient and interpretable model for representing complex molecules using graph grammars and random walks, addressing the challenges of molecular discovery in low-data scenarios. The proposed method demonstrates significant advantages in performance, efficiency, and synthesizability of predicted molecules, contributing to advancements in material design.
  - **Keywords:** molecular discovery, material design, complex molecular structures, graph grammars, random walks, context-sensitive grammar, gas-separation membranes, photovoltaics, data efficiency, low-data scenarios, scarcity of data and labels, interpretable model, efficient parameterization, property prediction, motifs, motif graph, hierarchical design space, chemical interpretability


- [On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control](https://icml.cc/virtual/2024/poster/35642) (Poster)
  - **Authors:** Amrit Singh Bedi, Anjaly Parayil, Junyu Zhang, Mengdi Wang, Alec Koppel
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Quantum Theory and Application of Contextual Optimal Transport](https://icml.cc/virtual/2024/poster/34782) (Poster)
  - **Authors:** [Nicola Mariella](http://openreview.net/profile?id=~Nicola_Mariella1), [Albert Akhriev](http://openreview.net/profile?id=albert_akhriev@ie.ibm.com), [Francesco Tacchino](http://openreview.net/profile?id=fta@zurich.ibm.com), [Christa Zoufal](http://openreview.net/profile?id=~Christa_Zoufal1), [Juan Gonzalez-Espitia](http://openreview.net/profile?id=juan.carlos.gonzalez.espitia@ibm.com), [Benedek Harsanyi](http://openreview.net/profile?id=benedek.harsanyi@ibm.com), [Eugene Koskin](http://openreview.net/profile?id=eugin.koskin@gmail.com), [Ivano Tavernelli](http://openreview.net/profile?id=ita@zurich.ibm.com), [Stefan Woerner](http://openreview.net/profile?id=~Stefan_Woerner1), [Marianna Rapsomaniki](http://openreview.net/profile?id=~Marianna_Rapsomaniki1), [Sergiy Zhuk](http://openreview.net/profile?id=~Sergiy_Zhuk1), [Jannis Born](http://openreview.net/profile?id=~Jannis_Born1)
  - **Affiliations:** IBM Quantum, IBM Research Europe - Dublin; None, IBM Quantum, IBM Research Europe - Dublin, IBM Quantum, IBM Research Europe - Zurich, Switzerland, IBM Quantum, IBM Research Europe - Zurich, Switzerland, IBM Quantum, IBM Research Europe - Zurich, Switzerland; Politecnico di Milano, Milan, Italy, IBM Research, IBM Research Europe - Zurich, Switzerland; École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, IBM Quantum, IBM Research Europe - Dublin; University College Dublin, Ireland, IBM Quantum, IBM Research Europe - Zurich, Switzerland, IBM Quantum, IBM Research Europe - Zurich, Switzerland, IBM Research, IBM Research Europe - Zurich, Switzerland; None, IBM Quantum, IBM Research Europe - Dublin, IBM Research, IBM Research Europe - Zurich, Switzerland
  - **TL;DR:** This study introduces a novel quantum computing approach for optimizing contextualized transportation plans using Optimal Transport (OT) theory, demonstrating superior performance over classical methods in predicting variations in cell type distributions based on drug dosage. The proposed method, QontOT, establishes a connection between OT and quantum computation, marking a significant advancement in the field.
  - **Keywords:** Optimal Transport, Quantum Computing, Machine Learning, Neural OT, Brenier’s theorem, Amortized Optimization, Conditional Monge Gap, Single-cell analysis, Drug dosage prediction, Biological data analysis, Conditional distribution learning, Mapping cellular distributions, Generalization to new contexts, QontOT method, Performance improvement over classical neural OT approaches, Synthetic data, Real data, Doubly stochastic matrices, Unitary operators, Input convex neural network (ICNN)


- [AegisFL: Efficient and Flexible Privacy-Preserving Byzantine-Robust Cross-silo Federated Learning](https://icml.cc/virtual/2024/poster/34131) (Poster)
  - **Authors:** [Dong Chen](http://openreview.net/profile?id=~Dong_Chen13), [Hongyuan Qu](http://openreview.net/profile?id=~Hongyuan_Qu1), [Guangwu Xu](http://openreview.net/profile?id=gxu4sdq@sdu.edu.cn)
  - **Affiliations:** School of Cyber Science and Technology, Shandong University, Qingdao, China; Key Laboratory of Cryptologic Technology and Information Security of Ministry of Education, Shandong University, Qingdao, China, School of Cyber Science and Technology, Shandong University, Qingdao, China; Key Laboratory of Cryptologic Technology and Information Security of Ministry of Education, Shandong University, Qingdao, China, School of Cyber Science and Technology, Shandong University, Qingdao, China; Key Laboratory of Cryptologic Technology and Information Security of Ministry of Education, Shandong University, Qingdao, China; Shandong Institute of Blockchain, Jinan, China; Quan Cheng Laboratory, Jinan, China
  - **TL;DR:** This paper presents AegisFL, an efficient and flexible privacy-preserving Byzantine-robust federated learning system that addresses privacy and poisoning attacks through a tailored packing scheme for homomorphic encryption. The proposed method allows for dynamic aggregation algorithm changes while ensuring that the global model remains secure and only accessible to honest clients.
  - **Keywords:** Federated Learning, Privacy-Preserving Learning, Homomorphic Encryption, Byzantine-Robust Aggregation, Cross-Silo Federated Learning, Privacy Attacks, Poisoning Attacks, Data Privacy, AegisFL, Efficient PBFL System, Flexible Aggregation, RLWE-based Fully Homomorphic Encryption, Advanced Aggregation Algorithms


- [Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs](https://icml.cc/virtual/2024/poster/33362) (Poster)
  - **Authors:** [Jordan Dotzel](http://openreview.net/profile?id=~Jordan_Dotzel1), [Yuzong Chen](http://openreview.net/profile?id=yc2367@cornell.edu), [Bahaa Kotb](http://openreview.net/profile?id=~Bahaa_Kotb1), [Sushma Prasad](http://openreview.net/profile?id=sushmahp@google.com), [Gang Wu](http://openreview.net/profile?id=~Gang_Wu7), [Sheng Li](http://openreview.net/profile?id=~Sheng_Li18), [Mohamed Abdelfattah](http://openreview.net/profile?id=~Mohamed_S_Abdelfattah1), [Zhiru Zhang](http://openreview.net/profile?id=~Zhiru_Zhang2)
  - **Affiliations:** School of ECE, Cornell University; Google, School of ECE, Cornell University, School of ECE, Cornell University, Google, Google, Google, School of ECE, Cornell University, School of ECE, Cornell University
  - **TL;DR:** This study analyzes the weight and activation distributions of large language models to propose a new format, Student Float (SF4), which enhances model accuracy while maintaining efficiency. The findings reveal a tradeoff between model accuracy and chip area, enabling more applications of LLMs at lower precision formats.
  - **Keywords:** Large Language Models (LLMs), Quantization, Student Float (SF4), Normal Float (NF4), E2M1, supernormal support, Model accuracy, chip area, power consumption, Improved accuracy on LLaMA2-7B, Pareto curve for model accuracy and chip area, LLaMA2-7B, None, INT4, FP8, E4M3, Additive Powers of Two (APoT4)


- [Position: Embracing Negative Results in Machine Learning](https://icml.cc/virtual/2024/poster/35063) (Oral)
  - **Authors:** [Florian Karl](http://openreview.net/profile?id=~Florian_Karl1), [Malte Kemeter](http://openreview.net/profile?id=malte.kemeter@iis.fraunhofer.de), [Gabriel Dax](http://openreview.net/profile?id=~Gabriel_Dax1), [Paulina Sierak](http://openreview.net/profile?id=paulina.sierak@iis.fraunhofer.de)
  - **Affiliations:** Fraunhofer Institute for Integrated Circuits IIS, Ludwig-Maximilians-Universität München, Munich Center for Machine Learning, Fraunhofer Institute for Integrated Circuits IIS, Fraunhofer Institute for Integrated Circuits IIS, Fraunhofer Institute for Integrated Circuits IIS
  - **TL;DR:** This position paper argues for the importance of publishing negative results in machine learning research, asserting that reliance on predictive performance alone creates inefficiencies and misaligned incentives within the research community. The authors advocate for a shift towards normalizing the publication of negative results to enhance scientific output and foster more meaningful discussions.
  - **Keywords:** negative results in machine learning, predictive performance, publication practices, inefficiencies in machine learning research, wrong incentives for researchers, advantages of publishing negative results, normalization of negative result publications, empirical machine learning, null hypothesis, state-of-the-art methods


- [Position: On the Societal Impact of Open Foundation Models](https://icml.cc/virtual/2024/poster/33305) (Oral)
  - **Authors:** [Sayash Kapoor](http://openreview.net/profile?id=~Sayash_Kapoor2), [Rishi Bommasani](http://openreview.net/profile?id=~Rishi_Bommasani1), [Kevin Klyman](http://openreview.net/profile?id=~Kevin_Klyman1), [Shayne Longpre](http://openreview.net/profile?id=~Shayne_Longpre1), [Ashwin Ramaswami](http://openreview.net/profile?id=aramaswamis@gmail.com), [Peter Cihon](http://openreview.net/profile?id=pcihon@github.com), [Aspen Hopkins](http://openreview.net/profile?id=~Aspen_K_Hopkins1), [Kevin Bankston](http://openreview.net/profile?id=kbankston@cdt.org), [Stella Biderman](http://openreview.net/profile?id=~Stella_Biderman1), [Miranda Bogen](http://openreview.net/profile?id=~Miranda_Bogen1), [Rumman Chowdhury](http://openreview.net/profile?id=~Rumman_Chowdhury1), [Alex Engler](http://openreview.net/profile?id=alexcengler@gmail.com), [Peter Henderson](http://openreview.net/profile?id=~Peter_Henderson1), [Yacine Jernite](http://openreview.net/profile?id=~Yacine_Jernite1), [Seth Lazar](http://openreview.net/profile?id=~Seth_Lazar1), [Stefano Maffulli](http://openreview.net/profile?id=stefano@opensource.org), [Alondra Nelson](http://openreview.net/profile?id=anelson@ias.edu), [Joelle Pineau](http://openreview.net/profile?id=~Joelle_Pineau1), [Aviya Skowron](http://openreview.net/profile?id=~Aviya_Skowron1), [Dawn Song](http://openreview.net/profile?id=~Dawn_Song1), [Victor Storchan](http://openreview.net/profile?id=~Victor_Storchan1), [Daniel Zhang](http://openreview.net/profile?id=dzhang105@stanford.edu), [Daniel Ho](http://openreview.net/profile?id=~Daniel_E._Ho1), [Percy Liang](http://openreview.net/profile?id=~Percy_Liang1), [Arvind Narayanan](http://openreview.net/profile?id=~Arvind_Narayanan1)
  - **Affiliations:** Princeton University, Stanford University, Stanford University, Massachusetts Institute of Technology, Georgetown University, GitHub, Massachusetts Institute of Technology, Center for Democracy and Technology, Eleuther AI, Center for Democracy and Technology, Humane Intelligence, Brookings Institution, Princeton University, Hugging Face, Australian National University, Open Source Initiative, Institute for Advanced Study, Meta, Eleuther AI, University of California, Berkeley, Mozilla AI, Stanford University, Stanford University, Stanford University, Princeton University
  - **TL;DR:** This paper examines the societal impact of open foundation models, highlighting their benefits and risks, particularly in terms of innovation and governance. The authors propose a risk assessment framework to better understand the marginal risks associated with these models compared to existing technologies.
  - **Keywords:** Open foundation models, Societal impact, AI governance, Risk assessment framework, Cybersecurity, Biosecurity, Disinformation, Misuse risks, Marginal risk characterization, Benefits and risks of open foundation models, Foundation models, Model weights, Innovation, Competition, Decision-making power, Transparency


- [Scalable Real-Time Recurrent Learning Using Columnar-Constructive Networks](https://icml.cc/virtual/2024/poster/35622) (Poster)
  - **Authors:** Khurram Javed, Haseeb Shah, Richard Sutton, Martha White
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Effect-Invariant Mechanisms for Policy Generalization](https://icml.cc/virtual/2024/poster/35626) (Poster)
  - **Authors:** Sorawit Saengkyongam, Niklas Pfister, Predag Klasnja, Susan Murphy, Jonas Peters
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy](https://icml.cc/virtual/2024/poster/33666) (Oral)
  - **Authors:** [Lucas Spangher](http://openreview.net/profile?id=~Lucas_Spangher1), [Allen Wang](http://openreview.net/profile?id=~Allen_M._Wang1), [Andrew Maris](http://openreview.net/profile?id=maris@psfc.mit.edu), [Myles Stapelberg](http://openreview.net/profile?id=myless@psfc.mit.edu), [Viraj Mehta](http://openreview.net/profile?id=~Viraj_Mehta1), [Alex Saperstein](http://openreview.net/profile?id=saperstein@psfc.mit.edu), [Stephen Lane-Walsh](http://openreview.net/profile?id=slwalsh@psfc.mit.edu), [Akshata Moharir](http://openreview.net/profile?id=~Akshata_Kishore_Moharir2), [Alessandro Pau](http://openreview.net/profile?id=alessandro.pau@epfl.ch), [Cristina Rea](http://openreview.net/profile?id=crea@psfc.mit.edu)
  - **Affiliations:** Plasma Science and Fusion Center, Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139, USA, Plasma Science and Fusion Center, Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139, USA, Plasma Science and Fusion Center, Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139, USA, Plasma Science and Fusion Center, Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139, USA, Department of Electrical Engineering and Computer Science, Carnegie Mellon University, 5000 Forbes Avenue Pittsburgh, PA 15213, USA, Plasma Science and Fusion Center, Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139, USA, Plasma Science and Fusion Center, Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139, USA, Robert H. Smith School of Business, University of Maryland, College Park, 7901 Regents Drive College Park, MD 20742-5025, USA, Swiss Plasma Center, Swiss Federal Institute of Technology, Chemin du Barrage 16, 1015 Ecublens, Lausanne, Switzerland, Plasma Science and Fusion Center, Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139, USA
  - **TL;DR:** This position paper identifies six key research challenges in magnetic fusion energy that are suitable for Machine Learning applications, emphasizing the potential of ML to address issues such as disruption prediction and materials discovery. The authors argue that collaboration between the ML community and fusion energy researchers is crucial for advancing clean energy solutions.
  - **Keywords:** Machine Learning, Magnetic Fusion Energy, Disruption Prediction, Simulation and Dynamics Modeling, Fusion Energy, Tokamak, Disruption Prediction, Resolving Partially Observed Data, Improving Controls, Guiding Experiments, Enhancing Materials Discovery, Tokamak, High Temperature Superconductors, Carbon-Free Energy, Decarbonization, Climate Change, Energy Insecurity


- [VideoPoet: A Large Language Model for Zero-Shot Video Generation](https://icml.cc/virtual/2024/poster/34296) (Best Paper)
  - **Authors:** [Dan Kondratyuk](http://openreview.net/profile?id=~Dan_Kondratyuk1), [Lijun Yu](http://openreview.net/profile?id=~Lijun_Yu1), [Xiuye Gu](http://openreview.net/profile?id=~Xiuye_Gu1), [Jose Lezama](http://openreview.net/profile?id=~Jose_Lezama1), [Jonathan Huang](http://openreview.net/profile?id=~Jonathan_Huang1), [Grant Schindler](http://openreview.net/profile?id=~Grant_Schindler3), [Rachel Hornung](http://openreview.net/profile?id=~Rachel_Hornung1), [Vighnesh N Birodkar](http://openreview.net/profile?id=~Vighnesh_Birodkar1), [Jimmy Yan](http://openreview.net/profile?id=~Jimmy_Yan1), [Ming-Chang Chiu](http://openreview.net/profile?id=~Ming-Chang_Chiu1), [Krishna Somandepalli](http://openreview.net/profile?id=~Krishna_Somandepalli3), [Hassan Akbari](http://openreview.net/profile?id=~Hassan_Akbari1), [Yair Alon](http://openreview.net/profile?id=~Yair_Alon1), [Yong Cheng](http://openreview.net/profile?id=~Yong_Cheng3), [Joshua V Dillon](http://openreview.net/profile?id=~Joshua_V._Dillon1), [Agrim Gupta](http://openreview.net/profile?id=~Agrim_Gupta1), [Meera Hahn](http://openreview.net/profile?id=~Meera_Hahn1), [Anja Hauth](http://openreview.net/profile?id=~Anja_Hauth1), [David Hendon](http://openreview.net/profile?id=hendon@google.com), [Alonso Martinez](http://openreview.net/profile?id=~Alonso_Martinez2), [David Minnen](http://openreview.net/profile?id=~David_Minnen1), [Mikhail Sirotenko](http://openreview.net/profile?id=~Mikhail_Sirotenko1), [Kihyuk Sohn](http://openreview.net/profile?id=~Kihyuk_Sohn1), [Xuan Yang](http://openreview.net/profile?id=~Xuan_Yang6), [Hartwig Adam](http://openreview.net/profile?id=~Hartwig_Adam1), [Ming-Hsuan Yang](http://openreview.net/profile?id=~Ming-Hsuan_Yang1), [Irfan Essa](http://openreview.net/profile?id=~Irfan_Essa1), [Huisheng Wang](http://openreview.net/profile?id=~Huisheng_Wang1), [David Ross](http://openreview.net/profile?id=~David_A_Ross1), [Bryan Seybold](http://openreview.net/profile?id=~Bryan_Seybold1), [Lu Jiang](http://openreview.net/profile?id=~Lu_Jiang1)
  - **Affiliations:** Google, Google; Carnegie Mellon University, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google, Google; Carnegie Mellon University
  - **TL;DR:** The study introduces VideoPoet, a model designed for synthesizing high-quality videos using a decoder-only transformer architecture that processes various multimodal inputs. It demonstrates state-of-the-art capabilities in zero-shot video generation, particularly in generating high-fidelity motions.
  - **Keywords:** video generation, multimodal inputs, large language models, decoder-only transformer architecture, autoregressive Transformer framework, text-to-video, image-to-video, video editing, video-to-video stylization, temporal consistency across video frames, high-fidelity motion generation, state-of-the-art capabilities in zero-shot video generation


- [LaMAGIC: Language-Model-based Topology Generation for Analog Integrated Circuits](https://icml.cc/virtual/2024/poster/34228) (Poster)
  - **Authors:** [Chen-Chia Chang](http://openreview.net/profile?id=~Chen-Chia_Chang1), [Yikang Shen](http://openreview.net/profile?id=~Yikang_Shen1), [Shaoze Fan](http://openreview.net/profile?id=sf392@njit.edu), [Jing Li](http://openreview.net/profile?id=~Jing_Li21), [Shun Zhang](http://openreview.net/profile?id=~Shun_Zhang6), [Ningyuan Cao](http://openreview.net/profile?id=ncao@nd.edu), [Yiran Chen](http://openreview.net/profile?id=~Yiran_Chen1), [Xin Zhang](http://openreview.net/profile?id=~Xin_Zhang49)
  - **Affiliations:** IBM T. J. Watson Research Center; Duke University, MIT-IBM Watson AI Lab, New Jersey Institute of Technology, New Jersey Institute of Technology, MIT-IBM Watson AI Lab, University of Notre Dame, Duke University, IBM T. J. Watson Research Center; MIT-IBM Watson AI Lab
  - **TL;DR:** This study introduces LaMAGIC, a language model-based approach for automated analog circuit design that efficiently generates optimized circuit topologies in a single pass. The model demonstrates a high success rate of 96% in generating custom power converters, addressing the complexities and demands of modern electronic applications.
  - **Keywords:** analog circuit design, automated circuit design, language model-based topology generation, supervised finetuning, power converters, electronic devices, electrical systems, complexity of circuit design, time-consuming design processes, LaMAGIC model, optimized circuit design, adjacency matrix-based circuit formulation


- [Language Models as Science Tutors](https://icml.cc/virtual/2024/poster/33881) (Poster)
  - **Authors:** [Alexis Chevalier](http://openreview.net/profile?id=~Alexis_Chevalier1), [Jiayi Geng](http://openreview.net/profile?id=~Jiayi_Geng1), [Alexander Wettig](http://openreview.net/profile?id=~Alexander_Wettig1), [Howard Chen](http://openreview.net/profile?id=~Howard_Chen1), [Sebastian Mizera](http://openreview.net/profile?id=~Sebastian_Mizera1), [Toni Annala](http://openreview.net/profile?id=~Toni_Annala1), [Max Aragon](http://openreview.net/profile?id=~Max_Aragon1), [Arturo Fanlo](http://openreview.net/profile?id=~Arturo_Rodriguez_Fanlo1), [Simon Frieder](http://openreview.net/profile?id=~Simon_Frieder1), [Simon Machado](http://openreview.net/profile?id=~Simon_Machado1), [Akshara P](http://openreview.net/profile?id=~Akshara_Prabhakar1), [Ellie Thieu](http://openreview.net/profile?id=elliethieu.amherst@gmail.com), [Jiachen Wang](http://openreview.net/profile?id=~Jiachen_T._Wang1), [Zirui Wang](http://openreview.net/profile?id=~Zirui_Wang5), [Xindi Wu](http://openreview.net/profile?id=~Xindi_Wu1), [Mengzhou Xia](http://openreview.net/profile?id=~Mengzhou_Xia1), [Wenhan Xia](http://openreview.net/profile?id=~Wenhan_Xia1), [Jiatong Yu](http://openreview.net/profile?id=~Jiatong_Yu1), [Junjie Zhu](http://openreview.net/profile?id=~Junjie_Zhu4), [Zhiyong Ren](http://openreview.net/profile?id=~Zhiyong_Ren1), [Sanjeev Arora](http://openreview.net/profile?id=~Sanjeev_Arora1), [Danqi Chen](http://openreview.net/profile?id=~Danqi_Chen1)
  - **Affiliations:** Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University, School of Natural Sciences, Institute for Advanced Study, School of Mathematics, Institute for Advanced Study, Neuroscience Institute, Princeton University, Hebrew University of Jerusalem, Department of Computer Science, Oxford University, FIM - Institute for Mathematical Research, ETH Zürich, Princeton Language and Intelligence, Princeton University, Department of Computer Science, University of Wisconsin-Madison, Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University, Department of Civil and Environmental Engineering, Princeton University; Andlinger Center for Energy and the Environment, Princeton University, Department of Civil and Environmental Engineering, Princeton University; Andlinger Center for Energy and the Environment, Princeton University, Princeton Language and Intelligence, Princeton University, Princeton Language and Intelligence, Princeton University
  - **TL;DR:** This study introduces TUTOR EVAL and TUTOR CHAT, benchmarks and datasets designed to evaluate and enhance the performance of language models as scientific tutors, particularly in processing long scientific texts. The findings demonstrate that specialized LM tutors excel in problem-solving tasks while addressing real-life usability challenges in educational contexts.
  - **Keywords:** Language Models, Scientific Problem-Solving, Education, Fine-tuning, Question-Answering, Long Context Processing, STEM Education, Scientific Assistance, Real-life Usability of LMs, Performance on Long Contexts, TUTOR EVAL, TUTOR CHAT, LM Tutors, 32K-token Context Window, TUTOR EVAL, TUTOR CHAT, GSM8K, MATH


- [Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning](https://icml.cc/virtual/2024/poster/32880) (Poster)
  - **Authors:** [Zhiheng Xi](http://openreview.net/profile?id=~Zhiheng_Xi1), [Wenxiang Chen](http://openreview.net/profile?id=~Wenxiang_Chen3), [Boyang Hong](http://openreview.net/profile?id=~Boyang_Hong1), [Senjie Jin](http://openreview.net/profile?id=~Senjie_Jin1), [Rui Zheng](http://openreview.net/profile?id=~Rui_Zheng1), [Wei He](http://openreview.net/profile?id=~Wei_He14), [Yiwen Ding](http://openreview.net/profile?id=~Yiwen_Ding3), [Shichun Liu](http://openreview.net/profile?id=~Shichun_Liu2), [Xin Guo](http://openreview.net/profile?id=~Xin_Guo13), [Junzhe Wang](http://openreview.net/profile?id=~Junzhe_Wang2), [Honglin Guo](http://openreview.net/profile?id=~Honglin_Guo1), [Wei Shen](http://openreview.net/profile?id=~Wei_Shen12), [Xiaoran Fan](http://openreview.net/profile?id=~Xiaoran_Fan3), [Yuhao Zhou](http://openreview.net/profile?id=~Yuhao_Zhou3), [Shihan Dou](http://openreview.net/profile?id=~Shihan_Dou1), [Xiao Wang](http://openreview.net/profile?id=~Xiao_Wang12), [Xinbo Zhang](http://openreview.net/profile?id=~Xinbo_Zhang1), [Peng Sun](http://openreview.net/profile?id=~peng_sun1), [Tao Gui](http://openreview.net/profile?id=~Tao_Gui1), [Qi Zhang](http://openreview.net/profile?id=~Qi_Zhang8), [Xuanjing Huang](http://openreview.net/profile?id=~Xuanjing_Huang1)
  - **Affiliations:** School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University, ByteDance Research, ByteDance Research, Institute of Modern Languages and Linguistics, Fudan University, School of Computer Science, Fudan University, School of Computer Science, Fudan University
  - **TL;DR:** This paper introduces R3, a novel method for training large language models in reasoning through Reverse Curriculum Reinforcement Learning, which utilizes outcome supervision to enhance model exploration and error identification. The method demonstrates improved performance on reasoning tasks compared to existing reinforcement learning baselines, particularly in program-based reasoning.
  - **Keywords:** Reasoning, Large Language Models, Reverse Curriculum Reinforcement Learning, Outcome Supervision, Process Supervision, Identifying sequences of actions for positive rewards, error accumulation in reasoning chains, complexity in reasoning tasks, R3 method, step-wise curriculum, improved performance on reasoning tasks, Llama2-7B


- [Dynamic Anisotropic Smoothing for Noisy Derivative-Free Optimization](https://icml.cc/virtual/2024/poster/34709) (Poster)
  - **Authors:** [Sam Reifenstein](http://openreview.net/profile?id=~Sam_Reifenstein1), [Timothee Leleu](http://openreview.net/profile?id=~Timothee_Leleu1), [Yoshihisa Yamamoto](http://openreview.net/profile?id=yoyshihisa.yamamoto@ntt-research.com)
  - **Affiliations:** NTT Research Inc, Stanford University; NTT Research Inc, NTT Research Inc
  - **TL;DR:** This paper presents a novel algorithm for noisy derivative-free optimization that adapts the smoothing kernel shape to account for heterogeneous curvature, significantly improving gradient estimation accuracy. The method shows enhanced performance in tuning NP-hard combinatorial optimization solvers compared to existing techniques.
  - **Keywords:** derivative-free optimization, noisy optimization, ball smoothing, Gaussian smoothing, dynamic anisotropic smoothing, machine learning, combinatorial optimization, algorithm tuning, optimization without gradient access, noisy evaluations, computationally costly evaluations, improved performance in tuning NP-hard combinatorial optimization solvers, error reduction in gradient estimation, Hessian, hyper-parameter tuning, NP-hard problems


- [Incorporating probabilistic domain knowledge into deep multiple instance learning](https://icml.cc/virtual/2024/poster/34493) (Poster)
  - **Authors:** [Ghadi S. Al Hajj](http://openreview.net/profile?id=~Ghadi_S._Al_Hajj1), [Aliaksandr Hubin](http://openreview.net/profile?id=~Aliaksandr_Hubin1), [Chakravarthi Kanduri](http://openreview.net/profile?id=skanduri@ifi.uio.no), [Milena Pavlović](http://openreview.net/profile?id=~Milena_Pavlovic1), [Knut Rand](http://openreview.net/profile?id=knutdr@math.uio.no), [Michael Widrich](http://openreview.net/profile?id=~Michael_Widrich2), [Anne Solberg](http://openreview.net/profile?id=~Anne_Schistad_Solberg1), [Victor Greiff](http://openreview.net/profile?id=~Victor_Greiff1), [Johan Pensar](http://openreview.net/profile?id=~Johan_Pensar2), [Günter Klambauer](http://openreview.net/profile?id=~Günter_Klambauer1), [Geir Kjetil Sandve](http://openreview.net/profile?id=~Geir_Kjetil_Sandve1)
  - **Affiliations:** Department of Informatics, University of Oslo, Oslo, Norway, Faculty of Chemistry, Biotechnology and Food Science, Norwegian University of Life Sciences, Ås, Norway, Department of Mathematics, University of Oslo, Oslo, Norway, Department of Informatics, University of Oslo, Oslo, Norway, Department of Informatics, University of Oslo, Oslo, Norway, Freenome, San Francisco, CA, USA, Department of Informatics, University of Oslo, Oslo, Norway, Department of Immunology, University of Oslo, Oslo, Norway, Department of Mathematics, University of Oslo, Oslo, Norway, LIT AI Lab Institute for Machine Learning, Johannes Kepler University, Vienna, Austria, Department of Informatics, University of Oslo, Oslo, Norway
  - **TL;DR:** This study proposes a framework called DeeMILIP to incorporate probabilistic domain knowledge into deep multiple instance learning, addressing the challenges of limited knowledge integration in deep learning models. The proposed strategies demonstrate the ability to learn generalizable models even in scenarios with weak signals and limited data.
  - **Keywords:** deep learning, multiple instance learning, domain knowledge incorporation, DeeMILIP framework, probabilistic strategies, medical image analysis, immune repertoire classification, limited ability to incorporate domain knowledge, weak signals, limited dataset size, limited compute, generalizable models, knowledge incorporation strategies, multiple instance learning (MIL), deep learning (DL)


- [Masked Face Recognition with Generative-to-Discriminative Representations](https://icml.cc/virtual/2024/poster/32848) (Spotlight Poster)
  - **Authors:** [Shiming Ge](http://openreview.net/profile?id=~Shiming_Ge1), [Weijia Guo](http://openreview.net/profile?id=guoweijia@iie.ac.cn), [Chenyu Li](http://openreview.net/profile?id=lichenyu@iie.ac.cn), [Zhang Junzheng](http://openreview.net/profile?id=~Zhang_Junzheng1), [Yong Li](http://openreview.net/profile?id=liyong@iie.ac.cn), [Dan Zeng](http://openreview.net/profile?id=~Dan_Zeng2)
  - **Affiliations:** Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100092, China; School of Cyber Security at University of Chinese Academy of Sciences, Beijing 100049, China, Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100092, China; School of Cyber Security at University of Chinese Academy of Sciences, Beijing 100049, China, Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100092, China; School of Cyber Security at University of Chinese Academy of Sciences, Beijing 100049, China; Cloud Music Inc., Hangzhou 311215, China, Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100092, China; School of Cyber Security at University of Chinese Academy of Sciences, Beijing 100049, China, Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100092, China; School of Cyber Security at University of Chinese Academy of Sciences, Beijing 100049, China, Department of Communication Engineering, Shanghai University, Shanghai 200040, China
  - **TL;DR:** This study presents a unified deep network that learns generative-to-discriminative representations to improve masked face recognition, addressing challenges posed by occlusions. The proposed method effectively recovers missing facial clues and enhances identity prediction accuracy, demonstrating strong performance on both synthetic and realistic datasets.
  - **Keywords:** Masked face recognition, Generative-to-discriminative representations, Generative encoder, Multi-layer convolutional network, Feature classifier, Face recognition, Insufficient or inaccurate representations, Occlusions, Key information loss, Occlusion-robust representations, Identity-aware vectors, Synthetic masked faces, Realistic datasets


- [Position: TrustLLM: Trustworthiness in Large Language Models](https://icml.cc/virtual/2024/poster/33637) (Poster)
  - **Authors:** [Yue Huang](http://openreview.net/profile?id=~Yue_Huang9), [Lichao Sun](http://openreview.net/profile?id=~Lichao_Sun1), [Haoran Wang](http://openreview.net/profile?id=~Haoran_Wang12), [Siyuan Wu](http://openreview.net/profile?id=~Siyuan_Wu6), [Qihui Zhang](http://openreview.net/profile?id=~Qihui_Zhang1), [Yuan Li](http://openreview.net/profile?id=~Yuan_Li18), [Chujie Gao](http://openreview.net/profile?id=~Chujie_Gao1), [Yixin Huang](http://openreview.net/profile?id=~Yixin_Huang2), [Wenhan Lyu](http://openreview.net/profile?id=~Wenhan_Lyu1), [Yixuan Zhang](http://openreview.net/profile?id=~Yixuan_Zhang7), [Xiner Li](http://openreview.net/profile?id=~Xiner_Li1), [Hanchi Sun](http://openreview.net/profile?id=~Hanchi_Sun2), [Zhengliang Liu](http://openreview.net/profile?id=~Zhengliang_Liu1), [Yixin Liu](http://openreview.net/profile?id=~Yixin_Liu4), [Yijue Wang](http://openreview.net/profile?id=~Yijue_Wang1), [Zhikun Zhang](http://openreview.net/profile?id=~Zhikun_Zhang2), [Bertie Vidgen](http://openreview.net/profile?id=~Bertie_Vidgen1), [Bhavya Kailkhura](http://openreview.net/profile?id=~Bhavya_Kailkhura1), [Caiming Xiong](http://openreview.net/profile?id=~Caiming_Xiong1), [Chaowei Xiao](http://openreview.net/profile?id=~Chaowei_Xiao2), [Chunyuan Li](http://openreview.net/profile?id=~Chunyuan_Li1), [Eric Xing](http://openreview.net/profile?id=~Eric_Xing1), [Furong Huang](http://openreview.net/profile?id=~Furong_Huang1), [Hao Liu](http://openreview.net/profile?id=~Hao_Liu1), [Heng Ji](http://openreview.net/profile?id=~Heng_Ji3), [Hongyi Wang](http://openreview.net/profile?id=~Hongyi_Wang1), [Huan Zhang](http://openreview.net/profile?id=~Huan_Zhang1), [Huaxiu Yao](http://openreview.net/profile?id=~Huaxiu_Yao1), [Manolis Kellis](http://openreview.net/profile?id=~Manolis_Kellis1), [Marinka Zitnik](http://openreview.net/profile?id=~Marinka_Zitnik1), [Meng Jiang](http://openreview.net/profile?id=~Meng_Jiang3), [Mohit Bansal](http://openreview.net/profile?id=~Mohit_Bansal2), [James Zou](http://openreview.net/profile?id=~James_Zou1), [Jian Pei](http://openreview.net/profile?id=~Jian_Pei1), [Jian Liu](http://openreview.net/profile?id=~Jian_Liu12), [Jianfeng Gao](http://openreview.net/profile?id=~Jianfeng_Gao1), [Jiawei Han](http://openreview.net/profile?id=~Jiawei_Han1), [Jieyu Zhao](http://openreview.net/profile?id=~Jieyu_Zhao1), [Jiliang Tang](http://openreview.net/profile?id=~Jiliang_Tang1), [Jindong Wang](http://openreview.net/profile?id=~Jindong_Wang1), [Joaquin Vanschoren](http://openreview.net/profile?id=~Joaquin_Vanschoren1), [John Mitchell](http://openreview.net/profile?id=~John_Mitchell1), [Kai Shu](http://openreview.net/profile?id=~Kai_Shu1), [Kaidi Xu](http://openreview.net/profile?id=~Kaidi_Xu1), [Kai-Wei Chang](http://openreview.net/profile?id=~Kai-Wei_Chang1), [Lifang He](http://openreview.net/profile?id=~Lifang_He1), [Lifu Huang](http://openreview.net/profile?id=~Lifu_Huang1), [Michael Backes](http://openreview.net/profile?id=~Michael_Backes3), [Neil Gong](http://openreview.net/profile?id=~Neil_Zhenqiang_Gong1), [Philip Yu](http://openreview.net/profile?id=~Philip_S._Yu1), [Pin-Yu Chen](http://openreview.net/profile?id=~Pin-Yu_Chen1), [Quanquan Gu](http://openreview.net/profile?id=~Quanquan_Gu1), [Ran Xu](http://openreview.net/profile?id=~Ran_Xu1), [ZHITAO YING](http://openreview.net/profile?id=~Zhitao_Ying1), [Shuiwang Ji](http://openreview.net/profile?id=~Shuiwang_Ji1), [Suman Jana](http://openreview.net/profile?id=~Suman_Jana1), [Tianlong Chen](http://openreview.net/profile?id=~Tianlong_Chen1), [Tianming Liu](http://openreview.net/profile?id=~Tianming_Liu3), [Tianyi Zhou](http://openreview.net/profile?id=~Tianyi_Zhou1), [William Wang](http://openreview.net/profile?id=~William_Yang_Wang2), [Xiang Li](http://openreview.net/profile?id=~Xiang_Li14), [Xiangliang Zhang](http://openreview.net/profile?id=~Xiangliang_Zhang1), [Xiao Wang](http://openreview.net/profile?id=~Xiao_Wang11), [Xing Xie](http://openreview.net/profile?id=~Xing_Xie3), [Xun Chen](http://openreview.net/profile?id=~Xun_Chen1), [Xuyu Wang](http://openreview.net/profile?id=~Xuyu_Wang1), [Yan Liu](http://openreview.net/profile?id=~Yan_Liu1), [Yanfang Ye](http://openreview.net/profile?id=~Yanfang_Ye1), [Yinzhi Cao](http://openreview.net/profile?id=~Yinzhi_Cao1), [Yong Chen](http://openreview.net/profile?id=~Yong_Chen9), [Yue Zhao](http://openreview.net/profile?id=~Yue_Zhao13)
  - **Affiliations:** Lehigh University; University of Notre Dame, Lehigh University, Illinois Institute of Technology, CISPA, CISPA, University of Cambridge, CISPA, Institut Polytechnique de Paris, William & Mary, William & Mary, Texas A&M University, Lehigh University, University of Georgia, Lehigh University, Samsung Research America, Stanford University, MLCommons; University of Oxford, Lawrence Livermore National Laboratory, Salesforce Research, University of Wisconsin, Madison, Microsoft Research, Carnegie Mellon University; Mohamed Bin Zayed University of Artificial Intelligence, University of Maryland, University of California, Berkeley, University of Illinois Urbana-Champaign, Rutgers University, UNC Chapel Hill, Massachusetts Institute of Technology, Harvard University, Duke University, University of Tennessee, Knoxville, University of Southern California, Stanford University, Michigan State University, Microsoft Research Asia, University of California, Los Angeles, University of Illinois Chicago, IBM Research, Yale University, Columbia University, University of California, Santa Barbara, Massachusetts General Hospital, Northwestern University, Florida International University, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None
  - **TL;DR:** This paper presents TRUST LLM, a comprehensive study on the trustworthiness of large language models, proposing principles and benchmarks for evaluation. The findings indicate a positive relationship between trustworthiness and capability, with proprietary models generally outperforming open-source ones, highlighting the need for transparency and collaboration in AI development.
  - **Keywords:** Trustworthiness in Large Language Models, Natural Language Processing, Benchmarking, Evaluation, Trustworthiness challenges in LLMs, Principles for trustworthy LLMs, Evaluation metrics, Over 30 datasets, Large Language Models, Proprietary LLMs, Open-source LLMs, AI alliance, Transparency in AI


- [From Classification Accuracy to Proper Scoring Rules: Elicitability of Probabilistic Top List Predictions](https://icml.cc/virtual/2024/poster/35625) (Poster)
  - **Authors:** Johannes Resin
  - **Affiliations:** Unknown
  - **TL;DR:** None
  - **Keywords:** None


- [PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs](https://icml.cc/virtual/2024/poster/35217) (Poster)
  - **Authors:** [Soroush Nasiriany](http://openreview.net/profile?id=~Soroush_Nasiriany1), [Fei Xia](http://openreview.net/profile?id=~Fei_Xia1), [Wenhao Yu](http://openreview.net/profile?id=~Wenhao_Yu1), [Ted Xiao](http://openreview.net/profile?id=~Ted_Xiao1), [Jacky Liang](http://openreview.net/profile?id=~Jacky_Liang1), [Ishita Dasgupta](http://openreview.net/profile?id=~Ishita_Dasgupta1), [Annie Xie](http://openreview.net/profile?id=~Annie_Xie1), [Danny Driess](http://openreview.net/profile?id=~Danny_Driess1), [Ayzaan Wahid](http://openreview.net/profile?id=~Ayzaan_Wahid1), [Zhuo Xu](http://openreview.net/profile?id=~Zhuo_Xu1), [Quan Vuong](http://openreview.net/profile?id=~Quan_Vuong2), [Tingnan Zhang](http://openreview.net/profile?id=~Tingnan_Zhang1), [Tsang-Wei Lee](http://openreview.net/profile?id=~Tsang-Wei_Edward_Lee1), [Kuang-Huei Lee](http://openreview.net/profile?id=~Kuang-Huei_Lee1), [Peng Xu](http://openreview.net/profile?id=~Peng_Xu9), [Sean Kirmani](http://openreview.net/profile?id=~Sean_Kirmani1), [Yuke Zhu](http://openreview.net/profile?id=~Yuke_Zhu1), [Andy Zeng](http://openreview.net/profile?id=~Andy_Zeng3), [Karol Hausman](http://openreview.net/profile?id=~Karol_Hausman2), [Nicolas Heess](http://openreview.net/profile?id=~Nicolas_Heess1), [Chelsea Finn](http://openreview.net/profile?id=~Chelsea_Finn1), [Sergey Levine](http://openreview.net/profile?id=~Sergey_Levine1), [brian ichter](http://openreview.net/profile?id=~brian_ichter1)
  - **Affiliations:** Google DeepMind; The University of Texas at Austin, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Stanford University, Google DeepMind, Google DeepMind, Google DeepMind; The University of Texas at Austin, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, The University of Texas at Austin, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind, Google DeepMind
  - **TL;DR:** This paper introduces PIVOT, a novel approach that enables Vision Language Models (VLMs) to perform robotic control and spatial reasoning tasks through iterative visual question answering without the need for task-specific training data. The findings demonstrate the potential for zero-shot control in various environments, highlighting both the capabilities and limitations of this method.
  - **Keywords:** Vision Language Models (VLMs), Robotic Control, Spatial Reasoning, Iterative Visual Question Answering, Visual Prompting, Optimization, Robotic Navigation, Real-World Manipulation, Instruction Following, Spatial Inference, Continuous Coordinates Output, Zero-Shot Control, Lack of Task-Specific Training Data, PIVOT Approach, Action Selection, Proposal Refinement

